[{"text": "Hello everyone. Uh, welcome to CS229.", "start": 3.5, "duration": 4.24}, {"text": "Um, today we're going to talk about,", "start": 7.74, "duration": 2.82}, {"text": "um, deep learning and neural networks.", "start": 10.56, "duration": 3.57}, {"text": "Um, we're going to have two lectures on that,", "start": 14.13, "duration": 2.85}, {"text": "one today and a little bit more of it on, ah, Monday.", "start": 16.98, "duration": 3.99}, {"text": "Um, don't hesitate to ask questions during the lecture.", "start": 20.97, "duration": 3.9}, {"text": "Ah, so stop me if you don't understand something and we'll try", "start": 24.87, "duration": 2.7}, {"text": "to build intuition around neural networks together.", "start": 27.57, "duration": 3.12}, {"text": "We will actually start with an algorithm that you guys have seen,", "start": 30.69, "duration": 2.97}, {"text": "uh, previously called logistic regression.", "start": 33.66, "duration": 2.355}, {"text": "Everybody remembers logistic regression?", "start": 36.015, "duration": 2.085}, {"text": "Yes.", "start": 38.1, "duration": 0.72}, {"text": "Okay. Remember it's a classification algorithm.", "start": 38.82, "duration": 2.21}, {"text": "Um, we're going to do that.", "start": 41.03, "duration": 2.655}, {"text": "Explain how logistic regression can be interpreted as", "start": 43.685, "duration": 3.255}, {"text": "a neural network- specific case of a neural network and then,", "start": 46.94, "duration": 4.2}, {"text": "we will go to neural networks. Sounds good?", "start": 51.14, "duration": 2.975}, {"text": "So the quick intro on deep learning.", "start": 54.115, "duration": 3.185}, {"text": "So deep learning is a- is a set of techniques that is let's say a subset of", "start": 62.24, "duration": 7.05}, {"text": "machine learning and it's one of the growing techniques that have been", "start": 69.29, "duration": 3.09}, {"text": "used in the industry specifically for problems in computer vision,", "start": 72.38, "duration": 3.779}, {"text": "natural language processing and speech recognition.", "start": 76.159, "duration": 2.221}, {"text": "So you guys have a lot of different tools and,", "start": 78.38, "duration": 3.46}, {"text": "and uh, plug-ins on your smartphones that uses this type of algorithm.", "start": 81.84, "duration": 4.19}, {"text": "Ah, the reason it came, uh,", "start": 86.03, "duration": 2.85}, {"text": "to work very well is primarily the,", "start": 88.88, "duration": 2.88}, {"text": "the new computational methods.", "start": 91.76, "duration": 2.01}, {"text": "So one thing we're going to see to- today, um,", "start": 93.77, "duration": 3.69}, {"text": "is that deep learning is really really computationally expensive and we- people had to", "start": 97.46, "duration": 7.17}, {"text": "find techniques in order to parallelize the code and", "start": 104.63, "duration": 3.435}, {"text": "use GPUs specifically in order to graphical processing units,", "start": 108.065, "duration": 3.765}, {"text": "in order to be able to compute,", "start": 111.83, "duration": 1.965}, {"text": "uh, the, the, the,", "start": 113.795, "duration": 1.965}, {"text": "the computations in deep learning.", "start": 115.76, "duration": 1.11}, {"text": "Ah, the second part is the data available has been growing after,", "start": 116.87, "duration": 8.59}, {"text": "after the Internet bubble,", "start": 125.46, "duration": 1.915}, {"text": "the digitalization of the world.", "start": 127.375, "duration": 1.885}, {"text": "So now people have access to large amounts of data and this type of algorithm has", "start": 129.26, "duration": 4.05}, {"text": "the specificity of being able to learn a lot when there is a lot of data.", "start": 133.31, "duration": 4.245}, {"text": "So these models are very flexible and the more you give them data,", "start": 137.555, "duration": 3.615}, {"text": "the more they will be able to understand the salient feature of the data.", "start": 141.17, "duration": 4.59}, {"text": "And finally algorithms.", "start": 145.76, "duration": 3.045}, {"text": "So people have come up with, with new techniques,", "start": 148.805, "duration": 3.235}, {"text": "uh, in order to use the data,", "start": 152.04, "duration": 3.04}, {"text": "use the computation power and build models.", "start": 155.08, "duration": 2.635}, {"text": "So we are going to touch a little bit on all of that,", "start": 157.715, "duration": 2.804}, {"text": "but let's go with logistic regression first.", "start": 160.519, "duration": 3.191}, {"text": "Can you guys see in the back?", "start": 168.41, "duration": 3.07}, {"text": "Yeah? Okay, perfect.", "start": 171.48, "duration": 2.625}, {"text": "So you remember,", "start": 174.105, "duration": 2.715}, {"text": "uh, what logistic regression is?", "start": 176.82, "duration": 2.07}, {"text": "What- we are going to fix a goal for us,", "start": 178.89, "duration": 2.95}, {"text": "uh, that, uh, is a classification goal.", "start": 181.84, "duration": 3.645}, {"text": "So let's try to,", "start": 185.485, "duration": 1.835}, {"text": "to find cats in images.", "start": 187.32, "duration": 2.25}, {"text": "So find cats in images.", "start": 189.57, "duration": 4.3}, {"text": "Meaning binary classification.", "start": 195.18, "duration": 3.445}, {"text": "If there is a cat in the image,", "start": 198.625, "duration": 2.665}, {"text": "we want to output a number that is close to 1, presence of the cat,", "start": 202.08, "duration": 5.93}, {"text": "and if there is no cat in the image, we wanna output 0.", "start": 208.01, "duration": 9.235}, {"text": "Let- let's say for now, ah,", "start": 217.245, "duration": 1.3}, {"text": "we're constrained to the fact that there is maximum one cat per image, there's no more.", "start": 218.545, "duration": 4.6}, {"text": "If you are to draw the logistic regression model,", "start": 223.145, "duration": 2.775}, {"text": "that's what you would do.", "start": 225.92, "duration": 1.485}, {"text": "You would take a cat.", "start": 227.405, "duration": 1.815}, {"text": "So this is an image of a cat.", "start": 229.22, "duration": 1.95}, {"text": "I'm very bad at that.", "start": 231.17, "duration": 2.385}, {"text": "Um, sorry.", "start": 233.555, "duration": 4.935}, {"text": "In computer science,", "start": 238.49, "duration": 1.945}, {"text": "you know that images can be represented as 3D matrices.", "start": 240.435, "duration": 4.175}, {"text": "So if I tell you that this is a color image of size 64 by 64,", "start": 244.61, "duration": 6.8}, {"text": "how many numbers do I have to represent those pixels?", "start": 251.41, "duration": 3.9}, {"text": "[BACKGROUND] Yeah, I heard it,", "start": 255.31, "duration": 6.53}, {"text": "64 by 64 by 3.", "start": 261.84, "duration": 2.295}, {"text": "Three for the RGB channel, red, green, blue.", "start": 264.135, "duration": 5.365}, {"text": "Every pixel in an image can be represented by three numbers.", "start": 269.5, "duration": 3.195}, {"text": "One representing the red filter,", "start": 272.695, "duration": 1.695}, {"text": "the green filter, and the, and the blue filter.", "start": 274.39, "duration": 2.61}, {"text": "So actually this image is of size 64 times 64 times 3.", "start": 277.0, "duration": 6.16}, {"text": "That makes sense? So the first thing we will do in", "start": 283.16, "duration": 3.56}, {"text": "order to use logistic regression to find if there is a cat in this image,", "start": 286.72, "duration": 3.315}, {"text": "we're going to flatten th- this into a vector.", "start": 290.035, "duration": 3.265}, {"text": "So I'm going to take all the numbers in this matrix and flatten them in a vector.", "start": 295.19, "duration": 5.225}, {"text": "Just an image to vector operation, nothing more.", "start": 300.415, "duration": 3.89}, {"text": "And now I can use my logistic regression because I have a vector input.", "start": 304.305, "duration": 4.76}, {"text": "So I'm going to, to take all of these and push them in an operation that", "start": 309.065, "duration": 7.455}, {"text": "we call th- the logistic operation which has one part that is wx plus b,", "start": 316.52, "duration": 7.8}, {"text": "where x is going to be the image.", "start": 324.32, "duration": 4.33}, {"text": "So wx plus b,", "start": 329.09, "duration": 2.545}, {"text": "and the second part is going to be the sigmoid.", "start": 331.635, "duration": 3.505}, {"text": "Everybody is familiar with the sigmoid function?", "start": 335.21, "duration": 3.07}, {"text": "Function that takes a number between", "start": 338.28, "duration": 1.99}, {"text": "minus infinity and plus infinity and maps it between 0 and 1.", "start": 340.27, "duration": 2.61}, {"text": "It is very convenient for classification problems.", "start": 342.88, "duration": 2.37}, {"text": "And this we are going to call it y hat,", "start": 345.25, "duration": 2.19}, {"text": "which is sigmoid of what you've seen in class previously,", "start": 347.44, "duration": 3.715}, {"text": "I think it's Theta transpose x.", "start": 351.155, "duration": 2.24}, {"text": "But here we will just separate the notation into w and b.", "start": 353.395, "duration": 4.195}, {"text": "So can someone tell me what's the shape of w?", "start": 363.51, "duration": 4.405}, {"text": "The matrix W, vector matrix.", "start": 367.915, "duration": 5.455}, {"text": "Um, what?", "start": 374.75, "duration": 4.79}, {"text": "Yes, 64 by 64 by 3 as a- yeah.", "start": 384.24, "duration": 5.155}, {"text": "So you know that this guy here is a vector of 64 by 64 by 3, a column vector.", "start": 389.395, "duration": 8.17}, {"text": "So the shape of x is going to be 64 by 64 by 3 times 1.", "start": 397.565, "duration": 8.755}, {"text": "This is the shape and this,", "start": 406.32, "duration": 1.965}, {"text": "I think it's- that if I don't know,", "start": 408.285, "duration": 3.72}, {"text": "12,288 and this indeed because we want y-hat to be one-by-one,", "start": 412.005, "duration": 8.39}, {"text": "this w has to be 1 by 12,288.", "start": 420.395, "duration": 6.485}, {"text": "That makes sense? So we have a row vector as our parameter.", "start": 426.88, "duration": 4.82}, {"text": "We're just changing the notations of the logistic regression that you guys have seen.", "start": 431.84, "duration": 5.08}, {"text": "And so once we have this model,", "start": 436.92, "duration": 2.0}, {"text": "we need to train it as you know.", "start": 438.92, "duration": 1.81}, {"text": "And the process of training is that first,", "start": 440.73, "duration": 2.51}, {"text": "we will initialize our parameters.", "start": 443.24, "duration": 6.6}, {"text": "These are what we call parameters.", "start": 449.84, "duration": 2.16}, {"text": "We will use the specific vocabulary of weights and bias.", "start": 452.0, "duration": 6.45}, {"text": "I believe you guys have heard this vocabulary before, weights and biases.", "start": 458.45, "duration": 5.865}, {"text": "So we're going to find the right w and the right b in order to be able,", "start": 464.315, "duration": 6.435}, {"text": "ah, to use this model properly.", "start": 470.75, "duration": 2.565}, {"text": "Once we initialized them,", "start": 473.315, "duration": 2.055}, {"text": "what we will do is that we will optimize them,", "start": 475.37, "duration": 3.44}, {"text": "find the optimal w and b,", "start": 478.81, "duration": 6.785}, {"text": "and after we found the optimal w and b,", "start": 485.595, "duration": 2.99}, {"text": "we will use them to predict.", "start": 488.585, "duration": 2.665}, {"text": "Does this process make sense? This training process?", "start": 500.76, "duration": 3.715}, {"text": "And I think the important part is to understand what this is.", "start": 504.475, "duration": 4.305}, {"text": "Find the optimal w and b means defining your loss function which is the objective.", "start": 508.78, "duration": 5.205}, {"text": "And in machine learning, we often have this, this,", "start": 513.985, "duration": 3.3}, {"text": "this specific problem where you have a function that you know you want to find,", "start": 517.285, "duration": 4.455}, {"text": "the network function, but you don't know the values of its parameters.", "start": 521.74, "duration": 3.45}, {"text": "In order to find them, you're going to use", "start": 525.19, "duration": 1.71}, {"text": "a proxy that is going to be your loss function.", "start": 526.9, "duration": 1.98}, {"text": "If you manage to minimize the loss function,", "start": 528.88, "duration": 2.31}, {"text": "you will find the right parameters.", "start": 531.19, "duration": 2.295}, {"text": "So you define a loss function,", "start": 533.485, "duration": 3.54}, {"text": "that is the logistic loss.", "start": 537.025, "duration": 3.205}, {"text": "Y log of y hat plus 1 minus y log of 1 minus y hat up.", "start": 541.05, "duration": 10.165}, {"text": "You guys have seen this one.", "start": 551.215, "duration": 1.74}, {"text": "You remember where it comes from?", "start": 552.955, "duration": 2.22}, {"text": "Comes from a maximum likelihood estimation,", "start": 555.175, "duration": 3.705}, {"text": "starting from a probabilistic model.", "start": 558.88, "duration": 2.95}, {"text": "And so the idea is how can I minimize this function.", "start": 562.14, "duration": 4.09}, {"text": "Minimize, because I've put the minus sign here.", "start": 566.23, "duration": 2.64}, {"text": "I want to find w and b that", "start": 568.87, "duration": 3.48}, {"text": "minimize this function and I'm going to use a gradient descent algorithm.", "start": 572.35, "duration": 4.155}, {"text": "Which means I'm going to", "start": 576.505, "duration": 1.74}, {"text": "iteratively compute the derivative of the loss with respect to my parameters.", "start": 578.245, "duration": 6.535}, {"text": "And at every step, I will update them to make", "start": 588.87, "duration": 3.73}, {"text": "this loss function go a little down at every iterative step.", "start": 592.6, "duration": 3.18}, {"text": "So in terms of implementation, this is a for loop.", "start": 595.78, "duration": 2.64}, {"text": "You will loop over a certain number of iteration and at every point,", "start": 598.42, "duration": 4.02}, {"text": "you will compute the derivative of the loss with respect to your parameters.", "start": 602.44, "duration": 3.495}, {"text": "Everybody remembers how to compute this number?", "start": 605.935, "duration": 3.715}, {"text": "Take the derivative here, you use the fact that the sigmoid function", "start": 609.84, "duration": 4.24}, {"text": "has a derivative that is sigmoid times 1 minus sigmoid,", "start": 614.08, "duration": 4.56}, {"text": "and you will compute the results.", "start": 618.64, "duration": 1.65}, {"text": "We- we're going to do some derivative later today.", "start": 620.29, "duration": 3.045}, {"text": "But just to set up the problem here.", "start": 623.335, "duration": 3.015}, {"text": "So, the few things that I wanna- that I wanna touch on here is,", "start": 626.35, "duration": 5.37}, {"text": "first, how many parameters does this model have? This logistic regression?", "start": 631.72, "duration": 4.33}, {"text": "If you have to, count them.", "start": 636.05, "duration": 2.18}, {"text": "So this is the numb- 089 yeah, correct.", "start": 645.51, "duration": 3.22}, {"text": "So 12,288 weights and 1 bias. That makes sense?", "start": 648.73, "duration": 5.04}, {"text": "So, actually, it's funny because you can quickly count it by just counting", "start": 653.77, "duration": 3.3}, {"text": "the number of edges on the- on the- on the drawing plus 1.", "start": 657.07, "duration": 4.255}, {"text": "Every circle has a bias.", "start": 661.325, "duration": 3.155}, {"text": "Every edge has a weight because ultimately this", "start": 664.48, "duration": 3.54}, {"text": "operation you can rewrite it like that, right?", "start": 668.02, "duration": 4.425}, {"text": "It means every weight has- every weight corresponds to an edge.", "start": 672.445, "duration": 5.175}, {"text": "So that's another way to count it,", "start": 677.62, "duration": 1.365}, {"text": "we are going to use it a little further.", "start": 678.985, "duration": 1.77}, {"text": "So we're starting with not too many parameters actually.", "start": 680.755, "duration": 2.91}, {"text": "And one thing that we notice is that the number of", "start": 683.665, "duration": 2.145}, {"text": "parameters of our model depends on the size of the input.", "start": 685.81, "duration": 3.225}, {"text": "We probably don't want that at some point,", "start": 689.035, "duration": 2.1}, {"text": "so we are going to change it later.", "start": 691.135, "duration": 2.88}, {"text": "So two equations that I want you to remember is,", "start": 694.015, "duration": 4.155}, {"text": "the first one is neuron equals linear plus activation.", "start": 698.17, "duration": 6.09}, {"text": "So this is the vocabulary we will use in neural networks.", "start": 704.26, "duration": 3.93}, {"text": "We define a neuron as an operation that has two parts,", "start": 708.19, "duration": 3.72}, {"text": "one linear part, and one activation", "start": 711.91, "duration": 2.25}, {"text": "part and it's exactly that. This is actually a neuron.", "start": 714.16, "duration": 3.31}, {"text": "We have a linear part, wx plus b and", "start": 718.71, "duration": 4.63}, {"text": "then we take the output of this linear part and we put it in an activation,", "start": 723.34, "duration": 3.87}, {"text": "that in this case, is the sigmoid function.", "start": 727.21, "duration": 2.07}, {"text": "It can be other functions, okay?", "start": 729.28, "duration": 3.21}, {"text": "So this is the first equation, not too hard.", "start": 732.49, "duration": 3.315}, {"text": "The second equation that I wanna set now is the model", "start": 735.805, "duration": 5.1}, {"text": "equals architecture plus parameters.", "start": 740.905, "duration": 7.795}, {"text": "What does that mean?", "start": 750.18, "duration": 2.65}, {"text": "It means here we're, we're trying to train a logistic regression in order to,", "start": 752.83, "duration": 4.185}, {"text": "to be able to use it.", "start": 757.015, "duration": 2.055}, {"text": "We need an architecture which is the following,", "start": 759.07, "duration": 3.0}, {"text": "a one neuron neural network and the parameters w and b.", "start": 762.07, "duration": 5.715}, {"text": "So basically, when people say we've shipped a model,", "start": 767.785, "duration": 3.735}, {"text": "like in the industry, what they're saying is that they found the right parameters,", "start": 771.52, "duration": 4.245}, {"text": "with the right architecture.", "start": 775.765, "duration": 1.71}, {"text": "They have two files and these two files are predicting a bunch of things, okay?", "start": 777.475, "duration": 5.005}, {"text": "One parameter file and one architecture file.", "start": 782.48, "duration": 3.08}, {"text": "The architecture will be modified a lot today.", "start": 785.56, "duration": 2.685}, {"text": "We will add neurons all over and the parameters will always be called w and b,", "start": 788.245, "duration": 5.64}, {"text": "but they will become bigger and bigger.", "start": 793.885, "duration": 1.83}, {"text": "Because we have more data,", "start": 795.715, "duration": 1.38}, {"text": "we want to be able to understand it.", "start": 797.095, "duration": 1.965}, {"text": "You can get that it's going to be hard to understand what a cat is with only that,", "start": 799.06, "duration": 4.755}, {"text": "that, that many parameters.", "start": 803.815, "duration": 1.615}, {"text": "We want to have more parameters.", "start": 805.43, "duration": 2.45}, {"text": "Any questions so far?", "start": 809.04, "duration": 2.41}, {"text": "So this was just to set up the problem with logistic regression.", "start": 811.45, "duration": 3.94}, {"text": "Let's try to set a new goal,", "start": 815.46, "duration": 2.8}, {"text": "after the first goal we have set prior to that.", "start": 818.26, "duration": 4.62}, {"text": "So the second goal would be, find cat,", "start": 822.88, "duration": 6.93}, {"text": "a lion, iguana in images.", "start": 829.81, "duration": 7.05}, {"text": "So a little different than before,", "start": 836.86, "duration": 2.549}, {"text": "only thing we changed is that we want to now to detect three types of animals.", "start": 839.409, "duration": 4.861}, {"text": "Either if there's a cat in the image,", "start": 844.27, "duration": 1.56}, {"text": "I wanna know there is a cat.", "start": 845.83, "duration": 1.035}, {"text": "If there's an iguana in the image,", "start": 846.865, "duration": 1.32}, {"text": "I wanna know there is an iguana.", "start": 848.185, "duration": 1.245}, {"text": "If there is a lion in the image,", "start": 849.43, "duration": 1.23}, {"text": "I wanna know it as well.", "start": 850.66, "duration": 2.175}, {"text": "So how would you modify the network", "start": 852.835, "duration": 2.73}, {"text": "that we previously had in order to take this into account?", "start": 855.565, "duration": 3.985}, {"text": "Yeah? Yeah, good idea.", "start": 862.95, "duration": 5.02}, {"text": "So put two more circles,", "start": 867.97, "duration": 1.29}, {"text": "so neurons, and do the same thing.", "start": 869.26, "duration": 2.325}, {"text": "So we have our picture here with the cats.", "start": 871.585, "duration": 4.425}, {"text": "So the cat is going to the right.", "start": 876.01, "duration": 3.91}, {"text": "64 by 64 by 3 we flatten it, from x1 to xn.", "start": 881.7, "duration": 6.07}, {"text": "Let's say n represents 64, 64 by 3 and what I will do,", "start": 887.77, "duration": 4.35}, {"text": "is that I will use three neurons that are all computing the same thing.", "start": 892.12, "duration": 5.37}, {"text": "They're all connected to all these inputs, okay?", "start": 897.49, "duration": 9.21}, {"text": "I connect all my inputs x1 to xn to each of these neurons,", "start": 906.7, "duration": 4.65}, {"text": "and I will use a specific set of notation here. Okay.", "start": 911.35, "duration": 11.29}, {"text": "Y_2 hat equals a_2_1 sigmoid of", "start": 943.22, "duration": 5.65}, {"text": "w_ 2_1 plus x plus b_2_1.", "start": 948.87, "duration": 7.275}, {"text": "And similarly, y_3 hat equals a_3_1,", "start": 956.145, "duration": 5.525}, {"text": "which is sigmoid of w_ 3_1 x plus b_3_1.", "start": 961.67, "duration": 7.49}, {"text": "So I'm introducing a few notations here and we'll,", "start": 970.65, "duration": 4.225}, {"text": "we'll get used to it, don't worry.", "start": 974.875, "duration": 1.2}, {"text": "So just, just write this down and we're going to go over it.", "start": 976.075, "duration": 3.18}, {"text": "So [NOISE] the square brackets here represent what we will call later on a layer.", "start": 979.255, "duration": 7.65}, {"text": "If you look at this network,", "start": 986.905, "duration": 2.055}, {"text": "it looks like there is one layer here.", "start": 988.96, "duration": 2.175}, {"text": "There's one layer in which neurons don't communicate with each other.", "start": 991.135, "duration": 4.065}, {"text": "We could add up to it,", "start": 995.2, "duration": 1.68}, {"text": "and we will do it later on,", "start": 996.88, "duration": 1.215}, {"text": "more neurons in other layers.", "start": 998.095, "duration": 1.92}, {"text": "We will denote with square brackets the index of the layer.", "start": 1000.015, "duration": 3.435}, {"text": "The index, that is, the subscript to this a", "start": 1003.45, "duration": 3.735}, {"text": "is the number identifying the neuron inside the layer.", "start": 1007.185, "duration": 3.795}, {"text": "So here we have one layer.", "start": 1010.98, "duration": 1.245}, {"text": "We have a_1, a_2,", "start": 1012.225, "duration": 2.054}, {"text": "and a_3 with square brackets one to identify the layer. Does that make sense?", "start": 1014.279, "duration": 4.681}, {"text": "And then we have our y-hat that instead of being a single number as it was before,", "start": 1018.96, "duration": 5.715}, {"text": "is now a vector of size three.", "start": 1024.675, "duration": 3.355}, {"text": "So how many parameters does this network have?", "start": 1028.55, "duration": 3.1}, {"text": "[NOISE]", "start": 1031.65, "duration": 13.92}, {"text": "How much?", "start": 1045.57, "duration": 0.45}, {"text": "[BACKGROUND]", "start": 1046.02, "duration": 3.3}, {"text": "Okay. How did you come up with that?", "start": 1049.32, "duration": 1.35}, {"text": "[BACKGROUND].", "start": 1050.67, "duration": 2.13}, {"text": "Okay. Yeah, correct. So we just have three times the,", "start": 1052.8, "duration": 2.505}, {"text": "the thing we had before because we added", "start": 1055.305, "duration": 1.875}, {"text": "two more neurons and they all have their own set of parameters.", "start": 1057.18, "duration": 3.36}, {"text": "Look like this edge is a separate edge as this one.", "start": 1060.54, "duration": 3.075}, {"text": "So we, we have to replicate parameters for each of these.", "start": 1063.615, "duration": 2.985}, {"text": "So w_1_1 would be the equivalent of what we had for the cat,", "start": 1066.6, "duration": 3.21}, {"text": "but we have to add two more,", "start": 1069.81, "duration": 2.565}, {"text": "ah, parameter vectors and biases.", "start": 1072.375, "duration": 2.565}, {"text": "[NOISE] So other question,", "start": 1074.94, "duration": 3.215}, {"text": "when you had to train this logistic regression,", "start": 1078.155, "duration": 3.03}, {"text": "what dataset did you need?", "start": 1081.185, "duration": 1.815}, {"text": "[NOISE]", "start": 1083.0, "duration": 11.04}, {"text": "Can someone try", "start": 1094.04, "duration": 0.64}, {"text": "to describe the dataset. Yeah.", "start": 1094.68, "duration": 4.77}, {"text": "[BACKGROUND]", "start": 1099.45, "duration": 4.56}, {"text": "Yeah, correct. So we need images and labels with it labeled as cat,", "start": 1104.01, "duration": 4.065}, {"text": "1 or no cat, 0.", "start": 1108.075, "duration": 1.89}, {"text": "So it is a binary classification with images and labels.", "start": 1109.965, "duration": 3.045}, {"text": "Now, what do you think should be the dataset to train this network? Yes.", "start": 1113.01, "duration": 9.09}, {"text": "[BACKGROUND]", "start": 1122.1, "duration": 7.5}, {"text": "That's a good idea. So just to repeat.", "start": 1129.6, "duration": 2.985}, {"text": "Uh, a label for an image that has a cat would probably be", "start": 1132.585, "duration": 6.63}, {"text": "a vector with a 1 and two 0s", "start": 1139.215, "duration": 4.05}, {"text": "where the 1 should represent the prese- the presence of a cat.", "start": 1143.265, "duration": 3.915}, {"text": "This one should represent the presence of", "start": 1147.18, "duration": 2.04}, {"text": "a lion and this one should represent the presence of an iguana.", "start": 1149.22, "duration": 3.94}, {"text": "So let's assume I use this scheme to label my dataset.", "start": 1154.76, "duration": 4.06}, {"text": "I train this network using the same techniques here.", "start": 1158.82, "duration": 3.42}, {"text": "Initialize all my weights and biases with a value, a starting value,", "start": 1162.24, "duration": 5.07}, {"text": "optimize a loss function by using gradient descent,", "start": 1167.31, "duration": 4.965}, {"text": "and then use y-hat equals, uh, log to predict.", "start": 1172.275, "duration": 3.895}, {"text": "What do you think this neuron is going to be responsible for?", "start": 1177.38, "duration": 4.97}, {"text": "If you had to describe the responsibility of this neuron.", "start": 1186.59, "duration": 3.49}, {"text": "[BACKGROUND]", "start": 1190.08, "duration": 3.75}, {"text": "Yes. Well, this one.", "start": 1193.83, "duration": 1.695}, {"text": "Lion.", "start": 1195.525, "duration": 1.225}, {"text": "Yeah, lion and this one iguana.", "start": 1197.81, "duration": 2.605}, {"text": "So basically the, the,", "start": 1200.415, "duration": 1.44}, {"text": "the way you- yeah, go for it.", "start": 1201.855, "duration": 1.605}, {"text": "[BACKGROUND]", "start": 1203.46, "duration": 3.8}, {"text": "That's a good question. We're going to talk about that now.", "start": 1207.26, "duration": 2.535}, {"text": "Multiple image contain different animals or not.", "start": 1209.795, "duration": 2.55}, {"text": "So going back on what you said,", "start": 1212.345, "duration": 1.665}, {"text": "because we decided to label our dataset like that,", "start": 1214.01, "duration": 3.495}, {"text": "after training, this neuron is naturally going to be there to detect cats.", "start": 1217.505, "duration": 3.945}, {"text": "If we had changed the labeling scheme and I said", "start": 1221.45, "duration": 2.74}, {"text": "that the second entry would correspond to the cat,", "start": 1224.19, "duration": 3.165}, {"text": "the presence of a cat, then after training,", "start": 1227.355, "duration": 2.625}, {"text": "you will detect that this neuron is responsible for detecting a cat.", "start": 1229.98, "duration": 3.25}, {"text": "So the network is going to evolve depending on the way you label your dataset.", "start": 1233.23, "duration": 4.08}, {"text": "Now, do you think that", "start": 1237.31, "duration": 2.57}, {"text": "this network can still be robust to different animals in the same picture?", "start": 1239.88, "duration": 4.89}, {"text": "So this cat now has,", "start": 1244.77, "duration": 2.28}, {"text": "uh, a friend [NOISE] that is a lion.", "start": 1247.05, "duration": 2.925}, {"text": "Okay, I have no idea how to draw a lion,", "start": 1249.975, "duration": 2.16}, {"text": "but let's say there is a lion here and because there is a lion,", "start": 1252.135, "duration": 6.15}, {"text": "I will add a 1 here.", "start": 1258.285, "duration": 1.26}, {"text": "Do you think this network is robust to this type of labeling?", "start": 1259.545, "duration": 3.975}, {"text": "[BACKGROUND]", "start": 1263.52, "duration": 13.95}, {"text": "It should be. The neurons aren't talking to each other.", "start": 1277.47, "duration": 1.86}, {"text": "That's a good answer actually. Another answer.", "start": 1279.33, "duration": 1.74}, {"text": "[BACKGROUND]", "start": 1281.07, "duration": 10.59}, {"text": "That's a good, uh, intuition because the network,", "start": 1291.66, "duration": 2.49}, {"text": "what it sees is just 1, 1,", "start": 1294.15, "duration": 1.35}, {"text": "0, and an image.", "start": 1295.5, "duration": 1.8}, {"text": "It doesn't see that this one corresponds to- the cat", "start": 1297.3, "duration": 3.27}, {"text": "corresponds to the first one and the second- and the lion corresponds to the second one.", "start": 1300.57, "duration": 3.975}, {"text": "So [NOISE] this is a property of neural networks,", "start": 1304.545, "duration": 3.15}, {"text": "it's the fact that you don't need to tell them everything.", "start": 1307.695, "duration": 2.25}, {"text": "If you have enough data,", "start": 1309.945, "duration": 1.17}, {"text": "they're going to figure it out.", "start": 1311.115, "duration": 1.35}, {"text": "So because you will have also cats with iguanas,", "start": 1312.465, "duration": 2.655}, {"text": "cats alone, lions with iguanas, lions alone,", "start": 1315.12, "duration": 3.285}, {"text": "ultimately, this neuron will understand what it's looking for,", "start": 1318.405, "duration": 3.63}, {"text": "and it will understand that this one corresponds to this lion.", "start": 1322.035, "duration": 4.02}, {"text": "Just needs a lot of data.", "start": 1326.055, "duration": 2.19}, {"text": "So yes, it's going to be robust.", "start": 1328.245, "duration": 2.37}, {"text": "And that's the reason you mentioned.", "start": 1330.615, "duration": 2.025}, {"text": "It's going to be robust to that because the three neurons aren't communicating together.", "start": 1332.64, "duration": 4.41}, {"text": "So we can totally train them independent- independently from each other.", "start": 1337.05, "duration": 4.005}, {"text": "And in fact, the sigmoid here,", "start": 1341.055, "duration": 2.07}, {"text": "doesn't depend on the sigmoid here and doesn't depend on the sigmoid here.", "start": 1343.125, "duration": 3.315}, {"text": "It means we can have one, one,", "start": 1346.44, "duration": 1.71}, {"text": "and one as an output.", "start": 1348.15, "duration": 1.11}, {"text": "[NOISE] Yes, question.", "start": 1349.26, "duration": 2.88}, {"text": "[BACKGROUND]", "start": 1352.14, "duration": 3.69}, {"text": "You could, you could, you could think about it as three logistic regressions.", "start": 1355.83, "duration": 3.27}, {"text": "So we wouldn't call that a neural network yet.", "start": 1359.1, "duration": 2.595}, {"text": "It's not ready yet, but it's", "start": 1361.695, "duration": 1.905}, {"text": "a three neural network or three logistic regression with each other.", "start": 1363.6, "duration": 5.34}, {"text": "[NOISE].", "start": 1368.94, "duration": 1.2}, {"text": "Now, following up on that,", "start": 1370.14, "duration": 1.32}, {"text": "uh, yeah, go for it. A question.", "start": 1371.46, "duration": 2.07}, {"text": "[BACKGROUND]", "start": 1373.53, "duration": 5.25}, {"text": "W and b are related to what?", "start": 1378.78, "duration": 1.56}, {"text": "[BACKGROUND]", "start": 1380.34, "duration": 2.73}, {"text": "Oh, yeah. Yeah. So, so usually you would have Theta transpose x,", "start": 1383.07, "duration": 4.695}, {"text": "which is sum of Theta_i_x_i, correct?", "start": 1387.765, "duration": 4.545}, {"text": "And what I will split it is,", "start": 1392.31, "duration": 1.815}, {"text": "I will spit it in sum of Theta_i_x_i plus Theta_0 times 1.", "start": 1394.125, "duration": 7.08}, {"text": "I'll split it like that.", "start": 1401.205, "duration": 1.65}, {"text": "Theta_0 would correspond to b and these Theta_is would correspond to w_is, make sense?", "start": 1402.855, "duration": 6.06}, {"text": "Okay. One more question and then we move on.", "start": 1408.915, "duration": 3.015}, {"text": "[BACKGROUND]", "start": 1411.93, "duration": 14.0}, {"text": "Good question. That's the next thing we're going to see.", "start": 1425.93, "duration": 3.18}, {"text": "So the question is a follow up on this,", "start": 1429.11, "duration": 3.24}, {"text": "is there cases where we have a constraint where there is only one possible outcome?", "start": 1432.35, "duration": 7.125}, {"text": "It means there is no cat and lion,", "start": 1439.475, "duration": 1.905}, {"text": "there is either a cat or a lion,", "start": 1441.38, "duration": 1.65}, {"text": "there is no iguana and lion,", "start": 1443.03, "duration": 1.68}, {"text": "there's either an iguana or a lion.", "start": 1444.71, "duration": 2.265}, {"text": "Think about health care.", "start": 1446.975, "duration": 1.95}, {"text": "There are many, there are many models that are made to detect,", "start": 1448.925, "duration": 5.82}, {"text": "uh, if a disease, skin disease is present on- based on cell microscopic images.", "start": 1454.745, "duration": 5.565}, {"text": "Usually, there is no overlap between these, it means,", "start": 1460.31, "duration": 3.255}, {"text": "you want to classify a specific disease among a large number of diseases.", "start": 1463.565, "duration": 3.69}, {"text": "So this model would still work but would not be optimal because it's longer to train.", "start": 1467.255, "duration": 5.145}, {"text": "Maybe one disease is super, super", "start": 1472.4, "duration": 1.98}, {"text": "rare and one of the neurons is never going to be trained.", "start": 1474.38, "duration": 2.49}, {"text": "Let's say you're working in a zoo where there is", "start": 1476.87, "duration": 2.19}, {"text": "only one iguana and there are thousands of lions and thousands of cats.", "start": 1479.06, "duration": 3.81}, {"text": "This guy will never train almost,", "start": 1482.87, "duration": 2.7}, {"text": "you know, it would be super hard to train this one.", "start": 1485.57, "duration": 1.77}, {"text": "So you want to start with another model that- where you put the constraint, that's, okay,", "start": 1487.34, "duration": 4.125}, {"text": "there is only one disease that we want to predict and let the model", "start": 1491.465, "duration": 3.735}, {"text": "learn, with all the neurons learn together by creating interaction between them.", "start": 1495.2, "duration": 5.655}, {"text": "Have you guys heard of softmax?", "start": 1500.855, "duration": 3.135}, {"text": "Yes? Somebody, ah, I see that in the back.", "start": 1503.99, "duration": 3.15}, {"text": "[LAUGHTER] Okay. So let's look at softmax a little bit together.", "start": 1507.14, "duration": 3.285}, {"text": "So we set a new goal now,", "start": 1510.425, "duration": 2.605}, {"text": "which is we add", "start": 1515.14, "duration": 3.7}, {"text": "a constraint which is an unique animal on an image.", "start": 1518.84, "duration": 9.07}, {"text": "So at most one animal on an image.", "start": 1530.35, "duration": 3.58}, {"text": "So I'm going to modify the network a little bit.", "start": 1533.93, "duration": 4.665}, {"text": "We're go- we have our cat and there is no lion on the image, we flatten it,", "start": 1538.595, "duration": 6.265}, {"text": "and now I'm going to use the same scheme with the three neurons, a_1, a_2, a_3.", "start": 1545.11, "duration": 10.28}, {"text": "But as an output,", "start": 1562.33, "duration": 3.37}, {"text": "what I am going to use is an exponent, a softmax function.", "start": 1565.7, "duration": 5.775}, {"text": "So let me be more precise, let me,", "start": 1571.475, "duration": 3.195}, {"text": "let me actually introduce another notation to make it easier.", "start": 1574.67, "duration": 4.135}, {"text": "As you know, the neuron is a linear part plus an activation.", "start": 1578.805, "duration": 5.285}, {"text": "So we are going to introduce a notation for the linear part,", "start": 1584.09, "duration": 5.415}, {"text": "I'm going to introduce Z_11 to represent the linear part of the first neuron.", "start": 1589.505, "duration": 6.175}, {"text": "Z_112 to introduce the linear part of the second neuron.", "start": 1596.17, "duration": 7.195}, {"text": "So now our neuron has two parts,", "start": 1603.365, "duration": 1.785}, {"text": "one which computes Z and one which computes a,", "start": 1605.15, "duration": 2.58}, {"text": "equals sigmoid of Z.", "start": 1607.73, "duration": 1.56}, {"text": "Now, I'm going to remove all the activations and make", "start": 1609.29, "duration": 4.89}, {"text": "these Zs and I'm going to use the specific formula.", "start": 1614.18, "duration": 7.72}, {"text": "So this, if you recall,", "start": 1643.81, "duration": 3.399}, {"text": "is exactly the softmax formula.", "start": 1647.209, "duration": 2.851}, {"text": "[NOISE] Yeah.", "start": 1650.06, "duration": 3.61}, {"text": "Okay. So now the network we have,", "start": 1670.09, "duration": 4.48}, {"text": "can you guys see or it's too small? Too small?", "start": 1674.57, "duration": 3.195}, {"text": "Okay, I'm going to just write this formula", "start": 1677.765, "duration": 2.97}, {"text": "bigger and then you can figure out the others, I guess, because,", "start": 1680.735, "duration": 3.66}, {"text": "e of Z_3, 1 divided by sum from", "start": 1684.395, "duration": 5.625}, {"text": "k equals 1 to 3 of e, exponential of ZK_1.", "start": 1690.02, "duration": 5.445}, {"text": "Okay, can you see this one?", "start": 1695.465, "duration": 4.134}, {"text": "So here is for the third one.", "start": 1699.599, "duration": 1.971}, {"text": "If you are doing it for the first one,", "start": 1701.57, "duration": 1.38}, {"text": "you will add- you'll just change this into a 2,", "start": 1702.95, "duration": 2.55}, {"text": "into a 1 and for the second one into a 2.", "start": 1705.5, "duration": 2.475}, {"text": "So why is this formula interesting and why is it not", "start": 1707.975, "duration": 2.835}, {"text": "robust to this labeling scheme anymore?", "start": 1710.81, "duration": 2.67}, {"text": "It's because the sum of the outputs of this network have to sum up to 1. You can try it.", "start": 1713.48, "duration": 7.2}, {"text": "If you sum the three outputs,", "start": 1720.68, "duration": 1.995}, {"text": "you get the same thing in the numerator and on", "start": 1722.675, "duration": 2.445}, {"text": "the denominator and you get 1. That makes sense?", "start": 1725.12, "duration": 4.825}, {"text": "So instead of getting a probabilistic output for each,", "start": 1729.945, "duration": 6.275}, {"text": "each of y, if,", "start": 1736.45, "duration": 2.095}, {"text": "each of y hat 1, y hat 2,", "start": 1738.545, "duration": 1.665}, {"text": "y hat 3, we will get a probability distribution over all the classes.", "start": 1740.21, "duration": 4.725}, {"text": "So that means we cannot get 0.7, 0.6,", "start": 1744.935, "duration": 4.035}, {"text": "0.1, telling us roughly that there is probably a cat and a lion but no iguana.", "start": 1748.97, "duration": 4.74}, {"text": "We have to sum these to 1.", "start": 1753.71, "duration": 2.115}, {"text": "So it means, if there is no cat and no lion,", "start": 1755.825, "duration": 3.6}, {"text": "it means there's very likely an iguana.", "start": 1759.425, "duration": 2.67}, {"text": "The three probabilities are dependent on each", "start": 1762.095, "duration": 2.475}, {"text": "other and for this one we have to label the following way,", "start": 1764.57, "duration": 7.035}, {"text": "1, 1, 0 for a cat, 0, 1,", "start": 1771.605, "duration": 3.075}, {"text": "0 for a lion or 0,", "start": 1774.68, "duration": 2.04}, {"text": "0, 1 for an iguana.", "start": 1776.72, "duration": 2.265}, {"text": "So this is called a softmax multi-class network.", "start": 1778.985, "duration": 7.755}, {"text": "[inaudible].", "start": 1786.74, "duration": 17.19}, {"text": "You assume there is at least one of the three classes,", "start": 1803.93, "duration": 2.145}, {"text": "otherwise you have to add a fourth input that will represent an absence of an animal.", "start": 1806.075, "duration": 4.83}, {"text": "But this way, you assume there is always one of these three animals on every picture.", "start": 1810.905, "duration": 6.055}, {"text": "And how many parameters does the network have?", "start": 1821.95, "duration": 3.61}, {"text": "The same as the second one.", "start": 1825.56, "duration": 2.475}, {"text": "We still have three neurons and although I didn't write it,", "start": 1828.035, "duration": 2.91}, {"text": "this Z_1 is equal to w_11,", "start": 1830.945, "duration": 3.15}, {"text": "x plus b_1, Z_2 same, Z_3 same.", "start": 1834.095, "duration": 3.825}, {"text": "So there's 3n plus 3 parameters.", "start": 1837.92, "duration": 3.88}, {"text": "So one question that we didn't talk about is,", "start": 1843.64, "duration": 3.64}, {"text": "how do we train these parameters?", "start": 1847.28, "duration": 3.73}, {"text": "These, these parameters, the 3n plus 3 parameters, how do we train them?", "start": 1854.56, "duration": 4.93}, {"text": "You think this scheme will work or no?", "start": 1859.49, "duration": 2.175}, {"text": "What's wrong, what's wrong with this scheme?", "start": 1861.665, "duration": 2.755}, {"text": "What's wrong with the loss function specifically?", "start": 1866.8, "duration": 4.52}, {"text": "There's only two outcomes.", "start": 1875.41, "duration": 2.185}, {"text": "So in this loss function,", "start": 1877.595, "duration": 2.22}, {"text": "y is a number between 0 and 1,", "start": 1879.815, "duration": 3.33}, {"text": "y hat same is the probability,", "start": 1883.145, "duration": 2.655}, {"text": "y is either a 0 or 1,", "start": 1885.8, "duration": 1.41}, {"text": "y hat is between 0 and 1,", "start": 1887.21, "duration": 1.59}, {"text": "so it cannot match this labeling.", "start": 1888.8, "duration": 2.865}, {"text": "So we need to modify the loss function.", "start": 1891.665, "duration": 2.725}, {"text": "So let's call it loss three neuron.", "start": 1894.73, "duration": 5.24}, {"text": "What I'm going to do is I'm going to just sum it up for the three neurons.", "start": 1900.76, "duration": 7.91}, {"text": "Does this make sense? So I am just doing three times this loss for each of the neurons.", "start": 1925.22, "duration": 8.875}, {"text": "So we have exactly three times this.", "start": 1934.095, "duration": 2.94}, {"text": "We sum them together.", "start": 1937.035, "duration": 2.31}, {"text": "And if you train this loss function,", "start": 1939.345, "duration": 2.19}, {"text": "you should be able to train the three neurons that you have.", "start": 1941.535, "duration": 4.005}, {"text": "And again, talking about scarcity of one of the classes.", "start": 1945.54, "duration": 3.54}, {"text": "If there is not many iguana,", "start": 1949.08, "duration": 2.01}, {"text": "then the third term of this sum is", "start": 1951.09, "duration": 4.71}, {"text": "not going to help this neuron train towards detecting an iguana.", "start": 1955.8, "duration": 4.635}, {"text": "It's going to push it to detect no iguana.", "start": 1960.435, "duration": 3.415}, {"text": "Any question on the loss function?", "start": 1965.93, "duration": 2.275}, {"text": "Does this one make sense? Yeah?", "start": 1968.205, "duration": 2.685}, {"text": "[inaudible]", "start": 1970.89, "duration": 10.92}, {"text": "Yeah. Usually, that's what will happen is", "start": 1981.81, "duration": 1.71}, {"text": "that the output of this network once it's trained,", "start": 1983.52, "duration": 3.21}, {"text": "is going to be a probability distribution.", "start": 1986.73, "duration": 1.44}, {"text": "You will pick the maximum of those,", "start": 1988.17, "duration": 1.68}, {"text": "and you will set it to 1 and the others to 0 as", "start": 1989.85, "duration": 2.49}, {"text": "your prediction. One more question, yeah.", "start": 1992.34, "duration": 5.55}, {"text": "[inaudible]", "start": 1997.89, "duration": 11.34}, {"text": "If you use the 2-1.", "start": 2009.23, "duration": 1.635}, {"text": "If you use this labeling scheme like 1-1-0 for this network,", "start": 2010.865, "duration": 5.055}, {"text": "what do you think it will happen?", "start": 2015.92, "duration": 2.47}, {"text": "It will probably not work.", "start": 2020.29, "duration": 2.86}, {"text": "And the reason is this sum is equal to 2,", "start": 2023.15, "duration": 3.585}, {"text": "the sum of these entries,", "start": 2026.735, "duration": 1.665}, {"text": "while the sum of these entries is equal to 1.", "start": 2028.4, "duration": 2.07}, {"text": "So you will never be able to match", "start": 2030.47, "duration": 2.625}, {"text": "the output to the input to the label. That makes sense?", "start": 2033.095, "duration": 3.63}, {"text": "So what the network is probably going to do", "start": 2036.725, "duration": 2.34}, {"text": "is it's probably going to send this one to one-half,", "start": 2039.065, "duration": 2.685}, {"text": "this one to one-half, and this one to 0 probably, which is not what you want.", "start": 2041.75, "duration": 4.3}, {"text": "Okay. Let's talk about the loss function for this softmax regression.", "start": 2047.02, "duration": 5.5}, {"text": "[NOISE] Because you know", "start": 2052.52, "duration": 9.84}, {"text": "what's interesting about this loss is if I take this derivative,", "start": 2062.36, "duration": 5.475}, {"text": "derivative of the loss 3N with respect to W2_1.", "start": 2067.835, "duration": 6.235}, {"text": "You think is going to be harder than this derivative,", "start": 2075.43, "duration": 3.475}, {"text": "than this one or no?", "start": 2078.905, "duration": 2.01}, {"text": "It's going to be exactly the same.", "start": 2080.915, "duration": 1.89}, {"text": "Because only one of these three terms depends on W12.", "start": 2082.805, "duration": 3.345}, {"text": "It means the derivative of the two others are 0.", "start": 2086.15, "duration": 2.85}, {"text": "So we are exactly at the same complexity during the derivation.", "start": 2089.0, "duration": 3.855}, {"text": "But this one, do you think if you try to compute,", "start": 2092.855, "duration": 5.189}, {"text": "let's say we define a loss function that corresponds roughly to that.", "start": 2098.044, "duration": 3.826}, {"text": "If you try to compute the derivative of the loss with respect to W2,", "start": 2101.87, "duration": 3.675}, {"text": "it will become much more complex.", "start": 2105.545, "duration": 2.58}, {"text": "Because this number, the output here that is going to impact the loss function directly,", "start": 2108.125, "duration": 6.09}, {"text": "not only depends on the parameters of W2,", "start": 2114.215, "duration": 2.895}, {"text": "it also depends on the parameters of W1 and W3.", "start": 2117.11, "duration": 2.955}, {"text": "And same for this output.", "start": 2120.065, "duration": 1.86}, {"text": "This output also depends on the parameters W2.", "start": 2121.925, "duration": 3.075}, {"text": "Does this makes sense? Because of this denominator.", "start": 2125.0, "duration": 3.42}, {"text": "So the softmax regression needs a different loss function and a different derivative.", "start": 2128.42, "duration": 5.94}, {"text": "So the loss function we'll define is a very common one in", "start": 2134.36, "duration": 3.0}, {"text": "deep learning, it's called the softmax cross entropy.", "start": 2137.36, "duration": 4.26}, {"text": "Cross entropy loss.", "start": 2141.62, "duration": 5.5}, {"text": "I'm not going to- to- into the details of where it comes from but you can", "start": 2147.55, "duration": 4.51}, {"text": "get the intuition of yklog.", "start": 2152.06, "duration": 5.17}, {"text": "So it, it surprisingly looks like the binary croissant,", "start": 2174.4, "duration": 5.215}, {"text": "the binary, uh, the logistic loss function.", "start": 2179.615, "duration": 3.24}, {"text": "The only difference is that we will sum it up on all the- on all the classes.", "start": 2182.855, "duration": 9.415}, {"text": "Now, we will take a derivative of something that looks like that later.", "start": 2193.24, "duration": 4.735}, {"text": "But I'd say if you can try it at home on this one,", "start": 2197.975, "duration": 3.555}, {"text": "uh, it would be a good exercise as well.", "start": 2201.53, "duration": 2.335}, {"text": "So this binary cross entropy loss is very", "start": 2203.865, "duration": 4.025}, {"text": "likely to be used in classification problems that are multi-class.", "start": 2207.89, "duration": 4.9}, {"text": "Okay. So this was the first part on logistic regression types of networks.", "start": 2214.86, "duration": 5.485}, {"text": "And I think we're ready now with the notation that we", "start": 2220.345, "duration": 2.595}, {"text": "introduced to jump on to neural networks.", "start": 2222.94, "duration": 3.545}, {"text": "Any question on this first part before we move on?", "start": 2226.485, "duration": 3.645}, {"text": "So one- one question I would have for you.", "start": 2234.06, "duration": 2.94}, {"text": "Let's say instead of trying to predict if there is a cat or no cat,", "start": 2237.0, "duration": 5.26}, {"text": "we were trying to predict the age of the cat based on the image.", "start": 2242.26, "duration": 4.72}, {"text": "What would you change? This network.", "start": 2246.98, "duration": 5.17}, {"text": "Instead of predicting 1-0,", "start": 2252.15, "duration": 1.87}, {"text": "you wanna predict the age of the cat.", "start": 2254.02, "duration": 2.44}, {"text": "What are the things you would change? Yes.", "start": 2257.68, "duration": 6.97}, {"text": "[inaudible].", "start": 2264.65, "duration": 9.24}, {"text": "Okay. So I repeat.", "start": 2273.89, "duration": 1.74}, {"text": "I, I basically make", "start": 2275.63, "duration": 2.715}, {"text": "several output nodes where each of them corresponds to one age of cats.", "start": 2278.345, "duration": 3.885}, {"text": "So would you use this network or the third one?", "start": 2282.23, "duration": 2.55}, {"text": "Would you use the three neurons neural network or the softmax regression?", "start": 2284.78, "duration": 6.15}, {"text": "Third one.", "start": 2290.93, "duration": 1.2}, {"text": "The third one. Why?", "start": 2292.13, "duration": 0.9}, {"text": "You have a unique age.", "start": 2293.03, "duration": 2.385}, {"text": "You have a unique age,", "start": 2295.415, "duration": 1.32}, {"text": "you cannot have two ages, right.", "start": 2296.735, "duration": 2.295}, {"text": "So we would use a softmax one because we want", "start": 2299.03, "duration": 2.43}, {"text": "the probability distribution along the edge, the ages.", "start": 2301.46, "duration": 3.42}, {"text": "Okay. That makes sense. That's a good approach.", "start": 2304.88, "duration": 4.14}, {"text": "There is also another approach which is using directly a regression to predict an age.", "start": 2309.02, "duration": 4.755}, {"text": "An age can be between zero and plus infi- not plus infinity-", "start": 2313.775, "duration": 3.435}, {"text": "[LAUGHTER].", "start": 2317.21, "duration": 1.17}, {"text": "-zero and a certain number.", "start": 2318.38, "duration": 1.23}, {"text": "[LAUGHTER] And, uh, so let's say you wanna do a regression,", "start": 2319.61, "duration": 6.345}, {"text": "how would you modify your network?", "start": 2325.955, "duration": 2.815}, {"text": "Change the Sigmoid.", "start": 2329.23, "duration": 2.05}, {"text": "The Sigmoid puts the Z between 0 and 1.", "start": 2331.28, "duration": 2.58}, {"text": "We don't want this to happen.", "start": 2333.86, "duration": 1.335}, {"text": "So I'd say we will change the Sigmoid.", "start": 2335.195, "duration": 2.385}, {"text": "Into what function would you change the Sigmoid?", "start": 2337.58, "duration": 2.22}, {"text": "[inaudible]", "start": 2339.8, "duration": 10.8}, {"text": "Yeah. So the second one you said was?", "start": 2350.6, "duration": 2.22}, {"text": "[inaudible]", "start": 2352.82, "duration": 2.34}, {"text": "Oh, to get a Poisson type of distribution.", "start": 2355.16, "duration": 1.605}, {"text": "Okay. So let's, let's go with linear. You mentioned linear.", "start": 2356.765, "duration": 3.36}, {"text": "We could just use a linear function,", "start": 2360.125, "duration": 3.284}, {"text": "right, for the Sigmoid.", "start": 2363.409, "duration": 2.041}, {"text": "But this becomes a linear regression.", "start": 2365.45, "duration": 1.995}, {"text": "The whole network becomes a linear regression.", "start": 2367.445, "duration": 2.355}, {"text": "Another one that is very common in,", "start": 2369.8, "duration": 1.86}, {"text": "in deep learning is called the Rayleigh function.", "start": 2371.66, "duration": 2.265}, {"text": "It's a function that is almost linear,", "start": 2373.925, "duration": 2.175}, {"text": "but for every input that is negative, it's equal to 0.", "start": 2376.1, "duration": 3.765}, {"text": "Because we cannot have negative h,", "start": 2379.865, "duration": 2.04}, {"text": "it makes sense to use this one.", "start": 2381.905, "duration": 2.155}, {"text": "Okay. So this is called rectified linear units, ReLU.", "start": 2384.07, "duration": 7.225}, {"text": "It's a very common one in deep learning.", "start": 2391.295, "duration": 2.385}, {"text": "Now, what else would you change?", "start": 2393.68, "duration": 2.58}, {"text": "We talked about linear regression.", "start": 2396.26, "duration": 1.89}, {"text": "Do you remember the loss function you are using in linear regression? What was it?", "start": 2398.15, "duration": 3.78}, {"text": "[inaudible]", "start": 2401.93, "duration": 4.11}, {"text": "It was probably one of these two;", "start": 2406.04, "duration": 1.875}, {"text": "y hat minus y.", "start": 2407.915, "duration": 2.085}, {"text": "This comparison between the output label and y-hat,", "start": 2410.0, "duration": 4.455}, {"text": "the prediction, or it was the L2 loss;", "start": 2414.455, "duration": 2.61}, {"text": "y-hat minus y in L2 norm.", "start": 2417.065, "duration": 3.18}, {"text": "So that's what we would use. We would modify", "start": 2420.245, "duration": 2.025}, {"text": "our loss function to fit the regression type of problem.", "start": 2422.27, "duration": 3.255}, {"text": "And the reason we would use this loss instead of the one we have", "start": 2425.525, "duration": 3.885}, {"text": "for a regression task is because in optimization,", "start": 2429.41, "duration": 3.63}, {"text": "the shape of this loss is much easier to optimize for", "start": 2433.04, "duration": 2.91}, {"text": "a regression task than it is for a classification task and vice versa.", "start": 2435.95, "duration": 3.51}, {"text": "I'm not going to go into the details of that but that's the intuition.", "start": 2439.46, "duration": 3.57}, {"text": "[NOISE] Okay.", "start": 2443.03, "duration": 2.01}, {"text": "Let's go have fun with neural networks.", "start": 2445.04, "duration": 2.7}, {"text": "[NOISE]", "start": 2447.74, "duration": 24.06}, {"text": "So we, we stick to our first goal.", "start": 2471.8, "duration": 3.64}, {"text": "Given an image, tell us if there is cat or no cat.", "start": 2476.47, "duration": 7.85}, {"text": "This is 1, this is 0.", "start": 2484.6, "duration": 2.98}, {"text": "But now we're going to make a network a little more complex.", "start": 2487.58, "duration": 2.76}, {"text": "We're going to add some parameters.", "start": 2490.34, "duration": 2.205}, {"text": "So I get my picture of the cat.", "start": 2492.545, "duration": 1.965}, {"text": "[NOISE] The cat is moving.", "start": 2494.51, "duration": 6.01}, {"text": "Okay. And what I'm going to do is that I'm going to put more neurons than before.", "start": 2503.89, "duration": 5.93}, {"text": "Maybe something like that.", "start": 2510.27, "duration": 2.02}, {"text": "[NOISE]", "start": 2512.29, "duration": 30.09}, {"text": "So using the same notation you see that", "start": 2542.38, "duration": 1.84}, {"text": "my square bracket-", "start": 2554.22, "duration": 1.975}, {"text": "So using the same notation,", "start": 2556.195, "duration": 1.905}, {"text": "you see that my square bracket here is two", "start": 2558.1, "duration": 2.38}, {"text": "indicating that there is a layer here which is the second layer", "start": 2560.48, "duration": 4.51}, {"text": "[NOISE] while this one is the first layer and this one is the third layer.", "start": 2564.99, "duration": 9.71}, {"text": "Everybody's, er, up to speed with the notations?", "start": 2576.12, "duration": 4.045}, {"text": "Cool. So now notice that when you make a choice of architecture,", "start": 2580.165, "duration": 6.78}, {"text": "you have to be careful of one thing,", "start": 2586.945, "duration": 2.295}, {"text": "is that the output layer has to have the same number of neurons as you", "start": 2589.24, "duration": 5.55}, {"text": "want, the number of classes to be for reclassification and one for a regression.", "start": 2594.79, "duration": 6.25}, {"text": "So, er, how many parameters does this - this network have?", "start": 2605.16, "duration": 5.845}, {"text": "Can someone quickly give me the thought process?", "start": 2611.005, "duration": 3.895}, {"text": "So how much here?", "start": 2616.53, "duration": 3.05}, {"text": "Yeah, like 3n plus 3 let's say.", "start": 2621.57, "duration": 2.68}, {"text": "[inaudible].", "start": 2624.25, "duration": 15.66}, {"text": "Yeah, correct. So here you would have 3n weights plus 3 biases.", "start": 2639.91, "duration": 5.76}, {"text": "Here you would have 2 times 3 weights plus 2 biases because you have", "start": 2645.67, "duration": 5.16}, {"text": "three neurons connected to two neurons and here you will have 2 times 1 plus 1 bias.", "start": 2650.83, "duration": 5.67}, {"text": "Makes sense. So this is the total number of parameters.", "start": 2656.5, "duration": 3.4}, {"text": "So you see that we didn't add too much parameters.", "start": 2660.03, "duration": 3.175}, {"text": "Most of the parameters are still in the input layer.", "start": 2663.205, "duration": 3.085}, {"text": "Um, let's define some vocabulary.", "start": 2667.53, "duration": 3.475}, {"text": "The first word is Layer.", "start": 2671.005, "duration": 2.16}, {"text": "Layer denotes neurons that are not connected to each other.", "start": 2673.165, "duration": 3.3}, {"text": "These two neurons are not connected to each other.", "start": 2676.465, "duration": 1.77}, {"text": "These two neurons are not connected to each other.", "start": 2678.235, "duration": 1.845}, {"text": "We call this cluster of neurons a layer.", "start": 2680.08, "duration": 2.49}, {"text": "And then this has three layers.", "start": 2682.57, "duration": 2.415}, {"text": "So we would use input layer to define the first layer,", "start": 2684.985, "duration": 3.895}, {"text": "output layer to define the third layer because it directly sees", "start": 2688.88, "duration": 3.61}, {"text": "the outputs and we would call the second layer a hidden layer.", "start": 2692.49, "duration": 4.96}, {"text": "And the reason we call it hidden is because", "start": 2698.91, "duration": 3.22}, {"text": "the inputs and the outputs are hidden from this layer.", "start": 2702.13, "duration": 2.85}, {"text": "It means the only thing that this layer sees", "start": 2704.98, "duration": 2.79}, {"text": "as input is what's the previous layer gave it.", "start": 2707.77, "duration": 3.225}, {"text": "So it's an abstraction of the inputs but it's not the inputs.", "start": 2710.995, "duration": 4.5}, {"text": "Does that make sense? And same, it doesn't see the output,", "start": 2715.495, "duration": 3.84}, {"text": "it just gives what it understood to the last neuron", "start": 2719.335, "duration": 3.0}, {"text": "that will compare the output to the ground truth.", "start": 2722.335, "duration": 3.915}, {"text": "So now, why are neural networks interesting?", "start": 2726.25, "duration": 2.925}, {"text": "And why do we call this hidden layer?", "start": 2729.175, "duration": 2.58}, {"text": "Um, it's because if you train this network", "start": 2731.755, "duration": 3.81}, {"text": "on cat classification with a lot of images of cats,", "start": 2735.565, "duration": 4.02}, {"text": "you would notice that the first layers", "start": 2739.585, "duration": 2.79}, {"text": "are going to understand the fundamental concepts of the image,", "start": 2742.375, "duration": 3.435}, {"text": "which is the edges.", "start": 2745.81, "duration": 1.605}, {"text": "This neuron is going to be able to detect this type of edges.", "start": 2747.415, "duration": 4.665}, {"text": "This neuron is probably going to detect some other type of edge.", "start": 2752.08, "duration": 4.575}, {"text": "This neuron, maybe this type of edge.", "start": 2756.655, "duration": 2.58}, {"text": "Then what's gonna happen, is that these neurons are going to communicate", "start": 2759.235, "duration": 3.045}, {"text": "what they found on the image to the next layer's neuron.", "start": 2762.28, "duration": 3.105}, {"text": "And this neuron is going to use the edges that these guys found to figure out that,", "start": 2765.385, "duration": 4.59}, {"text": "oh, there is a - their ears.", "start": 2769.975, "duration": 2.835}, {"text": "While this one is going to figure out, oh,", "start": 2772.81, "duration": 1.95}, {"text": "there is a mouth and so on if you have", "start": 2774.76, "duration": 3.24}, {"text": "several neurons and they're going to communicate what they understood to", "start": 2778.0, "duration": 3.69}, {"text": "the output neuron that is going to construct the face of", "start": 2781.69, "duration": 3.21}, {"text": "the cat based on what it received and be able to tell if there is a cat or not.", "start": 2784.9, "duration": 4.605}, {"text": "So the reason it's called hidden layer is because we - we", "start": 2789.505, "duration": 3.705}, {"text": "don't really know what it's going to figure out but with enough data,", "start": 2793.21, "duration": 3.255}, {"text": "it should understand very complex information about the data.", "start": 2796.465, "duration": 3.225}, {"text": "The deeper you go, the more complex information the neurons are able to understand.", "start": 2799.69, "duration": 4.965}, {"text": "Let me give you another example which is a house prediction example.", "start": 2804.655, "duration": 5.1}, {"text": "House price prediction.", "start": 2809.755, "duration": 1.995}, {"text": "[NOISE]", "start": 2811.75, "duration": 21.9}, {"text": "So let's assume that our inputs are number of bedrooms,", "start": 2833.65, "duration": 6.25}, {"text": "size of the house,", "start": 2840.54, "duration": 3.155}, {"text": "zip code, and wealth of the neighborhood, let's say.", "start": 2843.695, "duration": 8.355}, {"text": "What we will build is a network that has", "start": 2852.05, "duration": 2.8}, {"text": "three neurons in the first layer and one neuron in the output layer.", "start": 2854.85, "duration": 5.39}, {"text": "So what's interesting is that as a human if you were to build, uh,", "start": 2860.82, "duration": 5.005}, {"text": "this network and like hand engineer it,", "start": 2865.825, "duration": 2.34}, {"text": "you would say that, uh,", "start": 2868.165, "duration": 1.98}, {"text": "okay zip codes and wealth or - or sorry.", "start": 2870.145, "duration": 4.335}, {"text": "Let's do that.", "start": 2874.48, "duration": 1.545}, {"text": "Zip code and wealth are able to tell us about the school quality in the neighborhood.", "start": 2876.025, "duration": 7.62}, {"text": "The quality of the school that is next to the house probably.", "start": 2883.645, "duration": 6.51}, {"text": "As a human you would say these are probably good features to predict that.", "start": 2890.155, "duration": 4.155}, {"text": "The zip code is going to tell us if the neighborhood is walkable or not, probably.", "start": 2894.31, "duration": 8.8}, {"text": "The size and the number of bedrooms is going to tell", "start": 2903.36, "duration": 4.93}, {"text": "us what's the size of the family that can fit in this house.", "start": 2908.29, "duration": 4.725}, {"text": "And these three are probably", "start": 2913.015, "duration": 2.385}, {"text": "better information than these in order to finally predict the price.", "start": 2915.4, "duration": 4.725}, {"text": "So that's a way to hand engineer that by hand, as a human in order", "start": 2920.125, "duration": 5.055}, {"text": "to give human knowledge to the network to figure out the price.", "start": 2925.18, "duration": 5.94}, {"text": "In practice what we do here is that we use", "start": 2931.12, "duration": 4.275}, {"text": "a fully-connected layer - fully-connected.", "start": 2935.395, "duration": 7.245}, {"text": "What does that mean? It means that we connect every input of a layer,", "start": 2942.64, "duration": 5.415}, {"text": "every - every input to the first layer,", "start": 2948.055, "duration": 3.045}, {"text": "every output of the first layer to the input of the third layer and so on.", "start": 2951.1, "duration": 3.975}, {"text": "So all the neurons among lay - from one layer to another are connected with each other.", "start": 2955.075, "duration": 5.07}, {"text": "What we're saying is that we will let the network figure these out.", "start": 2960.145, "duration": 4.395}, {"text": "We will let the neurons of the first layer figure out", "start": 2964.54, "duration": 3.09}, {"text": "what's interesting for the second layer to make the price prediction.", "start": 2967.63, "duration": 3.12}, {"text": "So we will not tell these to the network,", "start": 2970.75, "duration": 2.505}, {"text": "instead we will fully connect the network [NOISE] and so on.", "start": 2973.255, "duration": 7.765}, {"text": "Okay. We'll fully connect", "start": 2981.3, "duration": 3.16}, {"text": "the network and let it figure out what are the interesting features.", "start": 2984.46, "duration": 3.36}, {"text": "And oftentimes, the network is going to be able better than", "start": 2987.82, "duration": 2.82}, {"text": "the humans to find these - what are the features that are representative.", "start": 2990.64, "duration": 3.195}, {"text": "Sometimes you may hear neural networks referred as,", "start": 2993.835, "duration": 3.99}, {"text": "uh, black box models.", "start": 2997.825, "duration": 1.92}, {"text": "The reason is we will not understand what this edge will correspond to.", "start": 2999.745, "duration": 4.65}, {"text": "It's - it's hard to figure out that this neuron is", "start": 3004.395, "duration": 3.285}, {"text": "detecting a weighted average of the input features. Does that make sense?", "start": 3007.68, "duration": 7.09}, {"text": "Another word you might hear is end-to-end learning.", "start": 3016.61, "duration": 4.63}, {"text": "The reason we talked about end-to-end learning is because we have an input,", "start": 3021.24, "duration": 4.365}, {"text": "a ground truth, and we don't constrain the network in the middle.", "start": 3025.605, "duration": 5.175}, {"text": "We let it learn whatever it has to learn and we call it", "start": 3030.78, "duration": 3.03}, {"text": "end-to-end learning because we are just training based on the input and the output.", "start": 3033.81, "duration": 3.72}, {"text": "[NOISE]", "start": 3037.53, "duration": 38.07}, {"text": "Let's delve more into the math of this network.", "start": 3075.6, "duration": 2.91}, {"text": "The neural network that we have here which has an input layer,", "start": 3078.51, "duration": 3.81}, {"text": "a hidden layer and an output layer.", "start": 3082.32, "duration": 2.01}, {"text": "Let's try to write down the equations that run", "start": 3084.33, "duration": 2.94}, {"text": "the inputs and pro - propagate it to the output.", "start": 3087.27, "duration": 4.15}, {"text": "We first have Z_1, that is the linear part of the first layer,", "start": 3091.67, "duration": 5.135}, {"text": "that is computed using W_1 times x plus b_1.", "start": 3096.805, "duration": 6.245}, {"text": "Then this Z_1 is given to an activation,", "start": 3104.0, "duration": 4.54}, {"text": "let's say sigmoid, which is sigmoid of Z_1.", "start": 3108.54, "duration": 4.02}, {"text": "Z_2 is then the linear part of the second neuron which is going to", "start": 3112.56, "duration": 6.36}, {"text": "take the output of the previous layer, multiply it by its weights and add the bias.", "start": 3118.92, "duration": 8.29}, {"text": "The second activation is going to take the sigmoid of Z_2.", "start": 3127.31, "duration": 5.9}, {"text": "And finally, we have the third layer which is going to multiply its weights,", "start": 3134.27, "duration": 6.155}, {"text": "with the output of the layer presenting it and add its bias.", "start": 3140.425, "duration": 6.335}, {"text": "And finally, we have the third activation which is simply the sigmoid of the three.", "start": 3146.76, "duration": 7.63}, {"text": "So what is interesting to notice between", "start": 3158.23, "duration": 3.55}, {"text": "these equations and the equations that we wrote here,", "start": 3161.78, "duration": 5.5}, {"text": "is that we put everything in matrices.", "start": 3167.86, "duration": 3.895}, {"text": "So it means this a_3 that I have here, sorry,", "start": 3171.755, "duration": 5.745}, {"text": "this here for three neurons I wrote three equations,", "start": 3177.5, "duration": 4.314}, {"text": "here for three neurons", "start": 3181.814, "duration": 3.816}, {"text": "in the second layer I just wrote a single equation to summarize it.", "start": 3185.63, "duration": 4.125}, {"text": "But the shape of these things are going to be vectors.", "start": 3189.755, "duration": 3.405}, {"text": "So let's go over the shapes,", "start": 3193.16, "duration": 1.485}, {"text": "let's try to define them.", "start": 3194.645, "duration": 1.44}, {"text": "Z_1 is going to be x which is n by 1 times", "start": 3196.085, "duration": 5.895}, {"text": "w which has to be 3 by n because it connects three neurons to the input.", "start": 3201.98, "duration": 7.455}, {"text": "So this z has to be 3 by 1.", "start": 3209.435, "duration": 3.645}, {"text": "It makes sense because we have three neurons.", "start": 3213.08, "duration": 3.7}, {"text": "Now let's go, let's go deeper.", "start": 3217.27, "duration": 3.73}, {"text": "A_1 is just the sigmoid of z_1 so it doesn't change the shape.", "start": 3221.0, "duration": 4.26}, {"text": "It keeps the 3 by 1.", "start": 3225.26, "duration": 2.325}, {"text": "Z_2 we know it,", "start": 3227.585, "duration": 2.745}, {"text": "it has to be 2 by 1 because there are two neurons in", "start": 3230.33, "duration": 3.18}, {"text": "the second layer and it helps us figure out what w_2 would be.", "start": 3233.51, "duration": 4.77}, {"text": "We know a_1 is 3 by 1.", "start": 3238.28, "duration": 2.325}, {"text": "It means that w_2 has to be 2 by 3.", "start": 3240.605, "duration": 3.855}, {"text": "And if you count the edges between the first and the second layer", "start": 3244.46, "duration": 3.84}, {"text": "here you will find six edges, 2 times 3.", "start": 3248.3, "duration": 4.59}, {"text": "A_2, same shape as z_2.", "start": 3252.89, "duration": 3.045}, {"text": "Z_3, 1 by 1,", "start": 3255.935, "duration": 2.22}, {"text": "a_3, 1 by 1,", "start": 3258.155, "duration": 2.475}, {"text": "w_3, it has to be 1 by 2,", "start": 3260.63, "duration": 3.18}, {"text": "because a_2 is 2 by 1 and same for b.", "start": 3263.81, "duration": 4.695}, {"text": "B is going to be the number of neurons so 3 by 1,", "start": 3268.505, "duration": 4.2}, {"text": "2 by 1, and finally 1 by 1.", "start": 3272.705, "duration": 4.725}, {"text": "So I think it's usually very helpful,", "start": 3277.43, "duration": 2.28}, {"text": "even when coding these type of equations,", "start": 3279.71, "duration": 2.64}, {"text": "to know all the shapes that are involved.", "start": 3282.35, "duration": 2.28}, {"text": "Are you guys, like, totally okay with the shapes,", "start": 3284.63, "duration": 2.94}, {"text": "super-easy to figure out? Okay, cool.", "start": 3287.57, "duration": 3.095}, {"text": "So now what is interesting is that we will try to vectorize the code even more.", "start": 3290.665, "duration": 8.335}, {"text": "Does someone remember the difference between stochastic gradient descent", "start": 3299.0, "duration": 3.615}, {"text": "and gradient descent. What's the difference?", "start": 3302.615, "duration": 3.555}, {"text": "[inaudible]", "start": 3306.17, "duration": 8.13}, {"text": "Exactly. Stochastic gradient descent is updates,", "start": 3314.3, "duration": 4.29}, {"text": "the weights and the bias after you see every example.", "start": 3318.59, "duration": 3.195}, {"text": "So the direction of the gradient is quite noisy.", "start": 3321.785, "duration": 2.985}, {"text": "It doesn't represent very well the entire batch,", "start": 3324.77, "duration": 2.445}, {"text": "while gradient descent or batch gradient descent is", "start": 3327.215, "duration": 2.835}, {"text": "updates after you've seen the whole batch of examples.", "start": 3330.05, "duration": 3.705}, {"text": "And the gradient is much more precise.", "start": 3333.755, "duration": 2.175}, {"text": "It points to the direction you want to go to.", "start": 3335.93, "duration": 4.03}, {"text": "So what we're trying to do now is to write down these equations if", "start": 3340.63, "duration": 7.39}, {"text": "instead of giving one single cat image we", "start": 3348.02, "duration": 3.09}, {"text": "had given a bunch of images that either have a cat or not a cat.", "start": 3351.11, "duration": 3.84}, {"text": "So now our input x.", "start": 3354.95, "duration": 4.215}, {"text": "So what happens for", "start": 3359.165, "duration": 7.725}, {"text": "an input batch of m examples?", "start": 3366.89, "duration": 9.25}, {"text": "So now our x is not anymore a single column vector,", "start": 3380.86, "duration": 7.539}, {"text": "it's a matrix with the first image corresponding to x_1,", "start": 3388.399, "duration": 6.121}, {"text": "the second image corresponding to x_2 and so on until the nth image corresponding to x_n.", "start": 3394.52, "duration": 7.935}, {"text": "And I'm introducing a new notation which is the parentheses", "start": 3402.455, "duration": 4.815}, {"text": "superscript corresponding to the ID of the example.", "start": 3407.27, "duration": 6.34}, {"text": "So square brackets for the layer,", "start": 3415.75, "duration": 3.655}, {"text": "round brackets for the idea of the example we are talking about.", "start": 3419.405, "duration": 5.605}, {"text": "So just to give more context on what we're trying to do.", "start": 3425.49, "duration": 3.805}, {"text": "We know that this is a bunch of operations.", "start": 3429.295, "duration": 3.18}, {"text": "We just have a,", "start": 3432.475, "duration": 1.67}, {"text": "a network with inputs, hidden, and output layer.", "start": 3434.145, "duration": 3.365}, {"text": "We could have a network with 1,000 layer.", "start": 3437.51, "duration": 2.37}, {"text": "The more layers we have the more computation and it quickly goes up.", "start": 3439.88, "duration": 4.815}, {"text": "So what we wanna do is to be able to parallelize our code or,", "start": 3444.695, "duration": 3.945}, {"text": "or our computation as much as possible by giving", "start": 3448.64, "duration": 2.61}, {"text": "batches of inputs and parallelizing these equations.", "start": 3451.25, "duration": 2.775}, {"text": "So let's see how these equations are modified when we give it a batch of m inputs.", "start": 3454.025, "duration": 5.575}, {"text": "I will use capital letters to", "start": 3460.81, "duration": 4.3}, {"text": "denote the equivalent of the lowercase letters but for a batch of input.", "start": 3465.11, "duration": 6.75}, {"text": "So Z_1 as an example would be W_1,", "start": 3471.86, "duration": 5.595}, {"text": "let's use the same actually,", "start": 3477.455, "duration": 2.235}, {"text": "W_1 times X plus B_1.", "start": 3479.69, "duration": 4.2}, {"text": "So let's analyze what Z_1 would look like.", "start": 3483.89, "duration": 3.375}, {"text": "Z_1 we know that for every,", "start": 3487.265, "duration": 3.925}, {"text": "for every input example of the batch we will get one Z_1 which should look like this.", "start": 3491.35, "duration": 7.7}, {"text": "Then we have to figure out what have to be the shapes", "start": 3508.96, "duration": 3.025}, {"text": "of this equation in order to end up with this.", "start": 3511.985, "duration": 2.4}, {"text": "We know that Z_1 was 3 by 1.", "start": 3514.385, "duration": 2.94}, {"text": "It mean, it means capital Z_1 has to be 3 by", "start": 3517.325, "duration": 5.625}, {"text": "m because each of these column vectors are 3 by 1 and we have m of them.", "start": 3522.95, "duration": 7.155}, {"text": "Because for each input we forward propagate through the network, we get these equations.", "start": 3530.105, "duration": 4.56}, {"text": "So for the first cat image we get these equations,", "start": 3534.665, "duration": 2.31}, {"text": "for the second cat image we get again equations like that and so on.", "start": 3536.975, "duration": 4.735}, {"text": "So what is the shape of x? We have it above.", "start": 3546.34, "duration": 3.805}, {"text": "We know that it's n by n. What is the shape of w_1?", "start": 3550.145, "duration": 7.59}, {"text": "It didn't change. W_1 doesn't change.", "start": 3557.735, "duration": 3.27}, {"text": "It's not because I will give 1,000 inputs to", "start": 3561.005, "duration": 2.655}, {"text": "my network that the parameters are going to be more.", "start": 3563.66, "duration": 3.885}, {"text": "So the parameter number stays the same even if I give more inputs.", "start": 3567.545, "duration": 4.32}, {"text": "And so this has to be 3 by n in order to match Z_1.", "start": 3571.865, "duration": 4.53}, {"text": "Now the interesting thing is that there is an algebraic problem here.", "start": 3576.395, "duration": 6.435}, {"text": "What is the algebraic problem?", "start": 3582.83, "duration": 1.935}, {"text": "We said that the number of parameters doesn't change.", "start": 3584.765, "duration": 3.145}, {"text": "It means that w has the same shape as it has before, as it had before.", "start": 3588.1, "duration": 6.445}, {"text": "B should have the same shape as it had before, right?", "start": 3594.545, "duration": 3.75}, {"text": "It should be 3 by 1. What's the problem of this equation?", "start": 3598.295, "duration": 5.395}, {"text": "Exactly. We're summing a 3 by m matrix to a 3 by 1 vector.", "start": 3606.19, "duration": 8.38}, {"text": "This is not possible in math. It doesn't work.", "start": 3614.57, "duration": 2.58}, {"text": "It doesn't match. When you do some summations or subtraction,", "start": 3617.15, "duration": 3.735}, {"text": "you need the two terms to be the same shape because you will", "start": 3620.885, "duration": 3.855}, {"text": "do an element-wise addition or an ele- element-wise subtraction.", "start": 3624.74, "duration": 4.29}, {"text": "So what's the trick that is used here?", "start": 3629.03, "duration": 2.205}, {"text": "It's a, it's a technique called broadcasting.", "start": 3631.235, "duration": 3.235}, {"text": "Broadcasting is that- is the fact that we", "start": 3641.23, "duration": 3.58}, {"text": "don't want to change the number of parameters, it should stay the same.", "start": 3644.81, "duration": 3.135}, {"text": "But we still want this operation to be able to be written in parallel version.", "start": 3647.945, "duration": 5.805}, {"text": "So we still want to write this equation because we want to parallelize our code,", "start": 3653.75, "duration": 3.345}, {"text": "but we don't want to add more parameters, it doesn't make sense.", "start": 3657.095, "duration": 2.79}, {"text": "So what we're going to do is that we are going to create a vector b tilde", "start": 3659.885, "duration": 5.16}, {"text": "1 which is going to be b_1 repeated three times.", "start": 3665.045, "duration": 6.48}, {"text": "Sorry, repeated m times.", "start": 3671.525, "duration": 3.475}, {"text": "So we just keep the same number of parameters but just repeat", "start": 3683.03, "duration": 4.33}, {"text": "them in order to be able to write my code in parallel.", "start": 3687.36, "duration": 4.42}, {"text": "This is called broadcasting. And what is convenient is that", "start": 3691.94, "duration": 4.48}, {"text": "for those of you who, uh, the homeworks are in Matlab or Python?", "start": 3696.42, "duration": 4.215}, {"text": "Matlab. Okay. So in Matlab, no Python? [LAUGHTER].", "start": 3700.635, "duration": 3.615}, {"text": "[NOISE]", "start": 3704.25, "duration": 0.1}, {"text": "Thank you. Um, Python. So in Python,", "start": 3704.35, "duration": 4.64}, {"text": "there is a package that is often used to to code these equations. It's numPy.", "start": 3708.99, "duration": 4.5}, {"text": "Some people call it numPy, I'm not sure why.", "start": 3713.49, "duration": 2.34}, {"text": "So numPy, basically numerical Python,", "start": 3715.83, "duration": 5.42}, {"text": "we directly do the broadcasting.", "start": 3721.25, "duration": 2.55}, {"text": "It means if you sum this 3 by m matrix with a 3 by 1 parameter vector,", "start": 3723.8, "duration": 8.305}, {"text": "it's going to automatically reproduce", "start": 3732.105, "duration": 1.605}, {"text": "the parameter vector m times so that the equation works.", "start": 3733.71, "duration": 3.24}, {"text": "It's called broadcasting. Does that make sense?", "start": 3736.95, "duration": 3.375}, {"text": "So because we're using this technique,", "start": 3740.325, "duration": 1.86}, {"text": "we're able to rewrite all these equations with capital letters.", "start": 3742.185, "duration": 4.92}, {"text": "Do you wanna do it together or do you want to do it on your own?", "start": 3747.105, "duration": 3.805}, {"text": "Who wants to do it on their own?", "start": 3751.19, "duration": 3.59}, {"text": "Okay. So let's do it on their own [LAUGHTER] on your own.", "start": 3755.69, "duration": 5.215}, {"text": "So rewrite these with capital letters and figure out the shapes.", "start": 3760.905, "duration": 4.455}, {"text": "I think you can do it at home, wherever,", "start": 3765.36, "duration": 1.62}, {"text": "we're not going to do here, but make sure you understand all the shapes. Yeah.", "start": 3766.98, "duration": 2.85}, {"text": "[inaudible] How how is this", "start": 3769.83, "duration": 6.72}, {"text": "[inaudible]?", "start": 3776.55, "duration": 9.15}, {"text": "So the question is how is this different from principal component analysis?", "start": 3785.7, "duration": 4.02}, {"text": "This is a supervised learning algorithm that", "start": 3789.72, "duration": 2.4}, {"text": "will be used to predict the price of a house.", "start": 3792.12, "duration": 2.475}, {"text": "Principal component analysis doesn't predict anything.", "start": 3794.595, "duration": 2.85}, {"text": "It gets an input matrix X normalizes it, ah,", "start": 3797.445, "duration": 4.77}, {"text": "computes the covariance matrix and then figures out what are", "start": 3802.215, "duration": 2.985}, {"text": "the pri- principal components by doing the the eigenvalue decomposition.", "start": 3805.2, "duration": 3.93}, {"text": "But the outcome of PCA is,", "start": 3809.13, "duration": 2.865}, {"text": "you know that the most important features of", "start": 3811.995, "duration": 2.295}, {"text": "your dataset X are going to be these features.", "start": 3814.29, "duration": 4.8}, {"text": "Here we're not looking at the features.", "start": 3819.09, "duration": 2.01}, {"text": "We're only looking at the output.", "start": 3821.1, "duration": 1.365}, {"text": "That is what is important to us. Yes.", "start": 3822.465, "duration": 4.135}, {"text": "In the first lecture when did you say that the first layers is", "start": 3828.17, "duration": 3.43}, {"text": "the edges in an [inaudible].", "start": 3831.6, "duration": 6.54}, {"text": "So the question is, can you explain why the first layer would see the edges?", "start": 3838.14, "duration": 3.705}, {"text": "Is there an intuition behind it?", "start": 3841.845, "duration": 1.29}, {"text": "It's not always going to see the edges,", "start": 3843.135, "duration": 2.07}, {"text": "but it's oftentimes going to see edges because um,", "start": 3845.205, "duration": 3.585}, {"text": "in order to detect a human face,", "start": 3848.79, "duration": 2.55}, {"text": "let's say, you will train an algorithm to find out whose face it is.", "start": 3851.34, "duration": 3.99}, {"text": "So it has to understand the faces very well.", "start": 3855.33, "duration": 2.385}, {"text": "Um, you need the network to be complex enough", "start": 3857.715, "duration": 2.715}, {"text": "to understand very detailed features of the face.", "start": 3860.43, "duration": 2.58}, {"text": "And usually, this neuron,", "start": 3863.01, "duration": 2.925}, {"text": "what it sees as input are pixels.", "start": 3865.935, "duration": 3.105}, {"text": "So it means every edge here is the multiplication of the weight by a pixel.", "start": 3869.04, "duration": 5.52}, {"text": "So it sees pixels.", "start": 3874.56, "duration": 2.595}, {"text": "It cannot understand the face as a whole because it sees only pixels.", "start": 3877.155, "duration": 5.1}, {"text": "It's very granular information for it.", "start": 3882.255, "duration": 2.55}, {"text": "So it's going to check if pixels nearby have", "start": 3884.805, "duration": 3.615}, {"text": "the same color and understand that there is an edge there, okay?", "start": 3888.42, "duration": 3.75}, {"text": "But it's too complicated to understand the whole face in the first layer.", "start": 3892.17, "duration": 3.72}, {"text": "However, if it understands a little more than a pixel information,", "start": 3895.89, "duration": 4.32}, {"text": "it can give it to the next neuron.", "start": 3900.21, "duration": 1.845}, {"text": "This neuron will receive more than pixel information.", "start": 3902.055, "duration": 3.06}, {"text": "It would receive a little more complex-like edges,", "start": 3905.115, "duration": 3.945}, {"text": "and then it will use this information to build on top of", "start": 3909.06, "duration": 2.7}, {"text": "it and build the features of the face.", "start": 3911.76, "duration": 2.475}, {"text": "So what I'm trying to sum up is that these neurons only see the pixels,", "start": 3914.235, "duration": 3.09}, {"text": "so they're not able to build more than the edges.", "start": 3917.325, "duration": 2.475}, {"text": "That's the minimum thing that they can- the maximum thing they can do.", "start": 3919.8, "duration": 3.435}, {"text": "And it's it's a complex topic,", "start": 3923.235, "duration": 2.55}, {"text": "like interpretation of neural network is", "start": 3925.785, "duration": 1.545}, {"text": "a highly researched topic, it's a big research topic.", "start": 3927.33, "duration": 2.94}, {"text": "So nobody figured out exactly how all the neurons evolve.", "start": 3930.27, "duration": 5.895}, {"text": "Yeah. One more question and then we move on.", "start": 3936.165, "duration": 3.835}, {"text": "Ah, how [inaudible].", "start": 3943.97, "duration": 7.39}, {"text": "So the question is how [OVERLAPPING].", "start": 3951.36, "duration": 2.34}, {"text": "[inaudible].", "start": 3953.7, "duration": 0.1}, {"text": "-how do you decide how many neurons per layer? How many layers?", "start": 3953.8, "duration": 3.53}, {"text": "What's the architecture of your neural network?", "start": 3957.33, "duration": 1.98}, {"text": "There are two things to take into consideration", "start": 3959.31, "duration": 1.95}, {"text": "I would say. First and nobody knows the right answer, so you have to test it.", "start": 3961.26, "duration": 3.93}, {"text": "So you you guys talked about training set,", "start": 3965.19, "duration": 3.105}, {"text": "validation set, and test set.", "start": 3968.295, "duration": 1.755}, {"text": "So what we would do is,", "start": 3970.05, "duration": 1.35}, {"text": "we would try ten different architectures,", "start": 3971.4, "duration": 3.18}, {"text": "train it, train the network on these,", "start": 3974.58, "duration": 2.025}, {"text": "looking at the validation set accuracy of all these,", "start": 3976.605, "duration": 2.745}, {"text": "and decide which one seems to be the best.", "start": 3979.35, "duration": 1.995}, {"text": "That's how we figure out what's the right network size.", "start": 3981.345, "duration": 2.955}, {"text": "On top of that, using experience is often valuable.", "start": 3984.3, "duration": 3.0}, {"text": "So if you give me a problem,", "start": 3987.3, "duration": 2.564}, {"text": "I try always to gauge how complex is the problem.", "start": 3989.864, "duration": 3.736}, {"text": "Like cat classification, do you", "start": 3993.6, "duration": 3.39}, {"text": "think it's easier or harder than day and night classification?", "start": 3996.99, "duration": 3.705}, {"text": "So day and night classification is I give you an image,", "start": 4000.695, "duration": 2.52}, {"text": "I asked you to predict if it was taken during the day or during the night,", "start": 4003.215, "duration": 3.025}, {"text": "and on the other hand you want there's a cat on the image or not.", "start": 4006.24, "duration": 2.935}, {"text": "Which one is easier, which one is harder?", "start": 4009.175, "duration": 2.845}, {"text": "Who thinks cat classification is harder?", "start": 4014.65, "duration": 3.7}, {"text": "Okay. I think people agree.", "start": 4018.35, "duration": 2.115}, {"text": "Cat classification seems harder, why?", "start": 4020.465, "duration": 2.01}, {"text": "Because there are many breeds of cats.", "start": 4022.475, "duration": 1.74}, {"text": "Can look like different things.", "start": 4024.215, "duration": 1.545}, {"text": "There's not many breeds of nights. um, I guess.", "start": 4025.76, "duration": 2.34}, {"text": "[LAUGHTER]", "start": 4028.1, "duration": 0.96}, {"text": "Um, one thing that might be challenging in the day and night classification,", "start": 4029.06, "duration": 3.915}, {"text": "is if you want also to figure it out in house like i- inside,", "start": 4032.975, "duration": 3.99}, {"text": "you know maybe there is a tiny window there and I'm able to tell that is the day", "start": 4036.965, "duration": 5.04}, {"text": "but for a network to understand it you will need a lot more data", "start": 4042.005, "duration": 2.715}, {"text": "than if only you wanted to work outside, different.", "start": 4044.72, "duration": 2.85}, {"text": "So these problems all have their own complexity.", "start": 4047.57, "duration": 2.655}, {"text": "Based on their complexity,", "start": 4050.225, "duration": 2.085}, {"text": "I think the network should be deeper.", "start": 4052.31, "duration": 1.77}, {"text": "The comp- the more complex usually is the problem,", "start": 4054.08, "duration": 2.13}, {"text": "the more data you need in order to figure out the output,", "start": 4056.21, "duration": 3.18}, {"text": "the more deeper should be the network.", "start": 4059.39, "duration": 1.905}, {"text": "That's an intuition, let's say.", "start": 4061.295, "duration": 1.5}, {"text": "Okay. Let's move on guys because I think we have about what, 12 more minutes?", "start": 4062.795, "duration": 7.495}, {"text": "Okay. Let's try to write the loss function", "start": 4077.44, "duration": 4.82}, {"text": "for this problem.", "start": 4082.72, "duration": 1.87}, {"text": "[NOISE].", "start": 4084.59, "duration": 14.43}, {"text": "So now that we have our network,", "start": 4099.02, "duration": 1.68}, {"text": "we have written this propagation equation and I we call it forward propagation,", "start": 4100.7, "duration": 5.73}, {"text": "because it's going forward, it's going from the input to the output.", "start": 4106.43, "duration": 3.315}, {"text": "Later on when we will, we will derive these equations,", "start": 4109.745, "duration": 3.945}, {"text": "we will call them backward propagation,", "start": 4113.69, "duration": 1.86}, {"text": "because we are starting from the loss and going backwards.", "start": 4115.55, "duration": 3.81}, {"text": "So let's let's talk about the optimization problem.", "start": 4119.36, "duration": 4.23}, {"text": "Optimizing w_1, w_2, w_3, b_1, b_2, b_3.", "start": 4123.59, "duration": 13.68}, {"text": "We have a lot of stuff to optimize, right?", "start": 4137.27, "duration": 2.04}, {"text": "We have to find the right values for these and", "start": 4139.31, "duration": 1.77}, {"text": "remember model equals architecture plus parameter.", "start": 4141.08, "duration": 2.13}, {"text": "We have our architecture, if we have our parameters we're done.", "start": 4143.21, "duration": 3.34}, {"text": "So in order to do that, we have to define an objective function.", "start": 4146.59, "duration": 6.44}, {"text": "Sometimes called loss, sometimes called cost function.", "start": 4153.04, "duration": 4.7}, {"text": "So usually we would call it loss if there is only one example in the batch,", "start": 4158.35, "duration": 5.275}, {"text": "and cost, if there is multiple examples in a batch.", "start": 4163.625, "duration": 6.685}, {"text": "So the loss function that,", "start": 4172.3, "duration": 3.97}, {"text": "let- let's define the cost function.", "start": 4176.27, "duration": 1.815}, {"text": "The cost function J depends on y hat n_y.", "start": 4178.085, "duration": 4.905}, {"text": "Okay. So y hat,", "start": 4182.99, "duration": 3.165}, {"text": "y hat is a_3.", "start": 4186.155, "duration": 2.755}, {"text": "Okay. It depends on y hat n_y,", "start": 4190.0, "duration": 7.525}, {"text": "and we will set it to be the sum of the loss functions L_i,", "start": 4197.525, "duration": 8.295}, {"text": "and I will normalize it.", "start": 4205.82, "duration": 1.74}, {"text": "It's not mandatory, but normalize it with 1/n. So what does this mean?", "start": 4207.56, "duration": 7.35}, {"text": "It's that we are going for batch gradient descent.", "start": 4214.91, "duration": 3.415}, {"text": "We wanna compute the loss function for the whole batch, parallelize our code,", "start": 4218.325, "duration": 5.16}, {"text": "and then calculate the cost function that", "start": 4223.485, "duration": 3.575}, {"text": "will be then derived to give us the direction of the gradients.", "start": 4227.06, "duration": 4.605}, {"text": "That is, the average direction of", "start": 4231.665, "duration": 2.04}, {"text": "all the de-de- derivation with respect to the whole input batch.", "start": 4233.705, "duration": 4.815}, {"text": "And L_i will be the loss function corresponding to one parameter.", "start": 4238.52, "duration": 7.425}, {"text": "So what's the error on this specific one input,", "start": 4245.945, "duration": 4.44}, {"text": "sorry not parameter, and it will be the logistic loss.", "start": 4250.385, "duration": 4.555}, {"text": "You've already seen these equations, I believe.", "start": 4268.75, "duration": 3.59}, {"text": "So now, is it more complex to take", "start": 4273.73, "duration": 3.16}, {"text": "a derivative with respect to J like of J with respect to the parameters or of L?", "start": 4276.89, "duration": 5.5}, {"text": "What's the most complex between this one,", "start": 4282.39, "duration": 3.93}, {"text": "let's say we're taking the derivative with respect to w_2, compared to this one?", "start": 4286.32, "duration": 7.38}, {"text": "Which one is the hardest? Who thinks J is the hardest?", "start": 4298.66, "duration": 8.8}, {"text": "Who think it doesn't matter?", "start": 4307.5, "duration": 2.61}, {"text": "Yeah, it doesn't matter because derivation is is a linear operation, right?", "start": 4310.11, "duration": 6.275}, {"text": "So you can just take the derivative inside and you will see that if you know this,", "start": 4316.385, "duration": 4.29}, {"text": "you just have to take the sum over this.", "start": 4320.675, "duration": 3.0}, {"text": "So instead of computing all derivatives on J,", "start": 4323.675, "duration": 3.21}, {"text": "we will com- compute them on L,", "start": 4326.885, "duration": 1.71}, {"text": "but it's totally equivalent.", "start": 4328.595, "duration": 1.365}, {"text": "There's just one more step at the end.", "start": 4329.96, "duration": 2.56}, {"text": "Okay. So now we defined our loss function, super.", "start": 4332.74, "duration": 6.11}, {"text": "We defined our loss function and the next step is optimize.", "start": 4339.61, "duration": 4.045}, {"text": "So we have to compute a lot of derivatives. [NOISE]", "start": 4343.655, "duration": 18.045}, {"text": "And that's called backward propagation.", "start": 4361.7, "duration": 3.3}, {"text": "[NOISE] So the question", "start": 4365.0, "duration": 8.19}, {"text": "is why is it called backward propagation?", "start": 4373.19, "duration": 2.37}, {"text": "It's because what we want to do ultimately is this.", "start": 4375.56, "duration": 4.095}, {"text": "For any l equals 1-3,", "start": 4379.655, "duration": 5.61}, {"text": "we want to do that,", "start": 4385.265, "duration": 4.35}, {"text": "wl equals wl minus Alpha derivative of j with respect to wl,", "start": 4389.615, "duration": 11.495}, {"text": "and bl equals bl minus Alpha derivative of j with respect to bl.", "start": 4401.11, "duration": 7.81}, {"text": "So we want to do that for every parameter in layer 1, 2, and 3.", "start": 4409.0, "duration": 6.91}, {"text": "So it means, we have to compute all these derivatives,", "start": 4415.91, "duration": 3.42}, {"text": "we have to compute derivative of the cost with respect to w1,", "start": 4419.33, "duration": 2.64}, {"text": "w2, w3, b1, b2, b3.", "start": 4421.97, "duration": 2.91}, {"text": "You've done it with logistic regression,", "start": 4424.88, "duration": 2.235}, {"text": "we're going to do it with a neural network,", "start": 4427.115, "duration": 2.205}, {"text": "and you're going to understand why it's called backward propagation.", "start": 4429.32, "duration": 3.015}, {"text": "Which one do you want to start with? Which derivative?", "start": 4432.335, "duration": 3.495}, {"text": "You wanna start with the derivative with respect to w1,", "start": 4435.83, "duration": 2.76}, {"text": "w2, or w3, let's say.", "start": 4438.59, "duration": 2.055}, {"text": "Assuming we'll do the bias later. W what?", "start": 4440.645, "duration": 5.58}, {"text": "W1? You think w1 is a good idea.", "start": 4446.225, "duration": 3.465}, {"text": "I do- don't wanna do w1.", "start": 4449.69, "duration": 2.92}, {"text": "I think we should do w3,", "start": 4453.28, "duration": 2.53}, {"text": "and the reason is because if you look at this loss function,", "start": 4455.81, "duration": 5.26}, {"text": "do you think the relation between w3 and", "start": 4462.49, "duration": 3.76}, {"text": "this loss function is easier to understand or", "start": 4466.25, "duration": 1.98}, {"text": "the relation between w1 and this loss function?", "start": 4468.23, "duration": 3.09}, {"text": "It's the relation between w3 and this loss function.", "start": 4471.32, "duration": 3.06}, {"text": "Because w3 happens much later in the- in the network.", "start": 4474.38, "duration": 2.775}, {"text": "So if you want to understand how much should we move w1 in order to make the loss move?", "start": 4477.155, "duration": 5.085}, {"text": "It's much more complicated than answering the question", "start": 4482.24, "duration": 2.49}, {"text": "how much should w3 move to move the loss.", "start": 4484.73, "duration": 2.925}, {"text": "Because there's much more connections if you wanna compete with w1.", "start": 4487.655, "duration": 5.265}, {"text": "So that's why we call it backward propagation", "start": 4492.92, "duration": 1.875}, {"text": "is because we will start with the top layer,", "start": 4494.795, "duration": 1.89}, {"text": "the one that's the closest to the loss function,", "start": 4496.685, "duration": 2.055}, {"text": "derive the derivative of j with respect to w1.", "start": 4498.74, "duration": 9.16}, {"text": "Once we've computed this derivative which we are going to do next week,", "start": 4508.6, "duration": 5.92}, {"text": "once we computed this number,", "start": 4514.52, "duration": 2.295}, {"text": "we can then tackle this one.", "start": 4516.815, "duration": 2.845}, {"text": "Oh, sorry. Yeah. Thanks. Yeah. Once we computed this number,", "start": 4522.46, "duration": 5.605}, {"text": "we will be able to compute this one very easily. Why very easily?", "start": 4528.065, "duration": 5.73}, {"text": "Because we can use the chain rule of calculus.", "start": 4533.795, "duration": 2.88}, {"text": "So let's see how it works.", "start": 4536.675, "duration": 1.635}, {"text": "What we're- I'm just going to give you, uh,", "start": 4538.31, "duration": 2.505}, {"text": "the one-minute pitch on- on backprop,", "start": 4540.815, "duration": 2.325}, {"text": "but, uh, we'll do it next week together.", "start": 4543.14, "duration": 2.235}, {"text": "So if we had to compute this derivative,", "start": 4545.375, "duration": 2.79}, {"text": "what I will do is that I will separate it into several derivatives that are easier.", "start": 4548.165, "duration": 5.145}, {"text": "I will separate it into the derivative of j with respect to something,", "start": 4553.31, "duration": 4.35}, {"text": "with the something, with respect to w3.", "start": 4557.66, "duration": 3.165}, {"text": "And the question is, what should this something be?", "start": 4560.825, "duration": 4.2}, {"text": "I will look at my equations.", "start": 4565.025, "duration": 2.25}, {"text": "I know that j depends on Y-hat,", "start": 4567.275, "duration": 3.09}, {"text": "and I know that Y-hat depends on z3.", "start": 4570.365, "duration": 3.315}, {"text": "Y-hat is the same thing as a3,", "start": 4573.68, "duration": 1.665}, {"text": "I know it depends on z3.", "start": 4575.345, "duration": 1.995}, {"text": "So why don't- why don't I include z3 in my equation?", "start": 4577.34, "duration": 3.825}, {"text": "I also know that z3 depends on w3,", "start": 4581.165, "duration": 2.25}, {"text": "and the derivative of z3 with respect to w2 is super easy,", "start": 4583.415, "duration": 3.345}, {"text": "it's just a2 transpose.", "start": 4586.76, "duration": 2.205}, {"text": "So I will just make a quick hack and say that", "start": 4588.965, "duration": 3.855}, {"text": "this derivative is the same as taking it with respect to a3,", "start": 4592.82, "duration": 5.13}, {"text": "taking derivative of a3 with respect to z3,", "start": 4597.95, "duration": 4.229}, {"text": "and taking the derivative of z3 with respect to w3.", "start": 4602.179, "duration": 7.276}, {"text": "So you see? Same, same derivative,", "start": 4609.455, "duration": 3.39}, {"text": "calculated in different ways.", "start": 4612.845, "duration": 2.205}, {"text": "And I know this, I know these are pretty easy to compute.", "start": 4615.05, "duration": 6.46}, {"text": "So that's why we call it backpropagation,", "start": 4621.51, "duration": 2.405}, {"text": "it's because I will use the chain rule to compute the derivative of w3,", "start": 4623.915, "duration": 3.705}, {"text": "and then when I want to do it for w2,", "start": 4627.62, "duration": 2.715}, {"text": "I'm going to insert,", "start": 4630.335, "duration": 2.16}, {"text": "I'm going to insert the derivative with z3 times the derivative of", "start": 4632.495, "duration": 7.245}, {"text": "z3 with respect to a2 times the derivative of a2 with respect to z2,", "start": 4639.74, "duration": 8.55}, {"text": "and derivative of z2 with respect to w2.", "start": 4648.29, "duration": 5.145}, {"text": "Does this make sense that this thing here is the same thing as this?", "start": 4653.435, "duration": 8.545}, {"text": "It means, if I wanna compute the derivative of w2,", "start": 4662.89, "duration": 3.655}, {"text": "I don't need to compute this anymore,", "start": 4666.545, "duration": 1.62}, {"text": "I already did this for w3.", "start": 4668.165, "duration": 2.01}, {"text": "I just need to compute those which are easy ones, and so on.", "start": 4670.175, "duration": 4.07}, {"text": "If I wanna compute the derivative of j with respect to w1,", "start": 4674.245, "duration": 5.625}, {"text": "I'm going to- I'm not going to decompose all the thing again,", "start": 4679.87, "duration": 3.645}, {"text": "I'm just going to take the derivative of j with respect to", "start": 4683.515, "duration": 2.805}, {"text": "z2 which is equal to this whole thing.", "start": 4686.32, "duration": 4.33}, {"text": "And then I'm gonna multiply it by the derivative of", "start": 4690.65, "duration": 2.43}, {"text": "z2 with respect to a1 times derivative of", "start": 4693.08, "duration": 4.965}, {"text": "a1 with respect to z1 times the derivative of z1 with respect to w1.", "start": 4698.045, "duration": 7.995}, {"text": "And again, this thing I know it already,", "start": 4706.04, "duration": 2.955}, {"text": "I computed it previously just for this one.", "start": 4708.995, "duration": 3.355}, {"text": "So what's, what's interesting about it is that I'm not gonna redo the work I did,", "start": 4712.87, "duration": 5.14}, {"text": "I'm just gonna store the right values while back-propagating and continue to derivate.", "start": 4718.01, "duration": 4.635}, {"text": "One thing that you need to notice though is that, look,", "start": 4722.645, "duration": 4.26}, {"text": "you need this forward propagation equation in order to", "start": 4726.905, "duration": 3.015}, {"text": "remember what should be the path to take in", "start": 4729.92, "duration": 3.09}, {"text": "your chain rule because you know that this derivative of j with respect to w3,", "start": 4733.01, "duration": 6.165}, {"text": "I cannot use it as it is because w3 is not connected to the previous layer.", "start": 4739.175, "duration": 4.755}, {"text": "If you look at this equation,", "start": 4743.93, "duration": 1.83}, {"text": "a2 doesn't depend on w3,", "start": 4745.76, "duration": 2.775}, {"text": "it depends on z3.", "start": 4748.535, "duration": 2.085}, {"text": "Sorry, like, uh, my bad,", "start": 4750.62, "duration": 1.635}, {"text": "it depends- no, sorry,", "start": 4752.255, "duration": 1.815}, {"text": "what I wanted to say is that z2 is connected to w2,", "start": 4754.07, "duration": 6.284}, {"text": "but a1 is not connected to w2.", "start": 4760.354, "duration": 4.816}, {"text": "So you wanna choose the path that you're going", "start": 4765.17, "duration": 3.15}, {"text": "through in the proper way so that there's no cancellation in these derivatives.", "start": 4768.32, "duration": 5.175}, {"text": "You- you cannot compute derivative of w2 with", "start": 4773.495, "duration": 5.655}, {"text": "respect to- to a1, right?", "start": 4779.15, "duration": 5.925}, {"text": "You cannot compute that, you don't know it.", "start": 4785.075, "duration": 3.21}, {"text": "Okay. So I think we're done for today.", "start": 4788.285, "duration": 3.3}, {"text": "So one thing that I'd like you to do if you have time is", "start": 4791.585, "duration": 2.985}, {"text": "just think about the things that can be tweaked in a neural network.", "start": 4794.57, "duration": 4.29}, {"text": "When you build a neural network, you are not done,", "start": 4798.86, "duration": 3.39}, {"text": "you have to tweak it, you have to tweak", "start": 4802.25, "duration": 1.41}, {"text": "the activations, you have to tweak the loss function.", "start": 4803.66, "duration": 1.65}, {"text": "There's many things you can tweak,", "start": 4805.31, "duration": 1.2}, {"text": "and that's what we're going to see next week. Okay. Thanks.", "start": 4806.51, "duration": 3.55}]