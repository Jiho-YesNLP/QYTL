[{"text": "Welcome back, everyone. I hope you had a good Thanksgiving.", "start": 3.47, "duration": 3.805}, {"text": "Um, I actually didn't ask, I'm not sure why this chair is here.", "start": 7.275, "duration": 2.43}, {"text": "All right. Let's get rid of this.", "start": 9.705, "duration": 1.065}, {"text": "Um, by the way, not sure- um, thanks, Anand.", "start": 10.77, "duration": 4.11}, {"text": "I'm not sure if you guys are following the news, but in,", "start": 14.88, "duration": 2.88}, {"text": "in reinforcement learning, we chat a lot about robotics, right?", "start": 17.76, "duration": 3.06}, {"text": "And one of the, you know, uh,", "start": 20.82, "duration": 2.04}, {"text": "constant problems a lot of people use reinforcement learning to solve is robotics and,", "start": 22.86, "duration": 4.485}, {"text": "um, I think, ah, uh, back in May,", "start": 27.345, "duration": 3.0}, {"text": "um, the InSight Mars lander had launched from, um,", "start": 30.345, "duration": 3.57}, {"text": "here in California and it's about to make an attempt at landing", "start": 33.915, "duration": 2.955}, {"text": "on the planet Mars in the next 2.5 hours or so,", "start": 36.87, "duration": 3.03}, {"text": "so excited about that, uh,", "start": 39.9, "duration": 1.845}, {"text": "I think that is actually one of the grandest,", "start": 41.745, "duration": 2.265}, {"text": "um, applications of robotics because, you know,", "start": 44.01, "duration": 2.79}, {"text": "with a- with 20 minute light-speed from Earth to Mars,", "start": 46.8, "duration": 2.955}, {"text": "you know, once it starts its landing,", "start": 49.755, "duration": 1.98}, {"text": "there is nothing anyone on Earth can do and so I think that's", "start": 51.735, "duration": 2.745}, {"text": "one of the most exciting applications of autonomous robotics.", "start": 54.48, "duration": 2.99}, {"text": "When you launch this thing, it's now about 20,", "start": 57.47, "duration": 2.0}, {"text": "20 light minutes away from planet Earth,", "start": 59.47, "duration": 1.99}, {"text": "so you actually can't control it in real time,", "start": 61.46, "duration": 1.95}, {"text": "uh, and you just have to hope like crazy that your software", "start": 63.41, "duration": 3.225}, {"text": "works well enough for it to land on this planet, you know.", "start": 66.635, "duration": 3.82}, {"text": "Uh, and then so we, we will find out a little bit", "start": 70.455, "duration": 2.445}, {"text": "afternoon if the landing happened successfully or not.", "start": 72.9, "duration": 3.17}, {"text": "I, I think, um, so I,", "start": 76.07, "duration": 2.035}, {"text": "I just get excited about stuff like this,", "start": 78.105, "duration": 1.535}, {"text": "I, I hope you guys do too.", "start": 79.64, "duration": 1.26}, {"text": "And for those of you that are from California, I mean,", "start": 80.9, "duration": 2.43}, {"text": "take some pride that it launched from the home state of California and,", "start": 83.33, "duration": 2.94}, {"text": "and is now nearing its,", "start": 86.27, "duration": 1.47}, {"text": "er, landing on Mars.", "start": 87.74, "duration": 1.5}, {"text": "Okay, um, all right.", "start": 89.24, "duration": 3.085}, {"text": "So, um, what I wanna do today is,", "start": 92.325, "duration": 3.435}, {"text": "uh, continue our discussion on reinforcement learning.", "start": 95.76, "duration": 3.76}, {"text": "Do a quick recap of the MDP or the Markov decision process framework.", "start": 99.52, "duration": 5.78}, {"text": "Um, and then we'll start to talk about algorithms for solving MDPs.", "start": 105.3, "duration": 4.35}, {"text": "In particular, we need to define, uh,", "start": 109.65, "duration": 2.115}, {"text": "something called the value function which tells you how good it is to be", "start": 111.765, "duration": 4.235}, {"text": "in different states of the MDP and then, um,", "start": 116.0, "duration": 4.28}, {"text": "we'll define the value function and then talk about an algorithm", "start": 120.28, "duration": 2.89}, {"text": "called value iteration for computing the value function", "start": 123.17, "duration": 3.18}, {"text": "and this will help us figure out how to actually", "start": 126.35, "duration": 2.91}, {"text": "find a good controller or find a good policy for an MDP,", "start": 129.26, "duration": 3.62}, {"text": "and then we'll wrap up with our learning state transition probabilities", "start": 132.88, "duration": 2.47}, {"text": "and how to put all these", "start": 135.35, "duration": 1.215}, {"text": "together into an actual reinforcement learning algorithm that you can implement.", "start": 136.565, "duration": 5.55}, {"text": "Um, to recap, um,", "start": 142.115, "duration": 2.215}, {"text": "our motivating example- running example from the last time,", "start": 144.33, "duration": 3.93}, {"text": "from before Thanksgiving was,", "start": 148.26, "duration": 1.49}, {"text": "uh, this 11-state MDP.", "start": 149.75, "duration": 2.31}, {"text": "And we said that an MDP comprises a five tuple,", "start": 152.06, "duration": 3.995}, {"text": "a lists of five things with, er, states.", "start": 156.055, "duration": 2.595}, {"text": "So that example had 11 states.", "start": 158.65, "duration": 2.745}, {"text": "Um, actions, and in this example the actions were the compass directions;", "start": 161.395, "duration": 4.895}, {"text": "North, South, East, and West,", "start": 166.29, "duration": 1.21}, {"text": "I can try to go in each of the four compass directions.", "start": 167.5, "duration": 2.335}, {"text": "The state transition probabilities and in the example,", "start": 169.835, "duration": 3.359}, {"text": "if the robot attempts to go North,", "start": 173.194, "duration": 2.296}, {"text": "it has an 80% chance of heading North and a 0.1%", "start": 175.49, "duration": 3.89}, {"text": "chance of veering off to the left and a 0.1 chance of veering off to the right.", "start": 179.38, "duration": 4.91}, {"text": "Um, Gamma is a number slightly less than 1,", "start": 184.29, "duration": 4.29}, {"text": "um, usually slightly less than 1,", "start": 188.58, "duration": 1.755}, {"text": "there is a discount factor, think of this as 0.99,", "start": 190.335, "duration": 3.195}, {"text": "um and R is the reward function that helps us specify where we want the robot to end up.", "start": 193.53, "duration": 8.645}, {"text": "Um, and so what we said last time was that, um,", "start": 202.175, "duration": 4.065}, {"text": "the way an MDP works is you start off in some state S_0,", "start": 206.24, "duration": 3.855}, {"text": "um, this one's much better, you choose an action, uh,", "start": 210.095, "duration": 3.09}, {"text": "a_0, and as a result of that,", "start": 213.185, "duration": 2.695}, {"text": "it transitions to a new state, S_1,", "start": 215.88, "duration": 2.235}, {"text": "which is drawn according to P_s_0 a_0.", "start": 218.115, "duration": 2.73}, {"text": "Um, and then you choose a new action a_1 and as a result", "start": 220.845, "duration": 5.045}, {"text": "the MDP transitions to some new state P_s_1 a_1,", "start": 225.89, "duration": 6.825}, {"text": "um, and the total payoff is the sum of rewards, right?", "start": 232.715, "duration": 10.435}, {"text": "Um, and the goal is to come up with a way, um,", "start": 243.15, "duration": 5.6}, {"text": "and formally the goal is to come up with a policy, Pi,", "start": 248.75, "duration": 5.725}, {"text": "which is a mapping from the states to the actions, uh,", "start": 254.475, "duration": 3.945}, {"text": "that will tell you how to choose actions from whatever stage you are in so that", "start": 258.42, "duration": 4.31}, {"text": "the policy maximizes the expected value of the total payoff, okay?", "start": 262.73, "duration": 5.5}, {"text": "Um, and so I think last time I,", "start": 268.23, "duration": 3.205}, {"text": "I kinda claimed that this is the optimal policy for this MDP, right?", "start": 271.435, "duration": 9.39}, {"text": "Um, and what this means for example is,", "start": 280.825, "duration": 3.07}, {"text": "if you look at this state, um,", "start": 283.895, "duration": 2.655}, {"text": "this policy is telling you that Pi of 3, 1 equals,", "start": 286.55, "duration": 5.94}, {"text": "uh, West, I guess,", "start": 292.49, "duration": 1.53}, {"text": "or you can write West or left, well,", "start": 294.02, "duration": 1.96}, {"text": "what do you call that left arrow, right,", "start": 295.98, "duration": 1.77}, {"text": "where from this state, um,", "start": 297.75, "duration": 2.415}, {"text": "from the state 3,1, you know,", "start": 300.165, "duration": 3.01}, {"text": "the best action to take is to go left, it's to go West.", "start": 303.175, "duration": 3.15}, {"text": "And so if you're executing this policy what that means is that, um,", "start": 306.325, "duration": 4.99}, {"text": "on every step the action you choose would be, you know,", "start": 311.315, "duration": 3.57}, {"text": "Pi, right, of the,", "start": 314.885, "duration": 1.965}, {"text": "the state that you're in, okay?", "start": 316.85, "duration": 2.115}, {"text": "So, um, what I'd like to do is now,", "start": 318.965, "duration": 4.105}, {"text": "uh, to find the value function.", "start": 323.07, "duration": 2.31}, {"text": "So, how, how, how, how did I come up with this, right?", "start": 325.38, "duration": 2.55}, {"text": "Well, what I'd like to do is, have you,", "start": 327.93, "duration": 1.8}, {"text": "um, learn given an MDP,", "start": 329.73, "duration": 2.445}, {"text": "given this five tuple,", "start": 332.175, "duration": 1.375}, {"text": "how do you compute the optimal policy?", "start": 333.55, "duration": 4.025}, {"text": "And one of the challenges with, um,", "start": 337.575, "duration": 4.05}, {"text": "finding the optimal policy is that, you know,", "start": 341.625, "duration": 1.875}, {"text": "there's a- there's an exponentially large number of possible policies, right?", "start": 343.5, "duration": 3.555}, {"text": "If you have 11 states and four actions per state,", "start": 347.055, "duration": 3.155}, {"text": "the number of possible policies is, er,", "start": 350.21, "duration": 1.92}, {"text": "4 to the power of 11 which is not that big because 11 is a small MDP, right?", "start": 352.13, "duration": 4.41}, {"text": "Because the number of, of policies- possible policies for,", "start": 356.54, "duration": 3.175}, {"text": "for an MDP is combinatorially large, is,", "start": 359.715, "duration": 1.965}, {"text": "uh, number of actions,  the power of the number of states.", "start": 361.68, "duration": 2.39}, {"text": "So how do you find the best policy?", "start": 364.07, "duration": 2.505}, {"text": "Okay. So what you learn today is,", "start": 366.575, "duration": 2.91}, {"text": "um, how to compute the optimal policy.", "start": 369.485, "duration": 4.41}, {"text": "Now, in order to develop an algorithm for computing an optimal policy,", "start": 373.895, "duration": 5.985}, {"text": "um, we'll need to define three things.", "start": 379.88, "duration": 3.33}, {"text": "So just as a roadmap.", "start": 383.21, "duration": 1.59}, {"text": "Um, what I'm about to do is define V_Pi,", "start": 384.8, "duration": 4.77}, {"text": "V_star, and Pi_star, okay?", "start": 389.57, "duration": 3.34}, {"text": "Um, and based on these definitions we'll see that- we'll,", "start": 392.91, "duration": 3.44}, {"text": "we'll come to the, uh, definition.", "start": 396.35, "duration": 2.355}, {"text": "We will- uh, derive that Pi_star is the optimal policy, okay?", "start": 398.705, "duration": 4.575}, {"text": "But so let's, let's go through these few definitions.", "start": 403.28, "duration": 2.95}, {"text": "Um, first V_Pi.", "start": 406.23, "duration": 2.62}, {"text": "So for a policy Pi,", "start": 408.98, "duration": 4.45}, {"text": "V_Pi is a function mapping from states to the rules, uh,", "start": 413.43, "duration": 9.71}, {"text": "[NOISE] is such that V_Pi of S is the expected total payoff,", "start": 423.14, "duration": 10.01}, {"text": "um, for starting in state S and executing Pi.", "start": 433.15, "duration": 20.55}, {"text": "And so sometimes we write this as V_Pi of S is", "start": 453.7, "duration": 3.52}, {"text": "the expected total payoff", "start": 457.22, "duration": 4.27}, {"text": "given that you execute the policy Pi and the initial state,", "start": 464.21, "duration": 7.68}, {"text": "S_0 is equal to S, okay?", "start": 471.89, "duration": 3.61}, {"text": "So the definition of V_Pi,", "start": 475.5, "duration": 1.845}, {"text": "this is called the, um,", "start": 477.345, "duration": 1.665}, {"text": "value function for a policy.", "start": 479.01, "duration": 2.085}, {"text": "Well, this is called the value function.", "start": 481.095, "duration": 2.175}, {"text": "[NOISE]", "start": 483.27, "duration": 7.43}, {"text": "For the policy Pi, okay?", "start": 490.7, "duration": 3.41}, {"text": "Um, and so what the value function for a policy Pi denoted v_Pi is?", "start": 494.11, "duration": 6.18}, {"text": "Is it tells you for any state you might start in,", "start": 500.29, "duration": 3.225}, {"text": "there's a function mapping of states to rewards, right?", "start": 503.515, "duration": 1.845}, {"text": "For any state you might start in what's", "start": 505.36, "duration": 1.56}, {"text": "your expected total payoff if you start off your robot in that state,", "start": 506.92, "duration": 4.47}, {"text": "and if you execute the policy Pi?", "start": 511.39, "duration": 2.64}, {"text": "And execute the policy Pi means take actions according to the policy Pi.", "start": 514.03, "duration": 4.095}, {"text": "Right? So here's a, here's a specific example.", "start": 518.125, "duration": 2.46}, {"text": "Um, this policy.", "start": 520.585, "duration": 4.59}, {"text": "So let's consider the follo- following policy Pi, right.", "start": 525.175, "duration": 3.945}, {"text": "Um,", "start": 529.12, "duration": 0.54}, {"text": "[NOISE]", "start": 529.66, "duration": 10.47}, {"text": "so this is not a great policy.", "start": 540.13, "duration": 2.385}, {"text": "You know, from some of these states,", "start": 542.515, "duration": 1.905}, {"text": "it looks like it's heading for the minus 1 reward or sorry.", "start": 544.42, "duration": 2.73}, {"text": "So if one of the reward was plus 1 that we get here.", "start": 547.15, "duration": 2.415}, {"text": "And secondly, this is called an absorbing state.", "start": 549.565, "duration": 2.715}, {"text": "Meaning that if you ever get to the plus 1 and minus 1,", "start": 552.28, "duration": 2.625}, {"text": "then the world ends and then there are no more rewards or penalties after that.", "start": 554.905, "duration": 3.555}, {"text": "Right? So but so this is actually not a very good policy,", "start": 558.46, "duration": 3.06}, {"text": "so the policy is any function mapping from the states to the actions.", "start": 561.52, "duration": 3.39}, {"text": "So this is one policy that says, uh,", "start": 564.91, "duration": 2.7}, {"text": "in this state, you know,", "start": 567.61, "duration": 4.23}, {"text": "this policy tells you in this state for one go north,", "start": 571.84, "duration": 2.895}, {"text": "which is actually a pretty bad thing to do, right, is take you to the minus 1 reward.", "start": 574.735, "duration": 3.285}, {"text": "So this is not a great policy,", "start": 578.02, "duration": 1.845}, {"text": "um, but, but this is just a policy.", "start": 579.865, "duration": 3.51}, {"text": "And v_Pi for this policy,", "start": 583.375, "duration": 6.795}, {"text": "um, looks like this.", "start": 590.17, "duration": 5.65}, {"text": "Okay. Um, don't worry too much about the specific numbers.", "start": 609.12, "duration": 4.66}, {"text": "But you've- if you look at this policy,", "start": 613.78, "duration": 2.25}, {"text": "you see that from this set of states it's", "start": 616.03, "duration": 2.76}, {"text": "pretty efficient at getting you to the really bad reward,", "start": 618.79, "duration": 2.925}, {"text": "and from this set of states it's pretty efficient at getting you to the good reward right,", "start": 621.715, "duration": 5.105}, {"text": "with some mixing because of the noise in the robot veering off to the side.", "start": 626.82, "duration": 4.305}, {"text": "And so, you know,", "start": 631.125, "duration": 1.575}, {"text": "these numbers are all negative.", "start": 632.7, "duration": 2.37}, {"text": "And those numbers are at least somewhat positive.", "start": 635.07, "duration": 3.1}, {"text": "Right. So but so v_Pi is just,", "start": 638.17, "duration": 3.645}, {"text": "um, if you start from say this state,", "start": 641.815, "duration": 2.625}, {"text": "from the state 1, 1 on expectation,", "start": 644.44, "duration": 2.79}, {"text": "you're expecting some these counts of rewards will be negative 0.88.", "start": 647.23, "duration": 4.59}, {"text": "Okay? Um, so that's what v_Pi is.", "start": 651.82, "duration": 5.805}, {"text": "Right. Now, um,", "start": 657.625, "duration": 16.065}, {"text": "the following equation.", "start": 673.69, "duration": 2.49}, {"text": "Let me think, uh,", "start": 676.18, "duration": 2.805}, {"text": "governs, um, the value function.", "start": 678.985, "duration": 9.595}, {"text": "It's called, it's called Bellman's equation.", "start": 696.66, "duration": 3.805}, {"text": "Um, and this says", "start": 700.465, "duration": 2.955}, {"text": "that your expected payoff", "start": 703.42, "duration": 5.82}, {"text": "at a given state is the reward that you receive plus the discount factor,", "start": 709.24, "duration": 5.01}, {"text": "times the future reward.", "start": 714.25, "duration": 1.47}, {"text": "So let me, let me actually explain,", "start": 715.72, "duration": 1.92}, {"text": "um, the intuition behind this, right?", "start": 717.64, "duration": 3.03}, {"text": "Which is that, um,", "start": 720.67, "duration": 1.575}, {"text": "let's say you start off at some state s_0, right?", "start": 722.245, "duration": 3.345}, {"text": "So and again, let's,", "start": 725.59, "duration": 1.35}, {"text": "let's say s is equal to s_0.", "start": 726.94, "duration": 2.1}, {"text": "So v_Pi of s is equal to,", "start": 729.04, "duration": 3.285}, {"text": "well, just for your robot waking up in that- I'm going to add to that in a second, okay?", "start": 732.325, "duration": 7.035}, {"text": "But just for the sake,", "start": 739.36, "duration": 1.35}, {"text": "just for this- for the fact that your robot woke up, um,", "start": 740.71, "duration": 4.17}, {"text": "in this state s,", "start": 744.88, "duration": 2.055}, {"text": "you get the immediate- you get a reward R of s_0 right away.", "start": 746.935, "duration": 4.02}, {"text": "This is something that's called- this is also called the immediate reward.", "start": 750.955, "duration": 3.075}, {"text": "[NOISE] Right.", "start": 754.03, "duration": 3.87}, {"text": "Uh, because, you know,", "start": 757.9, "duration": 1.395}, {"text": "just for the, for the, uh,", "start": 759.295, "duration": 1.875}, {"text": "good fortune or bad fortune of starting off in this state,", "start": 761.17, "duration": 3.615}, {"text": "the robot gets a reward right away.", "start": 764.785, "duration": 2.175}, {"text": "This is called the immediate reward.", "start": 766.96, "duration": 1.89}, {"text": "And then it will take some action and get to some new state s_1.", "start": 768.85, "duration": 8.84}, {"text": "Where it will receive, you know,", "start": 777.69, "duration": 1.905}, {"text": "Gamma times the reward of s_1.", "start": 779.595, "duration": 3.015}, {"text": "And then [NOISE]. Right.", "start": 782.61, "duration": 7.63}, {"text": "And then it will get some future reward at the next step and so on.", "start": 790.24, "duration": 3.075}, {"text": "Um, and just to flesh out the definition,", "start": 793.315, "duration": 3.15}, {"text": "the value function v_Pi is really this.", "start": 796.465, "duration": 3.315}, {"text": "Given that you execute the policy Pi and", "start": 799.78, "duration": 3.24}, {"text": "our s_0 equals s, right, and you start off in this state as 0.", "start": 803.02, "duration": 5.16}, {"text": "Now, what I'm going to do is rewrite this part of the equation little bit.", "start": 808.18, "duration": 3.78}, {"text": "I'm going to factor out.", "start": 811.96, "duration": 1.125}, {"text": "I'm just going to take the rest of this and factor out one factor of Gamma.", "start": 813.085, "duration": 5.07}, {"text": "So let me put parentheses around this,", "start": 818.155, "duration": 3.36}, {"text": "right, and just take out Gamma there.", "start": 821.515, "duration": 3.15}, {"text": "Okay. So I'm just, you know, taking this previously this was Gamma squared, right?", "start": 824.665, "duration": 5.955}, {"text": "But adding the parenthesis here,", "start": 830.62, "duration": 1.74}, {"text": "I'm just taking out one factor of Gamma,", "start": 832.36, "duration": 2.01}, {"text": "uh, that multiplies in the rest of that equation,", "start": 834.37, "duration": 3.45}, {"text": "okay? Does that make sense?", "start": 837.82, "duration": 2.13}, {"text": "No. So as Gamma R of s_1 plus gamma squared R of s_2,", "start": 839.95, "duration": 6.705}, {"text": "plus dot, dot, dot equals Gamma times R of s_1 plus.", "start": 846.655, "duration": 5.635}, {"text": "Okay. So that's, that's what I did down there, right,", "start": 853.59, "duration": 2.98}, {"text": "just factor out one, one factor of Gamma.", "start": 856.57, "duration": 2.94}, {"text": "And so, um, this is the,", "start": 859.51, "duration": 4.32}, {"text": "the value of state s is the immediate reward,", "start": 863.83, "duration": 2.73}, {"text": "plus Gamma times the expected future rewards.", "start": 866.56, "duration": 3.795}, {"text": "Right? So this, the expected value of this", "start": 870.355, "duration": 5.07}, {"text": "is really v_Pi of s_1.", "start": 875.425, "duration": 5.865}, {"text": "Right. So this- and,", "start": 881.29, "duration": 5.295}, {"text": "and so the second term here, this,", "start": 886.585, "duration": 2.505}, {"text": "this is the expected future rewards, right?", "start": 889.09, "duration": 4.27}, {"text": "So Bellman's equation says that,", "start": 898.26, "duration": 3.04}, {"text": "um, the value of a state,", "start": 901.3, "duration": 2.67}, {"text": "the value- the expected total payoff you get if", "start": 903.97, "duration": 3.78}, {"text": "your robot wakes up in a state s is the immediate reward plus Gamma,", "start": 907.75, "duration": 5.354}, {"text": "times the expected future rewards.", "start": 913.104, "duration": 3.286}, {"text": "Okay. Right. And, and this thing under,", "start": 916.39, "duration": 3.225}, {"text": "you know, above the curly braces is really, um,", "start": 919.615, "duration": 4.065}, {"text": "uh, asking if your robot wakes up at the state s_1,", "start": 923.68, "duration": 4.29}, {"text": "and executes Pi, what is the expected total payoff, right?", "start": 927.97, "duration": 3.45}, {"text": "And this when your robot wakes up in state s_1 then it'll take an action, gets s_2,", "start": 931.42, "duration": 5.07}, {"text": "take an action, get s_3,", "start": 936.49, "duration": 1.485}, {"text": "and this somewhat discounts the rewards for a bit,  starts off with the state s_1.", "start": 937.975, "duration": 4.935}, {"text": "Okay. Makes sense?", "start": 942.91, "duration": 2.43}, {"text": "So, um, uh,", "start": 945.34, "duration": 4.6}, {"text": "this- based on this,", "start": 950.52, "duration": 2.905}, {"text": "you can write out what- justify Bellman's equation,", "start": 953.425, "duration": 4.41}, {"text": "which is, um, and, excuse me.", "start": 957.835, "duration": 3.795}, {"text": "And the mapping from this equation to this equation.", "start": 961.63, "duration": 2.91}, {"text": "[NOISE].", "start": 964.54, "duration": 18.84}, {"text": "All right. The mapping from the equation on top to the equation at the bottom is that,", "start": 983.38, "duration": 4.725}, {"text": "S maps to S_0 and S prime maps to S_1, right?", "start": 988.105, "duration": 6.825}, {"text": "Um, and, what was I going to say, um, and so if we have that V_Pi of S equals,", "start": 994.93, "duration": 11.35}, {"text": "um, makes sense? [BACKGROUND]. So the value of,", "start": 1022.61, "duration": 5.17}, {"text": "um, state S is, uh,", "start": 1027.78, "duration": 2.43}, {"text": "R of S plus V_Pi of S prime,", "start": 1030.21, "duration": 2.4}, {"text": "where this is really S_0 and this is S_1.", "start": 1032.61, "duration": 5.22}, {"text": "Uh, and and in, in the notation of MDP,", "start": 1037.83, "duration": 2.805}, {"text": "if you want to write a long sequence of states,", "start": 1040.635, "duration": 1.965}, {"text": "we tend to use S_0,", "start": 1042.6, "duration": 1.185}, {"text": "S_1, S_2, S_3, and S_4,", "start": 1043.785, "duration": 1.575}, {"text": "and so on, but if you have, want to look at", "start": 1045.36, "duration": 2.25}, {"text": "just the current state and the state you'd get to after one time step,", "start": 1047.61, "duration": 3.12}, {"text": "we tend to use S and S prime for that.", "start": 1050.73, "duration": 1.95}, {"text": "So that's why there's this mapping between these two pieces of notation.", "start": 1052.68, "duration": 3.76}, {"text": "Uh, so S prime let's say you get to after one step,", "start": 1056.53, "duration": 3.86}, {"text": "well, let's see, what is S prime drawn from, right?", "start": 1060.39, "duration": 4.05}, {"text": "This so- the, the,", "start": 1064.44, "duration": 1.17}, {"text": "the state S prime or S_1 is the state you get to after one time step.", "start": 1065.61, "duration": 4.65}, {"text": "So what is, what is the distribution the S prime is drawn from?", "start": 1070.26, "duration": 3.57}, {"text": "S prime is drawn from P of what?", "start": 1073.83, "duration": 2.65}, {"text": "S.", "start": 1081.68, "duration": 1.27}, {"text": "Okay, P of S, and then?", "start": 1082.95, "duration": 2.74}, {"text": "Pi of S.", "start": 1086.39, "duration": 1.39}, {"text": "Pi of S, pretty cool. Does that make sense?", "start": 1087.78, "duration": 3.615}, {"text": "Because, um, in state S,", "start": 1091.395, "duration": 3.445}, {"text": "you will take action a equals Pi of s, right.", "start": 1095.6, "duration": 7.795}, {"text": "So we're executing the policy Pi.", "start": 1103.395, "duration": 2.955}, {"text": "So that means that when you're in a state S,", "start": 1106.35, "duration": 2.31}, {"text": "you're gonna take the action a given by Pi of S,", "start": 1108.66, "duration": 2.82}, {"text": "because Pi of S tells you,", "start": 1111.48, "duration": 1.56}, {"text": "please take this action a when you're in sate S. And so, um,", "start": 1113.04, "duration": 5.19}, {"text": "S prime is drawn from P of Sa,", "start": 1118.23, "duration": 3.64}, {"text": "where a is equal to Pi of S, right?", "start": 1121.94, "duration": 4.33}, {"text": "Because they- because that's the action you took,", "start": 1126.27, "duration": 1.83}, {"text": "which is why S prime,", "start": 1128.1, "duration": 1.905}, {"text": "the state you get to after one time step,", "start": 1130.005, "duration": 1.965}, {"text": "is drawn from a distribution S Pi of S, okay?", "start": 1131.97, "duration": 4.96}, {"text": "Wow, that pen really left a mark.", "start": 1139.82, "duration": 3.74}, {"text": "So putting all that together, that's why- well,", "start": 1148.13, "duration": 3.37}, {"text": "I just write out again, where Bellman's equation which is, um,", "start": 1151.5, "duration": 3.705}, {"text": "V_Pi of S equals R of S plus", "start": 1155.205, "duration": 3.555}, {"text": "the discount factor times the expected value of V_Pi of S prime.", "start": 1158.76, "duration": 5.835}, {"text": "And so this term here is just sum of", "start": 1164.595, "duration": 3.615}, {"text": "S prime V S Pi of S, V_Pi of S prime.", "start": 1168.21, "duration": 8.415}, {"text": "So that underlying term I guess is this just underline term here, okay?", "start": 1176.625, "duration": 5.58}, {"text": "Um, now, notice that this gives you", "start": 1182.205, "duration": 3.255}, {"text": "a linear system of equations for actually solving for the value function.", "start": 1185.46, "duration": 3.99}, {"text": "Um, so let's say I give you a policy, right?", "start": 1189.45, "duration": 3.51}, {"text": "It could be a good policy, could be a bad policy,", "start": 1192.96, "duration": 2.175}, {"text": "and you want to solve for V_Pi of S. What this, um, does is,", "start": 1195.135, "duration": 6.015}, {"text": "if you think of V_Pi of S as the unknown you're trying to solve for, um, given Pi,", "start": 1201.15, "duration": 10.51}, {"text": "right, these equations [NOISE] ,", "start": 1213.59, "duration": 6.1}, {"text": "um, these", "start": 1219.69, "duration": 9.03}, {"text": "equa- the Bellman's equations", "start": 1228.72, "duration": 1.26}, {"text": "defines a linear system of equations,", "start": 1229.98, "duration": 2.19}, {"text": "uh, in terms of V_Pi of S as the ve- values to be solved for.", "start": 1232.17, "duration": 7.32}, {"text": "So make sure- here's a, here's a specific example.", "start": 1239.49, "duration": 2.49}, {"text": "Um, let's take the state V1,", "start": 1241.98, "duration": 3.61}, {"text": "right, so this is the state V1, okay.", "start": 1246.68, "duration": 4.825}, {"text": "What this- what Bellman's equation this tells us is,", "start": 1251.505, "duration": 3.435}, {"text": "V_Pi of the state 3, 1 is", "start": 1254.94, "duration": 5.1}, {"text": "equal to the immediate reward you get at the state 3,1,", "start": 1260.04, "duration": 6.5}, {"text": "plus the discount factor times,", "start": 1266.54, "duration": 5.08}, {"text": "well, sum of S prime PS Pi of S V_Pi of S prime, right?", "start": 1271.62, "duration": 4.65}, {"text": "So, um, when- let's see- le,", "start": 1276.27, "duration": 3.63}, {"text": "le- let's say that Pi of 3,1 is north, right?", "start": 1279.9, "duration": 4.095}, {"text": "So let's say you try to go north.", "start": 1283.995, "duration": 2.055}, {"text": "If you try to go north from this state,", "start": 1286.05, "duration": 2.13}, {"text": "then you have a 0.8 chance of getting to 3, 2,", "start": 1288.18, "duration": 5.62}, {"text": "plus a 0.1 chance of, uh, veering, uh,", "start": 1293.8, "duration": 6.91}, {"text": "left, plus a 0.1 chance of veering right.", "start": 1300.71, "duration": 7.61}, {"text": "Um, let me just close out that parenthesis, okay.", "start": 1310.64, "duration": 4.735}, {"text": "So that's what Bellman's equation says about these values.", "start": 1315.375, "duration": 6.39}, {"text": "All right, and if your goal is to solve for the value function,", "start": 1321.765, "duration": 7.765}, {"text": "then these things I'm just circling in purple are the unknown variables [NOISE] okay?", "start": 1330.26, "duration": 8.545}, {"text": "And, um, if you have 11 states,", "start": 1338.805, "duration": 2.97}, {"text": "uh, like in our MDP,", "start": 1341.775, "duration": 1.815}, {"text": "then this gives you a system of 11 linear equations with 11 unknowns.", "start": 1343.59, "duration": 5.235}, {"text": "Um, uh, and so using sort of a linear algebra solver,", "start": 1348.825, "duration": 4.65}, {"text": "you could solve explicitly for the value of these 11 unknowns. Does that make sense?", "start": 1353.475, "duration": 4.74}, {"text": "Okay. So the way you would- so let's say I give you a policy Pi,", "start": 1358.215, "duration": 3.24}, {"text": "you know, any policy Pi.", "start": 1361.455, "duration": 1.62}, {"text": "Um, the way you can solve for the value function is,", "start": 1363.075, "duration": 3.885}, {"text": "create an, an 11 dimensional vector, um,", "start": 1366.96, "duration": 4.215}, {"text": "with V_Pi of, you know, 1, 1,", "start": 1371.175, "duration": 4.95}, {"text": "V_Pi of 1, 2 and so on,", "start": 1376.125, "duration": 4.38}, {"text": "down to the V_Pi of whether is the last thing.", "start": 1380.505, "duration": 2.37}, {"text": "You have 11 states, so V_Pi of 3, 3 or whatever, of 4, 3, right?", "start": 1382.875, "duration": 8.28}, {"text": "So if you want to,", "start": 1391.155, "duration": 1.665}, {"text": "er, solve for those, um,", "start": 1392.82, "duration": 1.32}, {"text": "11 numbers I wrote up just, uh,", "start": 1394.14, "duration": 1.785}, {"text": "in terms of defining V_Pi, what you can do is,", "start": 1395.925, "duration": 3.12}, {"text": "I'll give you a policy Pi,", "start": 1399.045, "duration": 1.545}, {"text": "you can then construct an 11 dimensional vector,", "start": 1400.59, "duration": 4.0}, {"text": "you know, 11 dimensional vector of unknown values that you want to solve for.", "start": 1404.63, "duration": 5.515}, {"text": "And Bellman's equations for each of the 11 states,", "start": 1410.145, "duration": 4.185}, {"text": "um, for each of the 11 states you could plug in on the left-hand side.", "start": 1414.33, "duration": 3.15}, {"text": "This gives you one equation for how one of the values is", "start": 1417.48, "duration": 4.56}, {"text": "determined as a linear function of a few other of the values in this vector, okay?", "start": 1422.04, "duration": 5.415}, {"text": "And so, um, what this does is it sets up", "start": 1427.455, "duration": 4.5}, {"text": "a linear system of equations with 11 variables and 11 unknowns, right?", "start": 1431.955, "duration": 4.785}, {"text": "And using a linear algebra solver, you,", "start": 1436.74, "duration": 2.505}, {"text": "you will be able to solve this linear system of equations. Does that make sense?", "start": 1439.245, "duration": 5.28}, {"text": "Okay. Um, all right.", "start": 1444.525, "duration": 5.775}, {"text": "And so this works so long as you have a discrete-", "start": 1450.3, "duration": 2.07}, {"text": "If you have 11 states, you know,", "start": 1452.37, "duration": 1.5}, {"text": "it takes like a, it,", "start": 1453.87, "duration": 1.41}, {"text": "it takes almost a- takes almost no time,", "start": 1455.28, "duration": 2.58}, {"text": "right, in a computer to solve a linear system of 11 equations.", "start": 1457.86, "duration": 2.52}, {"text": "So that's how you would actually get those values,", "start": 1460.38, "duration": 2.235}, {"text": "if you're ever called on to solve for V_Pi, okay?", "start": 1462.615, "duration": 3.615}, {"text": "[NOISE] Actually, the, the- did what I just say make sense?", "start": 1466.23, "duration": 7.56}, {"text": "Raise your hand if what I just explained made sense.", "start": 1473.79, "duration": 2.355}, {"text": "Okay, good, awesome, great.", "start": 1476.145, "duration": 2.065}, {"text": "All right, good.", "start": 1485.21, "duration": 2.14}, {"text": "So moving on our roadmap,", "start": 1487.35, "duration": 2.385}, {"text": "um, we've defined V_Pi,", "start": 1489.735, "duration": 2.144}, {"text": "let's now define V_star.", "start": 1491.879, "duration": 2.956}, {"text": "Um, so [NOISE].", "start": 1494.835, "duration": 17.399}, {"text": "So V star is the optimal value function.", "start": 1512.234, "duration": 10.706}, {"text": "And we'll define it as V star of S", "start": 1523.46, "duration": 4.64}, {"text": "equals max over all policies Pi of V Pi of S. Okay.", "start": 1529.4, "duration": 8.98}, {"text": "Um, one of the I don't know, slightly confusing things about", "start": 1538.38, "duration": 3.3}, {"text": "reinforcement learning terminology is that there are two types of value function.", "start": 1541.68, "duration": 3.615}, {"text": "There's value function for a given policy", "start": 1545.295, "duration": 2.265}, {"text": "Pi and there is the optimal value function V star.", "start": 1547.56, "duration": 3.105}, {"text": "So both of these are called value functions,", "start": 1550.665, "duration": 1.965}, {"text": "but one is a value function for a specific policy,", "start": 1552.63, "duration": 2.925}, {"text": "could be a great policy, could be a terrible policy, can be the optimal policy.", "start": 1555.555, "duration": 2.94}, {"text": "The other is V star which is the optimal- optimal value function.", "start": 1558.495, "duration": 3.72}, {"text": "So V star is defined as,", "start": 1562.215, "duration": 2.76}, {"text": "um, look at the value for, you know,", "start": 1564.975, "duration": 2.25}, {"text": "any- lo- lo- look across all of the possible policies you could have all, um, 4-11.", "start": 1567.225, "duration": 5.19}, {"text": "Over all the combinatorially large number of possible policies for this MDP.", "start": 1572.415, "duration": 4.35}, {"text": "And V star of this is,", "start": 1576.765, "duration": 2.22}, {"text": "well let's just take the max,", "start": 1578.985, "duration": 1.245}, {"text": "where was of all the possible- of all the policies", "start": 1580.23, "duration": 2.67}, {"text": "you know anyone could implement of all the possible policies,", "start": 1582.9, "duration": 3.045}, {"text": "let's take the value of the best possible policy for that state, so that's V star.", "start": 1585.945, "duration": 5.235}, {"text": "Okay. And that's the optimal- optimal,", "start": 1591.18, "duration": 2.25}, {"text": "um, optimal value function.", "start": 1593.43, "duration": 2.295}, {"text": "And it turns out that, um,", "start": 1595.725, "duration": 5.175}, {"text": "there is a different version of Bellman's equations for this.", "start": 1600.9, "duration": 6.28}, {"text": "And again, there's a Bellman's equation for V_Pi,", "start": 1611.0, "duration": 4.285}, {"text": "for value of a policy.", "start": 1615.285, "duration": 2.955}, {"text": "And then there's a different version of", "start": 1618.24, "duration": 1.5}, {"text": "Bellman's equations for the optimal value function, right?", "start": 1619.74, "duration": 2.67}, {"text": "So just as the two versions of value functions,", "start": 1622.41, "duration": 3.78}, {"text": "there are two versions of Bellman's equations.", "start": 1626.19, "duration": 2.715}, {"text": "But let me just write this out and hopefully this will make sense.", "start": 1628.905, "duration": 3.925}, {"text": "Um, actually let's think this through.", "start": 1632.87, "duration": 6.46}, {"text": "So let's say you start off your robot in a state S,", "start": 1639.33, "duration": 3.915}, {"text": "what is the best possible expected sum of discounted rewards?", "start": 1643.245, "duration": 4.355}, {"text": "What's the best possible payoff you could get, right?", "start": 1647.6, "duration": 2.64}, {"text": "Well, ah, just for the privilege of waking up in state S,", "start": 1650.24, "duration": 5.07}, {"text": "the robot will receive an immediate reward R of S, all right?", "start": 1655.31, "duration": 4.12}, {"text": "And then it has to take some action and after taking some action,", "start": 1659.43, "duration": 4.515}, {"text": "it will get to some other state S prime.", "start": 1663.945, "duration": 4.59}, {"text": "Um, you know, and after some other state S prime", "start": 1668.535, "duration": 4.95}, {"text": "it will receive, right, future expected rewards V star of S prime,", "start": 1673.485, "duration": 5.115}, {"text": "and we have to discount that by Gamma, right?", "start": 1678.6, "duration": 2.925}, {"text": "So, sorry. So well,", "start": 1681.525, "duration": 5.595}, {"text": "the state S prime was arrived at but [NOISE] you're taking", "start": 1687.12, "duration": 3.3}, {"text": "some action a from the initial state.", "start": 1690.42, "duration": 3.69}, {"text": "Um, and so whatever the action is you know,", "start": 1694.11, "duration": 6.345}, {"text": "for- if, if you take action a, right?", "start": 1700.455, "duration": 2.665}, {"text": "Okay, um, so if you take an action a in the state S,", "start": 1706.43, "duration": 8.29}, {"text": "then your total payoff will be- expected total payoff will be the immediate reward", "start": 1714.72, "duration": 4.185}, {"text": "plus Gamma times the expected value of the future payoff.", "start": 1718.905, "duration": 5.205}, {"text": "But what is the action a that we should plug it in here?", "start": 1724.11, "duration": 3.3}, {"text": "Right. Well, the optimal action to take in the MDP is", "start": 1727.41, "duration": 4.02}, {"text": "whatever action maximizes your expected total payoff,", "start": 1731.43, "duration": 4.275}, {"text": "maximizes the expected sum of rewards which is why", "start": 1735.705, "duration": 2.94}, {"text": "the action you want to plug in is just whatever action a maximizes that.", "start": 1738.645, "duration": 5.655}, {"text": "Okay. So this is Bellman's equations for the optimal value function,", "start": 1744.3, "duration": 6.555}, {"text": "which says that, ah,", "start": 1750.855, "duration": 1.68}, {"text": "the best possible expected total payoff you could receive", "start": 1752.535, "duration": 3.225}, {"text": "starting from state S is the immediate reward R of S,", "start": 1755.76, "duration": 4.155}, {"text": "plus max over all possible actions of whatever action allows you to maximize,", "start": 1759.915, "duration": 5.535}, {"text": "you know, your expected total payoff- expected future payoff, okay?", "start": 1765.45, "duration": 4.455}, {"text": "So this is the expected future payoff,", "start": 1769.905, "duration": 4.225}, {"text": "or expected future reward, okay.", "start": 1776.09, "duration": 8.83}, {"text": "Um, now based on the argument we just went through,", "start": 1784.92, "duration": 9.18}, {"text": "um, this allows us to figure out how to", "start": 1794.1, "duration": 4.8}, {"text": "compute Pi star of S as well, right?", "start": 1798.9, "duration": 5.55}, {"text": "Which is, um, let's say-", "start": 1804.45, "duration": 3.165}, {"text": "let's say we have a way of computing V star of S, but we don't yet.", "start": 1807.615, "duration": 3.33}, {"text": "But let's say I tell you what is the V star over S,", "start": 1810.945, "duration": 2.835}, {"text": "and then I ask you, you know,", "start": 1813.78, "duration": 1.74}, {"text": "what is the action you should take in a given state?", "start": 1815.52, "duration": 2.7}, {"text": "So remember, Pi, Pi star,", "start": 1818.22, "duration": 2.265}, {"text": "oh Pi star is going to be optimal policy, right?", "start": 1820.485, "duration": 7.785}, {"text": "And so, um, what should Pi star of S be, right?", "start": 1828.27, "duration": 3.29}, {"text": "Which is le- let's say- let's say we're computing V star.", "start": 1831.56, "duration": 2.91}, {"text": "Um, and now I'll see you,", "start": 1834.47, "duration": 2.655}, {"text": "\"Hey, my robot's in state S,", "start": 1837.125, "duration": 1.845}, {"text": "what is the best action I should take from the state S, right?", "start": 1838.97, "duration": 5.245}, {"text": "Then how do I- how do I decide what actions to take in the state S? What, what optimal?", "start": 1844.215, "duration": 6.435}, {"text": "What do you think is the best action to take from the state?", "start": 1850.65, "duration": 3.24}, {"text": "And the answer is almost given in the equation above, yeah.", "start": 1853.89, "duration": 4.47}, {"text": "[inaudible].", "start": 1858.36, "duration": 3.54}, {"text": "Yeah, cool. Awesome, right.", "start": 1861.9, "duration": 1.575}, {"text": "So the best action to take in state S,", "start": 1863.475, "duration": 3.6}, {"text": "and best means of maximizing respect to total payoff.", "start": 1867.075, "duration": 3.51}, {"text": "But the action that maximizes your expected total payoff is, you know,", "start": 1870.585, "duration": 2.985}, {"text": "what- whatever action we were choosing a up here.", "start": 1873.57, "duration": 3.285}, {"text": "And so it's just argmax over a of that.", "start": 1876.855, "duration": 8.305}, {"text": "And because Gamma is just a constant that,", "start": 1887.51, "duration": 3.1}, {"text": "that doesn't affect the argmax,", "start": 1890.61, "duration": 1.44}, {"text": "usually we just eliminate that since it's just a positive number, right?", "start": 1892.05, "duration": 5.41}, {"text": "So this gives us the strategy we will use for finding, um,", "start": 1900.29, "duration": 7.48}, {"text": "the optimal policy for an MDP, which is, um,", "start": 1907.77, "duration": 4.305}, {"text": "we're going to find a way to compute V star of S,", "start": 1912.075, "duration": 3.0}, {"text": "which we don't have a way of doing yet, right?", "start": 1915.075, "duration": 1.77}, {"text": "V star was defined as a max over a combinatorially or exponentially large number policy.", "start": 1916.845, "duration": 5.475}, {"text": "So we don't have a way of computing V star yet.", "start": 1922.32, "duration": 1.92}, {"text": "But if we can find the way to compute V star,", "start": 1924.24, "duration": 2.415}, {"text": "then you know, using this equation,", "start": 1926.655, "duration": 2.595}, {"text": "sorry, let me just scratch this out.", "start": 1929.25, "duration": 1.62}, {"text": "Using this equation gives you a way for every state of every state S,", "start": 1930.87, "duration": 4.86}, {"text": "to pretty efficiently compute this argmax, um,", "start": 1935.73, "duration": 3.51}, {"text": "and therefore figure out what is the optimal action for every state, okay?", "start": 1939.24, "duration": 6.09}, {"text": "[NOISE].", "start": 1945.33, "duration": 22.38}, {"text": "All right, um.", "start": 1967.71, "duration": 1.84}, {"text": "So all right. So just to practice with confusing notation.", "start": 1970.55, "duration": 6.98}, {"text": "All right, let's see if you understand this equation.", "start": 1981.05, "duration": 2.905}, {"text": "I'm, I'm just claiming this. I'm not proving this.", "start": 1983.955, "duration": 2.28}, {"text": "But for every state as V star of S equals V of Pi star of S,", "start": 1986.235, "duration": 7.02}, {"text": "is greater than V Pi of S, all right?", "start": 1993.255, "duration": 3.57}, {"text": "For every policy Pi in every state S, okay?", "start": 1996.825, "duration": 6.105}, {"text": "So ho- hope this equation makes sense.", "start": 2002.93, "duration": 2.505}, {"text": "Ah, this is what I'm claiming. I didn't prove this.", "start": 2005.435, "duration": 2.805}, {"text": "What I'm claiming is that, um,", "start": 2008.24, "duration": 1.92}, {"text": "the optimal value for state S is- this is the optimal value function on the left.", "start": 2010.16, "duration": 6.345}, {"text": "This is the value function for Pi star.", "start": 2016.505, "duration": 9.615}, {"text": "So this is- this is the optimal value function.", "start": 2026.12, "duration": 2.955}, {"text": "This is the value function for a specific policy Pi,", "start": 2029.075, "duration": 2.655}, {"text": "where the policy Pi happens to be Pi star.", "start": 2031.73, "duration": 2.7}, {"text": "And so what I'm claiming here is that- wh- what I'm writing here is that, um,", "start": 2034.43, "duration": 5.01}, {"text": "the optimal value for state S is equal to", "start": 2039.44, "duration": 2.25}, {"text": "the value function 4 Pi star applied to the state S,", "start": 2041.69, "duration": 3.765}, {"text": "and just as greater than equal to V Pi of S for any other policy Pi, okay?", "start": 2045.455, "duration": 4.29}, {"text": "Right. All right. So, um,", "start": 2049.745, "duration": 9.885}, {"text": "the strategy you can use for finding for optimal policy is: one, ah, find V star.", "start": 2059.63, "duration": 10.305}, {"text": "Two, you know, use", "start": 2069.935, "duration": 2.145}, {"text": "the argmax equation to", "start": 2072.08, "duration": 7.56}, {"text": "find Pi star, okay?", "start": 2079.64, "duration": 4.185}, {"text": "And so what we're going to do is- well, step two, right?", "start": 2083.825, "duration": 4.725}, {"text": "We, we know how to do from the argmax equation.", "start": 2088.55, "duration": 1.92}, {"text": "So what we're gonna do is talk about an algorithm for actually", "start": 2090.47, "duration": 2.7}, {"text": "computing V star because if you can compute V star,", "start": 2093.17, "duration": 3.06}, {"text": "then this equation helps- allows you to pretty quickly find the optimal, um,", "start": 2096.23, "duration": 7.215}, {"text": "action for every state [NOISE]. So, um.", "start": 2103.445, "duration": 10.135}, {"text": "So value iteration is,", "start": 2122.53, "duration": 7.66}, {"text": "ah, is an algorithm you can use to,", "start": 2130.19, "duration": 4.305}, {"text": "um, to find V star.", "start": 2134.495, "duration": 2.04}, {"text": "So let me just write out the algorithm, um.", "start": 2136.535, "duration": 3.855}, {"text": "So this is um-", "start": 2149.26, "duration": 9.56}, {"text": "Okay? So in the value iteration algorithm,", "start": 2174.7, "duration": 4.899}, {"text": "you initialize the estimated value of every state to 0,", "start": 2179.599, "duration": 4.741}, {"text": "and then you update these estimated values using Bellman's equation.", "start": 2184.34, "duration": 5.19}, {"text": "And this is the, uh, optimal value function,", "start": 2189.53, "duration": 2.55}, {"text": "the V star version of Bellman's equations, right?", "start": 2192.08, "duration": 3.195}, {"text": "And, um,", "start": 2195.275, "duration": 11.775}, {"text": "[NOISE] so to be concrete about how you implement this,", "start": 2207.05, "duration": 2.505}, {"text": "you know, if you're implementing this, right?", "start": 2209.555, "duration": 2.025}, {"text": "If you are implementing this in Python, um,", "start": 2211.58, "duration": 2.34}, {"text": "what you would do is create", "start": 2213.92, "duration": 1.38}, {"text": "a 11 dimensional vector to store all the values of V of S. So you create a,", "start": 2215.3, "duration": 4.65}, {"text": "you know, 11 dimensional vector, right?", "start": 2219.95, "duration": 2.46}, {"text": "That, that represent V of 1, 1,", "start": 2222.41, "duration": 2.97}, {"text": "V of 1, 2, you know,", "start": 2225.38, "duration": 3.135}, {"text": "down to V of 4, 3, right?", "start": 2228.515, "duration": 3.315}, {"text": "So this is, um, 11 dimensional vector corresponding to the 11 states.", "start": 2231.83, "duration": 5.745}, {"text": "Um, [NOISE] oh, I'm sorry I shou - wait did I say 11?", "start": 2237.575, "duration": 3.6}, {"text": "We got 10 states in the MDP, don't we? Wait.", "start": 2241.175, "duration": 2.475}, {"text": "Yes, we have 10 states. We've been saying 11 all long?", "start": 2243.65, "duration": 2.805}, {"text": "Sorry. Okay, 10.", "start": 2246.455, "duration": 1.425}, {"text": "Um, uh, yeah, uh, wait.", "start": 2247.88, "duration": 8.33}, {"text": "[inaudible].", "start": 2256.21, "duration": 0.1}, {"text": "11?", "start": 2256.31, "duration": 0.41}, {"text": "[inaudible].", "start": 2256.72, "duration": 3.21}, {"text": "Oh, Yes. You're right. Sorry. Yes, 11.", "start": 2259.93, "duration": 1.675}, {"text": "Okay. Sorry. Yes, 11 states. Okay, It's all right.", "start": 2261.605, "duration": 3.765}, {"text": "Right. So 11 states MDP so you create an initial, ah,", "start": 2265.37, "duration": 3.48}, {"text": "create an 11 dimensional vector um,", "start": 2268.85, "duration": 3.06}, {"text": "and initialize all of these values to 0.", "start": 2271.91, "duration": 3.06}, {"text": "And then you will repeatedly update, um,", "start": 2274.97, "duration": 4.71}, {"text": "the estimated value of every state according to Bellman's equations, right?", "start": 2279.68, "duration": 7.38}, {"text": "Um, and so uh, there, there,", "start": 2287.06, "duration": 3.195}, {"text": "there are actually two ways to interpret this um,", "start": 2290.255, "duration": 2.76}, {"text": "and sim- similar to,", "start": 2293.015, "duration": 1.485}, {"text": "er, similar to gradient descent, right?", "start": 2294.5, "duration": 1.68}, {"text": "We've written out, you know,", "start": 2296.18, "duration": 1.425}, {"text": "a gradient descent rule for updating the Theta,", "start": 2297.605, "duration": 3.135}, {"text": "uh, the, the, vector parameters Theta.", "start": 2300.74, "duration": 3.015}, {"text": "And what you do is, you know, then you have,", "start": 2303.755, "duration": 3.0}, {"text": "um- and what you do is you update all of the components of Theta simultaneously, right?", "start": 2306.755, "duration": 5.55}, {"text": "And so that's called a synchronous update, er, in gradient descent.", "start": 2312.305, "duration": 3.3}, {"text": "So one way to- so the way you would, um, er,", "start": 2315.605, "duration": 4.245}, {"text": "update this equation in what's called a synchronous update,", "start": 2319.85, "duration": 5.02}, {"text": "would be if you compute the right hand side for", "start": 2327.55, "duration": 3.73}, {"text": "all 11 states and then you simultaneously overwrite all 11 values at the same time.", "start": 2331.28, "duration": 6.35}, {"text": "And then you compute all 11 values for", "start": 2337.63, "duration": 2.43}, {"text": "the right-hand side and then you simultaneously update all 11 values, okay?", "start": 2340.06, "duration": 3.87}, {"text": "Um, the alternative would be an asynchronous update.", "start": 2343.93, "duration": 5.18}, {"text": "And an asynchronous update,", "start": 2351.55, "duration": 2.47}, {"text": "what you do is you compute v of 1, 1, right?", "start": 2354.02, "duration": 3.12}, {"text": "And the value of v of 1, 1 depends on some of the,", "start": 2357.14, "duration": 2.925}, {"text": "the other values on the right hand side, right?", "start": 2360.065, "duration": 1.965}, {"text": "But the asynchronous update,", "start": 2362.03, "duration": 1.32}, {"text": "you compute v of 1, 1 and then you overwrite this value first.", "start": 2363.35, "duration": 4.335}, {"text": "And then you use that equation to compute v of 1, 2.", "start": 2367.685, "duration": 3.3}, {"text": "And then you update this and then you observe update these one at a time.", "start": 2370.985, "duration": 4.23}, {"text": "And the difference between synchronous and asynchronous is um, you know,", "start": 2375.215, "duration": 4.2}, {"text": "if you're using asynchronous update by the time you're using V", "start": 2379.415, "duration": 3.045}, {"text": "of 4, 3 which depends on some of the earlier values,", "start": 2382.46, "duration": 3.03}, {"text": "you'd be using a new and refreshed value of some of the earlier values on your list, okay?", "start": 2385.49, "duration": 5.58}, {"text": "Um, it turns out that", "start": 2391.07, "duration": 2.52}, {"text": "value iteration works fine with either synchronous update or asynchronous updates.", "start": 2393.59, "duration": 4.785}, {"text": "But, um, for the,", "start": 2398.375, "duration": 2.58}, {"text": "er, er, but, um,", "start": 2400.955, "duration": 1.335}, {"text": "er, because it vectorizes better,", "start": 2402.29, "duration": 2.4}, {"text": "because you can use more efficient matrix operations.", "start": 2404.69, "duration": 2.25}, {"text": "Most people use asynchronous update but it turns out that the algorithm will", "start": 2406.94, "duration": 3.3}, {"text": "work whether using a synchronous or an asynchronous update.", "start": 2410.24, "duration": 3.69}, {"text": "So I, I, I, I guess unless,", "start": 2413.93, "duration": 1.155}, {"text": "unless otherwise uh, uh,", "start": 2415.085, "duration": 2.025}, {"text": "you know, stated you should usually assume that.", "start": 2417.11, "duration": 2.535}, {"text": "Whe- when I talk about, uh, value iteration,", "start": 2419.645, "duration": 2.595}, {"text": "I'm referring to asynchronous update where you compute all the values,", "start": 2422.24, "duration": 3.0}, {"text": "all 11 values using the- a- an- and then update all 11 values at the same time, okay?", "start": 2425.24, "duration": 6.0}, {"text": "Was there a question just now, someone had, yeah.", "start": 2431.24, "duration": 1.47}, {"text": "[inaudible]", "start": 2432.71, "duration": 0.16}, {"text": "Yeah,", "start": 2432.87, "duration": 1.15}, {"text": "yes.", "start": 2453.42, "duration": 1.3}, {"text": "So I think there,", "start": 2454.72, "duration": 1.035}, {"text": "there, uh, uh, yes.", "start": 2455.755, "duration": 1.545}, {"text": "So how do you represent the absorbing state?", "start": 2457.3, "duration": 1.83}, {"text": "The sync state? We get to plus 1 minus 1 then the world ends.", "start": 2459.13, "duration": 2.565}, {"text": "Um, in this framework one way to code that up would be to say that um,", "start": 2461.695, "duration": 4.89}, {"text": "the state transition parameters from that to any other state is 0.", "start": 2466.585, "duration": 2.655}, {"text": "That is one way to, to, to- that, that will work.", "start": 2469.24, "duration": 2.43}, {"text": "Uh, another way would be, um,", "start": 2471.67, "duration": 2.21}, {"text": "less- done less often", "start": 2473.88, "duration": 1.535}, {"text": "maybe mathematically a bit cleaner but not how people tend to do this,", "start": 2475.415, "duration": 3.03}, {"text": "would be to take your, um,", "start": 2478.445, "duration": 3.015}, {"text": "11 state MDP and then create a 12 state,", "start": 2481.46, "duration": 3.045}, {"text": "and a 12 state always goes back to itself with no further rewards.", "start": 2484.505, "duration": 3.255}, {"text": "So both, both of these will give you the same result.", "start": 2487.76, "duration": 2.58}, {"text": "Mathematically, it's pretty more convenient to just set, you know,", "start": 2490.34, "duration": 3.12}, {"text": "P of Sa S prime equals 0 for all other states.", "start": 2493.46, "duration": 4.065}, {"text": "It's not [inaudible] probably but that,", "start": 2497.525, "duration": 2.01}, {"text": "that will give you the right answer as well.", "start": 2499.535, "duration": 1.575}, {"text": "Yeah. All right.", "start": 2501.11, "duration": 4.95}, {"text": "Cool. Um, so just as a point of notation,", "start": 2506.06, "duration": 5.28}, {"text": "if you're using synchronous updates,", "start": 2511.34, "duration": 2.325}, {"text": "you can think of this as, um,", "start": 2513.665, "duration": 2.61}, {"text": "taking the old value function,", "start": 2516.275, "duration": 2.655}, {"text": "er, O estimate, right?", "start": 2518.93, "duration": 5.895}, {"text": "And using it to compute the new estimate, right?", "start": 2524.825, "duration": 5.715}, {"text": "So this, this, you know,", "start": 2530.54, "duration": 1.365}, {"text": "assuming the synchronous update,", "start": 2531.905, "duration": 1.605}, {"text": "you have some, uh,", "start": 2533.51, "duration": 1.545}, {"text": "previous 11 dimensional vector with", "start": 2535.055, "duration": 2.865}, {"text": "your estimates of the value from the previous iteration.", "start": 2537.92, "duration": 3.18}, {"text": "And after doing one iteration of this,", "start": 2541.1, "duration": 2.31}, {"text": "you have a new set of estimates.", "start": 2543.41, "duration": 1.83}, {"text": "So one step of this algorithm is sometimes called the Bellman backup operator.", "start": 2545.24, "duration": 5.35}, {"text": "And so where you update V equals B of V, right?", "start": 2552.43, "duration": 7.06}, {"text": "Where, uh, where now V is,", "start": 2559.49, "duration": 1.62}, {"text": "a 11 dimensional vector.", "start": 2561.11, "duration": 1.365}, {"text": "So you have an order 11 dimensional vector,", "start": 2562.475, "duration": 2.295}, {"text": "compute the Bellman backup operator with", "start": 2564.77, "duration": 1.98}, {"text": "just that equation there and update V according to V of P. Um,", "start": 2566.75, "duration": 4.485}, {"text": "and so one thing that you see in the,", "start": 2571.235, "duration": 4.875}, {"text": "um, problem set, uh,", "start": 2576.11, "duration": 1.455}, {"text": "is prove- is, er, er, showing that,", "start": 2577.565, "duration": 4.605}, {"text": "um, this will make a V of S converge to V star, okay?", "start": 2582.17, "duration": 4.83}, {"text": "So it turns out that, um,", "start": 2587.0, "duration": 4.18}, {"text": "okay, so it turns out that, um,", "start": 2606.52, "duration": 3.97}, {"text": "er, you can prove and you'll see more details of this in the problem set,", "start": 2610.49, "duration": 4.035}, {"text": "that by repeatedly and forcing Bellman's,", "start": 2614.525, "duration": 3.315}, {"text": "er, equations, that this equa- this,", "start": 2617.84, "duration": 2.955}, {"text": "this algorithm will cause your vector of 11 values or cause", "start": 2620.795, "duration": 3.285}, {"text": "V to converge to your optimal value function of V star, okay?", "start": 2624.08, "duration": 4.77}, {"text": "Um, and more details. You- you'll see", "start": 2628.85, "duration": 1.62}, {"text": "in the homework and a little bit in the lecture notes.", "start": 2630.47, "duration": 1.575}, {"text": "And it turns out this algorithm actually converges quite quickly, right?", "start": 2632.045, "duration": 3.645}, {"text": "Um, to, to, to give you a flavor,", "start": 2635.69, "duration": 2.445}, {"text": "I think that, uh, with the discount factor,", "start": 2638.135, "duration": 2.385}, {"text": "the discount factor is 0.99,", "start": 2640.52, "duration": 1.965}, {"text": "it turns out that you can show that the error, er,", "start": 2642.485, "duration": 3.015}, {"text": "reduces, you know, by a factor of 0.99 on every iteration, um,", "start": 2645.5, "duration": 4.035}, {"text": "and so V actually converges quite,", "start": 2649.535, "duration": 2.475}, {"text": "quickly geometrically quickly or exponentially quickly,", "start": 2652.01, "duration": 2.925}, {"text": "um, to the optimal value function, V star.", "start": 2654.935, "duration": 2.805}, {"text": "And so if it's, you know, if the discount factor is 0.99, then we've like a few,", "start": 2657.74, "duration": 3.6}, {"text": "we've 100 iterations or a few hundred iterations,", "start": 2661.34, "duration": 2.655}, {"text": "V would be very close to V star, okay?", "start": 2663.995, "duration": 3.42}, {"text": "And, and the discount factor is 0.9, then we've just,", "start": 2667.415, "duration": 2.7}, {"text": "you know, 10 or a few dozens of iterations that'll be very close to V star.", "start": 2670.115, "duration": 3.585}, {"text": "So these algorithm actually converges quite quickly to V star, okay?", "start": 2673.7, "duration": 5.46}, {"text": "Um, so let's see.", "start": 2679.16, "duration": 5.31}, {"text": "[NOISE].", "start": 2684.47, "duration": 30.81}, {"text": "All right. So just to put everything together,", "start": 2715.28, "duration": 3.345}, {"text": "um, if you- if", "start": 2718.625, "duration": 7.595}, {"text": "you run value iteration on that MDP,", "start": 2726.22, "duration": 4.299}, {"text": "you end up with this. Um, er,", "start": 2737.7, "duration": 4.16}, {"text": "so this is V star, okay?", "start": 2760.57, "duration": 3.445}, {"text": "So it's a list of 11 numbers telling you what is the optimal, um,", "start": 2764.015, "duration": 4.44}, {"text": "expected pay off for starting off in each of the 11 possible states.", "start": 2768.455, "duration": 5.805}, {"text": "And so, um, I had previously said,", "start": 2774.26, "duration": 3.72}, {"text": "I think I said last week,", "start": 2777.98, "duration": 1.785}, {"text": "uh, o- of the week before Thanksgiving,", "start": 2779.765, "duration": 2.715}, {"text": "that this is the optimal policy, right?", "start": 2782.48, "duration": 9.585}, {"text": "So, you know, let's just use as a case study how", "start": 2792.065, "duration": 3.735}, {"text": "you compute the optimal action for that state,", "start": 2795.8, "duration": 3.525}, {"text": "um, given this V star, all right?", "start": 2799.325, "duration": 4.335}, {"text": "Well, what you do is you,", "start": 2803.66, "duration": 1.185}, {"text": "you actually just use this equation.", "start": 2804.845, "duration": 2.175}, {"text": "And so, um, if you were to go west,", "start": 2807.02, "duration": 4.26}, {"text": "then if you were to compute,", "start": 2811.28, "duration": 2.384}, {"text": "I guess this term, um,", "start": 2813.664, "duration": 3.616}, {"text": "sum of S prime west or left I guess, right?", "start": 2817.28, "duration": 4.68}, {"text": "P of S A, S prime V star of S prime is equal to,", "start": 2821.96, "duration": 5.61}, {"text": "um, if you were to go west, you have a, um-", "start": 2827.57, "duration": 12.52}, {"text": "Right.", "start": 2840.66, "duration": 5.23}, {"text": "Um, right.", "start": 2845.89, "duration": 4.56}, {"text": "So if you're in this state,", "start": 2850.45, "duration": 2.07}, {"text": "and if you attempt to go left,", "start": 2852.52, "duration": 2.025}, {"text": "then there's a 0.8 chance you end up there with,", "start": 2854.545, "duration": 4.485}, {"text": "ah, ah, V star of 0.75.", "start": 2859.03, "duration": 3.705}, {"text": "There's a 0.1 chance.", "start": 2862.735, "duration": 2.22}, {"text": "You know if you try to go left,", "start": 2864.955, "duration": 2.505}, {"text": "there's 0.1 chance you veer off to the north and have a 0.069.", "start": 2867.46, "duration": 4.23}, {"text": "And then there's 0.1 chance that you actually go", "start": 2871.69, "duration": 3.42}, {"text": "south and bounce off the wall and end up with a 0.71.", "start": 2875.11, "duration": 4.095}, {"text": "And so the expected future reward,", "start": 2879.205, "duration": 2.955}, {"text": "the expected future payoff given this equation is that if you tend to go west,", "start": 2882.16, "duration": 3.84}, {"text": "you end up with a 0.740 as expected future rewards.", "start": 2886.0, "duration": 4.95}, {"text": "Whereas if you were to go north,", "start": 2890.95, "duration": 2.16}, {"text": "and we do a similar computation.", "start": 2893.11, "duration": 2.28}, {"text": "[NOISE] You know, so 0.8 times 0.69,", "start": 2895.39, "duration": 6.389}, {"text": "plus 0.1 times 0.75,", "start": 2901.779, "duration": 1.471}, {"text": "plus 0.1 times 0.49,", "start": 2903.25, "duration": 1.485}, {"text": "is the appropriate weighted average.", "start": 2904.735, "duration": 1.605}, {"text": "You find that this is equal to 0.676.", "start": 2906.34, "duration": 3.76}, {"text": "Um, which is why the expected future rewards for if you go west, if you go no- ah,", "start": 2910.19, "duration": 5.57}, {"text": "left is 0.740 which is quite a bit higher than if you go north,", "start": 2915.76, "duration": 5.355}, {"text": "which is why we can conclude based on this little calculation,", "start": 2921.115, "duration": 3.835}, {"text": "um, that the optimal policy is to go left by that state, okay?", "start": 2924.95, "duration": 4.01}, {"text": "And- and really, and technically you check north,", "start": 2928.96, "duration": 2.34}, {"text": "south, east, and west and make sure that going west gives a high reward.", "start": 2931.3, "duration": 2.94}, {"text": "And that's how you can conclude that going west is actually the better action,", "start": 2934.24, "duration": 3.48}, {"text": "at this state, okay?", "start": 2937.72, "duration": 3.57}, {"text": "So that's the value iteration.", "start": 2941.29, "duration": 2.34}, {"text": "And based on this, if you,", "start": 2943.63, "duration": 3.03}, {"text": "um, ah, are given an MDP you can implement this,", "start": 2946.66, "duration": 3.99}, {"text": "ah, south of V star and, ah,", "start": 2950.65, "duration": 3.37}, {"text": "ah, be able to, ah,", "start": 2954.02, "duration": 2.015}, {"text": "compute Pi star, okay?", "start": 2956.035, "duration": 2.82}, {"text": "All right. Few more things to go over.", "start": 2958.855, "duration": 3.705}, {"text": "But before I move on, ah,", "start": 2962.56, "duration": 1.32}, {"text": "let me check if there any questions, yeah.", "start": 2963.88, "duration": 3.78}, {"text": "[inaudible]", "start": 2967.66, "duration": 4.68}, {"text": "Oh, sure yep. Is the number of states always finite?", "start": 2972.34, "duration": 2.34}, {"text": "So in what we're discussing so far, yes.", "start": 2974.68, "duration": 2.895}, {"text": "But what we'll see on Wednesday is how to generalize this framework.", "start": 2977.575, "duration": 4.32}, {"text": "I'll, I'll do this a little bit later but it", "start": 2981.895, "duration": 2.655}, {"text": "turns out if you have a continuous state MDP, ah,", "start": 2984.55, "duration": 2.82}, {"text": "one of the things that's often", "start": 2987.37, "duration": 3.33}, {"text": "done I guess is to discretize into finite number of states.", "start": 2990.7, "duration": 3.36}, {"text": "Ah, but then there are also some other versions of, um,", "start": 2994.06, "duration": 3.95}, {"text": "ah, you know, value iteration that applies directly to continuous states as well.", "start": 2998.01, "duration": 5.2}, {"text": "Okay, cool. All right.", "start": 3003.21, "duration": 6.675}, {"text": "So [NOISE].", "start": 3009.885, "duration": 8.115}, {"text": "Um, what I describe is an algorithm called value iteration.", "start": 3018.0, "duration": 4.065}, {"text": "The other, um, I know, common, ah,", "start": 3022.065, "duration": 3.09}, {"text": "sort of textbook algorithm for solving for MDP is,", "start": 3025.155, "duration": 3.39}, {"text": "is called policy iteration.", "start": 3028.545, "duration": 2.785}, {"text": "And let me just- I'll just write out what the algorithm is.", "start": 3034.82, "duration": 5.24}, {"text": "So here's the algorithm which is, um,", "start": 3041.21, "duration": 3.13}, {"text": "you know initialize Pi randomly, right?", "start": 3044.34, "duration": 4.44}, {"text": "[NOISE].", "start": 3048.78, "duration": 54.69}, {"text": "Okay, so let's see what this algorithm does.", "start": 3103.47, "duration": 2.67}, {"text": "So we'll talk of pros and cons of valuation versus policy iteration in a little bit.", "start": 3106.14, "duration": 3.51}, {"text": "Um, in policy iteration, ah,", "start": 3109.65, "duration": 2.955}, {"text": "instead of solving for the optimal policy V star,", "start": 3112.605, "duration": 4.29}, {"text": "so in- in value iteration our focus of attention was V star, right?", "start": 3116.895, "duration": 3.555}, {"text": "Where, um, you know,", "start": 3120.45, "duration": 1.38}, {"text": "you do a lot of work to try to find the value function.", "start": 3121.83, "duration": 2.385}, {"text": "And then once you solve for V star,", "start": 3124.215, "duration": 1.545}, {"text": "you then figure out the best policy.", "start": 3125.76, "duration": 2.415}, {"text": "In policy iteration, the focus of attention is", "start": 3128.175, "duration": 2.835}, {"text": "on the policy Pi rather than the value function.", "start": 3131.01, "duration": 3.12}, {"text": "And so initialize Pi randomly.", "start": 3134.13, "duration": 2.61}, {"text": "So that means for- for each of the 11 states pick a random action, right?", "start": 3136.74, "duration": 3.51}, {"text": "So a random initial Pi.", "start": 3140.25, "duration": 1.575}, {"text": "And then we're going to repeatedly carry out these two steps.", "start": 3141.825, "duration": 3.285}, {"text": "Um, the first step is, um,", "start": 3145.11, "duration": 2.85}, {"text": "solve for the value function for the policy Pi, right?", "start": 3147.96, "duration": 3.945}, {"text": "And remember, um, for V Pi,", "start": 3151.905, "duration": 3.585}, {"text": "this was a linear system of equations, right?", "start": 3155.49, "duration": 8.625}, {"text": "With 11 variables, with 11 unknowns", "start": 3164.115, "duration": 3.135}, {"text": "in a linear- there is a linear system of 11 equations with 11 unknowns.", "start": 3167.25, "duration": 3.27}, {"text": "And so using a sort of linear algebra solver or linear equation solver,", "start": 3170.52, "duration": 4.725}, {"text": "given a fixed policy Pi,", "start": 3175.245, "duration": 1.83}, {"text": "you could just, you know,", "start": 3177.075, "duration": 1.065}, {"text": "at the cost of inverting a matrix roughly, right?", "start": 3178.14, "duration": 2.97}, {"text": "You can solve for- you can solve for all of these 11 values.", "start": 3181.11, "duration": 3.705}, {"text": "And so in policy iteration,", "start": 3184.815, "duration": 2.25}, {"text": "um, you would, you know,", "start": 3187.065, "duration": 2.595}, {"text": "use a linear solver to solve for", "start": 3189.66, "duration": 2.52}, {"text": "the optimal value function for this policy Pi that we just randomly initialized.", "start": 3192.18, "duration": 4.335}, {"text": "And then set V to be the value function for that policy.", "start": 3196.515, "duration": 5.355}, {"text": "Okay, um, and so this is done quite efficiently with the linear solver.", "start": 3201.87, "duration": 6.48}, {"text": "And then the second step of policy iteration", "start": 3208.35, "duration": 3.135}, {"text": "is pretend that V is the optimal value function,", "start": 3211.485, "duration": 4.665}, {"text": "and update Pi of S,", "start": 3216.15, "duration": 3.45}, {"text": "you know, using the Bellman's equations for the optimal value function,", "start": 3219.6, "duration": 6.375}, {"text": "right, or updated, um,", "start": 3225.975, "duration": 1.845}, {"text": "as you saw right how you update Pi of S. And then you iterate,", "start": 3227.82, "duration": 4.755}, {"text": "and then give it a new policy,", "start": 3232.575, "duration": 1.815}, {"text": "you then solve that linear system equations for your new policy Pi.", "start": 3234.39, "duration": 3.9}, {"text": "So you get a new V_Pi and you keep on iterating these two steps,", "start": 3238.29, "duration": 3.93}, {"text": "um, until convergence, okay? Yeah.", "start": 3242.22, "duration": 2.94}, {"text": "[inaudible]", "start": 3245.16, "duration": 15.99}, {"text": "Yeah, yep. Yes, that's right.", "start": 3261.15, "duration": 1.095}, {"text": "So in, in, in value,", "start": 3262.245, "duration": 2.085}, {"text": "ah, yeah, yeah, yeah, yeah.", "start": 3264.33, "duration": 3.495}, {"text": "So in, in value iteration, um, ah,", "start": 3267.825, "duration": 2.865}, {"text": "actu- in value iteration think about", "start": 3270.69, "duration": 3.18}, {"text": "value iterations as waiting to the end to compute Pi of S, right?", "start": 3273.87, "duration": 2.82}, {"text": "Solve for v star first, and then compute Pi of S. Whereas in policy iteration,", "start": 3276.69, "duration": 3.825}, {"text": "we're coming up with a new policy on every single iteration, right?", "start": 3280.515, "duration": 3.795}, {"text": "Okay? So, um, pros and cons of poly- and,", "start": 3284.31, "duration": 5.505}, {"text": "and it turns out that this algorithm will also converge to the optimal policy.", "start": 3289.815, "duration": 4.365}, {"text": "Um, pros and cons of policy iteration versus value iteration.", "start": 3294.18, "duration": 3.885}, {"text": "Policy iteration requires solving this linear system of", "start": 3298.065, "duration": 3.585}, {"text": "equations in order to, um, get V_Pi.", "start": 3301.65, "duration": 3.585}, {"text": "And so it turns out that if you have a relatively small state space,", "start": 3305.235, "duration": 4.665}, {"text": "um, like if you have 11 states,", "start": 3309.9, "duration": 2.04}, {"text": "it's really easy to solve a linear system of equations,", "start": 3311.94, "duration": 4.05}, {"text": "ah, you know, of 11 equations in order to get V_Pi.", "start": 3315.99, "duration": 3.03}, {"text": "And so in a relatively small set of states like 11 states or really anything, you know,", "start": 3319.02, "duration": 4.575}, {"text": "like a few hundred states, um,", "start": 3323.595, "duration": 2.01}, {"text": "policy iteration would work quite quickly.", "start": 3325.605, "duration": 2.685}, {"text": "Ah, but if you have a [NOISE] relatively large set of states,", "start": 3328.29, "duration": 3.885}, {"text": "you know, like 10,000 states or,", "start": 3332.175, "duration": 1.605}, {"text": "or, or a million states.", "start": 3333.78, "duration": 1.785}, {"text": "Um, then this step would be much slower.", "start": 3335.565, "duration": 3.405}, {"text": "At least if you do it right by solving linear system of equations and then", "start": 3338.97, "duration": 3.27}, {"text": "I would favor a value iteration over policy iterations.", "start": 3342.24, "duration": 3.345}, {"text": "So for larger problems,", "start": 3345.585, "duration": 1.575}, {"text": "usually value iteration will, um, ah, ah,", "start": 3347.16, "duration": 3.84}, {"text": "usually I would use value iteration because solving this linear system of equations,", "start": 3351.0, "duration": 5.055}, {"text": "you know, is, is pretty expensive if it's- it's like a million Pi.", "start": 3356.055, "duration": 3.81}, {"text": "Is a million equations and a million unknowns, that's quite expensive.", "start": 3359.865, "duration": 3.285}, {"text": "But even 11 states 11 unknowns is a very small system of equations.", "start": 3363.15, "duration": 3.54}, {"text": "Um, and then one,", "start": 3366.69, "duration": 1.695}, {"text": "one other pros and cons,", "start": 3368.385, "duration": 1.335}, {"text": "one of the, ah,", "start": 3369.72, "duration": 1.02}, {"text": "ah, differences that- that's maybe,", "start": 3370.74, "duration": 2.405}, {"text": "maybe more academic and practical.", "start": 3373.145, "duration": 1.905}, {"text": "But it turns out that if you use value iteration, um,", "start": 3375.05, "duration": 3.93}, {"text": "V will converge towards V star,", "start": 3378.98, "duration": 3.385}, {"text": "but it won't ever get to exactly V star, right?", "start": 3382.365, "duration": 3.48}, {"text": "So just as, if you apply gradient descent for linear regression,", "start": 3385.845, "duration": 4.245}, {"text": "gradient descent gets closer and closer and closer to the global optimum,", "start": 3390.09, "duration": 4.29}, {"text": "but it never, you know,", "start": 3394.38, "duration": 1.335}, {"text": "gets exactly the global optimum.", "start": 3395.715, "duration": 1.83}, {"text": "It just gets really, really close, really, really fast.", "start": 3397.545, "duration": 2.085}, {"text": "Actually gradient descent, actually turns out asymptotically", "start": 3399.63, "duration": 2.385}, {"text": "converges geometrically quickly or exponentially quickly, right?", "start": 3402.015, "duration": 2.775}, {"text": "But they've been never quite gets, you know,", "start": 3404.79, "duration": 1.815}, {"text": "definitively to the optimal,", "start": 3406.605, "duration": 1.905}, {"text": "to the one optimal value.", "start": 3408.51, "duration": 1.38}, {"text": "Whereas, you, you saw using normal equations it just", "start": 3409.89, "duration": 2.55}, {"text": "jumped straight to the optimal value and there's no,", "start": 3412.44, "duration": 2.43}, {"text": "you know, converging slowly.", "start": 3414.87, "duration": 1.725}, {"text": "And so value iteration converges to a V star,", "start": 3416.595, "duration": 3.345}, {"text": "but it doesn't ever end up at exactly the value of V star.", "start": 3419.94, "duration": 3.39}, {"text": "Ah, this difference may be a bit academic because in practice it,", "start": 3423.33, "duration": 3.405}, {"text": "it doesn't have, ah, right?", "start": 3426.735, "duration": 1.395}, {"text": "Ah, ah, but in policy iteration, um,", "start": 3428.13, "duration": 3.945}, {"text": "if you iterate this algorithm then after a finite number of iterations, ah,", "start": 3432.075, "duration": 5.235}, {"text": "this algorithm will stop changing meaning that after a", "start": 3437.31, "duration": 3.705}, {"text": "certain number of iterations Pi of S will just not change anymore, right?", "start": 3441.015, "duration": 4.83}, {"text": "So you find Pi of S update the value function,", "start": 3445.845, "duration": 3.105}, {"text": "and then after another integration.", "start": 3448.95, "duration": 1.53}, {"text": "When you take these argmax's,", "start": 3450.48, "duration": 1.319}, {"text": "you end up with exactly the same policy.", "start": 3451.799, "duration": 2.206}, {"text": "And so, ah, just- just to solve for", "start": 3454.005, "duration": 1.905}, {"text": "the optimal value and the optimal policy, and then just,", "start": 3455.91, "duration": 2.37}, {"text": "you know, ah, ah,", "start": 3458.28, "duration": 2.0}, {"text": "it doesn't converge- it doesn't just converge to what the optimal value.", "start": 3460.28, "duration": 3.33}, {"text": "It just gets the optimal value when it- when it converges, okay?", "start": 3463.61, "duration": 5.015}, {"text": "Um, so I think in practice I actually see value iteration used much more,", "start": 3468.625, "duration": 5.495}, {"text": "ah, ah, ah, because, um,", "start": 3474.12, "duration": 3.05}, {"text": "solving these linear system equations gets expensive, you know,", "start": 3477.17, "duration": 2.875}, {"text": "if you have a larger state space but, um,", "start": 3480.045, "duration": 3.0}, {"text": "value iteration, excuse me,", "start": 3483.045, "duration": 1.395}, {"text": "val- I see value iteration used much more.", "start": 3484.44, "duration": 2.235}, {"text": "But if you have a small problem, you know,", "start": 3486.675, "duration": 2.055}, {"text": "I think you could also use policy iteration which may converge a little bit faster.", "start": 3488.73, "duration": 4.02}, {"text": "If, if you have a small problem, okay?", "start": 3492.75, "duration": 2.49}, {"text": "[NOISE] All right, good.", "start": 3495.24, "duration": 8.265}, {"text": "So the last thing is,", "start": 3503.505, "duration": 3.21}, {"text": "um, kinda putting it together, right?", "start": 3506.715, "duration": 2.94}, {"text": "And what if you don't know", "start": 3509.655, "duration": 1.215}, {"text": "[NOISE].", "start": 3510.87, "duration": 10.66}, {"text": "So it turns out that when you apply this to a practical problem,", "start": 3521.57, "duration": 6.46}, {"text": "you know, in- in- in robotics right.", "start": 3528.03, "duration": 3.48}, {"text": "Um, one common scenario you run into is if you do not know what is P of S, A.", "start": 3531.51, "duration": 7.44}, {"text": "If you don't know the state transition priorities right.", "start": 3538.95, "duration": 1.83}, {"text": "So when we built the MDP we said, well,", "start": 3540.78, "duration": 3.375}, {"text": "let's say the robot if you're going off you know,", "start": 3544.155, "duration": 4.065}, {"text": "has a 0.8 chance of going off and a 0.1 chance of veering off to the left or right.", "start": 3548.22, "duration": 4.26}, {"text": "If you actually- again it's a very simplified robot.", "start": 3552.48, "duration": 2.445}, {"text": "But, if you build a actual robot or build a helicopter or whatever,", "start": 3554.925, "duration": 5.385}, {"text": "play- play- play chess against an opponent.", "start": 3560.31, "duration": 1.95}, {"text": "Uh, the state transition probabilities are often not known in advance.", "start": 3562.26, "duration": 4.185}, {"text": "And so in many MDP implementations you need to estimate this from data.", "start": 3566.445, "duration": 9.51}, {"text": "And so the workflow of many reinforcement learning projects will be that,", "start": 3575.955, "duration": 5.535}, {"text": "um, you will have some policy and have the robot run around,", "start": 3581.49, "duration": 4.575}, {"text": "you know, just have a robot run around a maze and count", "start": 3586.065, "duration": 2.535}, {"text": "up of all the times you had to take the action north,", "start": 3588.6, "duration": 2.985}, {"text": "how often did it actually go north and how", "start": 3591.585, "duration": 2.265}, {"text": "often do they veer off to the left or right, right?", "start": 3593.85, "duration": 2.19}, {"text": "And so you use those statistics to estimate the state transition probabilities.", "start": 3596.04, "duration": 3.245}, {"text": "So let me just write this out.", "start": 3599.285, "duration": 1.5}, {"text": "So you estimate.", "start": 3600.785, "duration": 1.845}, {"text": "So after you're taking maybe a random policy it takes some policy,", "start": 3602.63, "duration": 4.455}, {"text": "executes some policy in the MDP for a while.", "start": 3607.085, "duration": 2.93}, {"text": "And then you would estimate this from data.", "start": 3610.015, "duration": 3.365}, {"text": "And so, the obvious formula would be,", "start": 3613.38, "duration": 2.25}, {"text": "estimate P of Sa S prime to be number of times took action a,", "start": 3615.63, "duration": 6.04}, {"text": "in the state S and got to S prime", "start": 3624.32, "duration": 10.67}, {"text": "and divide that by the number of times you took", "start": 3635.15, "duration": 6.19}, {"text": "action a in state S,  right.", "start": 3641.34, "duration": 7.2}, {"text": "So P of Sa S prime estimates- does actually a maximum likelihood estimate.", "start": 3648.54, "duration": 4.98}, {"text": "When you look at the number of times,", "start": 3653.52, "duration": 1.29}, {"text": "you took action a in state S,", "start": 3654.81, "duration": 2.205}, {"text": "and of that was a fraction of times you got to the state S prime right.", "start": 3657.015, "duration": 5.665}, {"text": "Or one over S and the above is 0, 0 right.", "start": 3665.33, "duration": 7.78}, {"text": "[NOISE] And a common heuristic is,", "start": 3673.11, "duration": 3.555}, {"text": "if you've never taken this action in this state before,", "start": 3676.665, "duration": 4.14}, {"text": "if the number of times you try action A in state S is 0.", "start": 3680.805, "duration": 3.315}, {"text": "So you've never tried this action in this state.", "start": 3684.12, "duration": 1.65}, {"text": "So you have no idea what it's going to do.", "start": 3685.77, "duration": 1.74}, {"text": "They just assume that the state transition probability is 1 over 11, right?", "start": 3687.51, "duration": 4.875}, {"text": "That it randomly takes you to another state.", "start": 3692.385, "duration": 2.61}, {"text": "So this would be common heuristics that people", "start": 3694.995, "duration": 2.295}, {"text": "use when implementing reinforcement learning algorithms, okay?", "start": 3697.29, "duration": 3.37}, {"text": "And it turns out that you can use Laplace smoothing for this if you wish,", "start": 3701.45, "duration": 7.645}, {"text": "but you don't have to.", "start": 3709.095, "duration": 1.59}, {"text": "Because, so you're in Laplace smoothing right.", "start": 3710.685, "duration": 2.355}, {"text": "So it would be, you know, adds 1 to the numerator and add 11 to the denominator would be,", "start": 3713.04, "duration": 5.955}, {"text": "if you were to use Laplace smoothing,", "start": 3718.995, "duration": 2.04}, {"text": "which avoids the problems of 0 over 0s as well.", "start": 3721.035, "duration": 2.82}, {"text": "But it turns out that unlike the Naive Bayes algorithm,", "start": 3723.855, "duration": 3.955}, {"text": "these solvers of MDPs are not that sensitive to 0 values.", "start": 3728.15, "duration": 5.305}, {"text": "So if- if one of your estimates were probably a 0, you know,", "start": 3733.455, "duration": 3.9}, {"text": "unlike Naive Bayes' where having a 0 probability was very", "start": 3737.355, "duration": 3.765}, {"text": "problematic for the classifications made by Naive Bayes,", "start": 3741.12, "duration": 3.84}, {"text": "it turns out that MDP solvers,", "start": 3744.96, "duration": 2.49}, {"text": "including evaluation of policy iteration,", "start": 3747.45, "duration": 1.995}, {"text": "they do not give sort of", "start": 3749.445, "duration": 2.025}, {"text": "nonsensical/horrible results just because of a few probabilities that are exactly 0.", "start": 3751.47, "duration": 5.385}, {"text": "And so in practice,", "start": 3756.855, "duration": 2.13}, {"text": "you can use Laplace smoothing if you wish.", "start": 3758.985, "duration": 2.805}, {"text": "But because the reinforcement learning algorithms don't- don't perform", "start": 3761.79, "duration": 4.53}, {"text": "that badly if these estimates often will be a zero", "start": 3766.32, "duration": 2.52}, {"text": "in practice, Laplace moving is not commonly unison.", "start": 3768.84, "duration": 2.94}, {"text": "What I just wrote is- is more common.", "start": 3771.78, "duration": 2.52}, {"text": "Okay.", "start": 3774.3, "duration": 1.66}, {"text": "So to put it together.", "start": 3781.55, "duration": 4.4}, {"text": "All right, if I give you", "start": 3802.25, "duration": 6.46}, {"text": "a robot and asked you to implement a MDP Solver to find the good policy for this robot,", "start": 3808.71, "duration": 6.315}, {"text": "what you will do is the following.", "start": 3815.025, "duration": 2.935}, {"text": "Take actions with respect to some policy pi.", "start": 3822.53, "duration": 4.37}, {"text": "To get the experience in the MDP.", "start": 3834.49, "duration": 3.68}, {"text": "Right. So go ahead and let your robot lose and have it execute some policy for awhile.", "start": 3840.31, "duration": 7.97}, {"text": "And then update estimates of P of Sa.", "start": 3852.35, "duration": 4.52}, {"text": "Based on the observations of whether robot goes and takes different states,", "start": 3858.59, "duration": 4.81}, {"text": "update- update the estimates of P of Sa.", "start": 3863.4, "duration": 3.16}, {"text": "Solve, um, Bellman's equation using value iteration", "start": 3868.07, "duration": 11.929}, {"text": "to get V and then update.", "start": 3887.27, "duration": 6.8}, {"text": "So this is the value iteration we are putting together.", "start": 3906.56, "duration": 4.735}, {"text": "If you want to plug in policy innovation instead in this step that's also okay.", "start": 3911.295, "duration": 4.86}, {"text": "But so if you actually get the robot, um,", "start": 3916.155, "duration": 4.495}, {"text": "you know, yeah right- right.", "start": 3921.05, "duration": 6.37}, {"text": "If you actually get a robot, uh,", "start": 3927.42, "duration": 1.53}, {"text": "where you do not know in advance the state transition probabilities,", "start": 3928.95, "duration": 3.585}, {"text": "then this is what you would do in order to,", "start": 3932.535, "duration": 2.7}, {"text": "um, iterate a few times I guess.", "start": 3935.235, "duration": 2.715}, {"text": "Repeatedly find a- find a-", "start": 3937.95, "duration": 3.0}, {"text": "find a policy given your current estimate of the state transition probabilities.", "start": 3940.95, "duration": 3.63}, {"text": "Get some experience, update your estimates,", "start": 3944.58, "duration": 2.04}, {"text": "find a new policy and kind of repeat this process", "start": 3946.62, "duration": 2.4}, {"text": "until hopefully it converges to a good policy.", "start": 3949.02, "duration": 3.39}, {"text": "Okay.", "start": 3952.41, "duration": 1.81}, {"text": "Now just to", "start": 3963.2, "duration": 3.22}, {"text": "add more color and more richness to this,", "start": 3966.42, "duration": 3.225}, {"text": "we usually think of-", "start": 3969.645, "duration": 3.265}, {"text": "we usually think of the reward function as being given,", "start": 3976.97, "duration": 5.32}, {"text": "right, as part of the problem specification.", "start": 3982.29, "duration": 2.34}, {"text": "But sometimes you see that the reward function may be unknown.", "start": 3984.63, "duration": 5.07}, {"text": "And so for example,", "start": 3989.7, "duration": 1.245}, {"text": "if you're building a stock trading application", "start": 3990.945, "duration": 2.925}, {"text": "and the reward is the returns on a certain day,", "start": 3993.87, "duration": 3.03}, {"text": "it may not be a function of the state and it may be a little bit random.", "start": 3996.9, "duration": 2.985}, {"text": "Um, or if your robot is running around but depending on where it goes,", "start": 3999.885, "duration": 5.205}, {"text": "it may hit different bumps in the road and you", "start": 4005.09, "duration": 1.8}, {"text": "want to give it a penalty every time it hits the bump.", "start": 4006.89, "duration": 2.01}, {"text": "We're going to build a self-driving car right,", "start": 4008.9, "duration": 1.89}, {"text": "every time it hits a bump, hits a pothole,", "start": 4010.79, "duration": 2.19}, {"text": "you give it a negative reward,", "start": 4012.98, "duration": 1.02}, {"text": "then sometimes the rewards are a random function of the environments.", "start": 4014.0, "duration": 3.36}, {"text": "And so sometimes you can also estimate the expected value of a reward.", "start": 4017.36, "duration": 4.02}, {"text": "But- but in- in some applications,", "start": 4021.38, "duration": 2.219}, {"text": "if the reward is a random function of the state,", "start": 4023.599, "duration": 2.146}, {"text": "then this process allows you to also estimate the expected value of the reward from", "start": 4025.745, "duration": 4.755}, {"text": "every state and then running this will help you to converge. Okay yeah.", "start": 4030.5, "duration": 4.71}, {"text": "[inaudible]", "start": 4035.21, "duration": 15.91}, {"text": "Yeah, cool. [NOISE].", "start": 4051.12, "duration": 2.58}, {"text": "[inaudible]", "start": 4053.7, "duration": 0.1}, {"text": "Yeah, cool. Great question. So let me,", "start": 4053.8, "duration": 1.205}, {"text": "let me talk about exploration, right.", "start": 4055.005, "duration": 1.515}, {"text": "So it turns out that, um,", "start": 4056.52, "duration": 2.1}, {"text": "this one [NOISE] so it turns out", "start": 4058.62, "duration": 3.78}, {"text": "this algorithm will work okay for some problems but the- the- there's one other,", "start": 4062.4, "duration": 4.5}, {"text": "ah, again to add richness to this,", "start": 4066.9, "duration": 2.295}, {"text": "there's one other, um,", "start": 4069.195, "duration": 1.44}, {"text": "issue that this is not solving which is the exploration problem.", "start": 4070.635, "duration": 4.455}, {"text": "And [NOISE] in, in reinforcement learning sometimes you hear", "start": 4075.09, "duration": 2.25}, {"text": "the term exploration versus exploitation, [NOISE] right?", "start": 4077.34, "duration": 4.665}, {"text": "Which is, um, let me use a different MDP example, right.", "start": 4082.005, "duration": 7.875}, {"text": "Which is, um, if your robot, you know,", "start": 4089.88, "duration": 2.85}, {"text": "starts off here and if there is a, um,", "start": 4092.73, "duration": 5.745}, {"text": "plus 1 reward here,", "start": 4098.475, "duration": 2.715}, {"text": "right and maybe a plus 10 reward here.", "start": 4101.19, "duration": 4.245}, {"text": "If just by chance during the first time you run the robot it", "start": 4105.435, "duration": 5.385}, {"text": "happens to find its way to the plus 1 then if you run this algorithm,", "start": 4110.82, "duration": 5.685}, {"text": "it may figure out that going to the plus 1 is a good way, right?", "start": 4116.505, "duration": 4.56}, {"text": "We were giving it a discount factor and there is", "start": 4121.065, "duration": 1.785}, {"text": "a fuel surcharge of minus 0.02 on every step.", "start": 4122.85, "duration": 3.105}, {"text": "So if just by chance your robot happens to find its way to", "start": 4125.955, "duration": 4.095}, {"text": "the plus 1 the first few times you run this algorithm then this algorithm is,", "start": 4130.05, "duration": 4.365}, {"text": "um, is uh, locally greedy, right.", "start": 4134.415, "duration": 2.16}, {"text": "Ah, it may figure out that this is a great way to get to plus", "start": 4136.575, "duration": 4.695}, {"text": "1 reward and then the world ends, it stops giving these minus 0.02 surcharges for fuel.", "start": 4141.27, "duration": 5.055}, {"text": "And so this particular algorithm may converge to a bad,", "start": 4146.325, "duration": 5.145}, {"text": "you know, kind of local optima where it's always heading to the plus 1.", "start": 4151.47, "duration": 5.31}, {"text": "And as it hits the plus 1,", "start": 4156.78, "duration": 1.62}, {"text": "it sometimes will veer off randomly right and get a little bit more experience", "start": 4158.4, "duration": 4.395}, {"text": "in the right half of the state space and end up with pretty good estimates of,", "start": 4162.795, "duration": 4.545}, {"text": "ah, what happens in the right half of this state space.", "start": 4167.34, "duration": 2.475}, {"text": "And, um, and it may never find this hard-to-define", "start": 4169.815, "duration": 3.525}, {"text": "plus 10 pot of gold over on the lower left, okay?", "start": 4173.34, "duration": 4.02}, {"text": "So this problem is sometimes called actually, well,", "start": 4177.36, "duration": 4.275}, {"text": "it is called the exploration versus exploitation problem which is, um,", "start": 4181.635, "duration": 5.475}, {"text": "when you're acting in an MDP, you know,", "start": 4187.11, "duration": 2.88}, {"text": "how aggressively or how greedy should you be", "start": 4189.99, "duration": 2.7}, {"text": "at just taking actions to maximize your rewards?", "start": 4192.69, "duration": 3.285}, {"text": "And so the algorithm we describe is relatively greedy, right?", "start": 4195.975, "duration": 6.38}, {"text": "Meaning that, um, is taking your best estimate of the state transition probabilities", "start": 4202.355, "duration": 4.605}, {"text": "and rewards and is just taking whatever actions and this is really saying, you know,", "start": 4206.96, "duration": 5.1}, {"text": "pick the policy that maximizes", "start": 4212.06, "duration": 2.67}, {"text": "your current estimate of the expected rewards and it's just acting greedily,", "start": 4214.73, "duration": 3.895}, {"text": "meaning on every step it's just executing the policy that", "start": 4218.625, "duration": 3.66}, {"text": "it thinks allows it to maximize the expected payoff, right?", "start": 4222.285, "duration": 4.35}, {"text": "And what this algorithm does not do at all is explore which is", "start": 4226.635, "duration": 5.235}, {"text": "the process of taking actions that may appear less optimal at the outset,", "start": 4231.87, "duration": 5.07}, {"text": "um, such as if the robot hasn't seen this plus 10 reward, it doesn't know how to get there,", "start": 4236.94, "duration": 5.385}, {"text": "maybe it should, you know,", "start": 4242.325, "duration": 1.545}, {"text": "just try going left a couple of times just for the heck of it,", "start": 4243.87, "duration": 2.82}, {"text": "right, to see what happens.", "start": 4246.69, "duration": 1.71}, {"text": "Because even if it seems less,", "start": 4248.4, "duration": 2.475}, {"text": "even if going left from", "start": 4250.875, "duration": 1.875}, {"text": "the perspective of the current state of the knowledge of the robot,", "start": 4252.75, "duration": 2.7}, {"text": "um, maybe if it tries some new things it's never", "start": 4255.45, "duration": 2.79}, {"text": "tried before maybe it will find a new pot of gold, okay.", "start": 4258.24, "duration": 3.195}, {"text": "So this is called the exploration versus exploitation trade-off,", "start": 4261.435, "duration": 4.814}, {"text": "um, and this is actually not just an academic problem.", "start": 4266.249, "duration": 2.941}, {"text": "It turns out that some of the large online web advertising platforms,", "start": 4269.19, "duration": 4.485}, {"text": "ah, have the same problem as well.", "start": 4273.675, "duration": 1.965}, {"text": "And again, I, I, I, I have,", "start": 4275.64, "duration": 1.815}, {"text": "have mixed feelings about the advertising business.", "start": 4277.455, "duration": 1.875}, {"text": "It's very lucrative but it causes other problems, um, as well but,", "start": 4279.33, "duration": 3.045}, {"text": "but it turns out that for some of the large online ad platforms,", "start": 4282.375, "duration": 3.585}, {"text": "um, ah, you know, when a,", "start": 4285.96, "duration": 2.175}, {"text": "when an advertiser, um,", "start": 4288.135, "duration": 2.07}, {"text": "starts selling a new ad or your posts and", "start": 4290.205, "duration": 2.325}, {"text": "you add on one of the large online ad platforms,", "start": 4292.53, "duration": 2.355}, {"text": "the ad platform does not know who is most likely to click on this ad, right?", "start": 4294.885, "duration": 4.74}, {"text": "And so pure explo- pure exploitation,", "start": 4299.625, "duration": 3.9}, {"text": "boy exploitation has such horrible connotations", "start": 4303.525, "duration": 2.355}, {"text": "especially [LAUGHTER] for online ad platforms.", "start": 4305.88, "duration": 2.22}, {"text": "Ah, it's the technical term, not a,", "start": 4308.1, "duration": 1.695}, {"text": "not a social term when used in this context.", "start": 4309.795, "duration": 2.145}, {"text": "But the pure, you know,", "start": 4311.94, "duration": 1.32}, {"text": "reinforcement learning sends exploitation policy not,", "start": 4313.26, "duration": 2.995}, {"text": "not the other even more horrible sense of exploitation.", "start": 4316.255, "duration": 2.77}, {"text": "Um, would be to always just show you,", "start": 4319.025, "duration": 2.54}, {"text": "show, show users the ads that, you know,", "start": 4321.565, "duration": 2.3}, {"text": "they are most likely to click on to drive short-term revenues", "start": 4323.865, "duration": 2.91}, {"text": "because we want to just show people the ad they're most likely to click on to drive short-term revenue.", "start": 4326.775, "duration": 3.48}, {"text": "Whereas an exploration policy for large,", "start": 4330.255, "duration": 2.775}, {"text": "you know, some of these large online ad platforms,", "start": 4333.03, "duration": 1.95}, {"text": "is to show people some ads that may not be", "start": 4334.98, "duration": 2.22}, {"text": "what we think you are most likely to click on in this moment", "start": 4337.2, "duration": 2.58}, {"text": "in time but by showing you that ad or by showing", "start": 4339.78, "duration": 2.76}, {"text": "the pool of users an ad that you might be less likely to click on,", "start": 4342.54, "duration": 2.82}, {"text": "maybe we'll learn more about your interests.", "start": 4345.36, "duration": 1.845}, {"text": "And that, um, increases the effectiveness of these large or", "start": 4347.205, "duration": 3.66}, {"text": "these ad platforms at finding more relevant ads, right?", "start": 4350.865, "duration": 3.51}, {"text": "And for example, I don't know,", "start": 4354.375, "duration": 1.39}, {"text": "um, probably not- I, I, I guess,", "start": 4355.765, "duration": 2.27}, {"text": "ah there are probably no advertisements for ah,", "start": 4358.035, "duration": 2.43}, {"text": "Mars landers as I know.", "start": 4360.465, "duration": 1.545}, {"text": "But if the large online ad platforms", "start": 4362.01, "duration": 2.01}, {"text": "don't know that I'm actually pretty interested in Mars landers", "start": 4364.02, "duration": 2.25}, {"text": "if it shows me an ad for a Mars lander which I don't think such a thing exists, right?", "start": 4366.27, "duration": 4.17}, {"text": "If I did I click on it and they may learn that", "start": 4370.44, "duration": 1.95}, {"text": "showing me ads for Mars landers is a great thing,", "start": 4372.39, "duration": 2.325}, {"text": "right, ah, or, or some other thing that you may not know you're interested in.", "start": 4374.715, "duration": 3.735}, {"text": "So this is actually a real problem.", "start": 4378.45, "duration": 1.665}, {"text": "There are, um, some of the large online ad platforms, ah, um,", "start": 4380.115, "duration": 4.755}, {"text": "actually do explicitly consider exploration versus exploitation and make", "start": 4384.87, "duration": 4.68}, {"text": "sure that sometimes it shows ads", "start": 4389.55, "duration": 1.89}, {"text": "that may not be the most likely you'll click on but, you know,", "start": 4391.44, "duration": 2.94}, {"text": "allows us to gather information to then be better", "start": 4394.38, "duration": 3.09}, {"text": "situated to figure out where the future rewards to be better positioned to,", "start": 4397.47, "duration": 4.02}, {"text": "ah, learn how to match ads not just to you but to other users like you, right?", "start": 4401.49, "duration": 5.415}, {"text": "Um, sorry.", "start": 4406.905, "duration": 1.995}, {"text": "Okay but so in order to make sure their reinforcement learning algorithm,", "start": 4408.9, "duration": 4.155}, {"text": "um, ah, explores as was exploits a, um, ah,", "start": 4413.055, "duration": 3.915}, {"text": "a common a, a modification to", "start": 4416.97, "duration": 4.08}, {"text": "this would be tak- instead of taking actions with respect to Pi,", "start": 4421.05, "duration": 4.995}, {"text": "you may have a, um, a 0.9 chance.", "start": 4426.045, "duration": 7.755}, {"text": "[NOISE] Respect to Pi and 0.1 chance,", "start": 4433.8, "duration": 2.97}, {"text": "[NOISE] take an action randomly, okay.", "start": 4436.77, "duration": 6.03}, {"text": "And so, um, this particular,", "start": 4442.8, "duration": 2.88}, {"text": "[NOISE] exploration policy is called", "start": 4445.68, "duration": 5.655}, {"text": "Epsilon-greedy where on every time step and on every time step you toss a biased coin.", "start": 4451.335, "duration": 6.015}, {"text": "But on every time step,", "start": 4457.35, "duration": 1.785}, {"text": "let's say 90% of the chance you execute whatever you think is", "start": 4459.135, "duration": 4.665}, {"text": "the current best policy and with 10% chance you just take a random action.", "start": 4463.8, "duration": 4.5}, {"text": "And this type of exploration policy, um,", "start": 4468.3, "duration": 2.97}, {"text": "increases the odds that you know,", "start": 4471.27, "duration": 1.68}, {"text": "every now and then maybe just by chance, right,", "start": 4472.95, "duration": 2.385}, {"text": "it'll find it's way to the plus 10 pot of", "start": 4475.335, "duration": 2.925}, {"text": "gold, and learn state transition probabilities and,", "start": 4478.26, "duration": 3.255}, {"text": "and, and then eventually, um,", "start": 4481.515, "duration": 1.935}, {"text": "end up exploring the state-space more thoroughly, okay.", "start": 4483.45, "duration": 3.675}, {"text": "Um, this is called Epsilon-greedy exploration and,", "start": 4487.125, "duration": 3.765}, {"text": "um, it's a little bit of a misnomer I think.", "start": 4490.89, "duration": 2.07}, {"text": "So in, in, in the way we think of Epsilon-greedy Epsilon is, um,", "start": 4492.96, "duration": 4.59}, {"text": "say 0.1 is the chance of taking a random action instead of the greedy action.", "start": 4497.55, "duration": 5.25}, {"text": "Um, this algorithm is,", "start": 4502.8, "duration": 2.25}, {"text": "has always been a little bit strangely named because, ah,", "start": 4505.05, "duration": 2.745}, {"text": "if 0, 0.1 is actually the chance of you acting randomly, right.", "start": 4507.795, "duration": 4.155}, {"text": "So Epsilon greedy sounds like you're being greedy 0.1 of the time but,", "start": 4511.95, "duration": 3.84}, {"text": "but you're actually taking actions randomly 0.1 at a time", "start": 4515.79, "duration": 3.09}, {"text": "so Epsilon-greedy is actually maybe 1 minus Epsilon-greedy.", "start": 4518.88, "duration": 3.51}, {"text": "So th- these name has always been a little bit,", "start": 4522.39, "duration": 2.235}, {"text": "um, off but that's what,", "start": 4524.625, "duration": 1.935}, {"text": "that's, that's how people use this term.", "start": 4526.56, "duration": 1.695}, {"text": "Epsilon-greedy exploration means Epsilon of the time which is the hyperparameter,", "start": 4528.255, "duration": 3.78}, {"text": "which is the parameter of the algorithm you act randomly into- instead", "start": 4532.035, "duration": 3.465}, {"text": "of going to what you think is the best policy, okay.", "start": 4535.5, "duration": 3.75}, {"text": "And it turns out that, um,", "start": 4539.25, "duration": 2.175}, {"text": "if you implement this algorithm with, um,", "start": 4541.425, "duration": 3.66}, {"text": "Epsilon-greedy exploration then this,", "start": 4545.085, "duration": 3.24}, {"text": "ah, ah, this algorithm,", "start": 4548.325, "duration": 2.324}, {"text": "ah, will converge to the optimal policy for any discrete state MDP, right.", "start": 4550.649, "duration": 5.671}, {"text": "Ah, sometimes they take a long time because, you know, if there's a,", "start": 4556.32, "duration": 3.315}, {"text": "if it takes a long time to randomly find plus 10, it, it,", "start": 4559.635, "duration": 3.255}, {"text": "it could take a long time before it randomly stumbles upon the plus 10 pot of gold.", "start": 4562.89, "duration": 3.81}, {"text": "But, um, this algorithm with an,", "start": 4566.7, "duration": 2.46}, {"text": "with an exploration policy will converge to the optimal,", "start": 4569.16, "duration": 3.72}, {"text": "um, will, will converge to the optimal policy for any MDP. What is your question?", "start": 4572.88, "duration": 4.98}, {"text": "[inaudible]", "start": 4577.86, "duration": 9.869}, {"text": "Yeah, yeah, so, right, should you always keep epsilon constant or should you use a dynamic epsilon.", "start": 4587.729, "duration": 3.811}, {"text": "So yes, ah, there are, there, there are.", "start": 4591.54, "duration": 2.25}, {"text": "There are many heuristics for how to explore, ah.", "start": 4593.79, "duration": 3.105}, {"text": "One reasonable thing to do would be we start with", "start": 4596.895, "duration": 1.905}, {"text": "a large value of epsilon and we slowly shrink it.", "start": 4598.8, "duration": 2.28}, {"text": "Um, another common heuristic would be,", "start": 4601.08, "duration": 2.04}, {"text": "um, there is a different,", "start": 4603.12, "duration": 1.56}, {"text": "ah, type of exploration called Boltzmann exploration,", "start": 4604.68, "duration": 2.55}, {"text": "which you can look up if you want which is, ah,", "start": 4607.23, "duration": 1.92}, {"text": "if you think that the value of going north is,", "start": 4609.15, "duration": 2.49}, {"text": "um, you know, 10 and the value of going south is 1,", "start": 4611.64, "duration": 3.315}, {"text": "then there is such a huge difference that you", "start": 4614.955, "duration": 2.655}, {"text": "should bias your action to upgrading to the bigger result,", "start": 4617.61, "duration": 3.69}, {"text": "the, the bigger reward and,", "start": 4621.3, "duration": 1.38}, {"text": "ah, you could have the probability be f E to the value basically time,", "start": 4622.68, "duration": 4.32}, {"text": "ah, divide, times of a times the scaling factor, right?", "start": 4627.0, "duration": 3.57}, {"text": "So that's called Boltzmann exploration where instead", "start": 4630.57, "duration": 2.25}, {"text": "of having a 10% chance of taking an action completely at random,", "start": 4632.82, "duration": 3.36}, {"text": "ah, you could just, you know,", "start": 4636.18, "duration": 1.38}, {"text": "have a very strong bias to,", "start": 4637.56, "duration": 2.025}, {"text": "heading toward the higher values but also have some probability to go into", "start": 4639.585, "duration": 4.275}, {"text": "lower values but where", "start": 4643.86, "duration": 1.11}, {"text": "the exact probability depends on the difference in ideal values is.", "start": 4644.97, "duration": 3.405}, {"text": "So another probably the, I think Epsilon-greedy,", "start": 4648.375, "duration": 3.36}, {"text": "I feel like I see this used the most often", "start": 4651.735, "duration": 2.415}, {"text": "for these types of MDPs and then Boltzmann exploration", "start": 4654.15, "duration": 2.475}, {"text": "which is why I just drive this also. Two more questions before we wrap up, go ahead.", "start": 4656.625, "duration": 4.905}, {"text": "[inaudible]", "start": 4661.53, "duration": 6.72}, {"text": "Yes, can you get a reward for reaching states you've never seen before?", "start": 4668.25, "duration": 2.82}, {"text": "Yes, there is a fascinating line of research called intrinsic reinforcement learning.", "start": 4671.07, "duration": 4.695}, {"text": "Ah, and it really started by search indexing.", "start": 4675.765, "duration": 2.88}, {"text": "If you Google for intrinsic,", "start": 4678.645, "duration": 1.585}, {"text": "intrinsic motivation, you find some research papers on.", "start": 4680.23, "duration": 3.64}, {"text": "Um, and then there was some recent followup work I think by", "start": 4683.87, "duration": 2.28}, {"text": "DeepMind or some other groups but intrinsic motivation", "start": 4686.15, "duration": 3.09}, {"text": "is the term to Google where you reward", "start": 4689.24, "duration": 2.01}, {"text": "a reinforcement learning algorithm for finding new things about the world.", "start": 4691.25, "duration": 2.695}, {"text": "Just one last question.", "start": 4693.945, "duration": 1.615}, {"text": "How many actions you should take with respect to Pi?", "start": 4695.56, "duration": 3.4}, {"text": "Sorry, say that again?", "start": 4698.96, "duration": 2.04}, {"text": "How many actions you should take with respect to Pi before updating the Pi?", "start": 4701.0, "duration": 5.51}, {"text": "I see, right. How often,", "start": 4706.51, "duration": 1.55}, {"text": "how many actions you should you take before updating Pi?", "start": 4708.06, "duration": 2.85}, {"text": "Um, there's no harm  to do it as frequently as possible.", "start": 4710.91, "duration": 4.635}, {"text": "Ah, in the, if you're doing this with a real robot what,", "start": 4715.545, "duration": 3.375}, {"text": "you know, I've seen is, um,", "start": 4718.92, "duration": 2.1}, {"text": "this is sometimes going to physical robot and so, you know, I don't know,", "start": 4721.02, "duration": 4.05}, {"text": "when we're flying helicopters you go out to the field for the day,", "start": 4725.07, "duration": 2.88}, {"text": "collect a lot of data, and they go back to", "start": 4727.95, "duration": 1.5}, {"text": "the lab in the evening and rerun the algorithms.", "start": 4729.45, "duration": 2.01}, {"text": "Ah, but if there's no barrier to running this all the time,", "start": 4731.46, "duration": 3.45}, {"text": "then it doesn't hurt the performance,", "start": 4734.91, "duration": 1.59}, {"text": "it's just running as frequently as it can.", "start": 4736.5, "duration": 1.995}, {"text": "All right, that's it for basis of MDP.", "start": 4738.495, "duration": 3.045}, {"text": "Um, on Wednesday, we'll continue with generalizing all these to continuous state MDPs.", "start": 4741.54, "duration": 6.375}, {"text": "Okay, let's break, I'll see you on Wednesday.", "start": 4747.915, "duration": 2.605}]