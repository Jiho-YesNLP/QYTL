[{"text": "Couple of announcements, uh,", "start": 3.5, "duration": 2.455}, {"text": "before we get started.", "start": 5.955, "duration": 1.185}, {"text": "So, uh, first of all, PS1 is out.", "start": 7.14, "duration": 2.865}, {"text": "Uh, problem set 1, um,", "start": 10.005, "duration": 2.385}, {"text": "it is due on the 17th.", "start": 12.39, "duration": 3.66}, {"text": "That's two weeks from today.", "start": 16.05, "duration": 2.0}, {"text": "You have, um, exactly two weeks to work on it.", "start": 18.05, "duration": 2.555}, {"text": "You can take up to,", "start": 20.605, "duration": 1.485}, {"text": "um, two or three late days.", "start": 22.09, "duration": 1.62}, {"text": "I think you can take up to, uh,", "start": 23.71, "duration": 1.395}, {"text": "three late days, um.", "start": 25.105, "duration": 1.995}, {"text": "There is, uh, there's a good amount", "start": 27.1, "duration": 4.02}, {"text": "of programming and a good amount of math you, uh, you need to do.", "start": 31.12, "duration": 3.86}, {"text": "So PS1 needs to be uploaded.", "start": 34.98, "duration": 2.1}, {"text": "Uh, the solutions need to be uploaded to Gradescope.", "start": 37.08, "duration": 2.56}, {"text": "Um, you'll have to make two submissions.", "start": 39.64, "duration": 3.045}, {"text": "One submission will be a PDF file,", "start": 42.685, "duration": 2.85}, {"text": "uh, which you can either, uh,", "start": 45.535, "duration": 2.485}, {"text": "which you can either use a LaTeX template that we provide or you can", "start": 48.02, "duration": 4.26}, {"text": "handwrite it as well but you're strongly encouraged to use the- the LaTeX template.", "start": 52.28, "duration": 4.605}, {"text": "Um, and there is a separate coding assignment, uh,", "start": 56.885, "duration": 3.69}, {"text": "for which you'll have to submit code as a separate,", "start": 60.575, "duration": 2.865}, {"text": "uh, Gradescope assignment.", "start": 63.44, "duration": 1.2}, {"text": "So they're gonna- you're gonna see two assignments in Gradescope.", "start": 64.64, "duration": 2.49}, {"text": "One is for the written part.", "start": 67.13, "duration": 1.44}, {"text": "The other is for the, uh,", "start": 68.57, "duration": 1.545}, {"text": "is for the programming part.", "start": 70.115, "duration": 2.275}, {"text": "Uh, with that, let's- let's jump right into today's topics.", "start": 72.52, "duration": 4.19}, {"text": "So, uh, today, we're gonna cover,", "start": 76.71, "duration": 2.295}, {"text": "uh- briefly we're gonna cover, uh,", "start": 79.005, "duration": 1.59}, {"text": "the perceptron, uh, algorithm.", "start": 80.595, "duration": 2.46}, {"text": "Um, and then, you know,", "start": 83.055, "duration": 2.045}, {"text": "a good chunk of today is gonna be exponential family and,", "start": 85.1, "duration": 3.025}, {"text": "uh, generalized linear models.", "start": 88.125, "duration": 1.865}, {"text": "And, uh, we'll- we'll end it with, uh,", "start": 89.99, "duration": 2.46}, {"text": "softmax regression for multi-class classification.", "start": 92.45, "duration": 3.435}, {"text": "So, uh, perceptron, um,", "start": 95.885, "duration": 3.575}, {"text": "we saw in logistic regression, um.", "start": 99.46, "duration": 2.81}, {"text": "So first of all, the perceptron algorithm, um,", "start": 102.27, "duration": 2.62}, {"text": "I should mention is not something that is widely used in practice.", "start": 104.89, "duration": 3.88}, {"text": "Uh, we study it mostly for, um, historical reasons.", "start": 108.77, "duration": 4.455}, {"text": "And also because it is- it's nice and simple and, you know,", "start": 113.225, "duration": 3.355}, {"text": "it's easy to analyze and,", "start": 116.58, "duration": 2.22}, {"text": "uh, we also have homework questions on it.", "start": 118.8, "duration": 2.225}, {"text": "So, uh, logistic regression.", "start": 121.025, "duration": 3.075}, {"text": "Uh, we saw logistic regression uses,", "start": 124.1, "duration": 2.71}, {"text": "uh, the sigmoid function.", "start": 126.81, "duration": 3.79}, {"text": "Right. So, uh, the logistic regression,", "start": 153.5, "duration": 3.1}, {"text": "uh, using the sigmoid function which, uh,", "start": 156.6, "duration": 3.09}, {"text": "which essentially squeezes the entire real line", "start": 159.69, "duration": 3.16}, {"text": "from minus infinity to infinity between 0 and 1.", "start": 162.85, "duration": 2.91}, {"text": "Um, and - and the 0 and 1 kind of represents,", "start": 165.76, "duration": 2.52}, {"text": "uh, the probability right?", "start": 168.28, "duration": 2.595}, {"text": "Um, you could also think of,", "start": 170.875, "duration": 2.84}, {"text": "uh, a variant of that, uh,", "start": 173.715, "duration": 1.605}, {"text": "which will be, um,", "start": 175.32, "duration": 2.055}, {"text": "like the perceptron where, um.", "start": 177.375, "duration": 2.34}, {"text": "So in- in- in the sigmoid function at, um,", "start": 179.715, "duration": 3.425}, {"text": "at z equals 0- at z equals 0- g of z is a half.", "start": 183.14, "duration": 9.81}, {"text": "And as z tends to minus infinity,", "start": 192.95, "duration": 2.55}, {"text": "g tends to 0 and as z tends to plus infinity,", "start": 195.5, "duration": 3.345}, {"text": "g tends to 1.", "start": 198.845, "duration": 2.575}, {"text": "The perceptron, um, algorithm uses,", "start": 201.92, "duration": 4.45}, {"text": "uh, uh, a somewhat similar but, uh,", "start": 206.37, "duration": 2.76}, {"text": "different, uh, function which,", "start": 209.13, "duration": 9.525}, {"text": "uh, let's say this is z.", "start": 218.655, "duration": 2.305}, {"text": "Right. So, uh, g of z in this case", "start": 228.95, "duration": 6.685}, {"text": "is 1 if z is greater than or equal to 0 and 0 if z is less than 0, right?", "start": 235.635, "duration": 9.39}, {"text": "So you ca- you can think of this as the hard version of-", "start": 245.025, "duration": 2.55}, {"text": "of- of the sigmoid function, right?", "start": 247.575, "duration": 2.775}, {"text": "And this leads to, um,", "start": 250.35, "duration": 1.92}, {"text": "um, this leads to the hypothesis function, uh,", "start": 252.27, "duration": 6.42}, {"text": "here being, uh, h Theta of x is equal to,", "start": 258.69, "duration": 6.015}, {"text": "um, g of Theta transpose x.", "start": 264.705, "duration": 5.425}, {"text": "So, uh, Theta transpose x,", "start": 270.35, "duration": 2.8}, {"text": "um, your Theta has the parameter,", "start": 273.15, "duration": 2.295}, {"text": "x is the, um,", "start": 275.445, "duration": 1.395}, {"text": "x is the input and h Theta of x will be 0 or 1,", "start": 276.84, "duration": 4.705}, {"text": "depending on whether Theta transpose x was less than 0 or- or,", "start": 281.545, "duration": 4.235}, {"text": "uh, greater than 0.", "start": 285.78, "duration": 1.425}, {"text": "And you tol- and,", "start": 287.205, "duration": 1.26}, {"text": "um, similarly in, uh,", "start": 288.465, "duration": 1.505}, {"text": "logistic regression we had a state of x is equal to,", "start": 289.97, "duration": 6.05}, {"text": "um, 1 over 1 plus e to the minus Theta transpose x. Yeah.", "start": 296.02, "duration": 5.48}, {"text": "That's essentially, uh, g of- g of z where g is s,", "start": 301.5, "duration": 3.44}, {"text": "uh, the sigma- sigmoid function.", "start": 304.94, "duration": 2.93}, {"text": "Um, both of them have a common update rule,", "start": 307.87, "duration": 4.09}, {"text": "uh, which, you know,", "start": 311.96, "duration": 1.83}, {"text": "on the surface looks similar.", "start": 313.79, "duration": 1.83}, {"text": "So Theta j equal to Theta j plus Alpha times y_i", "start": 315.62, "duration": 10.275}, {"text": "minus h Theta of- of", "start": 325.895, "duration": 5.565}, {"text": "x_i times x_ij, right?", "start": 331.46, "duration": 6.745}, {"text": "So the update rules for,", "start": 338.205, "duration": 1.395}, {"text": "um, the perceptron and logistic regression,", "start": 339.6, "duration": 2.505}, {"text": "they look the same except h Theta of x means", "start": 342.105, "duration": 3.18}, {"text": "different things in- in- in- in the two different, um, uh, scenarios.", "start": 345.285, "duration": 4.94}, {"text": "We also saw that it was similar for linear regression as well.", "start": 350.225, "duration": 3.3}, {"text": "And we're gonna see why this- this is, um, you know,", "start": 353.525, "duration": 2.745}, {"text": "that this is actually a- a more common- common theme.", "start": 356.27, "duration": 3.645}, {"text": "So, uh, what's happening here?", "start": 359.915, "duration": 2.355}, {"text": "So, uh, if you inspect this equation, um,", "start": 362.27, "duration": 4.655}, {"text": "to get a better sense of what's happening in- in the perceptron algorithm,", "start": 366.925, "duration": 4.235}, {"text": "this quantity over here is a scalar, right?", "start": 371.44, "duration": 5.3}, {"text": "It's the difference between y_i which can be either 0 and 1", "start": 376.74, "duration": 3.285}, {"text": "and h Theta of x_i which can either be 0 or 1, right?", "start": 380.025, "duration": 4.065}, {"text": "So when the algorithm makes a prediction of h Theta of- h Theta of x_i for a given x_i,", "start": 384.09, "duration": 8.82}, {"text": "this quantity will either be zero if- if, uh,", "start": 392.91, "duration": 8.43}, {"text": "the algorithm got it right already, right?", "start": 401.34, "duration": 8.44}, {"text": "And it would be either plus 1 or minus 1 if- if y_i- if- if the actual, uh,", "start": 409.78, "duration": 9.275}, {"text": "if the ground truth was plus 1 and the algorithm predicted 0,", "start": 419.055, "duration": 3.41}, {"text": "then it, uh, uh,", "start": 422.465, "duration": 2.235}, {"text": "this will evaluate to 1 if", "start": 424.7, "duration": 3.75}, {"text": "wrong and y_i equals 1 and similarly it is,", "start": 428.45, "duration": 6.73}, {"text": "uh, minus 1 if", "start": 435.18, "duration": 6.75}, {"text": "wrong and y_i is 0.", "start": 441.93, "duration": 4.54}, {"text": "So what's happening here? Um, to see what's- what's- what's happening,", "start": 447.51, "duration": 4.96}, {"text": "uh, it's useful to see this picture, right?", "start": 452.47, "duration": 3.81}, {"text": "So this is the input space, right?", "start": 458.43, "duration": 4.015}, {"text": "And, uh, let's imagine there are two, uh,", "start": 462.445, "duration": 3.45}, {"text": "two classes, boxes and,", "start": 465.895, "duration": 6.42}, {"text": "let's say, circles, right?", "start": 472.315, "duration": 2.19}, {"text": "And you want to learn,", "start": 474.505, "duration": 1.905}, {"text": "I wanna learn an algorithm that can separate these two classes, right?", "start": 476.41, "duration": 4.425}, {"text": "And, uh, if you imagine that the, uh, uh,", "start": 480.835, "duration": 5.59}, {"text": "what- what the algorithm has learned so far is a Theta that", "start": 486.425, "duration": 4.36}, {"text": "represents this decision boundary, right?", "start": 490.785, "duration": 3.9}, {"text": "So this represents, uh,", "start": 494.685, "duration": 2.325}, {"text": "Theta transpose x equals 0.", "start": 497.01, "duration": 2.85}, {"text": "And, uh, anything above is Theta transpose,", "start": 499.86, "duration": 4.57}, {"text": "uh, x is greater than 0.", "start": 504.43, "duration": 2.265}, {"text": "And anything below is Theta transpose x less than 0, all right?", "start": 506.695, "duration": 6.24}, {"text": "And let's say, um,", "start": 512.935, "duration": 1.23}, {"text": "the algorithm is learning one example at a time,", "start": 514.165, "duration": 2.58}, {"text": "and a new example comes in.", "start": 516.745, "duration": 1.635}, {"text": "Uh, and this time it happens to be- the new example happens to be a square, uh, or a box.", "start": 518.38, "duration": 9.02}, {"text": "And, uh, but the algorithm has mis- misclassified it, right?", "start": 527.4, "duration": 4.515}, {"text": "Now, um, this line, the separating boundary,", "start": 531.915, "duration": 4.725}, {"text": "um, if- if- if the vector equivalent of that would be a vector that's normal to the line.", "start": 536.64, "duration": 6.775}, {"text": "So, uh, this was- would be Theta, all right?", "start": 543.415, "duration": 2.775}, {"text": "And this is our new x,", "start": 546.19, "duration": 2.01}, {"text": "right? This is the new x.", "start": 548.2, "duration": 5.29}, {"text": "So this got misclassified, this, uh, uh,", "start": 553.74, "duration": 3.745}, {"text": "this is lying to, you know,", "start": 557.485, "duration": 1.59}, {"text": "lying on the bottom of the decision boundary.", "start": 559.075, "duration": 1.86}, {"text": "So what- what- what's gonna happen here?", "start": 560.935, "duration": 1.905}, {"text": "Um, y_i, let's call this the one class and this is- this is the zero class, right?", "start": 562.84, "duration": 6.195}, {"text": "So y_i minus- h state of i will be plus 1, right?", "start": 569.035, "duration": 5.475}, {"text": "And what the algorithm is doing is, uh,", "start": 574.51, "duration": 3.06}, {"text": "it sets Theta to be Theta plus Alpha times x, right?", "start": 577.57, "duration": 4.71}, {"text": "So this is the old Theta,", "start": 582.28, "duration": 2.07}, {"text": "this is x. Alpha is some small learning rate.", "start": 584.35, "duration": 3.51}, {"text": "So it adds- let me use a different color here.", "start": 587.86, "duration": 3.23}, {"text": "It adds, right, Alpha times x to", "start": 591.09, "duration": 6.09}, {"text": "Theta and now say this is- let's call it Theta prime, is the new vector.", "start": 597.18, "duration": 7.285}, {"text": "That's- that's the updated value, right?", "start": 604.465, "duration": 2.04}, {"text": "And the- and the separating, um, uh,", "start": 606.505, "duration": 2.475}, {"text": "hyperplane corresponding to this is something that is normal to it, right?", "start": 608.98, "duration": 5.745}, {"text": "Yeah. So- so it updated the, um,", "start": 614.725, "duration": 2.91}, {"text": "decision boundary such that x is now included in the positive class, right?", "start": 617.635, "duration": 4.425}, {"text": "The- the, um, idea here- here is that, um, Theta,", "start": 622.06, "duration": 7.395}, {"text": "we want Theta to be similar to x in general,", "start": 629.455, "duration": 4.635}, {"text": "where such- where y is 1.", "start": 634.09, "duration": 4.245}, {"text": "And we want Theta to be not similar to x when y equals 0.", "start": 638.335, "duration": 6.615}, {"text": "The reason is, uh,", "start": 644.95, "duration": 1.455}, {"text": "when two vectors are similar,", "start": 646.405, "duration": 1.485}, {"text": "the dot product is positive and they are not similar,", "start": 647.89, "duration": 2.34}, {"text": "the dot product is negative.", "start": 650.23, "duration": 1.29}, {"text": "Uh, what does that mean?", "start": 651.52, "duration": 1.155}, {"text": "If, uh, let's say this is,", "start": 652.675, "duration": 1.785}, {"text": "um, x and let's say you have Theta.", "start": 654.46, "duration": 3.195}, {"text": "If they're kind of, um,", "start": 657.655, "duration": 2.025}, {"text": "pointed outwards, their dot product would be, um, negative.", "start": 659.68, "duration": 4.05}, {"text": "And when- and if you have a Theta that looks like this,", "start": 663.73, "duration": 3.51}, {"text": "we call it Theta prime, then the dot product will be", "start": 667.24, "duration": 2.43}, {"text": "positive if the angle is- is less than r.", "start": 669.67, "duration": 2.145}, {"text": "So, um, this essentially means that as Theta is rotating,", "start": 671.815, "duration": 3.405}, {"text": "the, um, decision boundary is kind of perpendicular to Theta.", "start": 675.22, "duration": 3.885}, {"text": "And you wanna get all the positive x's on one side of the decision boundary.", "start": 679.105, "duration": 4.71}, {"text": "And what's the- what's the, uh,", "start": 683.815, "duration": 2.295}, {"text": "most naive way of- of- of taking Theta and given x,", "start": 686.11, "duration": 4.47}, {"text": "try to make Theta more kind of closer to x?", "start": 690.58, "duration": 2.76}, {"text": "A simple thing is to just add a component of x in that direction.", "start": 693.34, "duration": 4.365}, {"text": "You know, add it here and kind of make Theta.", "start": 697.705, "duration": 2.4}, {"text": "And so this- this is a very common technique used in lots of", "start": 700.105, "duration": 2.715}, {"text": "algorithms where if you add a vector to another vector,", "start": 702.82, "duration": 2.85}, {"text": "you make the second one kind of closer to the first one, essentially.", "start": 705.67, "duration": 4.14}, {"text": "So this is- this is,", "start": 709.81, "duration": 1.395}, {"text": "uh, the perceptron algorithm.", "start": 711.205, "duration": 1.335}, {"text": "Um, you go example by example in an online manner,", "start": 712.54, "duration": 4.125}, {"text": "and if the al- if the,", "start": 716.665, "duration": 1.935}, {"text": "um, example is already classified, you do nothing.", "start": 718.6, "duration": 2.595}, {"text": "You get a 0 over here.", "start": 721.195, "duration": 1.2}, {"text": "If it is misclassified,", "start": 722.395, "duration": 1.53}, {"text": "you either add the- add a small component of, uh,", "start": 723.925, "duration": 4.395}, {"text": "as, uh, you add the vector itself,", "start": 728.32, "duration": 2.415}, {"text": "the example itself to your Theta or you subtract it,", "start": 730.735, "duration": 2.355}, {"text": "depending on the class of the vector.", "start": 733.09, "duration": 2.115}, {"text": "This is about it. Any- any- any questions about the perceptron?", "start": 735.205, "duration": 3.925}, {"text": "Cool. So let's move on to the next topic, um, exponential families.", "start": 741.12, "duration": 8.215}, {"text": "Um, so, um, exponential family is essentially a class of- yeah.", "start": 749.335, "duration": 10.355}, {"text": "Why don't we use them in practice?", "start": 759.69, "duration": 3.81}, {"text": "Um, it's, um, not used in practice because,", "start": 763.5, "duration": 3.01}, {"text": "um, it- it does not have a probabilistic interpretation of what's- what's happening.", "start": 766.51, "duration": 5.28}, {"text": "You kinda have a geometrical feel of what's happening with- with", "start": 771.79, "duration": 2.55}, {"text": "the hyperplane but it- it doesn't have a probabilistic interpretation.", "start": 774.34, "duration": 3.09}, {"text": "Also, um, it's, um,", "start": 777.43, "duration": 3.705}, {"text": "it- it was- and I think the perceptron was,", "start": 781.135, "duration": 3.255}, {"text": "uh, pretty famous in, I think,", "start": 784.39, "duration": 1.38}, {"text": "the 1950s or the '60s where people thought this is a good model of how the brain works.", "start": 785.77, "duration": 4.875}, {"text": "And, uh, I think it was,", "start": 790.645, "duration": 2.175}, {"text": "uh, Marvin Minsky who wrote a paper saying, you know,", "start": 792.82, "duration": 2.66}, {"text": "the perceptron is- is kind of limited because it- it could never classify,", "start": 795.48, "duration": 4.87}, {"text": "uh, points like this.", "start": 800.35, "duration": 2.38}, {"text": "And there is no possible separating boundary that can,", "start": 803.07, "duration": 2.785}, {"text": "you know, do- do something as simple as this.", "start": 805.855, "duration": 1.635}, {"text": "And kind of people lost interest in it, but, um, yeah.", "start": 807.49, "duration": 3.765}, {"text": "And in fact, what- what we see is- is,", "start": 811.255, "duration": 2.385}, {"text": "uh, in logistic regression,", "start": 813.64, "duration": 1.47}, {"text": "it's like a software version of,", "start": 815.11, "duration": 1.77}, {"text": "uh, the perceptron itself in a way. Yeah.", "start": 816.88, "duration": 2.73}, {"text": "[inaudible]", "start": 819.61, "duration": 8.49}, {"text": "It's- it's, uh, it's up to,", "start": 828.1, "duration": 2.25}, {"text": "you know, it's- it's a design choice that you make.", "start": 830.35, "duration": 2.04}, {"text": "What you could do is you can- you can kind of,", "start": 832.39, "duration": 2.76}, {"text": "um, anneal your learning rate with every step, every time, uh,", "start": 835.15, "duration": 3.33}, {"text": "you see a new example decrease your learning rate until something,", "start": 838.48, "duration": 3.57}, {"text": "um, um, until you stop changing, uh, Theta by a lot.", "start": 842.05, "duration": 4.005}, {"text": "You can- you're not guaranteed that you'll- you'll be able to get every example right.", "start": 846.055, "duration": 4.035}, {"text": "For example here, no matter how long you learn you're- you're never gonna,", "start": 850.09, "duration": 2.925}, {"text": "you know, um, uh, find,", "start": 853.015, "duration": 1.725}, {"text": "uh, a learning boundary.", "start": 854.74, "duration": 1.26}, {"text": "So it's- it's up to you when you wanna stop training.", "start": 856.0, "duration": 2.76}, {"text": "Uh, a common thing is to just decrease the learning rate,", "start": 858.76, "duration": 3.09}, {"text": "uh, with every time step until you stop making changes.", "start": 861.85, "duration": 3.79}, {"text": "All right.", "start": 865.92, "duration": 3.16}, {"text": "Let's move on to exponential families.", "start": 869.08, "duration": 1.485}, {"text": "So, uh, exponential families is, uh,", "start": 870.565, "duration": 2.73}, {"text": "is a class of probability distributions,", "start": 873.295, "duration": 3.6}, {"text": "which are somewhat nice mathematically, right?", "start": 876.895, "duration": 2.115}, {"text": "Um, they're also very closely related to GLMs,", "start": 879.01, "duration": 4.845}, {"text": "which we will be going over next, right?", "start": 883.855, "duration": 3.54}, {"text": "But first we kind of take a deeper look at, uh,", "start": 887.395, "duration": 2.37}, {"text": "exponential families and, uh,", "start": 889.765, "duration": 2.475}, {"text": "and- and what they're about.", "start": 892.24, "duration": 1.185}, {"text": "So, uh, an exponential family is one, um, whose PDF,", "start": 893.425, "duration": 8.035}, {"text": "right? So whose PDF can be written in the", "start": 910.23, "duration": 3.55}, {"text": "form- by PDF I mean probability density function,", "start": 913.78, "duration": 4.02}, {"text": "but for a discrete, uh, distribution,", "start": 917.8, "duration": 1.93}, {"text": "then it would be the probability mass function, right?", "start": 919.73, "duration": 3.87}, {"text": "Whose PDF can be written in the form, um.", "start": 923.6, "duration": 3.61}, {"text": "All right. This looks pretty scary.", "start": 927.24, "duration": 23.005}, {"text": "Let's- let's- let's kind of, uh,", "start": 950.245, "duration": 1.455}, {"text": "break it down into, you know,", "start": 951.7, "duration": 1.17}, {"text": "what- what- what they actually mean.", "start": 952.87, "duration": 1.585}, {"text": "So y over here is the data, right?", "start": 954.455, "duration": 6.33}, {"text": "And there's a reason why we call it y because- yeah.", "start": 960.785, "duration": 2.655}, {"text": "Can you write a bit larger.", "start": 963.44, "duration": 2.76}, {"text": "A bit larger, sure.", "start": 966.2, "duration": 1.66}, {"text": "Is this better?", "start": 976.92, "duration": 13.615}, {"text": "Yeah. So y is the data.", "start": 990.535, "duration": 1.98}, {"text": "And the reason- there's a reason why we call it y and not x.", "start": 992.515, "duration": 2.67}, {"text": "And that- and that's because we're gonna use exponential families", "start": 995.185, "duration": 2.965}, {"text": "to model the output of your- of- of your data,", "start": 998.15, "duration": 2.945}, {"text": "you know, in a, uh, in a supervised learning setting.", "start": 1001.095, "duration": 1.995}, {"text": "Um, and- and you're gonna see x when we move on to GLMs.", "start": 1003.09, "duration": 3.73}, {"text": "Until, you know, until then we're just gonna deal with y's for now.", "start": 1006.82, "duration": 2.55}, {"text": "Uh, so y is the data.", "start": 1009.37, "duration": 2.19}, {"text": "Um, Eta is- is called the natural parameter.", "start": 1011.56, "duration": 5.59}, {"text": "T of y is called a sufficient statistic.", "start": 1023.03, "duration": 6.05}, {"text": "If you have a statistics background and you've learn- if", "start": 1029.99, "duration": 3.7}, {"text": "you come across the word sufficient statistic before, it's the exact same thing.", "start": 1033.69, "duration": 3.255}, {"text": "But you don't need to know much about this because", "start": 1036.945, "duration": 3.69}, {"text": "for all the distributions that we're gonna be seeing today,", "start": 1040.635, "duration": 4.065}, {"text": "uh, or in this class,", "start": 1044.7, "duration": 1.41}, {"text": "t of y will be equal to just y.", "start": 1046.11, "duration": 3.33}, {"text": "So you can, you can just replace t of y with y for,", "start": 1049.44, "duration": 3.435}, {"text": "um, for all the examples today and in the rest of the calcu- of the class.", "start": 1052.875, "duration": 5.205}, {"text": "Uh, b of y,", "start": 1058.08, "duration": 2.85}, {"text": "is called a base measure.", "start": 1060.93, "duration": 2.98}, {"text": "Right, and finally a of Eta,", "start": 1067.82, "duration": 5.095}, {"text": "is called the log-partition function.", "start": 1072.915, "duration": 3.625}, {"text": "And we're gonna be seeing a lot of this function, log-partition function.", "start": 1077.24, "duration": 5.155}, {"text": "Right, so, um, again, y is the data that,", "start": 1082.395, "duration": 4.905}, {"text": "uh, this probability distribution is trying to model.", "start": 1087.3, "duration": 2.775}, {"text": "Eta is the parameter of the distribution.", "start": 1090.075, "duration": 3.39}, {"text": "Um, t of y,", "start": 1093.465, "duration": 2.715}, {"text": "which will mostly be just y, um,", "start": 1096.18, "duration": 2.07}, {"text": "but technically you know, t of y is more, more correct.", "start": 1098.25, "duration": 3.24}, {"text": "Um, um, b of y,", "start": 1101.49, "duration": 3.66}, {"text": "which means it is a function of only y.", "start": 1105.15, "duration": 2.415}, {"text": "This function cannot involve Eta. All right.", "start": 1107.565, "duration": 2.715}, {"text": "And similarly t of y cannot involve Eta.", "start": 1110.28, "duration": 2.13}, {"text": "It should be purely a function of y. Um,", "start": 1112.41, "duration": 3.045}, {"text": "b of y is called the base measure,", "start": 1115.455, "duration": 2.355}, {"text": "and a of Eta,", "start": 1117.81, "duration": 1.44}, {"text": "which has to be a function of only Eta and, and constants.", "start": 1119.25, "duration": 3.03}, {"text": "No, no y can,", "start": 1122.28, "duration": 1.605}, {"text": "can, uh, can be part of a of, uh, Eta.", "start": 1123.885, "duration": 2.535}, {"text": "This is called the log-partition function.", "start": 1126.42, "duration": 2.205}, {"text": "Right. And, uh, the reason why this is called the log-partition function", "start": 1128.625, "duration": 5.085}, {"text": "is pretty easy to see because this can be written as b of y,", "start": 1133.71, "duration": 6.81}, {"text": "ex of Eta, times t of y over.", "start": 1140.52, "duration": 11.325}, {"text": "So these two are exactly the same.", "start": 1151.845, "duration": 2.175}, {"text": "Um, just take this out and, um, um.", "start": 1154.02, "duration": 5.49}, {"text": "Sorry, this should be the log.", "start": 1159.51, "duration": 3.01}, {"text": "I think it's fine. These two are exactly the same.", "start": 1171.68, "duration": 7.18}, {"text": "And, uh.", "start": 1178.86, "duration": 2.07}, {"text": "It should be the [inaudible] and that should be positive.", "start": 1180.93, "duration": 1.81}, {"text": "Oh, yeah, you're right. This should be positive, um. Thank you.", "start": 1182.78, "duration": 7.375}, {"text": "So, uh, this is, um,", "start": 1190.155, "duration": 2.97}, {"text": "you can think of this as a normalizing constant of the distribution such that the,", "start": 1193.125, "duration": 4.485}, {"text": "um, the whole thing integrates to 1, right?", "start": 1197.61, "duration": 2.535}, {"text": "Um, and, uh, therefore the log of this will be a of Eta,", "start": 1200.145, "duration": 4.395}, {"text": "that's why it's just called the log of the partition function.", "start": 1204.54, "duration": 1.92}, {"text": "So the partition function is a technical term to indicate", "start": 1206.46, "duration": 2.82}, {"text": "the normalizing constant of, uh, probability distributions.", "start": 1209.28, "duration": 3.42}, {"text": "Now, um, you can plug-in any definition of b,", "start": 1212.7, "duration": 8.144}, {"text": "a, and t. Yeah.", "start": 1220.844, "duration": 5.866}, {"text": "Sure. So why is your y,", "start": 1226.71, "duration": 3.87}, {"text": "and for most of, uh, most of our example is going to be a scalar.", "start": 1230.58, "duration": 4.17}, {"text": "Eta can be a vector.", "start": 1234.75, "duration": 3.24}, {"text": "But we will also be focusing, uh,", "start": 1237.99, "duration": 3.015}, {"text": "except maybe in Softmax,", "start": 1241.005, "duration": 1.83}, {"text": "um, this would be, uh, a scalar.", "start": 1242.835, "duration": 3.0}, {"text": "T of y has to match,", "start": 1245.835, "duration": 2.175}, {"text": "so these- the dimension of these two has to match [NOISE].", "start": 1248.01, "duration": 7.62}, {"text": "And these are scalars, right?", "start": 1255.63, "duration": 2.47}, {"text": "So for any choice of a,", "start": 1260.42, "duration": 3.97}, {"text": "b and t, that you've- that,", "start": 1264.39, "duration": 2.805}, {"text": "that, that can be your choice completely.", "start": 1267.195, "duration": 2.22}, {"text": "As long as the expression integrates to 1,", "start": 1269.415, "duration": 3.84}, {"text": "you have a family in the exponential family, right?", "start": 1273.255, "duration": 4.425}, {"text": "Uh, what does that mean?", "start": 1277.68, "duration": 1.605}, {"text": "For a specific choice of, say, for,", "start": 1279.285, "duration": 2.61}, {"text": "for, for some choice of a,", "start": 1281.895, "duration": 1.335}, {"text": "b, and t. This can actually- this will be equal to say the, uh,", "start": 1283.23, "duration": 4.215}, {"text": "PDF of the Gaussian, in which case you,", "start": 1287.445, "duration": 2.955}, {"text": "you got for that choice of t, a, and,", "start": 1290.4, "duration": 2.355}, {"text": "and b, you got the Gaussian distribution.", "start": 1292.755, "duration": 3.57}, {"text": "A family of Gaussian distribution such that for any value of the parameter,", "start": 1296.325, "duration": 4.74}, {"text": "you get a member of the Gaussian family. All right.", "start": 1301.065, "duration": 3.63}, {"text": "And this is mostly,", "start": 1304.695, "duration": 2.7}, {"text": "uh, to show that, uh,", "start": 1307.395, "duration": 2.115}, {"text": "a distribution is in the exponential family.", "start": 1309.51, "duration": 1.92}, {"text": "Um, the most straightforward way to do it", "start": 1311.43, "duration": 3.45}, {"text": "is to write out the PDF of the distribution in a form that you know,", "start": 1314.88, "duration": 3.76}, {"text": "and just do some algebraic massaging to bring it into this form, right?", "start": 1318.64, "duration": 4.495}, {"text": "And then you do a pattern match to, to and,", "start": 1323.135, "duration": 2.88}, {"text": "and, you know, conclude that it's a member of the exponential family.", "start": 1326.015, "duration": 3.855}, {"text": "So let's do it for a couple of examples.", "start": 1329.87, "duration": 2.86}, {"text": "So, uh, we have", "start": 1335.45, "duration": 2.59}, {"text": "[NOISE].", "start": 1338.04, "duration": 13.92}, {"text": "So, uh, a Bernoulli distribution is one you use to,", "start": 1351.96, "duration": 3.675}, {"text": "uh, model binary data.", "start": 1355.635, "duration": 3.265}, {"text": "Right. And it has a parameter, uh,", "start": 1361.31, "duration": 4.12}, {"text": "let's call it Phi, which is,", "start": 1365.43, "duration": 2.19}, {"text": "you know, the probability of the event happening or not.", "start": 1367.62, "duration": 2.73}, {"text": "Right, right. Now, the,", "start": 1370.35, "duration": 10.035}, {"text": "uh, what's the PDF of a Bernoulli distribution?", "start": 1380.385, "duration": 6.615}, {"text": "One way to, um,", "start": 1387.0, "duration": 1.74}, {"text": "write this is Phi of y,", "start": 1388.74, "duration": 4.935}, {"text": "times 1 minus Phi,", "start": 1393.675, "duration": 3.675}, {"text": "1 minus y. I think this makes sense.", "start": 1397.35, "duration": 3.465}, {"text": "This, this pattern is like, uh, uh,", "start": 1400.815, "duration": 3.165}, {"text": "a way of writing a programming- programmatic if else in,", "start": 1403.98, "duration": 4.125}, {"text": "in, in math. All right.", "start": 1408.105, "duration": 1.26}, {"text": "So whenever y is 1,", "start": 1409.365, "duration": 2.085}, {"text": "this term cancels out,", "start": 1411.45, "duration": 1.455}, {"text": "so the answer would be Phi.", "start": 1412.905, "duration": 2.205}, {"text": "And whenever y is 0 this term cancels out and the answer is 1 minus Phi.", "start": 1415.11, "duration": 4.575}, {"text": "So this is just a mathematical way to,", "start": 1419.685, "duration": 2.28}, {"text": "to represent an if else that you would do in programming, right.", "start": 1421.965, "duration": 3.81}, {"text": "So this is the PDF of, um, a Bernoulli.", "start": 1425.775, "duration": 3.45}, {"text": "And our goal is to take this form and massage it into that form, right,", "start": 1429.225, "duration": 6.63}, {"text": "and see what, what the individual t,", "start": 1435.855, "duration": 2.895}, {"text": "b, and a turn out to be, right.", "start": 1438.75, "duration": 2.145}, {"text": "So, uh, whenever you, you,", "start": 1440.895, "duration": 3.6}, {"text": "uh, see your distribution in this form, a common, um,", "start": 1444.495, "duration": 3.96}, {"text": "technique is to wrap", "start": 1448.455, "duration": 4.305}, {"text": "this with a log and then Exp.", "start": 1452.76, "duration": 7.35}, {"text": "Right, um, because these two cancel out so, uh,", "start": 1460.11, "duration": 3.81}, {"text": "this is actually exactly equal to this [NOISE].", "start": 1463.92, "duration": 8.97}, {"text": "And, uh, if you,", "start": 1472.89, "duration": 2.97}, {"text": "uh, do some more algebra and this, uh, we will see that,", "start": 1475.86, "duration": 3.66}, {"text": "this turns out to be Exp of log Phi over 1 minus Phi times y,", "start": 1479.52, "duration": 11.29}, {"text": "plus log of 1 minus Phi, right?", "start": 1491.03, "duration": 8.44}, {"text": "It's pretty straightforward to go from here to here.", "start": 1499.47, "duration": 2.355}, {"text": "Um, I'll, I'll let you guys,uh,", "start": 1501.825, "duration": 1.455}, {"text": "uh, verify it yourself.", "start": 1503.28, "duration": 2.235}, {"text": "But once we have it in this form, um,", "start": 1505.515, "duration": 2.445}, {"text": "it's easy to kind of start doing some pattern matching,", "start": 1507.96, "duration": 2.64}, {"text": "from this expression to, uh, that expression.", "start": 1510.6, "duration": 3.135}, {"text": "So what, what we see, um,", "start": 1513.735, "duration": 2.07}, {"text": "here is, uh, the base measure b of y is equal to.", "start": 1515.805, "duration": 5.215}, {"text": "If you match this with that,", "start": 1521.18, "duration": 2.83}, {"text": "b of y will be just 1.", "start": 1524.01, "duration": 2.535}, {"text": "Uh, because there's no b of y term here. All right.", "start": 1526.545, "duration": 3.99}, {"text": "And, um, so this would be b of y.", "start": 1530.535, "duration": 4.735}, {"text": "This would be Eta.", "start": 1535.88, "duration": 3.55}, {"text": "This would be t of y.", "start": 1539.43, "duration": 3.3}, {"text": "This would be a of Eta, right?", "start": 1542.73, "duration": 4.68}, {"text": "So that could be, uh, um,", "start": 1547.41, "duration": 3.27}, {"text": "you can see that the kind of matching pattern.", "start": 1550.68, "duration": 3.75}, {"text": "So b of y would be 1.", "start": 1554.43, "duration": 2.64}, {"text": "T of y is just y,", "start": 1557.07, "duration": 3.045}, {"text": "as, um, as expected.", "start": 1560.115, "duration": 3.295}, {"text": "Um, so Eta is equal to log Phi over 1 minus Phi.", "start": 1563.9, "duration": 10.61}, {"text": "And, uh, this is an equivalent statement is to invert this operation and say", "start": 1574.6, "duration": 8.975}, {"text": "Phi is equal to 1 over 1 plus e to the minus Eta.", "start": 1583.575, "duration": 9.625}, {"text": "I'm just flipping the operation from,", "start": 1593.33, "duration": 2.77}, {"text": "uh, this went from Phi to Eta here.", "start": 1596.1, "duration": 2.595}, {"text": "It's, it's, it's the equivalent.", "start": 1598.695, "duration": 2.385}, {"text": "Now, here it goes from Eta to Phi, right?", "start": 1601.08, "duration": 2.41}, {"text": "And a of Eta is going to be, um,", "start": 1603.49, "duration": 8.015}, {"text": "so here we have it as a function of Phi,", "start": 1611.505, "duration": 3.585}, {"text": "but we got an expression for Phi in terms of eta,", "start": 1615.09, "duration": 3.84}, {"text": "so you can plug this expression in here,", "start": 1618.93, "duration": 4.59}, {"text": "and that, uh, change of minus sign.", "start": 1623.52, "duration": 3.0}, {"text": "So, so, let, let me work out this,", "start": 1626.52, "duration": 1.71}, {"text": "minus log of 1 minus Phi.", "start": 1628.23, "duration": 4.665}, {"text": "This is, uh, just,", "start": 1632.895, "duration": 2.325}, {"text": "uh, the pattern matching there.", "start": 1635.22, "duration": 2.16}, {"text": "And minus log 1 minus,", "start": 1637.38, "duration": 4.86}, {"text": "this thing over, 1 over 1 plus Eta to the minus Eta.", "start": 1642.24, "duration": 4.44}, {"text": "The reason is because we want an expression in terms of Eta.", "start": 1646.68, "duration": 3.345}, {"text": "Here we got it in terms of Phi, but we need to,", "start": 1650.025, "duration": 2.49}, {"text": "uh, plug in, um, plug in Eta over here.", "start": 1652.515, "duration": 3.33}, {"text": "Uh, Eta, and this will just be,", "start": 1655.845, "duration": 2.775}, {"text": "uh, log of 1 plus e to the Eta.", "start": 1658.62, "duration": 5.17}, {"text": "Right. So there you go.", "start": 1664.13, "duration": 3.22}, {"text": "So this, this kind of, uh,", "start": 1667.35, "duration": 1.845}, {"text": "verifies that the Bernoulli distribution is a member of the exponential family.", "start": 1669.195, "duration": 4.8}, {"text": "Any questions here? So note that this may look familiar.", "start": 1673.995, "duration": 7.11}, {"text": "It looks like the, uh,", "start": 1681.105, "duration": 2.31}, {"text": "sigmoid function, somewhat like the sigmoid function,", "start": 1683.415, "duration": 2.745}, {"text": "and there's actually no accident.", "start": 1686.16, "duration": 1.44}, {"text": "We'll see, uh, why, why it's, uh,", "start": 1687.6, "duration": 2.115}, {"text": "actually the sigmoid- how,", "start": 1689.715, "duration": 3.195}, {"text": "how it kind of relates to,", "start": 1692.91, "duration": 1.26}, {"text": "uh, logistic regression in a minute.", "start": 1694.17, "duration": 1.725}, {"text": "So another example, um", "start": 1695.895, "duration": 2.745}, {"text": "[NOISE].", "start": 1698.64, "duration": 9.555}, {"text": "So, uh, a Gaussian with fixed variance.", "start": 1708.195, "duration": 10.675}, {"text": "Right, so, um, a Gaussian distribution,", "start": 1721.1, "duration": 3.594}, {"text": "um, has two parameters the mean and the variance, uh,", "start": 1724.694, "duration": 4.486}, {"text": "for our purposes we're gonna assume a constant variance, um,", "start": 1729.18, "duration": 4.29}, {"text": "you-you can, uh, have,", "start": 1733.47, "duration": 2.22}, {"text": "um, you can also consider Gaussians with,", "start": 1735.69, "duration": 3.39}, {"text": "with where the variance is also a variable,", "start": 1739.08, "duration": 2.145}, {"text": "but for-for, uh, our course we are go- we are only interested in, um,", "start": 1741.225, "duration": 4.08}, {"text": "Gaussians with fixed variance and we are going to assume,", "start": 1745.305, "duration": 4.465}, {"text": "assume that variance is equal to 1.", "start": 1751.19, "duration": 5.05}, {"text": "So, this gives the PDF of a Gaussian to look like this,", "start": 1756.24, "duration": 4.59}, {"text": "p of y parameterized as mu. So note here,", "start": 1760.83, "duration": 5.85}, {"text": "when we start writing out,", "start": 1766.68, "duration": 1.11}, {"text": "we start with the, uh,", "start": 1767.79, "duration": 2.01}, {"text": "parameters that we are, um,", "start": 1769.8, "duration": 2.94}, {"text": "commonly used to, and we- they are also called like the canonical parameters.", "start": 1772.74, "duration": 3.69}, {"text": "And then we set up a link between the canonical parameters and the natural parameters,", "start": 1776.43, "duration": 4.65}, {"text": "that's part of the massaging exercise that we do.", "start": 1781.08, "duration": 3.06}, {"text": "So we're going to start with the canonical parameters, um,", "start": 1784.14, "duration": 3.64}, {"text": "is equal to 1 over root 2", "start": 1787.78, "duration": 3.62}, {"text": "pi, minus over 2.", "start": 1791.4, "duration": 11.28}, {"text": "So this is the Gaussian PDF with,", "start": 1802.68, "duration": 3.615}, {"text": "um, with- with a variance equal to 1, right,", "start": 1806.295, "duration": 3.81}, {"text": "and this can be rewritten as- again,", "start": 1810.105, "duration": 5.235}, {"text": "I'm skipping a few algebra steps, you know,", "start": 1815.34, "duration": 2.925}, {"text": "straightforward no tricks there,", "start": 1818.265, "duration": 2.755}, {"text": "uh, any question? Yep?", "start": 1821.3, "duration": 2.62}, {"text": "[BACKGROUND].", "start": 1823.92, "duration": 2.61}, {"text": "Fixed variance. E to the minus y squared over 2, times EX.", "start": 1826.53, "duration": 9.94}, {"text": "Again, we go to the same exercise,", "start": 1843.77, "duration": 3.955}, {"text": "you know, pattern match, this is b of y,", "start": 1847.725, "duration": 4.345}, {"text": "this is eta, this is t of y,", "start": 1852.41, "duration": 6.82}, {"text": "and this would be a of eta, right?", "start": 1859.23, "duration": 6.08}, {"text": "So, uh, we have, uh,", "start": 1865.31, "duration": 1.5}, {"text": "b of y equals 1 over root 2", "start": 1866.81, "duration": 4.875}, {"text": "pi minus y squared by 2.", "start": 1871.685, "duration": 5.785}, {"text": "Note that this is a function of only y,", "start": 1877.47, "duration": 2.4}, {"text": "there's no eta here,", "start": 1879.87, "duration": 1.455}, {"text": "um, t of y is just y, and in this case,", "start": 1881.325, "duration": 4.785}, {"text": "the natural parameter is-is mu, eta is mu,", "start": 1886.11, "duration": 3.96}, {"text": "and the log partition function is equal to mu square by 2,", "start": 1890.07, "duration": 7.68}, {"text": "and when we-and we repeat the same exercise we did here,", "start": 1897.75, "duration": 5.55}, {"text": "we start with a log partition function that is parameterized by the canonical parameters,", "start": 1903.3, "duration": 6.615}, {"text": "and we use the,", "start": 1909.915, "duration": 2.04}, {"text": "the link between the canonical and, and,", "start": 1911.955, "duration": 2.43}, {"text": "uh, the natural parameters, invert it and,", "start": 1914.385, "duration": 3.255}, {"text": "um, um, so in this case it's- it's the- it's the same sets, eta over 2.", "start": 1917.64, "duration": 6.975}, {"text": "So, a of eta is a function of only eta,", "start": 1924.615, "duration": 2.73}, {"text": "again here a of eta was a function of only eta,", "start": 1927.345, "duration": 2.61}, {"text": "and, um, p of y is a function of only y,", "start": 1929.955, "duration": 3.435}, {"text": "and b of y is a function of only,", "start": 1933.39, "duration": 1.59}, {"text": "um, y as well.", "start": 1934.98, "duration": 2.32}, {"text": "Any questions on this? Yeah.", "start": 1938.66, "duration": 5.35}, {"text": "If the variance is unknown [inaudible].", "start": 1944.01, "duration": 4.35}, {"text": "Yeah, you- if, if the variance is unknown you can write it as", "start": 1948.36, "duration": 2.76}, {"text": "an exponential family in which case eta will now be a vector,", "start": 1951.12, "duration": 3.21}, {"text": "it won't be a scalar anymore,", "start": 1954.33, "duration": 1.17}, {"text": "it'll be- it'll have two, uh,", "start": 1955.5, "duration": 1.59}, {"text": "like eta1 and eta2,", "start": 1957.09, "duration": 1.575}, {"text": "and you will also have, um,", "start": 1958.665, "duration": 3.525}, {"text": "you will have a mapping between each of", "start": 1962.19, "duration": 2.7}, {"text": "the canonical parameters and each of the natural parameters,", "start": 1964.89, "duration": 3.449}, {"text": "you, you can do it, uh, you know, it's pretty straightforward.", "start": 1968.339, "duration": 3.286}, {"text": "Right, so this is- this is exponential- these are exponential families, right?", "start": 1971.625, "duration": 6.87}, {"text": "Uh, the reason why we are, uh,", "start": 1978.495, "duration": 2.055}, {"text": "why we use exponential families is because it has", "start": 1980.55, "duration": 2.22}, {"text": "some nice mathematical properties, right?", "start": 1982.77, "duration": 8.13}, {"text": "So, uh, so one property is now,", "start": 1990.9, "duration": 5.67}, {"text": "uh, if we perform maximum likelihood on,", "start": 1996.57, "duration": 4.035}, {"text": "um, on the exponential family,", "start": 2000.605, "duration": 2.085}, {"text": "um, as, as, uh,", "start": 2002.69, "duration": 3.18}, {"text": "when, when the exponential family is parameterized in the natural parameters,", "start": 2005.87, "duration": 4.109}, {"text": "then, uh, the optimization problem is concave.", "start": 2009.979, "duration": 4.246}, {"text": "So MLE with respect to eta is concave.", "start": 2014.225, "duration": 9.73}, {"text": "Similarly, if you, uh,", "start": 2023.955, "duration": 2.42}, {"text": "flip this sign and use the, the, uh,", "start": 2026.375, "duration": 2.205}, {"text": "what's called the negative log-likelihood,", "start": 2028.58, "duration": 1.829}, {"text": "so you take the log of the expression negate it and in this case,", "start": 2030.409, "duration": 3.556}, {"text": "the negative log-likelihood is like", "start": 2033.965, "duration": 1.815}, {"text": "the cost function equivalent of doing maximum likelihood,", "start": 2035.78, "duration": 2.925}, {"text": "so you're just flipping the sign, instead of maximizing,", "start": 2038.705, "duration": 1.935}, {"text": "you minimize the negative log likelihood,", "start": 2040.64, "duration": 1.59}, {"text": "so-and, and you know, uh,", "start": 2042.23, "duration": 2.085}, {"text": "the NLL is therefore convex, okay.", "start": 2044.315, "duration": 5.97}, {"text": "Um, the expectation of y.", "start": 2050.285, "duration": 8.065}, {"text": "What does this mean? Um, each of the distribution,", "start": 2065.71, "duration": 6.205}, {"text": "uh, we start with, uh, a of eta,", "start": 2071.915, "duration": 2.865}, {"text": "differentiate this with respect to eta,", "start": 2074.78, "duration": 2.715}, {"text": "the log partition function with respect to eta,", "start": 2077.495, "duration": 2.22}, {"text": "and you get another function with respect to eta,", "start": 2079.715, "duration": 3.465}, {"text": "and that function will- is,", "start": 2083.18, "duration": 1.89}, {"text": "is the mean of the distribution as parameterized by eta,", "start": 2085.07, "duration": 3.66}, {"text": "and similarly the variance of y parameterized by eta,", "start": 2088.73, "duration": 9.45}, {"text": "is just the second derivative,", "start": 2098.18, "duration": 1.59}, {"text": "this was the first derivative,", "start": 2099.77, "duration": 1.05}, {"text": "this is the second derivative, this is eta.", "start": 2100.82, "duration": 7.29}, {"text": "So, um, the reason why", "start": 2108.11, "duration": 5.01}, {"text": "this is nice is because in general for", "start": 2113.12, "duration": 2.4}, {"text": "probability distributions to calculate the mean and the variance,", "start": 2115.52, "duration": 2.715}, {"text": "you generally need to integrate something,", "start": 2118.235, "duration": 1.8}, {"text": "but over here you just need to differentiate,", "start": 2120.035, "duration": 1.635}, {"text": "which is a lot easier operation, all right?", "start": 2121.67, "duration": 2.19}, {"text": "And, um, and you", "start": 2123.86, "duration": 8.04}, {"text": "will be proving these properties in your first homework.", "start": 2131.9, "duration": 3.91}, {"text": "You're provided hints so it should be [LAUGHTER].", "start": 2138.91, "duration": 4.72}, {"text": "All right, so, um,", "start": 2143.63, "duration": 2.325}, {"text": "now we're going to move on to,", "start": 2145.955, "duration": 2.205}, {"text": "uh, generalized linear models, uh,", "start": 2148.16, "duration": 1.86}, {"text": "this- this is all we wanna talk about exponential families, any questions? Yep.", "start": 2150.02, "duration": 9.57}, {"text": "[inaudible].", "start": 2159.59, "duration": 6.54}, {"text": "Exactly, so, ah, if you're-if you're, um,", "start": 2166.13, "duration": 2.76}, {"text": "if you're- if it's a multi-variate Gaussian,", "start": 2168.89, "duration": 2.64}, {"text": "then this eta would be a vector,", "start": 2171.53, "duration": 2.505}, {"text": "and this would be the Hessian.", "start": 2174.035, "duration": 2.515}, {"text": "All right, let's move on to, uh, GLM's.", "start": 2182.44, "duration": 4.28}, {"text": "So the GLM is, is, um,", "start": 2195.21, "duration": 3.595}, {"text": "somewhat like a natural extension of the exponential families to include,", "start": 2198.805, "duration": 5.235}, {"text": "um, include covariates or include your input features in some way, right.", "start": 2204.04, "duration": 5.58}, {"text": "So over here, uh,", "start": 2209.62, "duration": 1.71}, {"text": "we are only dealing with,", "start": 2211.33, "duration": 1.14}, {"text": "uh, in, in the exponential families,", "start": 2212.47, "duration": 1.59}, {"text": "you're only dealing with like the y, uh, which in,", "start": 2214.06, "duration": 2.925}, {"text": "in our case, it- it'll kind of map to the outputs, um.", "start": 2216.985, "duration": 3.855}, {"text": "But, um, we can actually build a lot of many powerful models by,", "start": 2220.84, "duration": 7.74}, {"text": "by choosing, uh, an appropriate, um, um,", "start": 2228.58, "duration": 4.845}, {"text": "family in the exponential family and kind of plugging it onto a, a linear model.", "start": 2233.425, "duration": 5.985}, {"text": "So, so the, uh,", "start": 2239.41, "duration": 1.665}, {"text": "assumptions we are going to make for GLM is that one, um,", "start": 2241.075, "duration": 5.22}, {"text": "so these are the assumptions or", "start": 2246.295, "duration": 6.585}, {"text": "design choices that are gonna take us from exponential families to,", "start": 2252.88, "duration": 7.89}, {"text": "uh, generalized linear models.", "start": 2260.77, "duration": 1.57}, {"text": "So the most important assumption is that, uh, well, yeah.", "start": 2262.34, "duration": 4.09}, {"text": "Assumption is that y given x parameterized", "start": 2266.43, "duration": 5.52}, {"text": "by Theta is a member of an exponential family.", "start": 2271.95, "duration": 6.37}, {"text": "Right. By exponential family of Theta,", "start": 2286.41, "duration": 3.16}, {"text": "I mean that form.", "start": 2289.57, "duration": 2.025}, {"text": "It could, it could, uh, in,", "start": 2291.595, "duration": 2.16}, {"text": "in the particular, uh, uh, uh,", "start": 2293.755, "duration": 2.475}, {"text": "scenario that you have, it could take on any one of these, um, uh, distributions.", "start": 2296.23, "duration": 4.965}, {"text": "Um, we only, we only,", "start": 2301.195, "duration": 2.85}, {"text": "uh, talked about the Bernoullian Gaussian.", "start": 2304.045, "duration": 2.25}, {"text": "There are also, um,", "start": 2306.295, "duration": 1.845}, {"text": "other distributions that are- those are part of the, uh, exponential family.", "start": 2308.14, "duration": 4.8}, {"text": "For example, um, I forgot to mention this.", "start": 2312.94, "duration": 3.42}, {"text": "So if you have, uh,", "start": 2316.36, "duration": 2.28}, {"text": "real value data, you use a Gaussian.", "start": 2318.64, "duration": 4.78}, {"text": "If you have binary, a Bernoulli.", "start": 2323.85, "duration": 6.41}, {"text": "If you have count,", "start": 2331.44, "duration": 2.02}, {"text": "uh, like, counts here.", "start": 2333.46, "duration": 3.135}, {"text": "And so this is a real value.", "start": 2336.595, "duration": 1.8}, {"text": "It can take any value between zero and infinity by count.", "start": 2338.395, "duration": 2.595}, {"text": "That means just non-negative integers,", "start": 2340.99, "duration": 2.745}, {"text": "uh, but not anything between it.", "start": 2343.735, "duration": 1.65}, {"text": "So if you have counts, you can use a Poisson.", "start": 2345.385, "duration": 2.785}, {"text": "If you have uh, positive real value integers like say,", "start": 2348.21, "duration": 6.655}, {"text": "the volume of some object or a time to an event which,", "start": 2354.865, "duration": 3.72}, {"text": "you know, um, that you are only predicting into the future.", "start": 2358.585, "duration": 2.115}, {"text": "So here, you can use, uh,", "start": 2360.7, "duration": 1.86}, {"text": "like Gamma or exponential.", "start": 2362.56, "duration": 5.11}, {"text": "So, um, so there is the exponential family,", "start": 2369.3, "duration": 3.91}, {"text": "and there is also a distribution called the exponential distribution,", "start": 2373.21, "duration": 2.775}, {"text": "which are, you know, two distinct things.", "start": 2375.985, "duration": 1.935}, {"text": "The exponential distribution happens to be a member of the exponential family as well,", "start": 2377.92, "duration": 3.855}, {"text": "but no, they're not the same thing.", "start": 2381.775, "duration": 1.77}, {"text": "Um, the exponential and, um, yeah,", "start": 2383.545, "duration": 3.915}, {"text": "and you can also have, um,", "start": 2387.46, "duration": 1.62}, {"text": "you can also have probability distributions over probability distributions.", "start": 2389.08, "duration": 5.11}, {"text": "Like, uh, the Beta, the Dirichlet.", "start": 2394.92, "duration": 5.81}, {"text": "These mostly show up in Bayesian machine learning or Bayesian statistics.", "start": 2401.31, "duration": 5.035}, {"text": "Right. So depending on the kind of data that you have,", "start": 2406.345, "duration": 8.67}, {"text": "if your y-variable is, is,", "start": 2415.015, "duration": 2.31}, {"text": "is if you're trying to do a regression,", "start": 2417.325, "duration": 1.68}, {"text": "then your y is going to be say, say a Gaussian.", "start": 2419.005, "duration": 2.415}, {"text": "If you're trying to do a classification, then your y is,", "start": 2421.42, "duration": 3.285}, {"text": "and if it's a binary classification,", "start": 2424.705, "duration": 1.335}, {"text": "then the exponential family would be Bernoulli.", "start": 2426.04, "duration": 2.25}, {"text": "So depending on the problem that you have,", "start": 2428.29, "duration": 1.98}, {"text": "you can choose any member of the exponential family,", "start": 2430.27, "duration": 3.045}, {"text": "um, as, as parameterized by Eta.", "start": 2433.315, "duration": 5.085}, {"text": "And so that's the first assumption.", "start": 2438.4, "duration": 2.49}, {"text": "That y conditioned on y given x is a member of the exponential family.", "start": 2440.89, "duration": 5.835}, {"text": "And the, uh, second, the design choice that we are making here is", "start": 2446.725, "duration": 4.425}, {"text": "that Eta is equal to Theta transpose x.", "start": 2451.15, "duration": 5.055}, {"text": "So this is where your x now comes into the picture.", "start": 2456.205, "duration": 3.1}, {"text": "Right. So Theta is, um,", "start": 2459.305, "duration": 3.935}, {"text": "is in Rn, and x is also in Rn.", "start": 2463.85, "duration": 7.27}, {"text": "Now, this n has nothing to do with anything in the exponential family.", "start": 2471.12, "duration": 4.93}, {"text": "It's purely, um, a dimensions of your of,", "start": 2476.05, "duration": 3.315}, {"text": "of your data that you have,", "start": 2479.365, "duration": 1.335}, {"text": "of the x's of your inputs,", "start": 2480.7, "duration": 1.575}, {"text": "and,  and this does not show up anywhere else. And that, that- that's, um.", "start": 2482.275, "duration": 4.465}, {"text": "And, and, uh, Eta is, is, uh, we,", "start": 2487.02, "duration": 5.59}, {"text": "we make a design choice that Eta will be Theta transpose- transpose x. Um, and", "start": 2492.61, "duration": 7.23}, {"text": "another kind of assumption is that at test time, um, right.", "start": 2499.84, "duration": 9.45}, {"text": "When we want an output for a new x,", "start": 2509.29, "duration": 2.52}, {"text": "given a new x, we want to make an output, right.", "start": 2511.81, "duration": 2.39}, {"text": "So the output will be, right.", "start": 2514.2, "duration": 10.35}, {"text": "So given an x and,", "start": 2524.55, "duration": 1.26}, {"text": "um, given an x,", "start": 2525.81, "duration": 1.745}, {"text": "we get, uh, an exponential family distribution, right.", "start": 2527.555, "duration": 3.89}, {"text": "And the mean of that distribution will be the prediction that we make for a given,", "start": 2531.445, "duration": 4.905}, {"text": "for a given x. Um, this may sound a little abstract, but, you know,", "start": 2536.35, "duration": 3.585}, {"text": "uh, we're going to make this, uh, uh, more clear.", "start": 2539.935, "duration": 2.175}, {"text": "So this- what this essentially means is that the hypothesis function", "start": 2542.11, "duration": 4.21}, {"text": "is actually just, right.", "start": 2546.33, "duration": 6.85}, {"text": "This is our hypothesis function.", "start": 2553.18, "duration": 1.635}, {"text": "And we will see that, you know, what we do over here,", "start": 2554.815, "duration": 2.355}, {"text": "if you plug in the,", "start": 2557.17, "duration": 1.425}, {"text": "uh, um, exponential family,", "start": 2558.595, "duration": 2.055}, {"text": "uh, as, as Gaussian,", "start": 2560.65, "duration": 1.275}, {"text": "then the hypothesis will be the same, you know,", "start": 2561.925, "duration": 2.19}, {"text": "Gaussian hypothesis that we saw in linear regression.", "start": 2564.115, "duration": 2.535}, {"text": "If we plug in a Bernoulli,", "start": 2566.65, "duration": 2.07}, {"text": "then this will turn out to be the same hypothesis that we saw in logistic regression,", "start": 2568.72, "duration": 4.08}, {"text": "and so on, right.", "start": 2572.8, "duration": 2.28}, {"text": "So, uh, one way to kind of,", "start": 2575.08, "duration": 2.115}, {"text": "um, visualize this is,", "start": 2577.195, "duration": 3.295}, {"text": "right. So one way to think of is,", "start": 2620.37, "duration": 3.34}, {"text": "of- if this is, there is a model and there is a distribution, right.", "start": 2623.71, "duration": 3.66}, {"text": "So the model we are assuming it to be a linear model, right.", "start": 2627.37, "duration": 3.18}, {"text": "Given x, there is a learnable parameter Theta,", "start": 2630.55, "duration": 2.82}, {"text": "and Theta transpose x will give you a parameter, right.", "start": 2633.37, "duration": 3.615}, {"text": "This is the model,", "start": 2636.985, "duration": 1.23}, {"text": "and here is the distribution.", "start": 2638.215, "duration": 1.665}, {"text": "Now, the distribution, um,", "start": 2639.88, "duration": 2.385}, {"text": "is a member of the exponential family.", "start": 2642.265, "duration": 2.295}, {"text": "And the parameter for this distribution is the output of the linear model, right.", "start": 2644.56, "duration": 5.55}, {"text": "This, this is the picture you want to have in your mind.", "start": 2650.11, "duration": 2.28}, {"text": "And the exponential family,", "start": 2652.39, "duration": 2.385}, {"text": "we make, uh, depending on the data that we have.", "start": 2654.775, "duration": 2.655}, {"text": "Whether it's a, you know, whether it's, uh,", "start": 2657.43, "duration": 1.38}, {"text": "a classification problem or a regression problem or a time to vent problem,", "start": 2658.81, "duration": 3.345}, {"text": "you would choose an appropriate b,", "start": 2662.155, "duration": 3.63}, {"text": "a and t, uh,", "start": 2665.785, "duration": 2.13}, {"text": "based on the distribution of your choice, right.", "start": 2667.915, "duration": 3.54}, {"text": "So this entire thing, uh,", "start": 2671.455, "duration": 2.64}, {"text": "a-and from this, you can say, uh,", "start": 2674.095, "duration": 3.24}, {"text": "get the, uh, expectation of y given Eta.", "start": 2677.335, "duration": 7.995}, {"text": "And this is the same as expectation of y given Theta transpose x, right.", "start": 2685.33, "duration": 8.79}, {"text": "And this is essentially our hypothesis function.", "start": 2694.12, "duration": 4.42}, {"text": "Right.", "start": 2699.39, "duration": 10.06}, {"text": "Yep. [BACKGROUND] That's exactly right.", "start": 2712.56, "duration": 2.035}, {"text": "Uh, so, uh, so the question is,", "start": 2714.595, "duration": 2.325}, {"text": "um, are we training Theta to, uh, uh, um,", "start": 2716.92, "duration": 4.89}, {"text": "to predict the parameter of the, um,", "start": 2721.81, "duration": 3.21}, {"text": "exponential family distribution whose mean is,", "start": 2725.02, "duration": 3.3}, {"text": "um, the, uh, uh,", "start": 2728.32, "duration": 1.86}, {"text": "uh, prediction that we're gonna make for y.", "start": 2730.18, "duration": 1.41}, {"text": "That's, that's correct, right.", "start": 2731.59, "duration": 2.73}, {"text": "And, um, so this is what we do at test time, right.", "start": 2734.32, "duration": 5.955}, {"text": "And during training time,", "start": 2740.275, "duration": 2.845}, {"text": "how do we train this model?", "start": 2744.33, "duration": 2.29}, {"text": "So in this model,", "start": 2746.62, "duration": 1.14}, {"text": "the parameter that we are learning by doing gradient descent,", "start": 2747.76, "duration": 3.18}, {"text": "are these parameters, right.", "start": 2750.94, "duration": 1.95}, {"text": "So you're not learning any the parameters in the,", "start": 2752.89, "duration": 4.02}, {"text": "uh, in the, uh, uh, exponential family.", "start": 2756.91, "duration": 2.22}, {"text": "We're not learning Mu or Sigma square or, or Eta.", "start": 2759.13, "duration": 3.69}, {"text": "We are not learning those. We're learning Theta that's part of the model,", "start": 2762.82, "duration": 2.88}, {"text": "and not part of, uh, the distribution.", "start": 2765.7, "duration": 1.86}, {"text": "And the output of this will become the,", "start": 2767.56, "duration": 3.0}, {"text": "um, the distributions parameter.", "start": 2770.56, "duration": 1.92}, {"text": "It's unfortunate that we use the word parameter for this and that, but, uh, there,", "start": 2772.48, "duration": 5.7}, {"text": "there are- it's important to understand what,", "start": 2778.18, "duration": 2.91}, {"text": "what is being learned during training phase and, and, and what's not.", "start": 2781.09, "duration": 4.605}, {"text": "So this parameter is the output of a function.", "start": 2785.695, "duration": 2.985}, {"text": "It's not, it's not a variable that we,", "start": 2788.68, "duration": 2.955}, {"text": "that we, uh, do gradient descent on.", "start": 2791.635, "duration": 1.815}, {"text": "So during learning, what we do is maximum likelihood.", "start": 2793.45, "duration": 5.805}, {"text": "Maximize with respect to Theta of P of", "start": 2799.255, "duration": 6.945}, {"text": "y i given, right.", "start": 2806.2, "duration": 8.025}, {"text": "So you're doing gradient ascent on the log probability of,", "start": 2814.225, "duration": 6.495}, {"text": "of y where, um, the, the, um,", "start": 2820.72, "duration": 4.71}, {"text": "natural parameter was re-parameterized, uh,", "start": 2825.43, "duration": 2.49}, {"text": "with the linear model, right.", "start": 2827.92, "duration": 2.655}, {"text": "And we are doing gradient ascent by taking gradients on Theta, right.", "start": 2830.575, "duration": 4.5}, {"text": "Thi-this is like the big picture of what's happening with GLMs,", "start": 2835.075, "duration": 2.415}, {"text": "and how they kind of,", "start": 2837.49, "duration": 1.695}, {"text": "yeah, are an extension of exponential families.", "start": 2839.185, "duration": 2.355}, {"text": "You re-parameterize the parameters with the linear model,", "start": 2841.54, "duration": 2.805}, {"text": "and you get a GLM.", "start": 2844.345, "duration": 1.275}, {"text": "[NOISE].", "start": 2845.62, "duration": 14.61}, {"text": "So let's, let's look at, uh,", "start": 2861.05, "duration": 2.665}, {"text": "some more detail on what happens at train time.", "start": 2863.715, "duration": 2.055}, {"text": "[NOISE]", "start": 2865.77, "duration": 34.77}, {"text": "So another, um,", "start": 2900.54, "duration": 2.135}, {"text": "kind of incidental benefit of using, uh, uh,", "start": 2902.675, "duration": 3.115}, {"text": "GLMs is", "start": 2905.79, "duration": 8.1}, {"text": "that at train time,", "start": 2913.89, "duration": 4.125}, {"text": "we saw that you wanna do, um,", "start": 2918.015, "duration": 2.685}, {"text": "maximum likelihood on the log prob- using", "start": 2920.7, "duration": 2.97}, {"text": "the log probability with respect to Thetas, right?", "start": 2923.67, "duration": 3.255}, {"text": "Now, um, at first it may appear that,", "start": 2926.925, "duration": 4.62}, {"text": "you know, we need to do some more algebra, uh,", "start": 2931.545, "duration": 2.205}, {"text": "figure out what the expression for, you know,", "start": 2933.75, "duration": 2.505}, {"text": "P is, um, represented in the- in-", "start": 2936.255, "duration": 3.3}, {"text": "in- as a function of Theta transpose x and take the derivatives and,", "start": 2939.555, "duration": 3.495}, {"text": "you know, come up with a gradient update rule and so on.", "start": 2943.05, "duration": 3.645}, {"text": "But it turns out that,", "start": 2946.695, "duration": 1.89}, {"text": "uh, no matter which- uh,", "start": 2948.585, "duration": 4.485}, {"text": "what kind of GLM you're doing,", "start": 2953.07, "duration": 1.65}, {"text": "no matter which choice of distribution that you make,", "start": 2954.72, "duration": 2.265}, {"text": "the learning update rule is the same.", "start": 2956.985, "duration": 4.905}, {"text": "[NOISE] The learning update rule is Theta equals", "start": 2961.89, "duration": 10.845}, {"text": "Theta j plus Alpha times y_i", "start": 2972.735, "duration": 6.12}, {"text": "minus h Theta of x_i.", "start": 2978.855, "duration": 7.765}, {"text": "You guys have seen this so many times by now.", "start": 2987.2, "duration": 3.205}, {"text": "So this is- you can,", "start": 2990.405, "duration": 2.28}, {"text": "you can straight away just apply this learning rule without ever having to,", "start": 2992.685, "duration": 6.015}, {"text": "um, do any more algebra to figure out what", "start": 2998.7, "duration": 3.18}, {"text": "the gradients are or what the- what, what the loss is.", "start": 3001.88, "duration": 3.42}, {"text": "You can go straight to the update rule and do your learning.", "start": 3005.3, "duration": 2.79}, {"text": "You plug in the appropriate h Theta of x,", "start": 3008.09, "duration": 3.93}, {"text": "you plug in the appropriate h Theta of x, uh,", "start": 3013.0, "duration": 3.325}, {"text": "depending on the choice of distribution that you make and you can start learning.", "start": 3016.325, "duration": 3.69}, {"text": "Initialize your Theta to some random values and,", "start": 3020.015, "duration": 2.985}, {"text": "and, and you can start learning.", "start": 3023.0, "duration": 1.89}, {"text": "So um, any question on this? Yeah.", "start": 3024.89, "duration": 5.28}, {"text": "[inaudible]", "start": 3030.17, "duration": 4.74}, {"text": "You can do, uh- if you wanna do it for batch gradient descent,", "start": 3034.91, "duration": 4.56}, {"text": "then you just, um,", "start": 3039.47, "duration": 1.725}, {"text": "sum over all your examples.", "start": 3041.195, "duration": 1.815}, {"text": "[inaudible]", "start": 3043.01, "duration": 7.74}, {"text": "Yeah. So, um, the uh,", "start": 3050.75, "duration": 2.535}, {"text": "Newton method is, is, uh,", "start": 3053.285, "duration": 1.83}, {"text": "is probably the most common you would use with GLMs, uh,", "start": 3055.115, "duration": 3.12}, {"text": "and that again comes with the assumption that you're- the", "start": 3058.235, "duration": 2.655}, {"text": "dimensionality of your data is not extremely high.", "start": 3060.89, "duration": 3.075}, {"text": "As long as the number of features is less than a few thousand,", "start": 3063.965, "duration": 4.095}, {"text": "then you can do Newton's method.", "start": 3068.06, "duration": 2.47}, {"text": "Any other questions? Good. So, um,", "start": 3072.61, "duration": 8.48}, {"text": "so this is the same update rule for any, any,", "start": 3083.59, "duration": 4.555}, {"text": "um, any specific type of GLM based on the choice of distribution that you have.", "start": 3088.145, "duration": 3.99}, {"text": "Whether you are modeling, uh,", "start": 3092.135, "duration": 1.725}, {"text": "you know, um, you're doing classification,", "start": 3093.86, "duration": 2.28}, {"text": "whether you're doing regression, whether you're doing- you know,", "start": 3096.14, "duration": 2.25}, {"text": "a Poisson regression, the update rule is the same.", "start": 3098.39, "duration": 2.775}, {"text": "You just plug in a different h Theta of x and you get your learning rule.", "start": 3101.165, "duration": 4.195}, {"text": "Another, um, some more terminology.", "start": 3106.72, "duration": 6.14}, {"text": "So Eta is what we call the natural parameter.", "start": 3118.66, "duration": 5.32}, {"text": "[NOISE] So Eta is", "start": 3123.98, "duration": 8.88}, {"text": "the natural parameter and the function that", "start": 3132.86, "duration": 3.3}, {"text": "links the natural parameter", "start": 3136.16, "duration": 11.21}, {"text": "to the mean of the distribution and this has a name,", "start": 3147.37, "duration": 3.315}, {"text": "it's called the canonical response function.", "start": 3150.685, "duration": 2.875}, {"text": "Right. And, um, similarly,", "start": 3162.06, "duration": 4.12}, {"text": "you can also- let's call it Mu.", "start": 3166.18, "duration": 2.16}, {"text": "It's like the mean of the distribution.", "start": 3168.34, "duration": 1.95}, {"text": "Uh, similarly you can go from Mu back to Eta with the inverse of this,", "start": 3170.29, "duration": 7.705}, {"text": "and this is also called the canonical link function.", "start": 3177.995, "duration": 5.425}, {"text": "There's some, uh, terminology.", "start": 3186.22, "duration": 2.545}, {"text": "We also already saw that g of Eta is also equal to the,", "start": 3188.765, "duration": 9.87}, {"text": "the, the gradient of the log partition function with respect to Eta.", "start": 3198.635, "duration": 4.23}, {"text": "So a side-note g is equal to- [NOISE]", "start": 3202.865, "duration": 17.655}, {"text": "right. And it's also helpful to", "start": 3220.52, "duration": 3.615}, {"text": "make- explicit the distinction between", "start": 3224.135, "duration": 2.775}, {"text": "the three different kinds of parameterizations we have.", "start": 3226.91, "duration": 2.7}, {"text": "So we have three parameterizations.", "start": 3229.61, "duration": 3.07}, {"text": "So we have the model parameters, that's Theta,", "start": 3236.56, "duration": 9.79}, {"text": "the natural parameters, that's Eta,", "start": 3246.35, "duration": 8.04}, {"text": "and we have the canonical parameters.", "start": 3254.39, "duration": 3.52}, {"text": "And this is a Phi for Bernoulli,", "start": 3259.96, "duration": 4.435}, {"text": "Mu and Sigma square for Gaussian, Lambda for Poisson.", "start": 3264.395, "duration": 7.045}, {"text": "Right. So these are three different ways we are- we can parameterize,", "start": 3271.9, "duration": 5.245}, {"text": "um, either the exponential family or,", "start": 3277.145, "duration": 2.34}, {"text": "or, or the G- uh, GLM.", "start": 3279.485, "duration": 2.595}, {"text": "And whenever we are learning a GLM,", "start": 3282.08, "duration": 3.525}, {"text": "it is only this thing that we learn.", "start": 3285.605, "duration": 3.805}, {"text": "Right. That is the Theta in the linear model.", "start": 3289.51, "duration": 3.49}, {"text": "This is the Theta that is, that is learned.", "start": 3293.0, "duration": 2.97}, {"text": "Right. And, uh, the connection between these two is, is linear.", "start": 3295.97, "duration": 4.98}, {"text": "So Theta transpose x will give you a natural parameter.", "start": 3300.95, "duration": 4.56}, {"text": "Uh, and this is the design choice that we're making.", "start": 3305.51, "duration": 3.82}, {"text": "Right. We choose to reparameterize Eta by a linear model,", "start": 3311.47, "duration": 5.485}, {"text": "uh, a linear of- linear in your data.", "start": 3316.955, "duration": 2.715}, {"text": "And, um, between these two,", "start": 3319.67, "duration": 2.58}, {"text": "you have g to go this way and g inverse to come back this way where g is also the,", "start": 3322.25, "duration": 8.61}, {"text": "uh, uh, uh, derivative of the log partition.", "start": 3330.86, "duration": 3.525}, {"text": "So yeah. So it's important to,", "start": 3334.385, "duration": 2.28}, {"text": "to kind of realize.", "start": 3336.665, "duration": 1.335}, {"text": "It can get pretty confusing when you're seeing this for the first time because you", "start": 3338.0, "duration": 3.15}, {"text": "have so many parameters that are being swapped around and,", "start": 3341.15, "duration": 3.03}, {"text": "you know, getting reparameterized.", "start": 3344.18, "duration": 1.62}, {"text": "There are three kind of", "start": 3345.8, "duration": 2.37}, {"text": "spaces in which- three different ways in which we are parameterizing,", "start": 3348.17, "duration": 3.585}, {"text": "uh, uh, generalized linear models.", "start": 3351.755, "duration": 1.755}, {"text": "Uh, the model parameters,", "start": 3353.51, "duration": 1.68}, {"text": "the ones that we learn and the output of this is", "start": 3355.19, "duration": 3.3}, {"text": "the natural parameter for the exponential family and you can, you know,", "start": 3358.49, "duration": 4.38}, {"text": "do some algebraic manipulations and get the canonical parameters for, uh,", "start": 3362.87, "duration": 4.41}, {"text": "the distribution, uh, that we are choosing, uh,", "start": 3367.28, "duration": 3.21}, {"text": "depending on the task where there's classification or regression.", "start": 3370.49, "duration": 2.97}, {"text": "[NOISE]", "start": 3373.46, "duration": 8.145}, {"text": "Any questions on this?", "start": 3381.605, "duration": 1.455}, {"text": "[NOISE]", "start": 3383.06, "duration": 10.17}, {"text": "So no- now it's actually pretty,", "start": 3393.23, "duration": 1.905}, {"text": "you know, um, you can- you can see that, you know,", "start": 3395.135, "duration": 3.435}, {"text": "when you are doing logistic regression, [NOISE] right?", "start": 3398.57, "duration": 7.86}, {"text": "So h theta of X,", "start": 3406.43, "duration": 3.195}, {"text": "um, so h theta of X, um,", "start": 3409.625, "duration": 3.75}, {"text": "is the expected value of- of,", "start": 3413.375, "duration": 6.625}, {"text": "um, of Y, uh,", "start": 3422.5, "duration": 2.845}, {"text": "conditioned on X theta,", "start": 3425.345, "duration": 2.205}, {"text": "[NOISE] and this is equal to phi, right?", "start": 3427.55, "duration": 7.755}, {"text": "Because, um, here the choice of distribution is a Bernoulli.", "start": 3435.305, "duration": 3.975}, {"text": "And the mean of a Bernoulli distribution is", "start": 3439.28, "duration": 2.37}, {"text": "just phi the- in- in the canonical parameter space.", "start": 3441.65, "duration": 3.495}, {"text": "And if we, um,", "start": 3445.145, "duration": 3.36}, {"text": "write that as, um,", "start": 3448.505, "duration": 1.845}, {"text": "in terms of the, um,", "start": 3450.35, "duration": 1.8}, {"text": "h minus eta and this is", "start": 3452.15, "duration": 5.52}, {"text": "equal to 1 over minus theta transpose X, right?", "start": 3457.67, "duration": 5.565}, {"text": "So, ah, the logistic function which when we introduced,", "start": 3463.235, "duration": 3.764}, {"text": "ah, linear reg-, uh,", "start": 3466.999, "duration": 1.246}, {"text": "logistic regression we just, you know,", "start": 3468.245, "duration": 3.0}, {"text": "pulled out the logistic function out of thin air, and said, hey,", "start": 3471.245, "duration": 3.225}, {"text": "this is something that can squash minus infinity to infinity,", "start": 3474.47, "duration": 2.91}, {"text": "between 0 and 1,", "start": 3477.38, "duration": 1.17}, {"text": "seems like a good choice.", "start": 3478.55, "duration": 1.545}, {"text": "Bu-but now we see that it is- it is a natural outcome.", "start": 3480.095, "duration": 4.455}, {"text": "It just pops out from", "start": 3484.55, "duration": 1.56}, {"text": "this more elegant generalized linear model where if you choose Bernoulli to be, uh,", "start": 3486.11, "duration": 6.99}, {"text": "uh, to be the distribution of your, uh, output, then,", "start": 3493.1, "duration": 3.765}, {"text": "you know, the logistic regression just- just pops out naturally.", "start": 3496.865, "duration": 5.115}, {"text": "[NOISE] So,um, [NOISE] any questions? Yeah.", "start": 3501.98, "duration": 12.36}, {"text": "Maybe you speak a little bit more about choosing a distribution to be the output.", "start": 3514.34, "duration": 6.505}, {"text": "Yeah. So the, uh,", "start": 3520.845, "duration": 2.635}, {"text": "the choice of what distribution you are going to", "start": 3523.48, "duration": 2.43}, {"text": "choose is really dependent on the task that you have.", "start": 3525.91, "duration": 3.66}, {"text": "So if your task is regression,", "start": 3529.57, "duration": 2.054}, {"text": "where you want to output real valued numbers like,", "start": 3531.624, "duration": 2.886}, {"text": "you know, price of the house, or- or something,", "start": 3534.51, "duration": 1.69}, {"text": "uh, then you choose a distribution over the real va- real- real numbers like a Gaussian.", "start": 3536.2, "duration": 6.625}, {"text": "If your task is classification,", "start": 3542.825, "duration": 2.715}, {"text": "where your output is binary 0, or 1,", "start": 3545.54, "duration": 2.46}, {"text": "you choose a distribution that models binary data.", "start": 3548.0, "duration": 3.72}, {"text": "Right? So the task in a way influences you to pick the distribution.", "start": 3551.72, "duration": 6.96}, {"text": "And, you know, uh, most of the times that choice is pretty obvious.", "start": 3558.68, "duration": 3.18}, {"text": "[NOISE] If you want to model the number of visitors to a website which is like a count,", "start": 3561.86, "duration": 4.215}, {"text": "you know, you want to use a Poisson distribution,", "start": 3566.075, "duration": 1.605}, {"text": "because Poisson distribution is a distribution over integers. So the task deci-,", "start": 3567.68, "duration": 4.545}, {"text": "you know, pretty much tells you what distribution you want to choose,", "start": 3572.225, "duration": 3.63}, {"text": "and then you- you do the- you know, uh,", "start": 3575.855, "duration": 2.235}, {"text": "um, you do this, you know,", "start": 3578.09, "duration": 3.36}, {"text": "all- you- you go through this machinery of- of- of figuring out what are the, uh,", "start": 3581.45, "duration": 4.725}, {"text": "what h state of X is,", "start": 3586.175, "duration": 1.53}, {"text": "and you plug in h state of X over there and you have your learning rule.", "start": 3587.705, "duration": 5.035}, {"text": "Any more questions? So, uh, it-,", "start": 3593.35, "duration": 5.32}, {"text": "so we made some assumptions.", "start": 3598.67, "duration": 1.83}, {"text": "Uh, these assumptions.", "start": 3600.5, "duration": 3.36}, {"text": "Now it- it- it's also helpful to kind of get,", "start": 3603.86, "duration": 5.535}, {"text": "uh, a visualization of what these assumptions actually mean, right?", "start": 3609.395, "duration": 3.225}, {"text": "[NOISE]", "start": 3612.62, "duration": 24.06}, {"text": "So to expand upon your point, um, um.", "start": 3636.68, "duration": 4.23}, {"text": "You know if you think of the question,", "start": 3640.91, "duration": 1.29}, {"text": "\"Are GLMs used for classification,", "start": 3642.2, "duration": 1.98}, {"text": "or are they used for regression,", "start": 3644.18, "duration": 1.17}, {"text": "or are they used for,", "start": 3645.35, "duration": 1.185}, {"text": "you know, um, something else?\"", "start": 3646.535, "duration": 1.68}, {"text": "The answer really depends on what is", "start": 3648.215, "duration": 2.445}, {"text": "the choice of distribution that you're gonna choose, you know.", "start": 3650.66, "duration": 2.655}, {"text": "GLMs are just a general way to model data,", "start": 3653.315, "duration": 2.46}, {"text": "and that data could be, you know,", "start": 3655.775, "duration": 1.655}, {"text": "um, binary, it could be real value.", "start": 3657.43, "duration": 1.93}, {"text": "And- and, uh, as long as you have a distribution that can model,", "start": 3659.36, "duration": 3.54}, {"text": "ah, that kind of data,", "start": 3662.9, "duration": 1.47}, {"text": "and falls in the exponential family,", "start": 3664.37, "duration": 1.815}, {"text": "it can be just plugged into a GLM and everything just, uh,", "start": 3666.185, "duration": 2.595}, {"text": "uh, uh works out nicely.", "start": 3668.78, "duration": 2.76}, {"text": "Right. So, uh, [NOISE] so the assumptions that we made.", "start": 3671.54, "duration": 9.405}, {"text": "Let, uh, let's start with regression, [NOISE] right?", "start": 3680.945, "duration": 5.19}, {"text": "So for regression, we assume there is some X.", "start": 3686.135, "duration": 2.985}, {"text": "Uh, to simplify I'm, um,", "start": 3689.12, "duration": 3.33}, {"text": "I'm drawing X as one dimension but,", "start": 3692.45, "duration": 2.655}, {"text": "you know, X could be multi-dimensional.", "start": 3695.105, "duration": 2.37}, {"text": "And there exists a theta, right?", "start": 3697.475, "duration": 2.265}, {"text": "And theta transpose X would- would be some linear, um,", "start": 3699.74, "duration": 6.619}, {"text": "um, some linear, uh, uh, uh, hyperplane.", "start": 3706.359, "duration": 6.896}, {"text": "And this, we assume is Eta, right?", "start": 3713.255, "duration": 10.615}, {"text": "And in case of regression Eta was also Mu.", "start": 3723.88, "duration": 4.85}, {"text": "So Eta was also Mu, right?", "start": 3729.58, "duration": 3.22}, {"text": "Um, and then we are assuming that the Y,", "start": 3732.8, "duration": 2.895}, {"text": "for any given X,", "start": 3735.695, "duration": 1.575}, {"text": "is distributed as a Gaussian with Mu as the mean.", "start": 3737.27, "duration": 4.545}, {"text": "So which means, for every X,", "start": 3741.815, "duration": 3.45}, {"text": "every possible X, you have the appropriate, uh, um, um, Eta.", "start": 3745.265, "duration": 4.53}, {"text": "And with this as the mean,", "start": 3749.795, "duration": 1.92}, {"text": "let's- let's think of this as Y.", "start": 3751.715, "duration": 2.07}, {"text": "So that is, uh, a Gaussian distribution at", "start": 3753.785, "duration": 4.11}, {"text": "every possible- we assume a variance of 1.", "start": 3757.895, "duration": 7.245}, {"text": "So this is like, uh, a Gaussian with standard deviation or variance equal to 1, right?", "start": 3765.14, "duration": 4.17}, {"text": "So for every possible X,", "start": 3769.31, "duration": 1.56}, {"text": "there is a Y given X, um,", "start": 3770.87, "duration": 2.86}, {"text": "which is parameterized by- by- by theta transpose X as- as the mean, right?", "start": 3773.86, "duration": 6.34}, {"text": "And you assume that your data is generated from this process, right?", "start": 3780.2, "duration": 7.14}, {"text": "So what does it mean?", "start": 3787.34, "duration": 1.38}, {"text": "It means, um, you're given X,", "start": 3788.72, "duration": 4.11}, {"text": "and let's- let's say this is Y.", "start": 3792.83, "duration": 3.255}, {"text": "So you would have examples in your training set that- that may look like this, right?", "start": 3796.085, "duration": 8.295}, {"text": "The assumption here is that,", "start": 3804.38, "duration": 2.04}, {"text": "for every X there is, um,", "start": 3806.42, "duration": 2.52}, {"text": "um- let's say for this particular value of X,", "start": 3808.94, "duration": 3.93}, {"text": "um, there was a Gaussian distribution that started from the mean over here.", "start": 3812.87, "duration": 5.115}, {"text": "And from this Gaussian distribution this value was sampled, right?", "start": 3817.985, "duration": 6.075}, {"text": "You're - you're- you're- you're just sampling it from- from the distribution.", "start": 3824.06, "duration": 2.535}, {"text": "Now, the, um- this is how your data is generated.", "start": 3826.595, "duration": 4.65}, {"text": "Again, this is our assumption, [NOISE] right?", "start": 3831.245, "duration": 5.58}, {"text": "Now that- now based on", "start": 3836.825, "duration": 3.435}, {"text": "these assumptions what we are doing with the GLM is we start with the data.", "start": 3840.26, "duration": 3.96}, {"text": "We don't know anything else.", "start": 3844.22, "duration": 1.17}, {"text": "We make an assumption that there is", "start": 3845.39, "duration": 1.32}, {"text": "some linear model from which the data was-was- was- was generated in this format.", "start": 3846.71, "duration": 5.625}, {"text": "And we want to work backwards, right,", "start": 3852.335, "duration": 2.805}, {"text": "to find theta that will give us this line, right?", "start": 3855.14, "duration": 5.34}, {"text": "So for different choice of theta we get a different line, right?", "start": 3860.48, "duration": 3.975}, {"text": "We assume that, you know,", "start": 3864.455, "duration": 2.055}, {"text": "if -if that line represents the- the Mu's,", "start": 3866.51, "duration": 2.835}, {"text": "or the means of the Y's for that particular X, uh,", "start": 3869.345, "duration": 3.015}, {"text": "from which it's sampled from,", "start": 3872.36, "duration": 1.545}, {"text": "we are trying to find a line, [NOISE] ah,", "start": 3873.905, "duration": 5.565}, {"text": "which is- which will be like", "start": 3879.47, "duration": 1.74}, {"text": "your theta transpose X from which these Y's are most likely to have sampled.", "start": 3881.21, "duration": 4.47}, {"text": "That's- that's essentially what's happening when you do", "start": 3885.68, "duration": 1.92}, {"text": "maximum likelihood with- with -with the GLM, right?", "start": 3887.6, "duration": 3.0}, {"text": "Ah, similarly, um, [NOISE]", "start": 3890.6, "duration": 16.31}, {"text": "Similarly for, um, classification,", "start": 3906.91, "duration": 2.88}, {"text": "again let's assume there's an x, right?", "start": 3909.79, "duration": 3.045}, {"text": "And there are some Theta transpose x, right?", "start": 3912.835, "duration": 5.13}, {"text": "And, uh, and this Theta transpose x is equal- is Eta.", "start": 3917.965, "duration": 6.03}, {"text": "We assign this to be Eta, right?", "start": 3923.995, "duration": 2.58}, {"text": "And this Eta is,", "start": 3926.575, "duration": 2.325}, {"text": "uh, from this Eta,", "start": 3928.9, "duration": 2.025}, {"text": "we- we run this through the sigmoid function, uh,", "start": 3930.925, "duration": 4.47}, {"text": "1 over 1 plus e to the minus Eta to get Phi, right?", "start": 3935.395, "duration": 5.985}, {"text": "So if these are the Etas for each, um,", "start": 3941.38, "duration": 4.095}, {"text": "for each Eta we run it through the sigmoid and we get something like this, right?", "start": 3945.475, "duration": 7.575}, {"text": "So this tends to, uh, 1.", "start": 3953.05, "duration": 1.95}, {"text": "This tends to 0.", "start": 3955.0, "duration": 1.605}, {"text": "And, um, when- at this point when Eta is 0,", "start": 3956.605, "duration": 4.965}, {"text": "the sigmoid is- is 0.5.", "start": 3961.57, "duration": 2.865}, {"text": "This is 0.5, right?", "start": 3964.435, "duration": 3.855}, {"text": "And now, um, at each point- at- at- at any given choice of x,", "start": 3968.29, "duration": 7.17}, {"text": "we have a probability distribution.", "start": 3975.46, "duration": 3.61}, {"text": "In this case, it's- it's a- it's a binary.", "start": 3979.98, "duration": 3.355}, {"text": "So let's assume probability of y is the height to the sigmoid line and here it is low.", "start": 3983.335, "duration": 7.125}, {"text": "Um, right. Every x we have a different, uh,", "start": 3990.46, "duration": 2.685}, {"text": "Bernoulli distribution essentially, um, that's obtained where,", "start": 3993.145, "duration": 3.75}, {"text": "you know, the- the probability of y is- is the height to the,", "start": 3996.895, "duration": 3.015}, {"text": "uh, uh, sigmoid through the natural parameter.", "start": 3999.91, "duration": 3.27}, {"text": "And from this, you have a data generating distribution that would look like this.", "start": 4003.18, "duration": 5.235}, {"text": "So x and, uh,", "start": 4008.415, "duration": 2.565}, {"text": "you have a few xs in your training set.", "start": 4010.98, "duration": 2.16}, {"text": "And for those xs, you calc- you- you figure out what your,", "start": 4013.14, "duration": 4.44}, {"text": "you know, y distribution is and sample from it.", "start": 4017.58, "duration": 2.1}, {"text": "So let's say- right.", "start": 4019.68, "duration": 11.385}, {"text": "And now, um, again our goal is to stop- given- given this data,", "start": 4031.065, "duration": 5.94}, {"text": "so- so over here this is the x and this is y.", "start": 4037.005, "duration": 3.23}, {"text": "So this is- these are points for which y is 0.", "start": 4040.235, "duration": 2.595}, {"text": "These are points for which y is 1.", "start": 4042.83, "duration": 1.44}, {"text": "And so given- given this data,", "start": 4044.27, "duration": 1.68}, {"text": "we wanna work backwards to find out,", "start": 4045.95, "duration": 3.185}, {"text": "uh, what Theta was.", "start": 4049.135, "duration": 2.785}, {"text": "What's the Theta that would have resulted in a sigmoid like", "start": 4051.92, "duration": 4.74}, {"text": "curve from which these- these y's were most likely to have been sampled?", "start": 4056.66, "duration": 5.115}, {"text": "That's- and- and figuring out that y is- is- is essentially doing logistic regression.", "start": 4061.775, "duration": 5.17}, {"text": "Any questions?", "start": 4066.945, "duration": 2.425}, {"text": "All right.", "start": 4077.15, "duration": 1.27}, {"text": "So in the last 10 minutes or so,", "start": 4078.42, "duration": 2.52}, {"text": "we will, uh, go over softmax regression.", "start": 4080.94, "duration": 5.44}, {"text": "So softmax regression is,", "start": 4110.66, "duration": 2.83}, {"text": "um, so in the lecture notes,", "start": 4113.49, "duration": 2.7}, {"text": "softmax regression is, uh,", "start": 4116.19, "duration": 1.635}, {"text": "explained as, uh, as yet another member of the GLM family.", "start": 4117.825, "duration": 5.37}, {"text": "Uh, however, in- in- in today's lecture we'll be taking", "start": 4123.195, "duration": 3.075}, {"text": "a non-GLM approach and kind of, um,", "start": 4126.27, "duration": 2.655}, {"text": "seeing- and- and see how softmax is- is essentially doing,", "start": 4128.925, "duration": 4.065}, {"text": "uh, what's also called as cross entropy minimization.", "start": 4132.99, "duration": 3.67}, {"text": "We'll end up with the same- same formulas and equations.", "start": 4137.72, "duration": 3.55}, {"text": "You can- you can go through the GLM interpretation in the notes.", "start": 4141.27, "duration": 2.565}, {"text": "It's a little messy to kind of do it on the whiteboard.", "start": 4143.835, "duration": 2.235}, {"text": "So, um, whereas this has- has- has a nicer, um, um, interpretation.", "start": 4146.07, "duration": 4.995}, {"text": "Um, and it's good to kind of get this cross entropy interpretation as well.", "start": 4151.065, "duration": 3.855}, {"text": "So, uh, let's assume- so here we are talking about multiclass classification.", "start": 4154.92, "duration": 5.655}, {"text": "So let's assume we have three cat- three,", "start": 4160.575, "duration": 3.135}, {"text": "uh, classes of data.", "start": 4163.71, "duration": 1.98}, {"text": "Let's call them circles, um,", "start": 4165.69, "duration": 4.62}, {"text": "squares, and say triangles.", "start": 4170.31, "duration": 7.6}, {"text": "Now, uh, if- here and this is x1 and x2.", "start": 4180.59, "duration": 5.74}, {"text": "We're just- we're just visualizing your input space and the output space, y", "start": 4186.33, "duration": 3.84}, {"text": "is kind of implicit in the shape of this, so, um, um.", "start": 4190.17, "duration": 3.75}, {"text": "So, um, in- in,", "start": 4193.92, "duration": 2.1}, {"text": "um, in multicl- multiclass classification,", "start": 4196.02, "duration": 4.41}, {"text": "our goal is to start from this data and learn a model that can,", "start": 4200.43, "duration": 5.955}, {"text": "given a new data point, you know,", "start": 4206.385, "duration": 3.435}, {"text": "make a prediction of whether this point is a circle,", "start": 4209.82, "duration": 3.21}, {"text": "square or a triangle, right?", "start": 4213.03, "duration": 2.04}, {"text": "Uh, you're just looking at three because it's", "start": 4215.07, "duration": 3.12}, {"text": "easy to visualize but this can work over thousands of classes.", "start": 4218.19, "duration": 3.93}, {"text": "And, um, so what we have is", "start": 4222.12, "duration": 6.24}, {"text": "so you have x_is in R_n.", "start": 4228.36, "duration": 5.94}, {"text": "All right. So the label y", "start": 4234.3, "duration": 5.29}, {"text": "is, uh, is 0, 1_k.", "start": 4239.59, "duration": 6.155}, {"text": "So k is the number of classes, right?", "start": 4245.745, "duration": 6.625}, {"text": "So the labels y is- is- is a one-hot vector.", "start": 4257.99, "duration": 4.525}, {"text": "What would you call it as a one-hot vector?", "start": 4262.515, "duration": 1.815}, {"text": "Where it's a vector which indicates which class the,", "start": 4264.33, "duration": 5.19}, {"text": "uh, x corresponds to.", "start": 4269.52, "duration": 1.83}, {"text": "So each- each element in the vector,", "start": 4271.35, "duration": 2.49}, {"text": "uh, corresponds to one of the classes.", "start": 4273.84, "duration": 2.535}, {"text": "So this may correspond to the triangle class,", "start": 4276.375, "duration": 2.88}, {"text": "circle class, square class or maybe something else.", "start": 4279.255, "duration": 2.88}, {"text": "Uh, so the labels are, uh,", "start": 4282.135, "duration": 4.5}, {"text": "in this one-hot vector where we have a vector that's filled with 0s", "start": 4286.635, "duration": 3.57}, {"text": "except with a 1 in one of the places, right?", "start": 4290.205, "duration": 4.365}, {"text": "And- and- and- and the way we're gonna- the way we're gonna,", "start": 4294.57, "duration": 9.57}, {"text": "uh, um, think of softmax regression is that", "start": 4304.14, "duration": 4.14}, {"text": "each class has its- its own set of parameters.", "start": 4308.28, "duration": 4.695}, {"text": "So we have, uh,", "start": 4312.975, "duration": 1.635}, {"text": "Theta class, right, in R_n.", "start": 4314.61, "duration": 7.125}, {"text": "And there are k such things where class is in here,", "start": 4321.735, "duration": 9.855}, {"text": "triangle, circle, square, etc, right?", "start": 4331.59, "duration": 4.485}, {"text": "So in logistic regression,", "start": 4336.075, "duration": 1.41}, {"text": "we had just one Theta,", "start": 4337.485, "duration": 2.055}, {"text": "which would do a binary, you know, yes versus no.", "start": 4339.54, "duration": 2.7}, {"text": "Uh, in softmax, we have one such vector of Theta per class, right?", "start": 4342.24, "duration": 6.93}, {"text": "So you could also optionally represent them as a matrix.", "start": 4349.17, "duration": 3.705}, {"text": "There's an n by k matrix where, you know,", "start": 4352.875, "duration": 4.05}, {"text": "you have a Theta class- Theta class, right?", "start": 4356.925, "duration": 4.065}, {"text": "Uh, so in softmax, uh, regression, um,", "start": 4360.99, "duration": 3.735}, {"text": "it's- it's- it's a generalization of logistic regression where you have,", "start": 4364.725, "duration": 4.575}, {"text": "um, a set of parameters per class, right?", "start": 4369.3, "duration": 4.11}, {"text": "And we're gonna do something, um,", "start": 4373.41, "duration": 4.36}, {"text": "something similar to, uh, so, uh,", "start": 4381.98, "duration": 16.63}, {"text": "[NOISE] so corresponding", "start": 4398.61, "duration": 11.01}, {"text": "to each- each class,", "start": 4409.62, "duration": 2.1}, {"text": "uh, uh, of- of, uh, parameters that exists, um [NOISE]", "start": 4411.72, "duration": 10.095}, {"text": "So there's- there exists this line which represents say,", "start": 4421.815, "duration": 3.39}, {"text": "Theta triangle transpose x equals 0,", "start": 4425.205, "duration": 2.895}, {"text": "and anything to the left,", "start": 4428.1, "duration": 1.38}, {"text": "will be Theta triangle transpose x is greater than 0,", "start": 4429.48, "duration": 3.945}, {"text": "and over here it'll be less than 0, right?", "start": 4433.425, "duration": 2.205}, {"text": "So if, if- for, for- uh, uh,", "start": 4435.63, "duration": 2.085}, {"text": "the- Theta triangle class,", "start": 4437.715, "duration": 1.335}, {"text": "um, there is- uh,", "start": 4439.05, "duration": 1.53}, {"text": "there is this line, um,", "start": 4440.58, "duration": 2.55}, {"text": "which- which corresponds to,", "start": 4443.13, "duration": 1.935}, {"text": "uh, uh, Theta transpose x equals 0.", "start": 4445.065, "duration": 2.235}, {"text": "Anything to the left, uh will give you", "start": 4447.3, "duration": 2.04}, {"text": "a value greater than on- zero, anything to the right.", "start": 4449.34, "duration": 2.85}, {"text": "Similarly, there is also-.", "start": 4452.19, "duration": 1.5}, {"text": "Uh, so this corresponds to Theta,", "start": 4453.69, "duration": 4.8}, {"text": "uh, square transpose x equals 0.", "start": 4458.49, "duration": 3.93}, {"text": "Anything below will be greater than 0,", "start": 4462.42, "duration": 3.66}, {"text": "anything above will be less than 0.", "start": 4466.08, "duration": 3.21}, {"text": "Similarly you have another one for,", "start": 4469.29, "duration": 4.515}, {"text": "um, this corresponds to Theta circle transpose x equals 0.", "start": 4473.805, "duration": 6.915}, {"text": "And, and, and, and this half plane, we have,", "start": 4480.72, "duration": 3.18}, {"text": "uh, to be greater than 0,", "start": 4483.9, "duration": 1.935}, {"text": "and to the left, it is less than 0,", "start": 4485.835, "duration": 2.635}, {"text": "right? So we have, um,", "start": 4488.47, "duration": 2.395}, {"text": "a different set of parameters per class which,", "start": 4490.865, "duration": 3.945}, {"text": "um, which, which, which hopefully satisfies this property, um,", "start": 4494.81, "duration": 5.215}, {"text": "and now, um, our goal is to take these parameters and let's see what happens when,", "start": 4500.025, "duration": 10.695}, {"text": "when we field a new example.", "start": 4510.72, "duration": 2.1}, {"text": "So given an example x,", "start": 4512.82, "duration": 2.52}, {"text": "we get a set of- given x,", "start": 4515.34, "duration": 9.36}, {"text": "um, and over here we have classes, right?", "start": 4524.7, "duration": 4.815}, {"text": "So we have the circle class,", "start": 4529.515, "duration": 1.425}, {"text": "the triangle class, the square class, right?", "start": 4530.94, "duration": 2.685}, {"text": "So, um, over here,", "start": 4533.625, "duration": 2.055}, {"text": "we plot Theta class transpose x.", "start": 4535.68, "duration": 4.005}, {"text": "So we may get something that looks like this.", "start": 4539.685, "duration": 3.565}, {"text": "So let's say for a new point x over here,", "start": 4545.18, "duration": 5.335}, {"text": "uh, if that's our new x,", "start": 4550.515, "duration": 1.83}, {"text": "we would have Theta transpose,", "start": 4552.345, "duration": 3.06}, {"text": "um, Theta trans- Theta square transpose x to be positive.", "start": 4555.405, "duration": 4.965}, {"text": "So we- all right.", "start": 4560.37, "duration": 2.415}, {"text": "And maybe for, um, for the others,", "start": 4562.785, "duration": 3.255}, {"text": "we may have some negative and maybe something like this for this, right?", "start": 4566.04, "duration": 4.895}, {"text": "So- th- this space is,", "start": 4570.935, "duration": 2.865}, {"text": "is also called the logic space, right?", "start": 4573.8, "duration": 2.895}, {"text": "So these are real numbers, right?", "start": 4576.695, "duration": 1.665}, {"text": "Thi- this will, this will, uh,", "start": 4578.36, "duration": 1.605}, {"text": "this is not a value between 0 and 1,", "start": 4579.965, "duration": 1.875}, {"text": "this is between plus infinity and minus infinity, right?", "start": 4581.84, "duration": 3.15}, {"text": "And, and our goal is to get,", "start": 4584.99, "duration": 4.3}, {"text": "uh, a probability distribution over the classes.", "start": 4589.29, "duration": 3.735}, {"text": "Uh, and in order to do that,", "start": 4593.025, "duration": 1.995}, {"text": "we perform a few steps.", "start": 4595.02, "duration": 2.01}, {"text": "So we exponentiate the logics which would give us- so now it is x above Theta class", "start": 4597.03, "duration": 8.61}, {"text": "transpose x and this will make everything positive so", "start": 4605.64, "duration": 4.29}, {"text": "it should be a small one.", "start": 4609.93, "duration": 7.17}, {"text": "Squares, triangles and circles, right?", "start": 4617.1, "duration": 4.635}, {"text": "Now we've got a set of positive numbers.", "start": 4621.735, "duration": 1.905}, {"text": "And next, we normalize this.", "start": 4623.64, "duration": 3.01}, {"text": "By normalize, I mean, um,", "start": 4630.56, "duration": 2.68}, {"text": "divide everything by the sum of all of them.", "start": 4633.24, "duration": 4.53}, {"text": "So here we have Theta e to the Theta class", "start": 4637.77, "duration": 4.41}, {"text": "transpose x over the sum of i in triangle,", "start": 4642.18, "duration": 8.16}, {"text": "square, circle, e to the Theta i transpose x.", "start": 4650.34, "duration": 7.305}, {"text": "So n- once we do this operation,", "start": 4657.645, "duration": 2.265}, {"text": "we now get a probability distribution", "start": 4659.91, "duration": 3.43}, {"text": "where the sum of the heights will add up to 1, right?", "start": 4673.04, "duration": 4.645}, {"text": "So, uh- so given- so- if, if,", "start": 4677.685, "duration": 3.345}, {"text": "if- given a new point x and we run through this pipeline,", "start": 4681.03, "duration": 3.225}, {"text": "we get a probability output over the classes for which", "start": 4684.255, "duration": 4.995}, {"text": "class that example is most likely to belong to, right?", "start": 4689.25, "duration": 5.31}, {"text": "And this whole process,", "start": 4694.56, "duration": 2.615}, {"text": "so let's call this p hat of,", "start": 4697.175, "duration": 3.3}, {"text": "of, of, of y for the given x, right?", "start": 4700.475, "duration": 4.09}, {"text": "So this is like our hypothesis.", "start": 4704.565, "duration": 1.605}, {"text": "The output of the hypothesis function will output this probability distribution.", "start": 4706.17, "duration": 4.125}, {"text": "In the other cases, the output of the hypothesis function,", "start": 4710.295, "duration": 2.325}, {"text": "generally, output a scalar or a probability.", "start": 4712.62, "duration": 2.925}, {"text": "In this case, it's outputting a probability di- distribution over all the classes.", "start": 4715.545, "duration": 4.02}, {"text": "And now, the true y would look something like this, right?", "start": 4719.565, "duration": 5.49}, {"text": "Let's say, the point over there was- le- let's say it was a triangle,", "start": 4725.055, "duration": 5.445}, {"text": "for, for whatever reason, right?", "start": 4730.5, "duration": 1.83}, {"text": "If that was the triangle,", "start": 4732.33, "duration": 1.41}, {"text": "then the p of y which is also called the label,", "start": 4733.74, "duration": 5.56}, {"text": "you can think of that as a probability distribution which is 1 over", "start": 4739.49, "duration": 5.305}, {"text": "the correct class and 0 elsewhere, right?", "start": 4744.795, "duration": 5.025}, {"text": "So p of y. This is essentially", "start": 4749.82, "duration": 2.52}, {"text": "representing the one-hot representation as a probability distribution, right?", "start": 4752.34, "duration": 3.645}, {"text": "Now the goal or, or, um,", "start": 4755.985, "duration": 2.58}, {"text": "the learning approach that we're going to do is in a way minimize", "start": 4758.565, "duration": 4.65}, {"text": "the distance between these two distributions, right?", "start": 4763.215, "duration": 5.085}, {"text": "This is one distribution,", "start": 4768.3, "duration": 1.11}, {"text": "this is another distribution.", "start": 4769.41, "duration": 1.185}, {"text": "We want to change this distribution to look like that distribution, right?", "start": 4770.595, "duration": 4.335}, {"text": "Uh, and, and, uh, technically,", "start": 4774.93, "duration": 2.31}, {"text": "that- the term for that is minimize the cross entropy between the two distributions.", "start": 4777.24, "duration": 5.95}, {"text": "So the cross entropy", "start": 4795.38, "duration": 3.38}, {"text": "between p and p hat is equal to,", "start": 4800.84, "duration": 6.8}, {"text": "for y in circle, triangle, square,", "start": 4812.06, "duration": 7.43}, {"text": "p of y times log p hat of", "start": 4819.5, "duration": 5.71}, {"text": "y. I don't think", "start": 4825.21, "duration": 3.66}, {"text": "we'll have time to go over the interpretation of cross entropy but you can look that up.", "start": 4828.87, "duration": 3.825}, {"text": "So here we see that p of y will be", "start": 4832.695, "duration": 2.325}, {"text": "one for just one of the classes and zero for the others.", "start": 4835.02, "duration": 2.715}, {"text": "So let's say in this, this example,", "start": 4837.735, "duration": 1.905}, {"text": "p of- so y was say a triangle.", "start": 4839.64, "duration": 2.4}, {"text": "So this will essentially boil down to- there's a little min-", "start": 4842.04, "duration": 4.77}, {"text": "minus log p hat of y triangle, right?", "start": 4846.81, "duration": 9.27}, {"text": "And what we saw that this- the hypothesis is essentially that expression.", "start": 4856.08, "duration": 5.985}, {"text": "So that's equal to minus log x e x of", "start": 4862.065, "duration": 5.97}, {"text": "Theta triangle transpose x over sum of class in triangle,", "start": 4868.035, "duration": 7.665}, {"text": "square, circle, e to the triangle.", "start": 4875.7, "duration": 7.455}, {"text": "Right. And on this, you, you, you,", "start": 4883.155, "duration": 3.06}, {"text": "you treat this as the loss and do gradient descent.", "start": 4886.215, "duration": 4.645}, {"text": "Gradient descent with respect to the parameters.", "start": 4894.14, "duration": 3.79}, {"text": "Right, um, yeah.", "start": 4897.93, "duration": 5.235}, {"text": "With, with, with that I think, uh,", "start": 4903.165, "duration": 1.755}, {"text": "uh, any, any questions on softmax?", "start": 4904.92, "duration": 3.49}, {"text": "Okay. So we'll, we'll break for today in that case. Thanks.", "start": 4913.28, "duration": 4.82}]