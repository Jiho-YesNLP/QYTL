[{"text": "Professor Dave here, let\u2019s talk about orthogonality.", "start": 0.48, "duration": 2.92}, {"text": "We\u2019ve already touched on orthogonality during\nour lesson about the vector dot product, and", "start": 10.1, "duration": 5.02}, {"text": "we may simply think of orthogonal vectors\nas ones that are perpendicular.", "start": 15.121, "duration": 5.068}, {"text": "Of course that is true, but now it\u2019s time\nto expand on this basic understanding, so", "start": 20.189, "duration": 5.131}, {"text": "let\u2019s go in for a closer look.", "start": 25.32, "duration": 1.879}, {"text": "First, let\u2019s recall the basic definition\nwe just mentioned, whereby two vectors in", "start": 27.199, "duration": 5.981}, {"text": "a vector space are considered orthogonal if\nthey are perpendicular to one another, which", "start": 33.18, "duration": 5.96}, {"text": "means that the angle between them is half\npi radians, or ninety degrees.", "start": 39.14, "duration": 6.099}, {"text": "This situation will occur if the dot product\nbetween the two vectors is equal to zero,", "start": 45.239, "duration": 6.37}, {"text": "since the dot product is equal to the product\nof the two vector lengths, times the cosine", "start": 51.609, "duration": 6.66}, {"text": "of the angle between them, and the cosine\nof half pi is zero, which makes the dot product", "start": 58.269, "duration": 6.361}, {"text": "equal to zero.", "start": 64.63, "duration": 1.59}, {"text": "Let\u2019s now consider the vectors a = (4, 2,\n-1) and b = (1, -3, -2).", "start": 66.22, "duration": 9.35}, {"text": "To see if these vectors are orthogonal, we\nmust check that the dot product is zero.", "start": 75.57, "duration": 6.32}, {"text": "To take the vector dot product, apart from\nthe geometric definition we already mentioned,", "start": 81.89, "duration": 5.58}, {"text": "we can also use this algebraic one, where\nwe find the product of the first components,", "start": 87.47, "duration": 6.36}, {"text": "plus the product of the second components,\nplus the product of the third components.", "start": 93.83, "duration": 6.87}, {"text": "In this particular case, this makes a dot\nb equal to (4 times 1) + (2 times -3) + (-1", "start": 100.7, "duration": 8.3}, {"text": "times -2), which is 4 \u2013 6 + 2, or zero.", "start": 109.0, "duration": 6.34}, {"text": "Since the dot product is equal to zero, our\nvectors are indeed orthogonal.", "start": 115.34, "duration": 6.39}, {"text": "Now let\u2019s expand on this basic concept.", "start": 121.73, "duration": 2.67}, {"text": "In addition to vectors being orthogonal, we\ncan use another restriction on some vectors", "start": 124.4, "duration": 5.529}, {"text": "to create another useful concept, orthonormality.", "start": 129.929, "duration": 4.821}, {"text": "The \u201cnormal\u201d part here refers to the vectors\nhaving a length of 1.", "start": 134.75, "duration": 5.71}, {"text": "If a set of vectors are all of length 1 and\nalso orthogonal, they are said to be orthonormal.", "start": 140.46, "duration": 8.52}, {"text": "We have already seen that one way to get the\nlength of a vector is to take the square root", "start": 148.98, "duration": 5.66}, {"text": "of the dot product involving the vector and\nitself, meaning the length of vector A is", "start": 154.64, "duration": 6.109}, {"text": "equal to the square root of A dot A. Now,\nif we then divide the original vector by that", "start": 160.749, "duration": 7.02}, {"text": "length, we will get a new vector that will\nbe of length 1.", "start": 167.769, "duration": 5.201}, {"text": "With that understood, let\u2019s take our two\nvectors from before, which were a = (4, 2,", "start": 172.97, "duration": 5.659}, {"text": "-1) and b = (1, -3, -2).", "start": 178.629, "duration": 4.521}, {"text": "The length of A will be given by the square\nroot of 42 + 22 + (-1)2, which gives us the", "start": 183.15, "duration": 8.979}, {"text": "square root of 16 + 4 + 1, which is equal\nto root 21.", "start": 192.129, "duration": 6.47}, {"text": "The length of B will be given by the square\nroot of 12 + (-3)2 + (-2)2 which gives us", "start": 198.599, "duration": 8.351}, {"text": "the square root of 1 + 9 + 4, which is equal\nto root 14.", "start": 206.95, "duration": 7.56}, {"text": "Now we can divide the vectors A and B by the\nlengths we just calculated, and we get new", "start": 214.51, "duration": 6.16}, {"text": "\u201cunit\u201d vectors (4, 2, -1) over root 21,\nand (1, -3, -2) over root 14.", "start": 220.67, "duration": 9.899}, {"text": "Here, the word \u201cunit\u201d means that they\nhave lengths that are equal to one.", "start": 230.569, "duration": 6.201}, {"text": "This process of making vectors into unit vectors\nis often called \u201cnormalization\u201d.", "start": 236.77, "duration": 6.899}, {"text": "By doing this we have only changed the magnitude\nof the vectors, and not the direction.", "start": 243.669, "duration": 5.781}, {"text": "Because of this, they are still orthogonal\nas we previously verified.", "start": 249.45, "duration": 5.009}, {"text": "And since these two vectors are orthogonal\nand also have lengths equal to 1, that makes", "start": 254.459, "duration": 5.981}, {"text": "them orthonormal as well.", "start": 260.44, "duration": 5.23}, {"text": "It is actually not the case that only vectors\ncan be considered orthogonal, we can also", "start": 265.67, "duration": 5.21}, {"text": "have orthogonal subspaces.", "start": 270.88, "duration": 2.01}, {"text": "Two subspaces, A and B, are said to be orthogonal\nif every vector in A is orthogonal to every", "start": 272.89, "duration": 7.77}, {"text": "vector in B. So for any vector a in subspace\nA, and any vector b in subspace B, a dot b", "start": 280.66, "duration": 8.19}, {"text": "must be equal to zero.", "start": 288.85, "duration": 3.27}, {"text": "Consider the following subspaces of R3.", "start": 292.12, "duration": 2.61}, {"text": "Subspace A is made of the vectors of the form\n(a1, 0, 0), which can have any value as the", "start": 294.73, "duration": 6.86}, {"text": "first component but must have zeros as the\nsecond and third.", "start": 301.59, "duration": 4.74}, {"text": "Subspace B is made of the vectors of the form\n(0, b2, b3), which have a zero as the first", "start": 306.33, "duration": 6.47}, {"text": "component and can have any value as the second\nand third.", "start": 312.8, "duration": 3.82}, {"text": "While there are an infinite number of vectors\nthat can satisfy these conditions, taking", "start": 316.62, "duration": 5.101}, {"text": "the dot product of any possible vector in\nsubspace A and any possible vector in subspace", "start": 321.721, "duration": 6.609}, {"text": "B gives us (a1 times zero) + (zero times b2)\n+ (zero times b3), and this will be equal", "start": 328.33, "duration": 8.23}, {"text": "to zero no matter what those values are.", "start": 336.56, "duration": 4.16}, {"text": "Because of this, we have shown that the subspaces\nA and B are orthogonal.", "start": 340.72, "duration": 7.74}, {"text": "Square matrices can also be orthogonal.", "start": 348.46, "duration": 2.67}, {"text": "The condition for this is simply that the\ncolumns of the matrix make up an orthonormal set.", "start": 351.13, "duration": 5.91}, {"text": "Take for example the 2 by 2 matrix made up\nof 1 over root 2 in all elements, but the", "start": 358.66, "duration": 6.3}, {"text": "bottom right is negative.", "start": 364.97, "duration": 2.85}, {"text": "Taking the columns of this matrix as vectors,\n(1 over root 2, 1 over root 2) and (1 over", "start": 367.82, "duration": 6.88}, {"text": "root 2, -1 over root 2), let\u2019s verify that\nthey are orthonormal.", "start": 374.7, "duration": 7.67}, {"text": "First to check for orthogonality, we can take\nthe dot product, which will give us (1 over", "start": 382.37, "duration": 4.99}, {"text": "root 2 times 1 over root 2) + (1 over root\n2 times -1 over root 2).", "start": 387.36, "duration": 6.45}, {"text": "This becomes one half minus one half, which\nis simply zero, meaning that these vectors", "start": 393.81, "duration": 4.96}, {"text": "are orthogonal.", "start": 398.77, "duration": 1.9}, {"text": "Now we just need to verify that they both\nhave lengths of 1.", "start": 400.67, "duration": 4.04}, {"text": "The length of the first will be the square\nroot of (1 over root 2 quantity squared) +", "start": 404.71, "duration": 5.77}, {"text": "(1 over root 2 quantity squared), which is\nthe square root of one half plus one half,", "start": 410.48, "duration": 5.95}, {"text": "or the square root of 1, which is 1.", "start": 416.43, "duration": 3.72}, {"text": "Then for the second, the length is the square\nroot of (1 over root 2 quantity squared) +", "start": 420.15, "duration": 5.67}, {"text": "(-1 over root 2 quantity squared), which again\nbecomes the square root of one half plus one", "start": 425.82, "duration": 5.68}, {"text": "half, or the square root of 1, which is 1.", "start": 431.5, "duration": 5.0}, {"text": "Both vectors have a length of 1, and are orthogonal\nto each other.", "start": 436.5, "duration": 4.96}, {"text": "This makes the columns of the matrix an orthonormal\nset, meaning that the matrix itself is orthogonal.", "start": 441.46, "duration": 10.35}, {"text": "The main benefit to having an orthogonal matrix\nis that the inverse of the matrix is simply", "start": 451.81, "duration": 5.84}, {"text": "the same as its \u201ctranspose\u201d.", "start": 457.65, "duration": 2.74}, {"text": "By transpose, we mean that the columns of\nthe matrix become the rows and vice versa.", "start": 460.39, "duration": 6.19}, {"text": "For example, the transpose of a 2 by 2 matrix\nand a 3 by 3 matrix are given as shown.", "start": 466.58, "duration": 6.91}, {"text": "You can see that the first row becomes the\nfirst column, the second row becomes the second", "start": 473.49, "duration": 4.9}, {"text": "column, and so on.", "start": 478.39, "duration": 2.77}, {"text": "For an orthogonal matrix, generating the inverse\nis as simple as finding the transpose, and", "start": 481.16, "duration": 6.96}, {"text": "this makes it extremely easy to find the inverse\nof orthogonal matrices, as you simply swap", "start": 488.12, "duration": 6.57}, {"text": "some elements around.", "start": 494.69, "duration": 2.17}, {"text": "Lastly, we can even consider orthogonality\nbetween functions.", "start": 496.86, "duration": 5.12}, {"text": "However to do this, we must consider a definition\nfor something called the \u201cinner product\u201d", "start": 501.98, "duration": 6.05}, {"text": "from point a to point b for the functions\nf(x) and g(x).", "start": 508.03, "duration": 5.12}, {"text": "This inner product, written as f comma g between\nangular brackets, is defined to be the integral,", "start": 513.15, "duration": 6.9}, {"text": "from a to b, of f(x) times g(x), dx.", "start": 520.05, "duration": 5.229}, {"text": "Once again, if this inner product ends up\nbeing equal to zero, the functions are said", "start": 525.279, "duration": 5.031}, {"text": "to be orthogonal.", "start": 530.31, "duration": 1.319}, {"text": "However, in this case there is some dependence\non what range of values we are considering.", "start": 531.629, "duration": 6.491}, {"text": "Take for example the functions f(x) = x and\ng(x) = 1.", "start": 538.12, "duration": 5.31}, {"text": "Our inner product  will be equal to\nthe integral from a to b, of x dx.", "start": 543.43, "duration": 6.51}, {"text": "As we recall from our study of calculus, the\nantiderivative here will be x2/2, and we must", "start": 549.94, "duration": 6.589}, {"text": "evaluate this at b, and then subtract from\nthat the evaluation at a.", "start": 556.529, "duration": 4.821}, {"text": "Let\u2019s say our interval goes from a = -1\nto b = 1.", "start": 561.35, "duration": 6.149}, {"text": "Then our inner product becomes 12/2 - (-1)2/2,\nwhich is equal to zero.", "start": 567.499, "duration": 7.601}, {"text": "So our functions are orthogonal across this\nrange.", "start": 575.1, "duration": 4.51}, {"text": "However, if we consider the range from a = 0\nto b = 1, then our inner product is now 12/2", "start": 579.61, "duration": 9.149}, {"text": "- 02/2, which is equal to 1/2, and therefore\nno longer zero.", "start": 588.759, "duration": 5.32}, {"text": "So our functions are not orthogonal over this\nrange of values.", "start": 594.079, "duration": 5.781}, {"text": "There are many things to consider when looking\nat the inner product of two functions.", "start": 599.86, "duration": 4.11}, {"text": "In fact, often times an extra function, w(x),\nreferred to as the weight function, is added", "start": 603.97, "duration": 7.35}, {"text": "to the definition of the inner product so\nthat  equals the integral from a to", "start": 611.32, "duration": 5.709}, {"text": "b of f(x) times g(x) times w(x) dx.", "start": 617.029, "duration": 5.941}, {"text": "This weight function can be any of a variety\nof different things, meaning we can have functions", "start": 622.97, "duration": 4.69}, {"text": "that are orthogonal with respect to a variety\nof different functions.", "start": 627.66, "duration": 6.35}, {"text": "Orthogonality ends up playing a key role in\nseveral aspects of math and even science.", "start": 634.01, "duration": 4.86}, {"text": "It helps us break down systems into distinct\nelements, for easier understanding and problem solving.", "start": 638.87, "duration": 5.41}, {"text": "So to make sure we understand, let\u2019s check\ncomprehension.", "start": 645.02, "duration": 2.4}]