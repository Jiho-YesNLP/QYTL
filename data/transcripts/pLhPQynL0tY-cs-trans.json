[{"text": "All right. Hey, everyone,", "start": 3.47, "duration": 2.665}, {"text": "actually started a little bit late.", "start": 6.135, "duration": 2.085}, {"text": "So welcome to the, uh,", "start": 8.22, "duration": 2.175}, {"text": "final lecture of, uh,", "start": 10.395, "duration": 2.935}, {"text": "CS229 of this quarter or I guess,", "start": 13.33, "duration": 2.6}, {"text": "uh, to the home viewers,", "start": 15.93, "duration": 2.19}, {"text": "welcome to the season finale.", "start": 18.12, "duration": 2.92}, {"text": "So what I'd like to do today is, um,", "start": 22.19, "duration": 3.61}, {"text": "wrap up our discussion on reinforcement learning and then,", "start": 25.8, "duration": 3.945}, {"text": "um, and then we'll conclude the class.", "start": 29.745, "duration": 2.13}, {"text": "Um, so I think you know,", "start": 31.875, "duration": 2.925}, {"text": "over the last, uh,", "start": 34.8, "duration": 1.53}, {"text": "few lectures you saw a lot of,", "start": 36.33, "duration": 3.07}, {"text": "uh, uh, we- we saw a lot of NAV.", "start": 40.46, "duration": 2.68}, {"text": "So maybe as a brief interlude here are some videos.", "start": 43.14, "duration": 5.695}, {"text": "Um, so self-autonomous helicopter,", "start": 48.835, "duration": 3.614}, {"text": "um, you know, this is a project that, uh, I know Pieter Abbeel,", "start": 52.449, "duration": 3.726}, {"text": "Adam Coates, uh, some- some former students here,", "start": 56.175, "duration": 2.945}, {"text": "now some of the machine learning greats worked on when they were,", "start": 59.12, "duration": 3.105}, {"text": "um, PhD students here.", "start": 62.225, "duration": 2.615}, {"text": "Uh, and- and- and I think, uh,", "start": 64.84, "duration": 3.24}, {"text": "using algorithms similar to the ones you learned in this class,", "start": 68.08, "duration": 2.92}, {"text": "how do you make a helicopter fly?", "start": 71.0, "duration": 1.32}, {"text": "So just for fun, this is a video shot on top of", "start": 72.32, "duration": 2.34}, {"text": "one of the Stanford, uh, soccer fields.", "start": 74.66, "duration": 3.4}, {"text": "I was actually the camera man that day [LAUGHTER] ,", "start": 78.11, "duration": 3.28}, {"text": "um, and zooming out the camera.", "start": 81.39, "duration": 3.735}, {"text": "See the trees touching the sky.", "start": 85.125, "duration": 2.325}, {"text": "[BACKGROUND].", "start": 87.45, "duration": 12.24}, {"text": "Say, uh, um,", "start": 99.69, "duration": 1.83}, {"text": "it- it- it turns out- that's a small radio-controlled helicopter.", "start": 101.52, "duration": 3.85}, {"text": "It turns out that, uh,", "start": 105.37, "duration": 1.26}, {"text": "when you're very far away you can't tell if this is", "start": 106.63, "duration": 2.7}, {"text": "a small radio-controlled helicopter or if there's", "start": 109.33, "duration": 1.89}, {"text": "like a helicopter with people sitting in it [LAUGHTER].", "start": 111.22, "duration": 1.89}, {"text": "So, um, uh, there was- actually there's,", "start": 113.11, "duration": 2.93}, {"text": "uh, you know, foot is on, uh,", "start": 116.04, "duration": 2.25}, {"text": "a kind of a soccer field, the big,", "start": 118.29, "duration": 2.37}, {"text": "uh, grass field off San Hill Road and turns out across San Hill Road,", "start": 120.66, "duration": 4.91}, {"text": "um, one of the high rises there was a- there", "start": 125.57, "duration": 2.52}, {"text": "was an elderly lady that lives in one of those apartments.", "start": 128.09, "duration": 2.37}, {"text": "And when she saw that, she would call 9-1-1 and say,", "start": 130.46, "duration": 2.48}, {"text": "\"Hey, there's a helicopter about to crash.\"", "start": 132.94, "duration": 1.83}, {"text": "[LAUGHTER] And then the- the firemen would come out, so,", "start": 134.77, "duration": 4.05}, {"text": "[LAUGHTER] I had to tell them that and I- I think they were partly relieved,", "start": 138.82, "duration": 3.32}, {"text": "partly disappointed that there was no one for us for- for them to save.", "start": 142.14, "duration": 4.1}, {"text": "And, uh, um, and so- and- and I think, uh, let's see.", "start": 146.24, "duration": 5.955}, {"text": "Uh, uh, one of the things I promised to do, um,", "start": 152.195, "duration": 5.035}, {"text": "in the debugging learning algorithms lecture was just go over the um,", "start": 157.23, "duration": 5.705}, {"text": "reinforcement learning example again.", "start": 162.935, "duration": 2.055}, {"text": "So let me just do that now but, uh,", "start": 164.99, "duration": 2.025}, {"text": "with notation that I think you now understand compared to- oh, yes.", "start": 167.015, "duration": 3.675}, {"text": "Why is the helicopter flying upside down?", "start": 170.69, "duration": 2.8}, {"text": "Oh, uh, it was an aerobatic stunt.", "start": 173.49, "duration": 2.58}, {"text": "Uh, yeah, I- I don't think there's", "start": 176.07, "duration": 2.07}, {"text": "any good reason for flying a helicopter upside down [LAUGHTER] ,", "start": 178.14, "duration": 2.16}, {"text": "uh, other than that you can.", "start": 180.3, "duration": 2.55}, {"text": "Uh, there- there a lot of videos of self-autonomous helicopters flying", "start": 182.85, "duration": 3.285}, {"text": "all sorts of stunts, go to heli.stanford.edu,", "start": 186.135, "duration": 4.02}, {"text": "heli.stanford.edu and the Stanford autonomous helicopter did- did-", "start": 190.155, "duration": 5.25}, {"text": "did a lot more than flying upside down.", "start": 195.405, "duration": 2.645}, {"text": "Uh, it could, I mean,", "start": 198.05, "duration": 1.525}, {"text": "make some maneuvers that looked aerodynamically", "start": 199.575, "duration": 2.665}, {"text": "impossible such as the helicopter that looks like it's tumbling,", "start": 202.24, "duration": 3.25}, {"text": "just spinning randomly but staying same place in the air, right?", "start": 205.49, "duration": 3.55}, {"text": "Um, it's called a chaos maneuver and if you look you go, wow.", "start": 209.04, "duration": 3.375}, {"text": "This helicopter was turning upside down,", "start": 212.415, "duration": 1.545}, {"text": "spinning around the air in every single direction but it was", "start": 213.96, "duration": 1.91}, {"text": "just staying right there in the air not crashing,", "start": 215.87, "duration": 1.98}, {"text": "and so there are maneuvers like that- that, um,", "start": 217.85, "duration": 2.49}, {"text": "the very best human pilots in the world can fly with helicopters and I think,", "start": 220.34, "duration": 3.49}, {"text": "uh, this was just, uh, um, uh,", "start": 223.83, "duration": 1.65}, {"text": "a demonstration I guess, uh,", "start": 225.48, "duration": 3.18}, {"text": "and I think a lot of this work wound up influencing some of", "start": 228.66, "duration": 2.9}, {"text": "the later work on the quadcopter drones in a few research labs and.", "start": 231.56, "duration": 3.65}, {"text": "Yeah, I think, uh,", "start": 235.21, "duration": 1.63}, {"text": "it was a difficult control problem and it was, uh,", "start": 236.84, "duration": 2.61}, {"text": "it was one of those things you do when you're,", "start": 239.45, "duration": 1.69}, {"text": "you- you when you're a university and you want to solve the hardest problems around.", "start": 241.14, "duration": 4.22}, {"text": "But I wanted to step through a few of the debugging process", "start": 245.36, "duration": 4.86}, {"text": "that we went through as we were building a helicopter like this.", "start": 250.22, "duration": 3.9}, {"text": "So, uh, when you're trying to get the helicopter to fly upside down,", "start": 254.12, "duration": 3.6}, {"text": "fly stunts, you don't want to crash too often.", "start": 257.72, "duration": 1.74}, {"text": "So step one is build a model or build a simulator of a helicopter, right?", "start": 259.46, "duration": 4.515}, {"text": "Much- much as you saw, um,", "start": 263.975, "duration": 1.525}, {"text": "when we start to talk about fitted value iteration and then,", "start": 265.5, "duration": 4.28}, {"text": "um, choose a reward function, uh,", "start": 269.78, "duration": 2.445}, {"text": "like that, and it turns out that", "start": 272.225, "duration": 2.57}, {"text": "specific reward function for staying in place is not that high,", "start": 274.795, "duration": 4.105}, {"text": "you know, like the quadratic function like that works okay.", "start": 278.9, "duration": 3.425}, {"text": "But if you want the helicopter to fly aggressive maneuvers it's actually quite", "start": 282.325, "duration": 3.805}, {"text": "tricky to specify what is a good turn for a helicopter, right?", "start": 286.13, "duration": 4.81}, {"text": "Um, and then what you do is you run reinforcement learning algorithm, um,", "start": 290.94, "duration": 5.395}, {"text": "to try to maximize say", "start": 296.335, "duration": 2.125}, {"text": "the finite horizon MDP formulation and maximize sum of rewards over T timesteps,", "start": 298.46, "duration": 4.41}, {"text": "so you get a policy Pi.", "start": 302.87, "duration": 2.49}, {"text": "And then whenever you do this,", "start": 305.36, "duration": 1.89}, {"text": "the first time you do this,", "start": 307.25, "duration": 1.08}, {"text": "you find that the resulting controller does much worse than the human pilot,", "start": 308.33, "duration": 4.145}, {"text": "and the question is what do you do next, right?", "start": 312.475, "duration": 2.5}, {"text": "This is- by the way- this is almost- I think this is", "start": 314.975, "duration": 2.67}, {"text": "almost exactly the slide I showed you last time except I cleaned up the slide", "start": 317.645, "duration": 3.075}, {"text": "using reinforcement learning notation rather than", "start": 320.72, "duration": 2.46}, {"text": "the slightly simplified notation you saw", "start": 323.18, "duration": 1.885}, {"text": "before [NOISE] you learned about reinforcement learning.", "start": 325.065, "duration": 3.105}, {"text": "And so the question is, um,", "start": 328.17, "duration": 2.85}, {"text": "and- and again if you're working on", "start": 331.02, "duration": 1.52}, {"text": "the reinforcement learning problem yourself, you know, uh,", "start": 332.54, "duration": 2.74}, {"text": "there's a good chance you have to answer this question yourself for", "start": 335.28, "duration": 3.29}, {"text": "whatever robot or other reinforcement learning or", "start": 338.57, "duration": 2.58}, {"text": "factory automation or stock trading system or whatever it is,", "start": 341.15, "duration": 4.0}, {"text": "um, you are trying to get to work in reinforcement learning.", "start": 345.15, "duration": 2.17}, {"text": "But do you want to improve the model sim- model", "start": 347.32, "duration": 2.555}, {"text": "or do you want to modify the reward function or do you want to,", "start": 349.875, "duration": 2.825}, {"text": "uh, modify the reinforcement learning algorithm. All right.", "start": 352.7, "duration": 3.295}, {"text": "And modifying the reinforcement learning algorithm includes things like,", "start": 355.995, "duration": 3.615}, {"text": "uh, playing with the discretization that you're using.", "start": 359.61, "duration": 3.44}, {"text": "Um, if you're taking a continuous state MDP and discretizing it to solve over", "start": 363.05, "duration": 4.95}, {"text": "finite state MDP formulation or modifying the reinforcement learning algorithm", "start": 368.0, "duration": 3.99}, {"text": "includes also maybe choosing new features to use in fitted value iteration, right?", "start": 371.99, "duration": 4.15}, {"text": "There are a lot of things you could try.", "start": 376.14, "duration": 1.08}, {"text": "Or maybe instead of using a linear function approximator,", "start": 377.22, "duration": 3.06}, {"text": "instead of fitting a linear function for fitted value iteration.", "start": 380.28, "duration": 3.035}, {"text": "Maybe you want to use a bigger,", "start": 383.315, "duration": 1.785}, {"text": "you know, deep neural network, right?", "start": 385.1, "duration": 2.03}, {"text": "Um, but so which of these steps is the most useful thing to do?", "start": 387.13, "duration": 4.51}, {"text": "So this is the analysis of those three things, uh, you know,", "start": 391.64, "duration": 4.515}, {"text": "if, I'll give you a second to read this, right?", "start": 396.155, "duration": 4.965}, {"text": "But if these three statements are true,", "start": 401.12, "duration": 3.245}, {"text": "then the learn controller should have flown well on the helicopter.", "start": 404.365, "duration": 4.84}, {"text": "Right? Um, and so", "start": 409.205, "duration": 6.805}, {"text": "those three sentences correspond to the three things in yellow that you could work on,", "start": 416.01, "duration": 8.93}, {"text": "um, there's a problem that,", "start": 424.94, "duration": 1.965}, {"text": "you know, um, statement 1 is false,", "start": 426.905, "duration": 2.275}, {"text": "that the simulator isn't good enough,", "start": 429.18, "duration": 1.37}, {"text": "there's a problem that statement 2 is false.", "start": 430.55, "duration": 3.52}, {"text": "That, um, ah, oh,", "start": 434.25, "duration": 4.24}, {"text": "sorry I think actually two or three are reversed.", "start": 438.49, "duration": 1.77}, {"text": "But, uh, the three statements corresponds to the three things in yellow.", "start": 440.26, "duration": 2.55}, {"text": "I think the two and three are in, uh,", "start": 442.81, "duration": 1.44}, {"text": "are in, uh, opposite order, right?", "start": 444.25, "duration": 2.4}, {"text": "Ah, as the RL algorithm maximizing some rewards is a reward function,", "start": 446.65, "duration": 4.605}, {"text": "actually the right thing to maximize.", "start": 451.255, "duration": 2.34}, {"text": "And so here are the diagnostics you could use,", "start": 453.595, "duration": 2.415}, {"text": "um, to see if this helicopter simulator is accurate,", "start": 456.01, "duration": 3.42}, {"text": "uh, well, first check if,", "start": 459.43, "duration": 2.745}, {"text": "um, the policy flies well in simulation.", "start": 462.175, "duration": 3.48}, {"text": "If your policy flies well in simulation but not in real life,", "start": 465.655, "duration": 4.965}, {"text": "then this shows that the problem is with", "start": 470.62, "duration": 3.15}, {"text": "your simulator and you should try to learn a better model for your helicopter, right?", "start": 473.77, "duration": 4.275}, {"text": "And, and if you're using a linear model this with the matrices a and b, um,", "start": 478.045, "duration": 4.455}, {"text": "if, you know, st plus 1 equals ast plus bat,", "start": 482.5, "duration": 3.075}, {"text": "if you're [inaudible] try,", "start": 485.575, "duration": 1.875}, {"text": "try getting more data or maybe try a non-linear model,", "start": 487.45, "duration": 4.02}, {"text": "but if you find that the problem's not your simulator,", "start": 491.47, "duration": 3.375}, {"text": "if you find that, uh,", "start": 494.845, "duration": 1.86}, {"text": "your policy is flying poorly in simulation and flying poorly in real life,", "start": 496.705, "duration": 5.145}, {"text": "right, then this is the diagnostic I would use.", "start": 501.85, "duration": 3.585}, {"text": "Um, so I shall show these two lines.", "start": 505.435, "duration": 3.345}, {"text": "So let human be the human control policy,", "start": 508.78, "duration": 2.969}, {"text": "so hire a human pilot, right? Which, which we did.", "start": 511.749, "duration": 2.671}, {"text": "We're fortunate to have one of the best- one, one of,", "start": 514.42, "duration": 2.37}, {"text": "um, America's top, you know,", "start": 516.79, "duration": 2.835}, {"text": "aerobatic helicopter pilots working with us, and he,", "start": 519.625, "duration": 3.06}, {"text": "using his control sticks and radio control,", "start": 522.685, "duration": 2.175}, {"text": "can make a helicopter fly upside-down,", "start": 524.86, "duration": 1.815}, {"text": "tumble, do flips, loops, rolls.", "start": 526.675, "duration": 2.205}, {"text": "So we had a very good human pilot, um,", "start": 528.88, "duration": 2.415}, {"text": "help us, uh, fly the helicopter manually.", "start": 531.295, "duration": 3.06}, {"text": "So what you can do is, um,", "start": 534.355, "duration": 3.735}, {"text": "test whether or not the,", "start": 538.09, "duration": 2.565}, {"text": "uh- so this, this thing here, right?", "start": 540.655, "duration": 3.51}, {"text": "That's just a pay off of the,", "start": 544.165, "duration": 3.09}, {"text": "um, learn policy as measured on your reward function.", "start": 547.255, "duration": 5.04}, {"text": "So check if, um,", "start": 552.295, "duration": 2.085}, {"text": "the learn policy achieves a better or worse pay off than a human pilot can, right?", "start": 554.38, "duration": 7.215}, {"text": "And so that means, you know,", "start": 561.595, "duration": 1.5}, {"text": "go ahead and let the learn policy fly the helicopter and we get", "start": 563.095, "duration": 3.435}, {"text": "the human to fly the helicopter and compute the sum of rewards", "start": 566.53, "duration": 4.064}, {"text": "on the sequence of states that these two systems take the helicopter through and", "start": 570.594, "duration": 5.296}, {"text": "just see whether the human or the learn policy achieves a higher payoff,", "start": 575.89, "duration": 5.37}, {"text": "achieves a higher sum of rewards.", "start": 581.26, "duration": 2.715}, {"text": "And if, um, the payoff achieved", "start": 583.975, "duration": 3.585}, {"text": "by the learning algorithm is less than the payoff achieved by the human,", "start": 587.56, "duration": 3.285}, {"text": "then this shows that, um,", "start": 590.845, "duration": 3.03}, {"text": "the learn policy's not actually maximizing the sum of rewards, right?", "start": 593.875, "duration": 4.335}, {"text": "Because whether the human is doing, you know,", "start": 598.21, "duration": 1.845}, {"text": "he or she is doing a better job,", "start": 600.055, "duration": 1.395}, {"text": "maximizes the sum of rewards then the learn policy.", "start": 601.45, "duration": 3.21}, {"text": "So this means that you should, you know,", "start": 604.66, "duration": 2.625}, {"text": "consider working on the reinforcement learning algorithm to try to", "start": 607.285, "duration": 2.685}, {"text": "make it do a better job maximizing the sum of rewards, right?", "start": 609.97, "duration": 3.06}, {"text": "Um, and then on the flip side,", "start": 613.03, "duration": 3.09}, {"text": "this inequality goes the other way, right?", "start": 616.12, "duration": 3.105}, {"text": "Uh, so if pa- if, if the payoff or", "start": 619.225, "duration": 2.295}, {"text": "the RL algorithm is greater than the payoff of the human,", "start": 621.52, "duration": 3.69}, {"text": "then what that means is that, you know,", "start": 625.21, "duration": 3.06}, {"text": "RL algorithm is actually doing a better job,", "start": 628.27, "duration": 2.295}, {"text": "maximizing the sum of rewards,", "start": 630.565, "duration": 1.65}, {"text": "but it's still flying worse.", "start": 632.215, "duration": 1.59}, {"text": "So what this tells you is that,", "start": 633.805, "duration": 2.475}, {"text": "doing a really good job maximizing the", "start": 636.28, "duration": 1.89}, {"text": "sum of rewards does not correspond to how you actually want", "start": 638.17, "duration": 3.12}, {"text": "the helicopter to fly and so that means that maybe you should work on,", "start": 641.29, "duration": 5.655}, {"text": "um, improving the reward function,", "start": 646.945, "duration": 2.91}, {"text": "that the reward function is not capturing what's actually most", "start": 649.855, "duration": 2.715}, {"text": "important to fly a helicopter well and then,", "start": 652.57, "duration": 3.0}, {"text": "then you modify the reward function, right?", "start": 655.57, "duration": 2.475}, {"text": "So in a typical workflow,", "start": 658.045, "duration": 2.205}, {"text": "uh, hoping to describe to you what,", "start": 660.25, "duration": 2.025}, {"text": "what it feels like to work on a machine learning project like this,", "start": 662.275, "duration": 2.925}, {"text": "and this was a big multi-year machine learning project,", "start": 665.2, "duration": 2.55}, {"text": "but when you're working on a big complicated machine learning project like this,", "start": 667.75, "duration": 3.96}, {"text": "um, the bottleneck moves around meaning that you build a helicopter,", "start": 671.71, "duration": 4.08}, {"text": "you get a human pilot to fly it,", "start": 675.79, "duration": 1.605}, {"text": "you know, gets in the work,", "start": 677.395, "duration": 1.215}, {"text": "they run these diagnostics and maybe the first time you do this you'll find, wow,", "start": 678.61, "duration": 4.2}, {"text": "the simulation's really inaccurate,", "start": 682.81, "duration": 1.515}, {"text": "then you are going to work on improving the simulator for a couple months.", "start": 684.325, "duration": 3.3}, {"text": "And then, you know, and every now and then you come back and rerun", "start": 687.625, "duration": 3.225}, {"text": "this diagnostic and maybe for the first two months of the project,", "start": 690.85, "duration": 3.21}, {"text": "you keep on saying, \"Yup,", "start": 694.06, "duration": 1.155}, {"text": "simulator is not good enough, simulator", "start": 695.215, "duration": 1.275}, {"text": "is not good enough, simulator is not good enough.\"", "start": 696.49, "duration": 1.47}, {"text": "After working on the simulator for a couple months you,", "start": 697.96, "duration": 2.955}, {"text": "you may find that, um,", "start": 700.915, "duration": 1.665}, {"text": "item 1 is no longer the problem,", "start": 702.58, "duration": 2.115}, {"text": "you might then find that,", "start": 704.695, "duration": 1.26}, {"text": "um, item 3 is the problem,", "start": 705.955, "duration": 1.8}, {"text": "the simulator's now good enough,", "start": 707.755, "duration": 1.56}, {"text": "but when you run this diagnostic,", "start": 709.315, "duration": 1.92}, {"text": "two months into the project, you might say,", "start": 711.235, "duration": 1.905}, {"text": "\"Wow, looks like your RL algorithm, uh,", "start": 713.14, "duration": 2.355}, {"text": "is maximizing the reward function but this is not good flying.\"", "start": 715.495, "duration": 3.525}, {"text": "So now I think the biggest problem for the project or the biggest bottleneck", "start": 719.02, "duration": 3.54}, {"text": "for the project is that the ref- the reward function is not good enough,", "start": 722.56, "duration": 3.585}, {"text": "and then you might spend, you know,", "start": 726.145, "duration": 1.455}, {"text": "another one or two,", "start": 727.6, "duration": 1.11}, {"text": "or three, or, or,", "start": 728.71, "duration": 1.08}, {"text": "or sometimes longer months working to try to improve the reward function,", "start": 729.79, "duration": 3.84}, {"text": "then you might do that for a while,", "start": 733.63, "duration": 1.23}, {"text": "and then when the reward function is good enough then that exposes", "start": 734.86, "duration": 3.24}, {"text": "the next problem in your system which might be that the RL algorithm isn't good enough.", "start": 738.1, "duration": 3.51}, {"text": "And so the problem you should be working on", "start": 741.61, "duration": 2.655}, {"text": "actually moves around and it's different in different phases of the project.", "start": 744.265, "duration": 3.765}, {"text": "And, um, when you're working on this it feels like every time you", "start": 748.03, "duration": 4.26}, {"text": "solve the current problem that exposes the next most important problem to work", "start": 752.29, "duration": 4.47}, {"text": "on and then you work on that and you solve that then this helps you identify and", "start": 756.76, "duration": 4.14}, {"text": "expose the next most important problem to work on", "start": 760.9, "duration": 2.16}, {"text": "and you kind of keep doing that or you keep iterating,", "start": 763.06, "duration": 2.58}, {"text": "and keep solving problems until hopefully,", "start": 765.64, "duration": 2.145}, {"text": "you get a helicopter that does what you want it to, make sense?", "start": 767.785, "duration": 3.81}, {"text": "Okay. Um, but I think [NOISE] teams that have the discipline to, um,", "start": 771.595, "duration": 6.795}, {"text": "prioritize according to diagnostics like this,", "start": 778.39, "duration": 2.67}, {"text": "uh, tend to be much more efficient,", "start": 781.06, "duration": 1.53}, {"text": "the teams that kind of go by gut feeling in terms of selecting,", "start": 782.59, "duration": 3.765}, {"text": "you know, what to, what to spend the time on.", "start": 786.355, "duration": 2.985}, {"text": "All right, um, any,", "start": 789.34, "duration": 2.055}, {"text": "any questions about this?", "start": 791.395, "duration": 1.065}, {"text": "[inaudible].", "start": 792.46, "duration": 3.96}, {"text": "Oh, sorry, say that again.", "start": 796.42, "duration": 0.93}, {"text": "[inaudible] the simulator's", "start": 797.35, "duration": 4.14}, {"text": "accurate [inaudible].", "start": 801.49, "duration": 8.13}, {"text": "Yeah, uh, I, I kind of wanna say yes,", "start": 809.62, "duration": 2.91}, {"text": "um, let me think.", "start": 812.53, "duration": 1.665}, {"text": "Yeah, I would usually check step 1 first and then", "start": 814.195, "duration": 3.375}, {"text": "if I think simulator is okay then look at steps 2 and 3.", "start": 817.57, "duration": 3.765}, {"text": "Um, maybe one, one other thing, uh, er, about,", "start": 821.335, "duration": 2.685}, {"text": "when you work on these projects there is some judgment involved so I", "start": 824.02, "duration": 3.33}, {"text": "think I'm presenting these things as though- as a rigid mathematical formula,", "start": 827.35, "duration": 3.78}, {"text": "that's cut and dry, this formula says,", "start": 831.13, "duration": 1.8}, {"text": "now work on step 1, then this one says,", "start": 832.93, "duration": 1.74}, {"text": "now work on step 3.", "start": 834.67, "duration": 1.38}, {"text": "Um, there is, there is, um,", "start": 836.05, "duration": 1.86}, {"text": "more judgment involved because when you run these diagnostics you might say, well,", "start": 837.91, "duration": 3.93}, {"text": "it looks like the simulators not that good but it's kinda good,", "start": 841.84, "duration": 2.76}, {"text": "it's little bit ambiguous, and oh it looks like,", "start": 844.6, "duration": 2.13}, {"text": "you know, uh, and so that's what it often feels like.", "start": 846.73, "duration": 2.55}, {"text": "And so a team would get together,", "start": 849.28, "duration": 1.86}, {"text": "look at the evidence from all three steps and then say, you know, \"Well,", "start": 851.14, "duration": 3.51}, {"text": "maybe the simulator is not that good but it's maybe good", "start": 854.65, "duration": 2.34}, {"text": "enough and but both the reinforcement- the,", "start": 856.99, "duration": 2.37}, {"text": "the reward function is really bad, let's focus on that.\"", "start": 859.36, "duration": 2.4}, {"text": "So there is some,", "start": 861.76, "duration": 1.575}, {"text": "um- so rather than a hard and fast rule there,", "start": 863.335, "duration": 2.49}, {"text": "there is some judgment needed to,", "start": 865.825, "duration": 2.22}, {"text": "to make these decisions,", "start": 868.045, "duration": 1.5}, {"text": "uh, but having a,", "start": 869.545, "duration": 1.695}, {"text": "um- so when leading machine learning teams often my teams will, you know,", "start": 871.24, "duration": 3.735}, {"text": "run these diagnostics, get together and look at", "start": 874.975, "duration": 2.065}, {"text": "the evidence and then discuss and debate what's the best way to move forward,", "start": 877.04, "duration": 3.195}, {"text": "but I think the process in making sure that discussion and", "start": 880.235, "duration": 2.295}, {"text": "the debate is much better than the alternative,", "start": 882.53, "duration": 2.49}, {"text": "which is, you know,", "start": 885.02, "duration": 1.02}, {"text": "someone just picks something kind of at random and,", "start": 886.04, "duration": 2.865}, {"text": "and the team does that, right?", "start": 888.905, "duration": 2.145}, {"text": "Yeah, okay. Cool. Um-", "start": 891.05, "duration": 5.415}, {"text": "All right, cool.", "start": 896.465, "duration": 2.125}, {"text": "So, um, just, uh,", "start": 898.59, "duration": 4.38}, {"text": "yeah maybe you, while I have the laptop up, you know,", "start": 902.97, "duration": 3.33}, {"text": "a little bit for fun but a little bit because I'm,", "start": 906.3, "duration": 2.25}, {"text": "uh, to illustrate fitted value iteration.", "start": 908.55, "duration": 3.615}, {"text": "Um, let me just show another,", "start": 912.165, "duration": 2.31}, {"text": "um, reinforcement learning video.", "start": 914.475, "duration": 2.61}, {"text": "Um, oh, by the way,", "start": 917.085, "duration": 1.5}, {"text": "one of the- I- I think if I look at the future of AI,", "start": 918.585, "duration": 2.565}, {"text": "the future of machine learning, you know,", "start": 921.15, "duration": 1.6}, {"text": "there's a lot of hype about reinforcement learning for game playing which is fine.", "start": 922.75, "duration": 3.575}, {"text": "You know, we all like- we all love, uh,", "start": 926.325, "duration": 3.03}, {"text": "computers playing computer games,", "start": 929.355, "duration": 1.695}, {"text": "like that's a great thing I think or something, er.", "start": 931.05, "duration": 2.405}, {"text": "But- but I think that some of the most exciting applications of", "start": 933.455, "duration": 2.52}, {"text": "reinforcement learning coming down the pipe I think will be robotics.", "start": 935.975, "duration": 2.775}, {"text": "So I think over the next few years,", "start": 938.75, "duration": 1.26}, {"text": "even though there are", "start": 940.01, "duration": 1.5}, {"text": "only a few success stories of reinforcement learning applied to robotics.", "start": 941.51, "duration": 3.33}, {"text": "There are more and more right now.", "start": 944.84, "duration": 1.61}, {"text": "One of the trends I see, you know, when you look at, uh,", "start": 946.45, "duration": 2.795}, {"text": "the academic publications and some of the things making", "start": 949.245, "duration": 2.655}, {"text": "their way into industrial environments is I think in the next several years,", "start": 951.9, "duration": 3.27}, {"text": "just based on the stuff I see,", "start": 955.17, "duration": 1.2}, {"text": "my friends in many different companies,", "start": 956.37, "duration": 1.74}, {"text": "in many different institutes working on,", "start": 958.11, "duration": 1.59}, {"text": "I think there will be a rise of, uh,", "start": 959.7, "duration": 1.8}, {"text": "reinforcement learning algorithms applied to robotics.", "start": 961.5, "duration": 2.46}, {"text": "I think this would be one important area to- to- to watch out for. All right.", "start": 963.96, "duration": 4.67}, {"text": "Uh, but, uh, uh,", "start": 968.63, "duration": 2.91}, {"text": "so, you know, uh, uh,", "start": 971.54, "duration": 2.025}, {"text": "this is another Stanford video,", "start": 973.565, "duration": 1.935}, {"text": "this is again just using reinforcement learning to get a robot dog,", "start": 975.5, "duration": 3.21}, {"text": "um, to climb over obstacles like these.", "start": 978.71, "duration": 4.03}, {"text": "Uh, my friends that were less generous, um [NOISE] ,", "start": 982.74, "duration": 4.14}, {"text": "uh, uh, did not want to think of this as a robot dog.", "start": 986.88, "duration": 3.78}, {"text": "Uh, they thought it was more like a robot cockroach, uh, [LAUGHTER].", "start": 990.66, "duration": 4.26}, {"text": "But I think cockroaches don't have four legs, right,", "start": 994.92, "duration": 1.815}, {"text": "cockroaches have six legs [LAUGHTER].", "start": 996.735, "duration": 5.205}, {"text": "Um, yeah but so,", "start": 1001.94, "duration": 13.8}, {"text": "uh, how do you program a robot dog like this,", "start": 1015.74, "duration": 2.595}, {"text": "right, to, uh, climb over terrain?", "start": 1018.335, "duration": 3.375}, {"text": "So one of the key components, this is work by,", "start": 1021.71, "duration": 1.89}, {"text": "um, Zico Kolter, uh,", "start": 1023.6, "duration": 2.325}, {"text": "now a Carnegie Mellon professor, uh,", "start": 1025.925, "duration": 2.685}, {"text": "another one of the machine learning greats, uh, is,", "start": 1028.61, "duration": 3.09}, {"text": "ah, ah, a key part of this was,", "start": 1031.7, "duration": 2.94}, {"text": "ah, value function of approximation, uh,", "start": 1034.64, "duration": 2.37}, {"text": "where it- dog starts on the left and it goes get to the right then, uh,", "start": 1037.01, "duration": 4.455}, {"text": "the approximate value function kind of,", "start": 1041.465, "duration": 3.105}, {"text": "um, ah, I- I'm- I'm sort of finding a little bit, right?", "start": 1044.57, "duration": 2.97}, {"text": "But- but the approximate value function tells it,", "start": 1047.54, "duration": 2.7}, {"text": "uh, given the 3D shape of the terrain, uh,", "start": 1050.24, "duration": 3.09}, {"text": "the middle plot  is a height map where", "start": 1053.33, "duration": 2.19}, {"text": "the different shades tell you how- how- how tall is the terrain,", "start": 1055.52, "duration": 3.375}, {"text": "uh, but given the shape of the terrain, the dog, uh,", "start": 1058.895, "duration": 3.915}, {"text": "learns a value function that tells it what is the cost of putting", "start": 1062.81, "duration": 3.78}, {"text": "his feet on different locations to the terrain and it learns among other things,", "start": 1066.59, "duration": 4.2}, {"text": "you know, not to put his feet at the edge of a cliff because then it's", "start": 1070.79, "duration": 3.12}, {"text": "likely to slip off the edge of a cliff and fall over, right?", "start": 1073.91, "duration": 3.21}, {"text": "And so, um, but- but hopefully this gives a visualization of whether,", "start": 1077.12, "duration": 4.44}, {"text": "uh, learn value function for a very complicated function they'll say.", "start": 1081.56, "duration": 3.855}, {"text": "And- and the state is very high-dimensional,", "start": 1085.415, "duration": 2.174}, {"text": "this is all kind of projected onto a 2D space so you can visualize it.", "start": 1087.589, "duration": 3.016}, {"text": "But- but this is what, uh,", "start": 1090.605, "duration": 1.695}, {"text": "a simplified value function looks like for a robot like this.", "start": 1092.3, "duration": 2.925}, {"text": "Okay. All right.", "start": 1095.225, "duration": 3.855}, {"text": "So with that,", "start": 1099.08, "duration": 1.86}, {"text": "um, let me return to the white board [BACKGROUND] um.", "start": 1100.94, "duration": 4.44}, {"text": "So, um, there's just one class of algorithms I want to describe to you", "start": 1105.38, "duration": 22.47}, {"text": "today which are called policy search algorithms.", "start": 1127.85, "duration": 4.51}, {"text": "And uh, sometimes, uh, policy searches are also called,", "start": 1135.1, "duration": 5.86}, {"text": "uh, direct policy search.", "start": 1140.96, "duration": 2.83}, {"text": "And, um, to explain what this means,", "start": 1147.28, "duration": 4.78}, {"text": "so far our approach to reinforcement learning has", "start": 1152.06, "duration": 3.72}, {"text": "been to first learn or approximate the value function,", "start": 1155.78, "duration": 4.005}, {"text": "you know, approximate V star and then use that", "start": 1159.785, "duration": 2.925}, {"text": "to learn or at least hopefully approximate Pi star, right?", "start": 1162.71, "duration": 3.495}, {"text": "So we have- you saw value iteration,", "start": 1166.205, "duration": 2.205}, {"text": "top, we had policy iteration.", "start": 1168.41, "duration": 1.35}, {"text": "But philosophy to reinforcement learning was", "start": 1169.76, "duration": 1.92}, {"text": "to estimate the value function and then use that,", "start": 1171.68, "duration": 2.69}, {"text": "you know, that equation with the arg max to figure out what is Pi star.", "start": 1174.37, "duration": 3.405}, {"text": "So this is an indirect way of getting a policy", "start": 1177.775, "duration": 2.925}, {"text": "because we- we first try to figure out what's the value function.", "start": 1180.7, "duration": 3.72}, {"text": "In direct policy search, um,", "start": 1184.42, "duration": 2.715}, {"text": "we try to find a good policy directly,", "start": 1187.135, "duration": 9.175}, {"text": "hence the term direct policy search because you", "start": 1196.75, "duration": 3.04}, {"text": "don't- you go straight for trying to find a good policy", "start": 1199.79, "duration": 2.43}, {"text": "without the intermediate step of finding an approximation to the value function.", "start": 1202.22, "duration": 5.85}, {"text": "So, um, let's see.", "start": 1208.07, "duration": 3.345}, {"text": "I'm going to use, uh,", "start": 1211.415, "duration": 1.755}, {"text": "as the motivating example the inverted pendulum.", "start": 1213.17, "duration": 3.57}, {"text": "Right. So that is that thing with a free hinge here,", "start": 1216.74, "duration": 3.81}, {"text": "and let's say your actions are to accelerate left or to accelerate right, right?", "start": 1220.55, "duration": 5.32}, {"text": "And then you can have- and you can have states to accelerate strong,", "start": 1225.87, "duration": 3.68}, {"text": "accelerate less strong, accelerate right.", "start": 1229.55, "duration": 1.29}, {"text": "You got more than two actions but let's just say you've", "start": 1230.84, "duration": 2.19}, {"text": "inverted pendulum with, um, two actions.", "start": 1233.03, "duration": 3.675}, {"text": "So, um, if you", "start": 1236.705, "duration": 3.6}, {"text": "want to- I- I'll- I'll talk about pros and cons of direct policy search later.", "start": 1240.305, "duration": 3.375}, {"text": "But if you want to apply polic- direct policy search,", "start": 1243.68, "duration": 3.03}, {"text": "you want to apply policy search,", "start": 1246.71, "duration": 1.47}, {"text": "the first step is to,", "start": 1248.18, "duration": 1.89}, {"text": "um, come up with the class of policies you'll entertain or", "start": 1250.07, "duration": 3.87}, {"text": "come up with the set of functions you use to approximate the policy.", "start": 1253.94, "duration": 4.05}, {"text": "So, um, again to make an analogy,", "start": 1257.99, "duration": 2.91}, {"text": "when, uh, you saw logistic regression for the first time, you know,", "start": 1260.9, "duration": 4.41}, {"text": "we kind of said that we would approximate y as the hypothesis,", "start": 1265.31, "duration": 5.38}, {"text": "um, right, whose form was governed by this sigmoid function.", "start": 1272.8, "duration": 4.84}, {"text": "And you remember in week 2 when,", "start": 1277.64, "duration": 3.825}, {"text": "uh, I first described logistic regression,", "start": 1281.465, "duration": 2.55}, {"text": "I kind of pulled this out of a hat,", "start": 1284.015, "duration": 1.845}, {"text": "right, and said, \"Oh yeah, trust me,", "start": 1285.86, "duration": 1.65}, {"text": "let's use the logistic function,\" and- and then later,", "start": 1287.51, "duration": 2.37}, {"text": "we saw this was a special case of the generalized linear model.", "start": 1289.88, "duration": 3.015}, {"text": "Um, but, you know,", "start": 1292.895, "duration": 2.07}, {"text": "we just had to write down some form for how we will predict y as a function of x.", "start": 1294.965, "duration": 5.655}, {"text": "So in direct policy search,", "start": 1300.62, "duration": 2.64}, {"text": "we will have to come up with a form for Pi, right?", "start": 1303.26, "duration": 4.62}, {"text": "So we have to just come up with a function for algorithms in h. Um,", "start": 1307.88, "duration": 3.96}, {"text": "in direct policy search,", "start": 1311.84, "duration": 1.2}, {"text": "we'll have to come up with a way for how we approximate the policy Pi.", "start": 1313.04, "duration": 4.275}, {"text": "Right? And so, you know,", "start": 1317.315, "duration": 1.485}, {"text": "one thing we have to do is say, well,", "start": 1318.8, "duration": 1.77}, {"text": "maybe the action were approximate with some policy Pi, um,", "start": 1320.57, "duration": 5.535}, {"text": "maybe parameterized by Theta and is now a function of the state,", "start": 1326.105, "duration": 4.98}, {"text": "and maybe it'll be 1 over 1 plus e to the negative Theta transpose,", "start": 1331.085, "duration": 5.91}, {"text": "you know, to state vector.", "start": 1336.995, "duration": 2.295}, {"text": "Right? Where the same vector maybe something like,", "start": 1339.29, "duration": 3.285}, {"text": "um, x, x dot, uh,", "start": 1342.575, "duration": 1.935}, {"text": "and- and the angle- and the angle dot right", "start": 1344.51, "duration": 3.63}, {"text": "if- if this angle is Phi and maybe add an intercept there.", "start": 1348.14, "duration": 3.96}, {"text": "Okay. And- and I- I switch this from Theta to Phi to avoid,", "start": 1352.1, "duration": 4.59}, {"text": "uh, conflict in the notation.", "start": 1356.69, "duration": 2.07}, {"text": "Okay. Um, this isn't really the formative policy we'll write.", "start": 1358.76, "duration": 3.06}, {"text": "So let me- let me make one more definition and then I'll, um,", "start": 1361.82, "duration": 3.27}, {"text": "show you a form of a specific form of policy you can use,", "start": 1365.09, "duration": 3.18}, {"text": "but it's actually not quite this.", "start": 1368.27, "duration": 1.02}, {"text": "We'll- we'll need to tweak this a little bit.", "start": 1369.29, "duration": 2.31}, {"text": "So, uh, the direct policy search algorithm we'll use,", "start": 1371.6, "duration": 4.29}, {"text": "will use a stochastic policy.", "start": 1375.89, "duration": 2.31}, {"text": "So this is a new definition.", "start": 1378.2, "duration": 3.13}, {"text": "Um, so stochastic policy is a function.", "start": 1383.62, "duration": 12.385}, {"text": "Right.", "start": 1396.005, "duration": 14.925}, {"text": "Um, so we're going to use,", "start": 1410.93, "duration": 18.075}, {"text": "um, for the direct policy search algorithm that you see today,", "start": 1429.005, "duration": 3.285}, {"text": "we are going to use stochastic policies meaning that,", "start": 1432.29, "duration": 3.24}, {"text": "um, on every time step, uh,", "start": 1435.53, "duration": 3.4}, {"text": "the policy will tell you what's the chance you want to", "start": 1438.93, "duration": 3.53}, {"text": "accelerate left versus what's the chance you want to accelerate right,", "start": 1442.46, "duration": 3.84}, {"text": "and then you use a random gen- number generator to select either left or", "start": 1446.3, "duration": 4.08}, {"text": "right to accelerate on the inverted pendulum depending on the policies- no,", "start": 1450.38, "duration": 4.035}, {"text": "depending on the probabilities output by this policy.", "start": 1454.415, "duration": 3.54}, {"text": "Okay. Um, and so here's one example.", "start": 1457.955, "duration": 5.265}, {"text": "Um, let's see which is", "start": 1463.22, "duration": 5.255}, {"text": "you can have [BACKGROUND].", "start": 1468.475, "duration": 9.615}, {"text": "So, you know, continuing with the inverted pendulum,", "start": 1478.09, "duration": 2.8}, {"text": "here's one policy that, um,", "start": 1480.89, "duration": 2.13}, {"text": "[BACKGROUND] might be reasonable,", "start": 1483.02, "duration": 8.22}, {"text": "uh, where you say that, um, let's see.", "start": 1491.24, "duration": 6.7}, {"text": "Right. So, you know,", "start": 1501.85, "duration": 6.655}, {"text": "in a state s, the chance that you take", "start": 1508.505, "duration": 2.775}, {"text": "the accelerate right action is given by this sigmoid function.", "start": 1511.28, "duration": 4.65}, {"text": "And the chance that in a state s,", "start": 1515.93, "duration": 2.805}, {"text": "you take the accelerate left action is given by that.", "start": 1518.735, "duration": 7.26}, {"text": "Okay. Um, and here's one example for why this might be a reasonable policy.", "start": 1525.995, "duration": 5.205}, {"text": "So let's say the state vector s is 1, x,", "start": 1531.2, "duration": 3.015}, {"text": "x dot phi, phi dot, um,", "start": 1534.215, "duration": 4.335}, {"text": "where, you know, this angle of the inverted pendulum,", "start": 1538.55, "duration": 7.065}, {"text": "um, is the angle phi.", "start": 1545.615, "duration": 2.28}, {"text": "And let's say for the sake of argument that", "start": 1547.895, "duration": 3.36}, {"text": "we set the parameter of this policy phi to be,", "start": 1551.255, "duration": 3.735}, {"text": "um, 0, 0, 0, 1, 0.", "start": 1554.99, "duration": 6.27}, {"text": "So in this case,", "start": 1561.26, "duration": 2.04}, {"text": "this is saying that, um,", "start": 1563.3, "duration": 2.28}, {"text": "let's see, so theta transpose s is just equal to phi, right?", "start": 1565.58, "duration": 5.07}, {"text": "And so in this case, uh, right, because,", "start": 1570.65, "duration": 3.435}, {"text": "you know, theta transpose s is just 1 times phi,", "start": 1574.085, "duration": 2.025}, {"text": "everything else gets multiplied by 0.", "start": 1576.11, "duration": 1.5}, {"text": "And so in this case is saying that the chance you accelerate to", "start": 1577.61, "duration": 2.73}, {"text": "the right is equal to 1 over 1 plus e to the negative,", "start": 1580.34, "duration": 5.4}, {"text": "how far is the pole tilted over to the right.", "start": 1585.74, "duration": 3.255}, {"text": "Um, and so this policy gives you", "start": 1588.995, "duration": 2.925}, {"text": "the effect that the further the pole is tilted to the right,", "start": 1591.92, "duration": 3.525}, {"text": "the more aggressively you want to accelerate to the right, okay?", "start": 1595.445, "duration": 4.215}, {"text": "So this is a very simple policy,", "start": 1599.66, "duration": 1.71}, {"text": "it's not a great policy,", "start": 1601.37, "duration": 1.29}, {"text": "but it's not a totally unreasonable policy, which is well,", "start": 1602.66, "duration": 2.52}, {"text": "look at how far the pole is tilted to the left or the right, apply sigmoid function,", "start": 1605.18, "duration": 4.11}, {"text": "and then accelerate to the left or right, you know,", "start": 1609.29, "duration": 2.43}, {"text": "depending on how far it's tilted to the right.", "start": 1611.72, "duration": 2.79}, {"text": "Um, now, uh, and,", "start": 1614.51, "duration": 2.67}, {"text": "and, and because this is the, right,", "start": 1617.18, "duration": 2.55}, {"text": "so this is really the chance of taking the accelerate right action as a function of the,", "start": 1619.73, "duration": 8.355}, {"text": "um, pole angle Pi, right?", "start": 1628.085, "duration": 2.94}, {"text": "Now, this is not the best policy because it ignores all the features other than phi.", "start": 1631.025, "duration": 7.425}, {"text": "Um, but if you were to set theta equals,", "start": 1638.45, "duration": 3.974}, {"text": "you know, 0, negative 0.5,", "start": 1642.424, "duration": 2.866}, {"text": "0, 1, 0, then this policy,", "start": 1645.29, "duration": 5.549}, {"text": "um, the negative 0.5 now multiplies into the x position.", "start": 1650.839, "duration": 4.546}, {"text": "Right. Uh, now this new policy if you have this value of theta,", "start": 1655.385, "duration": 4.755}, {"text": "it takes into account how far is your cart is already to the right,", "start": 1660.14, "duration": 4.695}, {"text": "um, where I guess this is the x distance, right?", "start": 1664.835, "duration": 4.905}, {"text": "And the further your cart is already, I guess if,", "start": 1669.74, "duration": 2.745}, {"text": "if your cart is on a set of rails,", "start": 1672.485, "duration": 1.785}, {"text": "right, is on a set of railway track.", "start": 1674.27, "duration": 1.875}, {"text": "And you don't want to fall off the rail- and you want to keep the cart kind of centered,", "start": 1676.145, "duration": 3.405}, {"text": "you don't want it to fall off the end of your table.", "start": 1679.55, "duration": 2.07}, {"text": "But this now says the further this is to the right already well,", "start": 1681.62, "duration": 3.09}, {"text": "the less likely you should be to accelerate to the right.", "start": 1684.71, "duration": 2.67}, {"text": "Okay? And so maybe this is", "start": 1687.38, "duration": 1.8}, {"text": "a slightly better policy than with this set of parameters.", "start": 1689.18, "duration": 3.54}, {"text": "And more generally, what you would like is to", "start": 1692.72, "duration": 5.07}, {"text": "come up with five numbers that tells you how to trade off,", "start": 1697.79, "duration": 4.47}, {"text": "how much you should accelerate to the right based on the position, velocity, angle,", "start": 1702.26, "duration": 4.74}, {"text": "and angular velocity, um,", "start": 1707.0, "duration": 2.235}, {"text": "of the current state of the cart- of the,", "start": 1709.235, "duration": 2.88}, {"text": "of the inverted pendulum.", "start": 1712.115, "duration": 1.455}, {"text": "And what a direct policy search algorithm will do is, um,", "start": 1713.57, "duration": 3.975}, {"text": "help you come up with a set of numbers that results", "start": 1717.545, "duration": 3.795}, {"text": "in hopefully a reasonable policy for controlling the inverted pendulum.", "start": 1721.34, "duration": 3.93}, {"text": "Hope- and in a policy that hopefully result in a appropriate set of", "start": 1725.27, "duration": 3.87}, {"text": "probabilities that cause it to", "start": 1729.14, "duration": 1.755}, {"text": "accelerate to the right whenever it's good to do so and accelerate to the left,", "start": 1730.895, "duration": 2.895}, {"text": "you know, more often when it's good to do so.", "start": 1733.79, "duration": 2.22}, {"text": "Okay. So, um, all right.", "start": 1736.01, "duration": 7.24}, {"text": "So our goal is to find the parame- find parameters", "start": 1743.83, "duration": 7.48}, {"text": "theta so that when", "start": 1751.31, "duration": 6.6}, {"text": "we execute pi of s,", "start": 1757.91, "duration": 5.46}, {"text": "a, um, we maximize, well,", "start": 1763.37, "duration": 9.285}, {"text": "max over theta the expected value of R of s_0 is 0 plus dot,", "start": 1772.655, "duration": 5.055}, {"text": "dot, dot, plus, okay?", "start": 1777.71, "duration": 11.52}, {"text": "Um, and so the reward function could be negative", "start": 1789.23, "duration": 3.18}, {"text": "1 whenever the inverted pendulum falls over,", "start": 1792.41, "duration": 2.895}, {"text": "uh, and 9 whenever it stays up that of, of, of, whatever,", "start": 1795.305, "duration": 2.985}, {"text": "or something that measures how well your inverted pendulum is doing.", "start": 1798.29, "duration": 2.85}, {"text": "But the goal of a direct policy search algorithm is to", "start": 1801.14, "duration": 3.84}, {"text": "choose a set of parameters theta so that we execute the policy,", "start": 1804.98, "duration": 3.96}, {"text": "you maximize your expected payoff.", "start": 1808.94, "duration": 2.16}, {"text": "And I'm gonna use the finite horizon setting,", "start": 1811.1, "duration": 2.115}, {"text": "um, for the algorithm that we'll talk about today.", "start": 1813.215, "duration": 2.7}, {"text": "Okay? Uh, and then one,", "start": 1815.915, "duration": 2.4}, {"text": "one other difference between policy search compared to, um,", "start": 1818.315, "duration": 5.73}, {"text": "estimating the value function is that in direct policy search here s_0 is,", "start": 1824.045, "duration": 7.155}, {"text": "um, a fixed initial state, okay?", "start": 1831.2, "duration": 7.995}, {"text": "Um, it turns out that when we were estimating the value function v-star,", "start": 1839.195, "duration": 6.27}, {"text": "um, you found the best possible policy for starting from any state.", "start": 1845.465, "duration": 4.575}, {"text": "Right. And there's kind of no matter what state you start from is", "start": 1850.04, "duration": 2.31}, {"text": "simultaneously the best possible policy for all states.", "start": 1852.35, "duration": 3.06}, {"text": "In direct policy search,", "start": 1855.41, "duration": 1.245}, {"text": "we assume that either there's a fixed start state- fixed", "start": 1856.655, "duration": 3.345}, {"text": "initial state s0 or there's a fixed distribution over initial state.", "start": 1860.0, "duration": 4.17}, {"text": "So I'm gonna try to maximize the expected reward with respect to your initial state or", "start": 1864.17, "duration": 4.26}, {"text": "respect to your initial probability distribution over what is the initial state.", "start": 1868.43, "duration": 4.56}, {"text": "Okay. So that's, that's one other, um, difference.", "start": 1872.99, "duration": 5.41}, {"text": "So, um, let me think how I'm going to do this. All right.", "start": 1888.97, "duration": 5.89}, {"text": "So let's write this out.", "start": 1894.86, "duration": 5.01}, {"text": "The goal is to maximize overall theta,", "start": 1899.87, "duration": 4.005}, {"text": "the expected value of R of s_0,", "start": 1903.875, "duration": 2.865}, {"text": "a_0, plus R of s_1,", "start": 1906.74, "duration": 3.075}, {"text": "a_1, plus dot, dot,", "start": 1909.815, "duration": 1.815}, {"text": "dot up to R of sc, aT", "start": 1911.63, "duration": 1.77}, {"text": "um, you know, given pi theta.", "start": 1913.4, "duration": 8.32}, {"text": "And, um, in order to simplify the math we'll write on this board today,", "start": 1922.39, "duration": 7.42}, {"text": "um, I'm just going to set T equals 1 to simplify the math,", "start": 1929.81, "duration": 4.95}, {"text": "uh, in order to not carry such a long summation.", "start": 1934.76, "duration": 2.76}, {"text": "But it turns out that, um,", "start": 1937.52, "duration": 1.8}, {"text": "uh, so I'm just gonna do like a 2 times set MDP, uh,", "start": 1939.32, "duration": 3.495}, {"text": "just to simplify the derivation,", "start": 1942.815, "duration": 1.425}, {"text": "but everything works, you know,", "start": 1944.24, "duration": 1.53}, {"text": "just with a longer sum if you,", "start": 1945.77, "duration": 2.055}, {"text": "uh, have a more general version of T. Okay.", "start": 1947.825, "duration": 2.865}, {"text": "Um, and so this term here,", "start": 1950.69, "duration": 3.255}, {"text": "the expectation is equal to sum over all possible state action sequences, right?", "start": 1953.945, "duration": 6.645}, {"text": "And again, this will go up to sT and aT.", "start": 1960.59, "duration": 2.43}, {"text": "But as we said T equals 1 of,", "start": 1963.02, "duration": 3.36}, {"text": "um, what's the chance your MDP starts out in some state s_0?", "start": 1966.38, "duration": 4.62}, {"text": "So this is your initial state distribution times the chance that in", "start": 1971.0, "duration": 5.46}, {"text": "that state you take the first action a_0- oh, actually sorry.", "start": 1976.46, "duration": 6.345}, {"text": "Let me just- let me write this out.", "start": 1982.805, "duration": 2.04}, {"text": "Right. So the chance of your MDP going through this state action sequence, right,", "start": 1984.845, "duration": 7.59}, {"text": "times, times that, right.", "start": 1992.435, "duration": 9.75}, {"text": "So that's what it means to sort of compute the expected value of, uh, the payoff.", "start": 2002.185, "duration": 6.39}, {"text": "Um, and so instead of writing out this sum,", "start": 2008.575, "duration": 5.025}, {"text": "I'm just gonna call this the payoff, right?", "start": 2013.6, "duration": 4.35}, {"text": "And so this is equal to sum of s_0,", "start": 2017.95, "duration": 4.125}, {"text": "a_0, s_1, s_1, a_1 of the chance your MDP starts in state 0,", "start": 2022.075, "duration": 4.83}, {"text": "times the chance that in state 0,", "start": 2026.905, "duration": 2.895}, {"text": "you end up choosing the action a_0 times, um, uh,", "start": 2029.8, "duration": 4.935}, {"text": "the chance governed by", "start": 2034.735, "duration": 2.025}, {"text": "the state transition probabilities that you end up in state 1, uh, state s_1,", "start": 2036.76, "duration": 5.835}, {"text": "times the chance at state s_1 you end up choosing, let's see,", "start": 2042.595, "duration": 3.72}, {"text": "s_1 and then times the payoff, okay.", "start": 2046.315, "duration": 9.025}, {"text": "And so what we're going to really do is, um,", "start": 2055.77, "duration": 4.66}, {"text": "derive a gradient ascent algorithm- actually a stochastic gradient ascent algorithm as", "start": 2060.43, "duration": 5.28}, {"text": "a function of theta to maximize this thing- to maximize the expected value of this thing.", "start": 2065.71, "duration": 5.63}, {"text": "And that- and this is a, um,", "start": 2071.34, "duration": 2.415}, {"text": "this is how we'll do direct policy search.", "start": 2073.755, "duration": 3.21}, {"text": "Okay. So let me just write out the algorithm,", "start": 2076.965, "duration": 4.19}, {"text": "and then we'll go through why, um,", "start": 2081.155, "duration": 3.71}, {"text": "the algorithm that I write down is maximizing this expected payoff.", "start": 2084.865, "duration": 4.605}, {"text": "[NOISE].", "start": 2089.47, "duration": 8.725}, {"text": "So this algorithm is called the, um, reinforce algorithm.", "start": 2098.195, "duration": 10.125}, {"text": "Ah, the objective of the reinforce algorithm, um, uh,", "start": 2108.32, "duration": 3.42}, {"text": "had a few other bells and whistles, but,", "start": 2111.74, "duration": 2.07}, {"text": "but I'm gonna to explain the core of the idea.", "start": 2113.81, "duration": 4.15}, {"text": "But the reinforcing- the reinforce algorithm, um,", "start": 2118.0, "duration": 4.135}, {"text": "does the following which is you're going to run your MDP, right?", "start": 2122.135, "duration": 8.37}, {"text": "And just you know run it for a trajectory of T timesteps.", "start": 2130.505, "duration": 3.105}, {"text": "So, um, again, you know,", "start": 2133.61, "duration": 1.725}, {"text": "I'm just gonna, [NOISE] well.", "start": 2135.335, "duration": 2.25}, {"text": "Right. And and actually you would, uh, right.", "start": 2137.585, "duration": 5.94}, {"text": "Technically, you would, um,", "start": 2143.525, "duration": 2.595}, {"text": "run it for T timesteps but, you know,", "start": 2146.12, "duration": 3.39}, {"text": "let, let's just say for now,", "start": 2149.51, "duration": 1.14}, {"text": "we'll - we'll do only the thing in blue.", "start": 2150.65, "duration": 1.56}, {"text": "We run it for one timestep,", "start": 2152.21, "duration": 1.26}, {"text": "because we set capital T equal to 1.", "start": 2153.47, "duration": 2.16}, {"text": "Um, and then you would compute the payoff, right,", "start": 2155.63, "duration": 7.38}, {"text": "equals R of s0 + R of s1 and again,", "start": 2163.01, "duration": 4.725}, {"text": "in the more general case, you know,", "start": 2167.735, "duration": 1.95}, {"text": "plus dot dot dot plus R of st right?", "start": 2169.685, "duration": 3.645}, {"text": "[NOISE] And then you perform the following update which", "start": 2173.33, "duration": 7.35}, {"text": "is Theta gets updated as Theta plus the learning rate alpha, times.", "start": 2180.68, "duration": 9.52}, {"text": "Right? Um, and then times the payoff.", "start": 2211.21, "duration": 4.555}, {"text": "Right? And again, I'm just setting capital T equals 1.", "start": 2215.765, "duration": 4.74}, {"text": "If capital T was bigger,", "start": 2220.505, "duration": 1.635}, {"text": "you would just sum this all the way up to time T. Okay?", "start": 2222.14, "duration": 5.07}, {"text": "So that's the algorithm.", "start": 2227.21, "duration": 1.71}, {"text": "Um, that's on every iteration through the reinforce algorithm,", "start": 2228.92, "duration": 6.21}, {"text": "through the reinforce algorithm,", "start": 2235.13, "duration": 2.25}, {"text": "you will take your robot,", "start": 2237.38, "duration": 1.785}, {"text": "take your inverted pendulum, um,", "start": 2239.165, "duration": 2.415}, {"text": "run it through T timesteps,", "start": 2241.58, "duration": 2.43}, {"text": "uh, executing your current policy,", "start": 2244.01, "duration": 2.355}, {"text": "so choose actions randomly according to", "start": 2246.365, "duration": 1.785}, {"text": "the current stochastic policy using current values of the parameters Theta,", "start": 2248.15, "duration": 4.53}, {"text": "compute the total sum of rewards you receive, that's called a payoff", "start": 2252.68, "duration": 3.345}, {"text": "and then update Theta using this funny formula.", "start": 2256.025, "duration": 3.51}, {"text": "Right? Now, on every iteration of this algorithm,", "start": 2259.535, "duration": 5.849}, {"text": "um, you're going to update Theta.", "start": 2265.384, "duration": 2.911}, {"text": "And it turns out that reinforce is a stochastic gradient ascent algorithm.", "start": 2268.295, "duration": 6.525}, {"text": "Um, and you remember when we talked about,", "start": 2274.82, "duration": 3.765}, {"text": "uh, linear regression, right?", "start": 2278.585, "duration": 2.025}, {"text": "You saw me draw pictures like this.", "start": 2280.61, "duration": 1.56}, {"text": "It is a global minimum.", "start": 2282.17, "duration": 1.335}, {"text": "Then uh, gradient descents with just,", "start": 2283.505, "duration": 2.61}, {"text": "you know, take a straight path to the minimum,", "start": 2286.115, "duration": 2.145}, {"text": "but stochastic gradient descent would take", "start": 2288.26, "duration": 2.04}, {"text": "a more random path right towards the minimum and it kind of", "start": 2290.3, "duration": 3.465}, {"text": "oscillates around then, maybe it doesn't", "start": 2293.765, "duration": 3.045}, {"text": "quite converge unless you slowly decrease the learning rate alpha.", "start": 2296.81, "duration": 3.555}, {"text": "So this is what we have for stochastic gradient descent,", "start": 2300.365, "duration": 3.255}, {"text": "um, for linear regression.", "start": 2303.62, "duration": 2.385}, {"text": "What we'll see in a minute,", "start": 2306.005, "duration": 2.28}, {"text": "is that reinforce is a", "start": 2308.285, "duration": 1.605}, {"text": "stochastic gradient ascent algorithm meaning that each of these updates is random,", "start": 2309.89, "duration": 5.37}, {"text": "because it depends on what was", "start": 2315.26, "duration": 2.01}, {"text": "this state action sequence that you just saw and what was the payoff that you just saw.", "start": 2317.27, "duration": 4.53}, {"text": "But what will this show is that on expectation, the average update.", "start": 2321.8, "duration": 7.89}, {"text": "You know, this- this update to Theta.", "start": 2329.69, "duration": 2.205}, {"text": "This thing you are adding to theta,", "start": 2331.895, "duration": 1.545}, {"text": "that on average let's see,", "start": 2333.44, "duration": 2.085}, {"text": "that- that on average this update here is exactly in the direction of the, um, gradient.", "start": 2335.525, "duration": 7.77}, {"text": "So that on average,", "start": 2343.295, "duration": 1.65}, {"text": "um, you know, because, uh, every-every loop,", "start": 2344.945, "duration": 3.315}, {"text": "every time through this loop you're making", "start": 2348.26, "duration": 3.6}, {"text": "a random update to Theta and it's random and", "start": 2351.86, "duration": 2.55}, {"text": "noisy because it depends on this random state sequence.", "start": 2354.41, "duration": 3.045}, {"text": "Right? That and this state sequence is random because of", "start": 2357.455, "duration": 3.285}, {"text": "the state transition probabilities and also", "start": 2360.74, "duration": 1.65}, {"text": "because of the fact that you're choosing actions randomly.", "start": 2362.39, "duration": 2.895}, {"text": "But on- but the expected value of this update, uh,", "start": 2365.285, "duration": 3.75}, {"text": "you'll see in a little bit it turns out to be exactly the direction of the gradient.", "start": 2369.035, "duration": 4.665}, {"text": "Um, which is why this, uh,", "start": 2373.7, "duration": 2.475}, {"text": "reinforce algorithm is a gradient ascent algorithm.", "start": 2376.175, "duration": 3.57}, {"text": "Okay? So let's, uh,", "start": 2379.745, "duration": 2.85}, {"text": "let's show that now.", "start": 2382.595, "duration": 3.145}, {"text": "Okay.", "start": 2385.74, "duration": 2.12}, {"text": "So [NOISE] all right.", "start": 2395.62, "duration": 9.985}, {"text": "So what we want to do is maximize the expected payoff", "start": 2405.605, "duration": 3.375}, {"text": "which is a formula we derive up there and so,", "start": 2408.98, "duration": 3.48}, {"text": "um, we're going to,", "start": 2412.46, "duration": 1.71}, {"text": "want to take derivatives with respect to Theta of the expected pay-off.", "start": 2414.17, "duration": 5.2}, {"text": "Right? Of, uh, I'm just gonna copy", "start": 2420.58, "duration": 2.71}, {"text": "that formula up there", "start": 2423.29, "duration": 1.35}, {"text": "[NOISE].", "start": 2424.64, "duration": 13.08}, {"text": "Okay? So there's a chance", "start": 2437.72, "duration": 3.24}, {"text": "of that, you're going through that state-action sequence times the pay off.", "start": 2440.96, "duration": 3.225}, {"text": "And so we want to take derivatives of this and, you know,", "start": 2444.185, "duration": 2.775}, {"text": "so we can like go uphill using gradient ascent.", "start": 2446.96, "duration": 4.59}, {"text": "Um, so we're going to do this in, uh, four steps.", "start": 2451.55, "duration": 6.36}, {"text": "Um, now, first, um,", "start": 2457.91, "duration": 3.42}, {"text": "let me remind you when you take the derivative of three,", "start": 2461.33, "duration": 3.72}, {"text": "of- of a product of three things.", "start": 2465.05, "duration": 1.86}, {"text": "Right? So let's say that you have, uh, three functions,", "start": 2466.91, "duration": 3.464}, {"text": "f of Theta times g of Theta times h of Theta.", "start": 2470.374, "duration": 5.746}, {"text": "So by the product rule of,", "start": 2476.12, "duration": 3.435}, {"text": "um, you know, derivatives product rule from calculus,", "start": 2479.555, "duration": 3.795}, {"text": "the derivative of the product of three things is obtained by,", "start": 2483.35, "duration": 4.59}, {"text": "um, you know, taking the derivatives of each of them one at a time.", "start": 2487.94, "duration": 4.08}, {"text": "Right? So this is f prime times g times h plus,", "start": 2492.02, "duration": 6.21}, {"text": "um, g prime", "start": 2498.23, "duration": 3.39}, {"text": "here plus h prime.", "start": 2501.62, "duration": 7.2}, {"text": "Okay? So the product rule from calculus is", "start": 2508.82, "duration": 3.63}, {"text": "that if you want to take derivatives of a product of three things,", "start": 2512.45, "duration": 3.78}, {"text": "then you kind of take the derivatives one at a time and you end up with three sums.", "start": 2516.23, "duration": 5.88}, {"text": "Right? And so we're going to apply the product rule to this where, um,", "start": 2522.11, "duration": 6.165}, {"text": "we have- here we have two different terms that depend on, um,", "start": 2528.275, "duration": 6.795}, {"text": "Theta, and so when we take the derivative of this thing with respect to theta,", "start": 2535.07, "duration": 5.325}, {"text": "we're gonna have two terms.", "start": 2540.395, "duration": 1.89}, {"text": "Uh, that correspond to taking derivative of", "start": 2542.285, "duration": 1.515}, {"text": "this ones and taking the derivative of that ones.", "start": 2543.8, "duration": 2.115}, {"text": "Right? And so, um,", "start": 2545.915, "duration": 9.495}, {"text": "this derivative is equal to,", "start": 2555.41, "duration": 2.685}, {"text": "so the first term is the sum over all the state action sequences,", "start": 2558.095, "duration": 5.695}, {"text": "um, P of s0,", "start": 2566.68, "duration": 3.32}, {"text": "um, and then let's see.", "start": 2571.33, "duration": 3.745}, {"text": "So now we have pi of Theta, excuse me.", "start": 2575.075, "duration": 4.935}, {"text": "The derivative with respect to pi Theta,", "start": 2580.01, "duration": 2.985}, {"text": "s0, a0.", "start": 2582.995, "duration": 11.475}, {"text": "Right? And then plus, um,", "start": 2594.47, "duration": 3.03}, {"text": "[NOISE].", "start": 2597.5, "duration": 17.79}, {"text": "Right? And then times the payoff.", "start": 2615.29, "duration": 3.22}, {"text": "Right? So the whole thing here is then multiplied by the payoff.", "start": 2618.52, "duration": 5.755}, {"text": "Okay? So we just apply the product rule for calculus where,", "start": 2624.275, "duration": 3.795}, {"text": "uh, for the first term in the sum,", "start": 2628.07, "duration": 2.07}, {"text": "we kinda took the derivative of this first thing and then for", "start": 2630.14, "duration": 3.57}, {"text": "the second term of the sum we took the derivative of this second thing.", "start": 2633.71, "duration": 4.365}, {"text": "Okay? And now, um,", "start": 2638.075, "duration": 2.625}, {"text": "I'm gonna make one more algebraic trick which is,", "start": 2640.7, "duration": 4.35}, {"text": "I'm going to multiply and divide by that same term,", "start": 2645.05, "duration": 6.67}, {"text": "and then multiply and divide by the same thing here.", "start": 2653.62, "duration": 4.28}, {"text": "Right? So lots of multiply,", "start": 2662.74, "duration": 3.25}, {"text": "multiply and divide by the same thing.", "start": 2665.99, "duration": 1.875}, {"text": "Right? And then finally,", "start": 2667.865, "duration": 5.64}, {"text": "um, if you factor out.", "start": 2673.505, "duration": 3.325}, {"text": "So now, the final step is, um,", "start": 2678.36, "duration": 3.535}, {"text": "I'm- I'm gonna factor out these terms I'm underlining.", "start": 2681.895, "duration": 4.255}, {"text": "Ah, right?", "start": 2686.45, "duration": 2.965}, {"text": "Because this terms I underlined,", "start": 2689.415, "duration": 2.39}, {"text": "this is just you know,", "start": 2691.805, "duration": 2.205}, {"text": "the probability of the whole state sequence.", "start": 2694.01, "duration": 3.31}, {"text": "Right? And again, for the orange thing,", "start": 2700.87, "duration": 3.28}, {"text": "this this orange thing.", "start": 2704.15, "duration": 2.74}, {"text": "Right? These two orange things multiplied", "start": 2707.71, "duration": 3.25}, {"text": "together is equal to that orange thing on that box as well.", "start": 2710.96, "duration": 3.91}, {"text": "And so the final step is to factor out the orange box which is just P of s0,", "start": 2716.26, "duration": 8.395}, {"text": "a0, s1, a1, right?", "start": 2724.655, "duration": 4.23}, {"text": "So that's the thing I boxed-up in orange times,", "start": 2728.885, "duration": 7.785}, {"text": "then those two terms", "start": 2736.67, "duration": 1.38}, {"text": "involving the derivatives", "start": 2738.05, "duration": 1.38}, {"text": "[NOISE].", "start": 2739.43, "duration": 17.52}, {"text": "Times the payoff [NOISE].", "start": 2756.95, "duration": 7.11}, {"text": "Okay? And I think, ah, right,", "start": 2764.06, "duration": 5.76}, {"text": "where- because I guess this term goes there,", "start": 2769.82, "duration": 3.96}, {"text": "[NOISE] and this term goes there, okay?", "start": 2773.78, "duration": 8.71}, {"text": "And so this is just equal to,", "start": 2788.41, "duration": 4.195}, {"text": "um, well- and if you look at the reinforce algorithm,", "start": 2792.605, "duration": 5.115}, {"text": "right, that we wrote down,", "start": 2797.72, "duration": 1.365}, {"text": "ah, this is just equal to sum over, you know,", "start": 2799.085, "duration": 6.06}, {"text": "all the state action sequences times the", "start": 2805.145, "duration": 4.365}, {"text": "probability of the gradient update,", "start": 2809.51, "duration": 5.625}, {"text": "right.", "start": 2815.135, "duration": 6.495}, {"text": "[NOISE] Because, ah, I guess I'm running out of colors.", "start": 2821.63, "duration": 2.655}, {"text": "But, you know, this is a gradient update and that's just like equal to this thing, okay?", "start": 2824.285, "duration": 7.005}, {"text": "So what this shows is that, um,", "start": 2832.83, "duration": 6.565}, {"text": "even though on each iteration the direction of the gradient updates is random, um,", "start": 2839.395, "duration": 8.95}, {"text": "the, ah, the expected value of how you", "start": 2848.345, "duration": 6.87}, {"text": "update the parameters is exactly equal to the derivative of your objective,", "start": 2855.215, "duration": 6.675}, {"text": "of your expected total payoff, right.", "start": 2861.89, "duration": 2.535}, {"text": "So we started saying that this formula is your expected total payoff,", "start": 2864.425, "duration": 4.905}, {"text": "um, so let's figure out what's the derivative of your expected total payoff,", "start": 2869.33, "duration": 4.335}, {"text": "and we found that the expected- the- the derivative,", "start": 2873.665, "duration": 2.865}, {"text": "your expected total payoff,", "start": 2876.53, "duration": 1.14}, {"text": "the derivative of the thing you want to maximize is equal", "start": 2877.67, "duration": 2.37}, {"text": "to the expected value of your gradient update.", "start": 2880.04, "duration": 3.72}, {"text": "And so this proves that, um,", "start": 2883.76, "duration": 2.655}, {"text": "on average, you know,", "start": 2886.415, "duration": 1.86}, {"text": "if you have a very small learning rate,", "start": 2888.275, "duration": 1.395}, {"text": "you end up averaging over many steps, right?", "start": 2889.67, "duration": 1.71}, {"text": "But on average, the updates that reinforce is taking on", "start": 2891.38, "duration": 3.72}, {"text": "every iteration is exactly in the direction of the derivative of the,", "start": 2895.1, "duration": 6.615}, {"text": "um, expected total payoff that you're trying to maximize, okay makes sense?", "start": 2901.715, "duration": 5.625}, {"text": "Yes, any questions about this? Yeah.", "start": 2907.34, "duration": 4.35}, {"text": "[inaudible].", "start": 2911.69, "duration": 5.21}, {"text": "Oh, it is independent of the choice of its function.", "start": 2916.9, "duration": 2.52}, {"text": "Um, this is true for any form of a stochastic policy,", "start": 2919.42, "duration": 3.81}, {"text": "ah, where the definition is that, you know,", "start": 2923.23, "duration": 3.43}, {"text": "Pi Theta of s0, [NOISE] ah,", "start": 2926.66, "duration": 2.4}, {"text": "a0 has to be the chance of taking that action in that state,", "start": 2929.06, "duration": 3.87}, {"text": "but this could be any function you want.", "start": 2932.93, "duration": 2.16}, {"text": "Ah, it could be a softmax,", "start": 2935.09, "duration": 2.22}, {"text": "it could be a logistic function of many,", "start": 2937.31, "duration": 1.41}, {"text": "many different complicated features,", "start": 2938.72, "duration": 1.35}, {"text": "it could be- or it has to be a continuous de- or it has to be a differential function.", "start": 2940.07, "duration": 4.005}, {"text": "And actually one of the reasons we shifted to stochastic policies was because,", "start": 2944.075, "duration": 4.995}, {"text": "um, previously just have two actions,", "start": 2949.07, "duration": 2.1}, {"text": "is either left or right, right?", "start": 2951.17, "duration": 1.56}, {"text": "And so you can't define a derivative over a discontinuous function like either left or", "start": 2952.73, "duration": 4.89}, {"text": "right but now we have a probability that", "start": 2957.62, "duration": 2.43}, {"text": "shifts slowly between what's the probability to go left, versus", "start": 2960.05, "duration": 2.775}, {"text": "go right and by making this a continuous function of Theta,", "start": 2962.825, "duration": 3.615}, {"text": "you can then take derivatives and plot gradient ascent,", "start": 2966.44, "duration": 2.249}, {"text": "but it does need to be a logistic function.", "start": 2968.689, "duration": 1.996}, {"text": "Yeah, go ahead.", "start": 2970.685, "duration": 10.095}, {"text": "Ah, [inaudible]?", "start": 2980.78, "duration": 6.42}, {"text": "Sure. So, um, ah,", "start": 2987.2, "duration": 1.965}, {"text": "another way to train a, um,", "start": 2989.165, "duration": 2.595}, {"text": "helicopter controller is you use supervised learning,", "start": 2991.76, "duration": 2.55}, {"text": "where you have a human expert train,", "start": 2994.31, "duration": 2.595}, {"text": "um, you know, so you can also actually have a human pilot demonstrate in this state,", "start": 2996.905, "duration": 4.965}, {"text": "take this action, right,", "start": 3001.87, "duration": 1.695}, {"text": "and then you use supervised learning to just learn", "start": 3003.565, "duration": 2.055}, {"text": "directly a mapping from a state to the action.", "start": 3005.62, "duration": 2.415}, {"text": "Um, I think this, I don't know,", "start": 3008.035, "duration": 2.235}, {"text": "this might be okay for low speed helicopter flight,", "start": 3010.27, "duration": 3.135}, {"text": "I don't think it works super well, ah,", "start": 3013.405, "duration": 1.8}, {"text": "I bet you could do this and not crash a helicopter, but, ah, um, ah,", "start": 3015.205, "duration": 3.825}, {"text": "ah, but to get the best results,", "start": 3019.03, "duration": 3.48}, {"text": "I wouldn't use this approach, um, yeah.", "start": 3022.51, "duration": 3.9}, {"text": "It turns out for some of the maneuvers it'll actually", "start": 3026.41, "duration": 2.13}, {"text": "fly better than human pilots as well,", "start": 3028.54, "duration": 1.965}, {"text": "um, yeah, no.", "start": 3030.505, "duration": 2.64}, {"text": "Cool. All right.", "start": 3033.145, "duration": 2.58}, {"text": "Um, and so, um,", "start": 3035.725, "duration": 4.95}, {"text": "for other types of policies,", "start": 3040.675, "duration": 2.805}, {"text": "um, let's see, right.", "start": 3043.48, "duration": 4.68}, {"text": "[NOISE]", "start": 3048.16, "duration": 14.22}, {"text": "So, ah, direct policy search also works,", "start": 3062.38, "duration": 2.79}, {"text": "um, if you have continuous value actions and you don't want to discretize the actions.", "start": 3065.17, "duration": 5.25}, {"text": "So here's a simple example.", "start": 3070.42, "duration": 1.545}, {"text": "Let's say a is a real number, ah,", "start": 3071.965, "duration": 1.905}, {"text": "such as the magnitude of the force you apply to accelerating left or right. All right.", "start": 3073.87, "duration": 5.07}, {"text": "So run discretizing, you invert your pendulum,", "start": 3078.94, "duration": 2.43}, {"text": "you wanna output a continuous number of how hard you swerve to left  or right.", "start": 3081.37, "duration": 4.125}, {"text": "Um, or for a self-driving car maybe Theta is", "start": 3085.495, "duration": 2.745}, {"text": "the steering angle which is a real value number.", "start": 3088.24, "duration": 2.97}, {"text": "So simple policy would be a equals,", "start": 3091.21, "duration": 2.954}, {"text": "you know, Theta transpose S, um,", "start": 3094.164, "duration": 2.986}, {"text": "and then plus [NOISE] Gaussian noise.", "start": 3097.15, "duration": 6.24}, {"text": "And if just for the purpose of training,", "start": 3103.39, "duration": 1.965}, {"text": "you're willing to pretend that your policy is to", "start": 3105.355, "duration": 3.57}, {"text": "apply the action Theta transpose S and add a little bit of Gaussian noise to it,", "start": 3108.925, "duration": 4.275}, {"text": "then, um, the whole framework for", "start": 3113.2, "duration": 2.535}, {"text": "reinforce but this type of gradient descent also, ah, will,", "start": 3115.735, "duration": 3.42}, {"text": "will also work, great, um,", "start": 3119.155, "duration": 2.685}, {"text": "and then I guess if you're actually implementing this,", "start": 3121.84, "duration": 1.92}, {"text": "you can probably turn off the Gaussian noise variability,", "start": 3123.76, "duration": 2.085}, {"text": "there, there are little tricks like that as well.", "start": 3125.845, "duration": 1.575}, {"text": "Um, so let's see.", "start": 3127.42, "duration": 5.88}, {"text": "Some pros and cons of, um, so,", "start": 3133.3, "duration": 2.61}, {"text": "whe - whe- when should you use direct policy search and when should you", "start": 3135.91, "duration": 3.63}, {"text": "use value iteration or a value function based type of approach?", "start": 3139.54, "duration": 4.755}, {"text": "Um, so it, ah,", "start": 3144.295, "duration": 3.105}, {"text": "turns out there's one setting, ah,", "start": 3147.4, "duration": 2.145}, {"text": "actually there are two settings where direct policy search works much better.", "start": 3149.545, "duration": 4.065}, {"text": "One is if you have a, um,", "start": 3153.61, "duration": 3.555}, {"text": "POMDP, ah, PO in this case stands for partially observable.", "start": 3157.165, "duration": 4.215}, {"text": "[NOISE] And that's if for example, um, you know,", "start": 3161.38, "duration": 10.365}, {"text": "for the inverted pendulum,", "start": 3171.745, "duration": 2.4}, {"text": "um, does a polar angle Phi, you have,", "start": 3174.145, "duration": 2.94}, {"text": "you have a car and this is your position x. Um,", "start": 3177.085, "duration": 3.255}, {"text": "and what this is saying that the state space is, ah,", "start": 3180.34, "duration": 3.03}, {"text": "x, x dot Phi, Phi dot.", "start": 3183.37, "duration": 3.45}, {"text": "All right? [NOISE] But let's say that, um,", "start": 3186.82, "duration": 3.3}, {"text": "you have sensors on this inverted pendulum that allow you to", "start": 3190.12, "duration": 4.83}, {"text": "measure only the position and only the angle of the inverted pendulum.", "start": 3194.95, "duration": 4.77}, {"text": "[NOISE] Uh, so you might have an angle sensor, you know,", "start": 3199.72, "duration": 2.865}, {"text": "down here and you may have a position sensor for your inverted pendulum,", "start": 3202.585, "duration": 3.165}, {"text": "but maybe you don't know the velocity or you don't know the angular velocity, right.", "start": 3205.75, "duration": 4.35}, {"text": "So this is an example of a partially observable Markov decision process because,", "start": 3210.1, "duration": 4.68}, {"text": "ah, and what this means is that on every step,", "start": 3214.78, "duration": 2.61}, {"text": "you do not get to see the host state because you,", "start": 3217.39, "duration": 2.955}, {"text": "you don't have enough sensors to tell you", "start": 3220.345, "duration": 2.28}, {"text": "exactly what is the state of the entire system, okay?", "start": 3222.625, "duration": 3.915}, {"text": "So in a partially observable MDP,", "start": 3226.54, "duration": 2.37}, {"text": "um, at each step,", "start": 3228.91, "duration": 2.41}, {"text": "you get a partial and potentially", "start": 3231.96, "duration": 7.84}, {"text": "noisy measurement of the state,", "start": 3239.8, "duration": 9.37}, {"text": "right, and then have to take actions, or,", "start": 3249.84, "duration": 3.4}, {"text": "have to choose an action a.", "start": 3253.24, "duration": 1.38}, {"text": "[NOISE]", "start": 3254.62, "duration": 7.44}, {"text": "Using these partial and potentially noisy elements, right?", "start": 3262.06, "duration": 4.005}, {"text": "Which is, uh, maybe you only observe the position and the angle,", "start": 3266.065, "duration": 4.2}, {"text": "but your senses aren't even totally accurate.", "start": 3270.265, "duration": 2.205}, {"text": "So you get a slightly noisy,", "start": 3272.47, "duration": 1.62}, {"text": "you know, estimate of the position.", "start": 3274.09, "duration": 1.44}, {"text": "You get a slightly noisy estimate of the angle but you just have to choose", "start": 3275.53, "duration": 3.27}, {"text": "an action based on your noisy estimates of just two of the four state variables, right?", "start": 3278.8, "duration": 5.355}, {"text": "Um, it turns out", "start": 3284.155, "duration": 7.725}, {"text": "that there's been a lot of", "start": 3291.88, "duration": 1.455}, {"text": "academic literature trying to generalize value function base approaches,", "start": 3293.335, "duration": 4.515}, {"text": "the POMDPs, ah, and they're very complicated algorithms in", "start": 3297.85, "duration": 3.47}, {"text": "the literature on trying to apply value function based approaches of POMDPs.", "start": 3301.32, "duration": 4.215}, {"text": "But those algorithms despite their very high level of complexity,", "start": 3305.535, "duration": 4.19}, {"text": "you know, are not- are not widely in production, right?", "start": 3309.725, "duration": 3.62}, {"text": "Um, but if you use the direct policy search algorithm,", "start": 3313.345, "duration": 3.63}, {"text": "then there's actually very little problem.", "start": 3316.975, "duration": 2.25}, {"text": "Oh, let me just write this out.", "start": 3319.225, "duration": 1.2}, {"text": "So let's say the observation is on every timestep you", "start": 3320.425, "duration": 5.925}, {"text": "observe y equals x Phi plus noise, right?", "start": 3326.35, "duration": 6.18}, {"text": "So you just don't know whether it's a state.", "start": 3332.53, "duration": 1.65}, {"text": "And in a POMDP you cannot approximate a value function.", "start": 3334.18, "duration": 4.2}, {"text": "Or even if you knew what was V star, right?", "start": 3338.38, "duration": 4.515}, {"text": "You can't compute Pi-star because,", "start": 3342.895, "duration": 2.835}, {"text": "uh, and maybe you know what is Pi star best.", "start": 3345.73, "duration": 2.22}, {"text": "This can compute V star and Pi star.", "start": 3347.95, "duration": 1.785}, {"text": "But if you don't know what the state is,", "start": 3349.735, "duration": 1.875}, {"text": "you can't apply Pi star to the state because- so- so how do you choose an action.", "start": 3351.61, "duration": 4.335}, {"text": "Um, if you're using direct policy search,", "start": 3355.945, "duration": 2.94}, {"text": "then here's one thing you could do.", "start": 3358.885, "duration": 1.98}, {"text": "Which is you can say that, uh,", "start": 3360.865, "duration": 2.04}, {"text": "Pi of, um, given an observation,", "start": 3362.905, "duration": 4.455}, {"text": "the chance of going to the right given your current observation is equal", "start": 3367.36, "duration": 4.65}, {"text": "to 1 over 1 plus e to negative Theta transpose y,", "start": 3372.01, "duration": 4.785}, {"text": "where I guess y can be 1,", "start": 3376.795, "duration": 3.72}, {"text": "right, x plus noise, Phi plus noise.", "start": 3380.515, "duration": 3.705}, {"text": "But, sorry that's x plus noise,", "start": 3384.22, "duration": 3.375}, {"text": "Phi plus noise, right?", "start": 3387.595, "duration": 2.01}, {"text": "And so you could run reinforce using just the observations you have to, um,", "start": 3389.605, "duration": 6.24}, {"text": "try to- stochastically try to randomly choose an action,", "start": 3395.845, "duration": 2.985}, {"text": "and nothing in the framework we talked about prevents this algorithm from working.", "start": 3398.83, "duration": 3.765}, {"text": "And so direct policy search just works very", "start": 3402.595, "duration": 2.925}, {"text": "naturally even if you have only partial observations of the state.", "start": 3405.52, "duration": 4.425}, {"text": "Um, and more generally instead of plugging the direct observations this can", "start": 3409.945, "duration": 4.935}, {"text": "be any set of features, right?", "start": 3414.88, "duration": 6.0}, {"text": "I'll just make a side comment for those who don't know", "start": 3420.88, "duration": 2.31}, {"text": "what common filters are. Don't worry if you don't.", "start": 3423.19, "duration": 1.995}, {"text": "But one common- one common way of, uh, uh,", "start": 3425.185, "duration": 3.405}, {"text": "using direct policy search would be to use some estimates such as common filter,", "start": 3428.59, "duration": 4.2}, {"text": "or probabilistic graphical model or something to use your historical estimates.", "start": 3432.79, "duration": 4.17}, {"text": "Look, don't, don't just look at your one, uh,", "start": 3436.96, "duration": 2.22}, {"text": "set of measurements now but look at all the historical meas- measurements.", "start": 3439.18, "duration": 3.09}, {"text": "And then there are algorithms such as something we call", "start": 3442.27, "duration": 2.19}, {"text": "the common filter that lets you estimate whatever is the current state,", "start": 3444.46, "duration": 3.45}, {"text": "the full state vector.", "start": 3447.91, "duration": 1.02}, {"text": "You can plug that full state vector estimate into", "start": 3448.93, "duration": 2.43}, {"text": "the features you use to choose- to choose an action.", "start": 3451.36, "duration": 3.015}, {"text": "That's a common design paradigm.", "start": 3454.375, "duration": 1.515}, {"text": "If you don't know what the common filter is, don't worry about it.", "start": 3455.89, "duration": 2.04}, {"text": "Ah, we take- take one of Stephen Boyd's classes or something, I don't know. Yeah, right.", "start": 3457.93, "duration": 3.78}, {"text": "But, but that's one common paradigm where you can use your partial observations as", "start": 3461.71, "duration": 3.99}, {"text": "for the full state and plug that as a feature into the policy search, okay?", "start": 3465.7, "duration": 4.755}, {"text": "So that's one setting where direct policy search works.", "start": 3470.455, "duration": 3.585}, {"text": "Um, just, just applies in a way that", "start": 3474.04, "duration": 3.0}, {"text": "value function approximation is very difficult to even get to apply.", "start": 3477.04, "duration": 5.955}, {"text": "Um, now one last thing is,", "start": 3482.995, "duration": 3.765}, {"text": "uh, one last consideration", "start": 3486.76, "duration": 4.47}, {"text": "so should you apply search policy search algorithm", "start": 3491.23, "duration": 1.68}, {"text": "or a value function approximation algorithm?", "start": 3492.91, "duration": 1.785}, {"text": "Oh, it turns out, um,", "start": 3494.695, "duration": 2.175}, {"text": "the reinforce algorithm is,", "start": 3496.87, "duration": 1.545}, {"text": "is actually very inefficient.", "start": 3498.415, "duration": 1.605}, {"text": "Ah, as in, ah, you end up, you know, whe- when,", "start": 3500.02, "duration": 2.805}, {"text": "when you look at research papers on the reinforce algorithm,", "start": 3502.825, "duration": 2.745}, {"text": "it's not unusual for people that run the reinforce algorithm for like a million iterations,", "start": 3505.57, "duration": 3.78}, {"text": "or 10 million iterations.", "start": 3509.35, "duration": 1.65}, {"text": "So you just have to train. It turns out that gradient estimates", "start": 3511.0, "duration": 2.91}, {"text": "for the reinforce algorithm even though the expected value is right,", "start": 3513.91, "duration": 3.075}, {"text": "it's actually very noisy.", "start": 3516.985, "duration": 1.395}, {"text": "And so if you train the reinforce algorithm,", "start": 3518.38, "duration": 2.19}, {"text": "you end up just running it for a very,", "start": 3520.57, "duration": 1.83}, {"text": "very, very long time, right?", "start": 3522.4, "duration": 1.32}, {"text": "It does work but it is a pretty inefficient algorithm.", "start": 3523.72, "duration": 2.82}, {"text": "So that's one disadvantage of the reinforce algorithm is that", "start": 3526.54, "duration": 2.985}, {"text": "the gradient estimates on expectation are exactly what you want it to be,", "start": 3529.525, "duration": 4.44}, {"text": "but there's a lot of variance in the gradient.", "start": 3533.965, "duration": 2.445}, {"text": "So you have to run it for a long time for a very small learning range.", "start": 3536.41, "duration": 3.015}, {"text": "Um, but one other reason to use, um,", "start": 3539.425, "duration": 4.065}, {"text": "direct policy search is,", "start": 3543.49, "duration": 1.86}, {"text": "is kind of ask yourself,", "start": 3545.35, "duration": 1.695}, {"text": "do you think Pi star is simpler?", "start": 3547.045, "duration": 3.405}, {"text": "Or is V star simpler, right?", "start": 3550.45, "duration": 7.425}, {"text": "And so, um, here's what I mean,", "start": 3557.875, "duration": 2.565}, {"text": "there are, ah, ah, ah,", "start": 3560.44, "duration": 1.64}, {"text": "there- in, in robotics,", "start": 3562.08, "duration": 2.229}, {"text": "there's sometimes what we call low-level control tasks.", "start": 3564.309, "duration": 4.061}, {"text": "And, uh, one way to think of low-level control task is flying a helicopter.", "start": 3571.92, "duration": 8.17}, {"text": "Hovering a helicopter is example of a low-level control task.", "start": 3580.09, "duration": 3.105}, {"text": "And one way to inform of when you think of low-level control task is kind of a really skilled human,", "start": 3583.195, "duration": 5.115}, {"text": "um, you know, holding a joystick.", "start": 3588.31, "duration": 2.4}, {"text": "Control this thing, making seat of the pants decisions, right?", "start": 3590.71, "duration": 3.54}, {"text": "So those are kind of almost instinctual,", "start": 3594.25, "duration": 2.19}, {"text": "in a tiny fraction of a second,", "start": 3596.44, "duration": 1.365}, {"text": "almost by feel you could control the thing.", "start": 3597.805, "duration": 2.31}, {"text": "Those, those are- tend to be low-level control tasks.", "start": 3600.115, "duration": 2.415}, {"text": "Those are seat of the pants, holding a joystick,", "start": 3602.53, "duration": 2.04}, {"text": "a skilled person could balance the inverted pendulum or,", "start": 3604.57, "duration": 3.6}, {"text": "you know, steer a helicopter.", "start": 3608.17, "duration": 1.8}, {"text": "Those are low-level control tasks.", "start": 3609.97, "duration": 1.545}, {"text": "In contrast, um, playing chess is not a low-level control task.", "start": 3611.515, "duration": 4.515}, {"text": "You know, because for the most part,", "start": 3616.03, "duration": 1.785}, {"text": "a very good chess player is not really a seat of the pants, you know,", "start": 3617.815, "duration": 4.08}, {"text": "take that- make a decision in like- in,", "start": 3621.895, "duration": 2.865}, {"text": "in 0.1 seconds, right.", "start": 3624.76, "duration": 1.35}, {"text": "You kind of have to think multiple steps ahead.", "start": 3626.11, "duration": 2.085}, {"text": "Um, and in low-level control tasks,", "start": 3628.195, "duration": 3.48}, {"text": "there's usually some control policy that is quite simple.", "start": 3631.675, "duration": 3.825}, {"text": "A very fun- simple function mapping some state actions, that's pretty good.", "start": 3635.5, "duration": 3.945}, {"text": "And so that allows you to specify a relatively simple class of functions of", "start": 3639.445, "duration": 4.995}, {"text": "Pi star and direct policy search would be relatively promising for tasks like those.", "start": 3644.44, "duration": 6.705}, {"text": "Whereas in contrast, if you want to play chess or play Go,", "start": 3651.145, "duration": 3.705}, {"text": "or do these things where we have multiple steps of reasoning,", "start": 3654.85, "duration": 2.355}, {"text": "um, I think that,", "start": 3657.205, "duration": 1.695}, {"text": "if you're driving a car on a straight road,", "start": 3658.9, "duration": 2.97}, {"text": "that's a low-level control task.", "start": 3661.87, "duration": 1.74}, {"text": "Where you just look at the road and you just, you know,", "start": 3663.61, "duration": 1.335}, {"text": "you know turn the steering a little bit to stay on the road.", "start": 3664.945, "duration": 2.295}, {"text": "So that's a low-level control task.", "start": 3667.24, "duration": 1.71}, {"text": "But if you are planning how to, um, you know,", "start": 3668.95, "duration": 3.75}, {"text": "overtake this car and avoid that other car,", "start": 3672.7, "duration": 2.55}, {"text": "or there's a pedestrian and a bicycle is along the way,", "start": 3675.25, "duration": 2.49}, {"text": "then that's less of a low level control task.", "start": 3677.74, "duration": 3.06}, {"text": "Um, and that requires more multi-step reasoning, right?", "start": 3680.8, "duration": 3.48}, {"text": "I guess depends on how aggressive of a driver you are, right?", "start": 3684.28, "duration": 2.295}, {"text": "Driving on the highway, you know,", "start": 3686.575, "duration": 1.41}, {"text": "may require more or less multistep reasoning.", "start": 3687.985, "duration": 2.025}, {"text": "Where you want to, ah,", "start": 3690.01, "duration": 1.695}, {"text": "overtake this car before the truck comes in this lane.", "start": 3691.705, "duration": 2.385}, {"text": "So that- that type of thing is,", "start": 3694.09, "duration": 1.665}, {"text": "um, more multi-step reasoning.", "start": 3695.755, "duration": 1.95}, {"text": "Um, and approaches like that tend to be", "start": 3697.705, "duration": 3.165}, {"text": "difficult for a very simple like a linear function to be a good policy.", "start": 3700.87, "duration": 4.02}, {"text": "And for those things in playing chess, playing Go,", "start": 3704.89, "duration": 2.85}, {"text": "playing checkers, um, a value function approximation approach may be more promising.", "start": 3707.74, "duration": 5.34}, {"text": "Okay, um, cool.", "start": 3713.08, "duration": 7.98}, {"text": "So any, um,", "start": 3721.06, "duration": 1.08}, {"text": "questions about the- oh, and so, um,", "start": 3722.14, "duration": 2.94}, {"text": "okay for, for, ah, autonomous helicopter flight, ah,", "start": 3725.08, "duration": 3.45}, {"text": "actually, my first attempt for flying", "start": 3728.53, "duration": 3.45}, {"text": "helicopters were actually a direct policy search because flying helicopters,", "start": 3731.98, "duration": 4.379}, {"text": "are actually a seat of the pants thing.", "start": 3736.359, "duration": 1.501}, {"text": "Ah, but then when you try to fly more complex maneuvers,", "start": 3737.86, "duration": 3.36}, {"text": "then you end up using something maybe closer to", "start": 3741.22, "duration": 2.88}, {"text": "value function approximation methods if you want to fly a very complicated maneuver, right?", "start": 3744.1, "duration": 4.665}, {"text": "Um, oh, so the video you saw just now,", "start": 3748.765, "duration": 2.445}, {"text": "of the helicopter flying upside down,", "start": 3751.21, "duration": 1.59}, {"text": "the algorithm implemented on, you know,", "start": 3752.8, "duration": 1.95}, {"text": "for that particular video that was a direct policy search algorithm, right?", "start": 3754.75, "duration": 3.27}, {"text": "Not, not exactly this one, a little bit different.", "start": 3758.02, "duration": 2.04}, {"text": "But that was a direct policy search algorithm.", "start": 3760.06, "duration": 1.875}, {"text": "But if you want the helicopter to fly a very complicated maneuver,", "start": 3761.935, "duration": 2.445}, {"text": "then you need something maybe closer to a value function approximator.", "start": 3764.38, "duration": 3.195}, {"text": "And so the- and there is exciting research on how to blend", "start": 3767.575, "duration": 3.54}, {"text": "direct policy search approaches together with value function approximation approaches.", "start": 3771.115, "duration": 3.945}, {"text": "So actually AlphaGo.", "start": 3775.06, "duration": 1.875}, {"text": "Ah, ah, ah, ah, one of the reasons AlphaGo works was,", "start": 3776.935, "duration": 4.815}, {"text": "um, sorry, ah, you know Go playing program, right, by DeepMind,", "start": 3781.75, "duration": 4.23}, {"text": "ah, was, was a blend of ideas from both of these types of", "start": 3785.98, "duration": 3.33}, {"text": "literature which enabled it to scale to a much bigger system to play Go and,", "start": 3789.31, "duration": 4.14}, {"text": "you know clearly at a very, very impressive level.", "start": 3793.45, "duration": 2.98}, {"text": "All right. Any questions about this, anyone?", "start": 3796.65, "duration": 3.97}, {"text": "[NOISE]", "start": 3800.62, "duration": 6.12}, {"text": "All right. Um, so just final application examples, um, you know,", "start": 3806.74, "duration": 5.535}, {"text": "reinforcement learning today, um,", "start": 3812.275, "duration": 2.865}, {"text": "is, uh, making strong- let's see.", "start": 3815.14, "duration": 3.255}, {"text": "So there's a lot of work on reinforcement learning for game playing,", "start": 3818.395, "duration": 3.27}, {"text": "Checkers, Chess, um, uh, Go.", "start": 3821.665, "duration": 3.195}, {"text": "That is exciting, um,", "start": 3824.86, "duration": 1.875}, {"text": "reinforcement learning today is used in, uh,", "start": 3826.735, "duration": 2.685}, {"text": "is used in a growing number of robotics applications,", "start": 3829.42, "duration": 3.24}, {"text": "um, I think for controlling a lot of robots.", "start": 3832.66, "duration": 2.67}, {"text": "Um, there is a, uh- if you've go to the robotics conferences,", "start": 3835.33, "duration": 3.36}, {"text": "if you look at some of the projects being done by some of", "start": 3838.69, "duration": 2.28}, {"text": "the very large companies that make very large machines, right.", "start": 3840.97, "duration": 3.57}, {"text": "Uh, I have many friends in multiple, you know,", "start": 3844.54, "duration": 2.58}, {"text": "large companies making large machines that", "start": 3847.12, "duration": 2.55}, {"text": "are increasingly using reinforcement learning to control them.", "start": 3849.67, "duration": 2.85}, {"text": "Um, there is fascinating work, uh,", "start": 3852.52, "duration": 3.885}, {"text": "using reinforcement learning for optimizing,", "start": 3856.405, "duration": 2.79}, {"text": "um, entire factory deployments.", "start": 3859.195, "duration": 2.235}, {"text": "Um, there is, uh,", "start": 3861.43, "duration": 1.845}, {"text": "academic research work, uh,", "start": 3863.275, "duration": 1.575}, {"text": "still in research for a class, I know,", "start": 3864.85, "duration": 1.515}, {"text": "actually may- maybe Science to be deployed on", "start": 3866.365, "duration": 2.205}, {"text": "using reinforcement learning to build chatbots.", "start": 3868.57, "duration": 2.79}, {"text": "Um, uh, uh, and actually, on, on,", "start": 3871.36, "duration": 2.355}, {"text": "on using reinforcement learning to, uh,", "start": 3873.715, "duration": 2.22}, {"text": "build a, uh, AI-based guidance counselor,", "start": 3875.935, "duration": 2.385}, {"text": "for example, right, where,", "start": 3878.32, "duration": 1.26}, {"text": "uh, the actions you take up,", "start": 3879.58, "duration": 1.56}, {"text": "of what you say to students, and then,", "start": 3881.14, "duration": 1.695}, {"text": "and then the reward is, you know,", "start": 3882.835, "duration": 1.635}, {"text": "do you manage to help a student navigate their coursework and navigate their career.", "start": 3884.47, "duration": 4.44}, {"text": "Uh, there is, uh, uh,", "start": 3888.91, "duration": 1.995}, {"text": "and that's also starting to be applied to healthcare,", "start": 3890.905, "duration": 3.045}, {"text": "where- one of the keys of reinforcement learning is,", "start": 3893.95, "duration": 2.52}, {"text": "this is a sequential decision making process, right?", "start": 3896.47, "duration": 2.265}, {"text": "Where, do you have to take a sequence of decisions that may affect your reward over time?", "start": 3898.735, "duration": 4.695}, {"text": "And I think um,", "start": 3903.43, "duration": 1.08}, {"text": "uh, and, uh, in,", "start": 3904.51, "duration": 1.845}, {"text": "in healthcare, there is work on medical planning,", "start": 3906.355, "duration": 3.66}, {"text": "where, um, the goal is not to, you know,", "start": 3910.015, "duration": 3.255}, {"text": "send you to get, uh,", "start": 3913.27, "duration": 1.68}, {"text": "a blood test and then we're done, right?", "start": 3914.95, "duration": 1.95}, {"text": "In, in, in complicated, um,", "start": 3916.9, "duration": 2.175}, {"text": "medical procedures, we might essentially get a blood test,", "start": 3919.075, "duration": 3.195}, {"text": "then based on the outcome of the blood test,", "start": 3922.27, "duration": 1.83}, {"text": "we might send you to get a biopsy or not,", "start": 3924.1, "duration": 2.58}, {"text": "or we might ask you to take a drug and then come back in two weeks.", "start": 3926.68, "duration": 3.18}, {"text": "But this is a very complicated sequential decision-making process", "start": 3929.86, "duration": 3.18}, {"text": "for a treatment of complicated healthcare conditions,", "start": 3933.04, "duration": 2.775}, {"text": "and so there's fascinating work on trying to apply", "start": 3935.815, "duration": 2.415}, {"text": "reinforcement learning to this set of multi-step reasoning,", "start": 3938.23, "duration": 2.49}, {"text": "where it's not about, well,", "start": 3940.72, "duration": 1.29}, {"text": "we'll send you for a treatment and then you'll never see again for the rest your life.", "start": 3942.01, "duration": 2.97}, {"text": "It's about here's the first thing you do then come back,", "start": 3944.98, "duration": 2.4}, {"text": "let's see what state you get to after taking this blood test,", "start": 3947.38, "duration": 3.15}, {"text": "or let's see what- state you get to after trying a drug,", "start": 3950.53, "duration": 2.775}, {"text": "and then coming back in a week to see what has happened to symptoms.", "start": 3953.305, "duration": 2.955}, {"text": "But I think that, um,", "start": 3956.26, "duration": 1.5}, {"text": "these are all sectors where reinforcement learning,", "start": 3957.76, "duration": 2.82}, {"text": "uh, is making inroads, um,", "start": 3960.58, "duration": 2.04}, {"text": "or, or even actually, stock trading.", "start": 3962.62, "duration": 2.49}, {"text": "Okay, maybe not the most inspiring one,", "start": 3965.11, "duration": 1.68}, {"text": "but one of my friends, um,", "start": 3966.79, "duration": 1.17}, {"text": "on the East Coast was, uh, uh, in,", "start": 3967.96, "duration": 2.46}, {"text": "in- was, uh- and just actually,", "start": 3970.42, "duration": 2.7}, {"text": "if, if you or your parents,", "start": 3973.12, "duration": 1.53}, {"text": "uh, invest in mutual funds,", "start": 3974.65, "duration": 1.29}, {"text": "this may be being used to, um,", "start": 3975.94, "duration": 2.58}, {"text": "buy and sell shares with them today,", "start": 3978.52, "duration": 1.35}, {"text": "depending on what bank they are investing.", "start": 3979.87, "duration": 1.44}, {"text": "I know what bank is doing this, but I won't say it out loud.", "start": 3981.31, "duration": 2.685}, {"text": "Uh, but, um, uh, uh, but,", "start": 3983.995, "duration": 2.445}, {"text": "uh, if you want to buy or sell, you know, say,", "start": 3986.44, "duration": 2.88}, {"text": "a million shares of stock,", "start": 3989.32, "duration": 1.575}, {"text": "a, a very large volume of stock,", "start": 3990.895, "duration": 1.89}, {"text": "you may not want to do it in", "start": 3992.785, "duration": 1.845}, {"text": "a very public way because that will affect the price of the shares, right?", "start": 3994.63, "duration": 2.82}, {"text": "So everyone knows that a very large investor", "start": 3997.45, "duration": 2.7}, {"text": "is about to buy a million shares or buy 10 million shares or whatever,", "start": 4000.15, "duration": 3.075}, {"text": "that will, um, cause the price to increase, uh,", "start": 4003.225, "duration": 2.805}, {"text": "and this, this is,", "start": 4006.03, "duration": 1.2}, {"text": "this disadvantages the person wanting to buy shares.", "start": 4007.23, "duration": 2.67}, {"text": "But so that's been very interesting work on using reinforcement learning to, um,", "start": 4009.9, "duration": 5.025}, {"text": "decide how to sequence out your, your buy,", "start": 4014.925, "duration": 2.715}, {"text": "how to buy the stock in small lots,", "start": 4017.64, "duration": 2.985}, {"text": "and this trading market is called dark pools.", "start": 4020.625, "duration": 2.205}, {"text": "You could Google if you're curious.", "start": 4022.83, "duration": 1.125}, {"text": "Actually, don't bother, uh, uh, uh, to,", "start": 4023.955, "duration": 2.73}, {"text": "to try to, um,", "start": 4026.685, "duration": 1.26}, {"text": "buy a very large lot of shares.", "start": 4027.945, "duration": 2.34}, {"text": "Also, sell a very large lot of shares without affecting", "start": 4030.285, "duration": 2.685}, {"text": "the market price too much because the way you affect", "start": 4032.97, "duration": 1.95}, {"text": "the market price always breaks against,", "start": 4034.92, "duration": 1.965}, {"text": "you know, is always against you,", "start": 4036.885, "duration": 1.755}, {"text": "it's always bad, right.", "start": 4038.64, "duration": 1.74}, {"text": "Um, so there's work laid down as well.", "start": 4040.38, "duration": 2.175}, {"text": "So anyway, I think, um, uh,", "start": 4042.555, "duration": 2.1}, {"text": "many applications- I personally think that one of", "start": 4044.655, "duration": 2.505}, {"text": "the most exciting areas for reinforcement learning will be robotics,", "start": 4047.16, "duration": 2.79}, {"text": "but, uh, we'll, we'll see what,", "start": 4049.95, "duration": 1.83}, {"text": "what happens over the next few years, right?", "start": 4051.78, "duration": 3.75}, {"text": "Okay. All right.", "start": 4055.53, "duration": 1.335}, {"text": "So let's see,", "start": 4056.865, "duration": 1.44}, {"text": "well, just five more minutes.", "start": 4058.305, "duration": 1.56}, {"text": "Um, and, and just to wrap up, I think,", "start": 4059.865, "duration": 2.73}, {"text": "you know, um, uh,", "start": 4062.595, "duration": 2.205}, {"text": "we've gone through quite a lot of stuff.", "start": 4064.8, "duration": 1.515}, {"text": "I guess, uh, for supervised learning to, uh, learning theory,", "start": 4066.315, "duration": 4.095}, {"text": "and advice for applying learning algorithms to unsupervised learning,", "start": 4070.41, "duration": 3.105}, {"text": "although- was it, K-means, PCA, uh,", "start": 4073.515, "duration": 2.37}, {"text": "EM mixture of Gaussians, uh,", "start": 4075.885, "duration": 2.685}, {"text": "factor analysis, and PrinCo analysis to most recently,", "start": 4078.57, "duration": 3.39}, {"text": "reinforcement learning with the value function approaches,", "start": 4081.96, "duration": 3.464}, {"text": "fitted value iteration, policy search.", "start": 4085.424, "duration": 2.446}, {"text": "So, um, feels like we did, feels like,", "start": 4087.87, "duration": 2.46}, {"text": "feels like- I- feels like you've seen a lot of learning algorithms.", "start": 4090.33, "duration": 3.285}, {"text": "Um, go ahead.", "start": 4093.615, "duration": 1.125}, {"text": "[inaudible]", "start": 4094.74, "duration": 4.26}, {"text": "How is reinforcement learning compared to adversarial learning?", "start": 4099.0, "duration": 2.07}, {"text": "I think of those as pretty distinguished literatures.", "start": 4101.07, "duration": 3.315}, {"text": "Uh, uh, yeah, yeah,", "start": 4104.385, "duration": 2.46}, {"text": "so I think, uh, and again, actually, I, I,", "start": 4106.845, "duration": 2.355}, {"text": "I know a lot of", "start": 4109.2, "duration": 1.845}, {"text": "non-publicly known facts about the machine learning world, but, uh,", "start": 4111.045, "duration": 3.555}, {"text": "one of the things that I actually happen to know is that, er,", "start": 4114.6, "duration": 3.96}, {"text": "uh, some of the ideas in adversarial learning, uh, uh, you know,", "start": 4118.56, "duration": 4.965}, {"text": "so can you tweak a picture by,", "start": 4123.525, "duration": 1.68}, {"text": "you know, very little bit,", "start": 4125.205, "duration": 1.365}, {"text": "by tweaking a bunch of pixel values that are not visible to human eye,", "start": 4126.57, "duration": 2.805}, {"text": "that fools the learning algorithm into thinking that this picture is actually a cat,", "start": 4129.375, "duration": 3.465}, {"text": "when it's clearly not a cat or whatever.", "start": 4132.84, "duration": 1.575}, {"text": "So I actually know that there are attackers out in the world", "start": 4134.415, "duration": 2.385}, {"text": "today using techniques like that to attack,", "start": 4136.8, "duration": 2.565}, {"text": "you know, websites, to try to fool, um, uh, ah,", "start": 4139.365, "duration": 3.645}, {"text": "you know- some of the websites I'm pretty sure you guys use,", "start": 4143.01, "duration": 3.6}, {"text": "and fool their anti-spam,", "start": 4146.61, "duration": 1.95}, {"text": "anti-fraud, anti-undermining democracy types of algorithms into,", "start": 4148.56, "duration": 4.38}, {"text": "um, [LAUGHTER] into making poor decisions.", "start": 4152.94, "duration": 2.34}, {"text": "Uh, so, so it's an exciting to time to do machine learning, right?", "start": 4155.28, "duration": 3.6}, {"text": "[LAUGHTER] That, that, we get to fight battles like these [LAUGHTER]. Yeah, I'm sorry.", "start": 4158.88, "duration": 3.93}, {"text": "Um, uh, okay, um,", "start": 4162.81, "duration": 4.62}, {"text": "and, and I think, you know, I think with,", "start": 4167.43, "duration": 2.205}, {"text": "with- really, I think that with", "start": 4169.635, "duration": 1.425}, {"text": "the things that you guys have learned in machine learning,", "start": 4171.06, "duration": 1.695}, {"text": "I think all of you, um, uh,", "start": 4172.755, "duration": 1.695}, {"text": "are now very knowledgeable, right?", "start": 4174.45, "duration": 2.775}, {"text": "I think all of you are experts in all the ideas of core machine learning,", "start": 4177.225, "duration": 4.665}, {"text": "and I hope that, um- I,", "start": 4181.89, "duration": 1.575}, {"text": "I think you- when you look around the world,", "start": 4183.465, "duration": 1.515}, {"text": "there are so many worthwhile projects you", "start": 4184.98, "duration": 1.5}, {"text": "could do with machine learning and the number of", "start": 4186.48, "duration": 1.56}, {"text": "you that know these techniques is so small that I hope that,", "start": 4188.04, "duration": 3.165}, {"text": "um, you take these skills.", "start": 4191.205, "duration": 1.695}, {"text": "Um, and some of you will go, you know,", "start": 4192.9, "duration": 2.355}, {"text": "build businesses and make a lot of money, that's great.", "start": 4195.255, "duration": 2.115}, {"text": "Some of you will take these ideas and, uh,", "start": 4197.37, "duration": 2.43}, {"text": "help drive basic research at Stanford or at other institutions.", "start": 4199.8, "duration": 3.27}, {"text": "I think that's fantastic.", "start": 4203.07, "duration": 1.275}, {"text": "But I think whatever you are doing,", "start": 4204.345, "duration": 2.01}, {"text": "the number of worthwhile projects on the planet is so large and", "start": 4206.355, "duration": 3.975}, {"text": "the number of you that actually know how to use", "start": 4210.33, "duration": 1.8}, {"text": "these techniques is so small that I hope that,", "start": 4212.13, "duration": 2.64}, {"text": "um, you take these skills you're learning from", "start": 4214.77, "duration": 1.77}, {"text": "this course and go and do something meaningful,", "start": 4216.54, "duration": 2.55}, {"text": "go and do something that helps other people.", "start": 4219.09, "duration": 2.19}, {"text": "Um, I think we are seeing in the Silicon Valley that there a lot of ways, you know,", "start": 4221.28, "duration": 4.185}, {"text": "to build very valuable businesses, uh,", "start": 4225.465, "duration": 2.355}, {"text": "and some of you do that and that's great,", "start": 4227.82, "duration": 1.56}, {"text": "but I hope that you do it in a way that helps other people.", "start": 4229.38, "duration": 3.675}, {"text": "Um, uh, I think, er,", "start": 4233.055, "duration": 1.845}, {"text": "over the past few years we've seen,", "start": 4234.9, "duration": 2.655}, {"text": "uh, um- I think that,", "start": 4237.555, "duration": 2.01}, {"text": "er, in Silicon Valley,", "start": 4239.565, "duration": 1.53}, {"text": "maybe 10 years ago,", "start": 4241.095, "duration": 1.26}, {"text": "the contract we had with society was that people would", "start": 4242.355, "duration": 3.375}, {"text": "trust us with their data and then we'll use their data to help them.", "start": 4245.73, "duration": 3.735}, {"text": "But I think in the past year,", "start": 4249.465, "duration": 1.89}, {"text": "that contract feels like it has been broken and", "start": 4251.355, "duration": 3.165}, {"text": "the world's faith in Silicon Valley has been shaken up,", "start": 4254.52, "duration": 3.78}, {"text": "but I think that places even more pressure on all of us, on all of you,", "start": 4258.3, "duration": 3.795}, {"text": "to make sure that, um,", "start": 4262.095, "duration": 1.665}, {"text": "the work you go out in the world to do is", "start": 4263.76, "duration": 1.95}, {"text": "work that actually is respectful of individuals,", "start": 4265.71, "duration": 2.19}, {"text": "respectful of individual's privacy,", "start": 4267.9, "duration": 1.53}, {"text": "is transparent and open, and ultimately is,", "start": 4269.43, "duration": 2.79}, {"text": "uh, helping drive forward,", "start": 4272.22, "duration": 1.995}, {"text": "um, uh, humanity, or helping people,", "start": 4274.215, "duration": 2.505}, {"text": "or helping drive forward basic research,", "start": 4276.72, "duration": 1.62}, {"text": "or building products that actually help people,", "start": 4278.34, "duration": 1.98}, {"text": "rather than, um, exploit their foibles for profit but to their own harm.", "start": 4280.32, "duration": 5.25}, {"text": "So I hope that all of you will take your superpowers that you now have,", "start": 4285.57, "duration": 3.96}, {"text": "and, um, go out to,", "start": 4289.53, "duration": 2.055}, {"text": "to, to do meaningful work.", "start": 4291.585, "duration": 2.025}, {"text": "Um, and let's see,", "start": 4293.61, "duration": 1.89}, {"text": "um, and I think, uh- oh, and,", "start": 4295.5, "duration": 1.74}, {"text": "and lastly, just, just on a personal note,", "start": 4297.24, "duration": 2.07}, {"text": "I want, to, you know, thank all of you.", "start": 4299.31, "duration": 1.485}, {"text": "On behalf of the TAs and the whole teaching team and myself,", "start": 4300.795, "duration": 3.765}, {"text": "I want to thank all of you for your hard work.", "start": 4304.56, "duration": 2.115}, {"text": "Uh, sometimes, they go over the homework problems.", "start": 4306.675, "duration": 1.485}, {"text": "They look at probably some of these problems and go, \"Wow,", "start": 4308.16, "duration": 1.605}, {"text": "there she got that problem, I thought that was really hard,\" or the project milestones go,", "start": 4309.765, "duration": 3.285}, {"text": "\"Hey, that's really cool, look forward to seeing", "start": 4313.05, "duration": 1.89}, {"text": "your final project results at the final poster session.\"", "start": 4314.94, "duration": 3.09}, {"text": "So, um, I know that all of you have worked really hard.", "start": 4318.03, "duration": 3.66}, {"text": "Uh, uh, and if you didn't,", "start": 4321.69, "duration": 2.31}, {"text": "don't tell me, but I think almost all of you have [LAUGHTER] but,", "start": 4324.0, "duration": 2.115}, {"text": "but, but, I will make sure you know there's", "start": 4326.115, "duration": 2.505}, {"text": "a- I think it wasn't that long ago that I was a student,", "start": 4328.62, "duration": 2.535}, {"text": "you know, working late at night on homework problems and,", "start": 4331.155, "duration": 2.76}, {"text": "and I know that many of you have been doing that, uh, for the homework,", "start": 4333.915, "duration": 3.435}, {"text": "for studying for the midterm,", "start": 4337.35, "duration": 1.05}, {"text": "um, for working on your final term project.", "start": 4338.4, "duration": 2.76}, {"text": "So, um I want to make sure, um,", "start": 4341.16, "duration": 2.145}, {"text": "you know I'm very grateful for the hard work you put into this course,", "start": 4343.305, "duration": 3.375}, {"text": "and I hope that, um- I hope that, uh, your,", "start": 4346.68, "duration": 2.79}, {"text": "your hard earned skills will also reward you very well in the future,", "start": 4349.47, "duration": 4.05}, {"text": "and also help you do work that,", "start": 4353.52, "duration": 1.77}, {"text": "that you find as meaningful,", "start": 4355.29, "duration": 1.425}, {"text": "so thank you very much [APPLAUSE].", "start": 4356.715, "duration": 3.285}]