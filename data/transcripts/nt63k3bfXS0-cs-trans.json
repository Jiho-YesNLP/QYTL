[{"text": "Hey, morning everyone. Welcome back.", "start": 3.47, "duration": 3.41}, {"text": "Um, so last week you heard about uh,", "start": 6.88, "duration": 4.48}, {"text": "logistic regression and um,", "start": 11.36, "duration": 2.585}, {"text": "uh, generalized linear models.", "start": 13.945, "duration": 2.525}, {"text": "And it turns out all of the learning algorithms we've been learning about", "start": 16.47, "duration": 3.18}, {"text": "so far are called discriminative learning algorithms,", "start": 19.65, "duration": 3.12}, {"text": "which is one big bucket of learning algorithms.", "start": 22.77, "duration": 1.95}, {"text": "And today um, what I'd like to do is share with", "start": 24.72, "duration": 2.88}, {"text": "you how generative learning algorithms work.", "start": 27.6, "duration": 3.0}, {"text": "Um, and in particular you learned about", "start": 30.6, "duration": 1.59}, {"text": "Gaussian discriminant analysis so by the end of the day,", "start": 32.19, "duration": 2.43}, {"text": "you will know how to implement this.", "start": 34.62, "duration": 1.29}, {"text": "And it turns out that uh,", "start": 35.91, "duration": 1.38}, {"text": "compared to say logistic regression for classification,", "start": 37.29, "duration": 3.45}, {"text": "GDA is actually a um,", "start": 40.74, "duration": 2.61}, {"text": "simpler and maybe more computationally", "start": 43.35, "duration": 2.3}, {"text": "efficient algorithm to implement ah, in some cases.", "start": 45.65, "duration": 2.37}, {"text": "So um, and it sometimes works better if you have uh,", "start": 48.02, "duration": 3.63}, {"text": "very small data sets sometimes with some caveats.", "start": 51.65, "duration": 2.79}, {"text": "Um, and there was a helpful comparison between generative learning algorithms,", "start": 54.44, "duration": 3.81}, {"text": "which is a new class of algorithms you hear about today,", "start": 58.25, "duration": 2.01}, {"text": "versus discriminative learn- learning algorithms.", "start": 60.26, "duration": 2.235}, {"text": "And then we'll talk about naive Bayes and how you can use that to uh,", "start": 62.495, "duration": 4.115}, {"text": "build a spam filter, for example.", "start": 66.61, "duration": 2.62}, {"text": "Okay? So um, we'll use binary classification as the motivating example for today.", "start": 69.23, "duration": 8.115}, {"text": "And um, if you have a data set that looks like this with two classes,", "start": 77.345, "duration": 6.985}, {"text": "then what a discriminative learning algorithm,", "start": 84.33, "duration": 2.96}, {"text": "like logistic regression would do,", "start": 87.29, "duration": 1.95}, {"text": "is use gradient descent to search for", "start": 89.24, "duration": 2.52}, {"text": "a line that separates the positive-negative examples, right?", "start": 91.76, "duration": 2.91}, {"text": "So if you randomish - randomly initialize parameters,", "start": 94.67, "duration": 3.48}, {"text": "maybe starts with some digital boundary", "start": 98.15, "duration": 3.03}, {"text": "like that and over the course of gradient descent, you know,", "start": 101.18, "duration": 2.7}, {"text": "the line migrates or evolves until you get maybe a line like that,", "start": 103.88, "duration": 3.795}, {"text": "that separates the positive and negative examples.", "start": 107.675, "duration": 2.025}, {"text": "And um, logistic regression is really searching for a line,", "start": 109.7, "duration": 4.27}, {"text": "searching for a decision boundary that separates the positive and negative examples.", "start": 113.97, "duration": 4.19}, {"text": "Um, and so if this was the uh,", "start": 118.16, "duration": 2.475}, {"text": "malignant tumors [NOISE] and the benign tumors example,", "start": 120.635, "duration": 5.175}, {"text": "right, that's - that's what logistic regression would do.", "start": 125.81, "duration": 4.155}, {"text": "Now, there's a different class of algorithm which isn't searching for this separation,", "start": 129.965, "duration": 4.875}, {"text": "which isn't trying to maximize the likelihood that you -", "start": 134.84, "duration": 2.75}, {"text": "the way you saw last week, which is um,", "start": 137.59, "duration": 2.91}, {"text": "here's an alternative, just call it generative learning algorithm;", "start": 140.5, "duration": 3.47}, {"text": "which is rather than looking at two classes and trying to find the separation.", "start": 143.97, "duration": 3.81}, {"text": "Instead, the algorithm is going to look at the classes one at a time.", "start": 147.78, "duration": 2.96}, {"text": "First, we'll look at all of the malignant tumors, right?", "start": 150.74, "duration": 3.475}, {"text": "In the cancer example and try to build a model for what malignant tumors look like.", "start": 154.215, "duration": 5.025}, {"text": "So you might say, ah, it looks like all the malignant tumors um,", "start": 159.24, "duration": 3.62}, {"text": "roughly [NOISE] all the malignant tumors roughly live in that ellipse.", "start": 162.86, "duration": 7.065}, {"text": "And then you look at all the benign tumors in isolation and say,", "start": 169.925, "duration": 4.155}, {"text": "ah, it looks like all the benign tumors roughly live in that ellipse.", "start": 174.08, "duration": 4.035}, {"text": "And then at classification time,", "start": 178.115, "duration": 2.055}, {"text": "if there's a new patient in your office with those features, uh,", "start": 180.17, "duration": 6.0}, {"text": "it would then look at this new patient and compare it", "start": 186.17, "duration": 3.645}, {"text": "to the malignant tumor model compared to the benign tumor model and then say,", "start": 189.815, "duration": 5.505}, {"text": "in this case, ah, it looks like this one.", "start": 195.32, "duration": 1.5}, {"text": "Looks a lot more like the benign tumors I had previously seen,", "start": 196.82, "duration": 3.33}, {"text": "so we're gonna classify that as a benign tumor.", "start": 200.15, "duration": 2.25}, {"text": "Okay? So um, rather", "start": 202.4, "duration": 4.21}, {"text": "than looking at both classes simultaneously and searching for a way to separate them,", "start": 206.61, "duration": 5.3}, {"text": "a generative learning algorithm, uh,", "start": 211.91, "duration": 2.6}, {"text": "instead builds a model of what each of the classes looks like,", "start": 214.51, "duration": 3.85}, {"text": "kind of almost in isolation,", "start": 218.36, "duration": 1.8}, {"text": "with some details we'll learn about later.", "start": 220.16, "duration": 2.205}, {"text": "And then at test time uh,", "start": 222.365, "duration": 1.785}, {"text": "it evaluates a new example against the benign model,", "start": 224.15, "duration": 3.475}, {"text": "evaluates against the malignant model and tries to see which of", "start": 227.625, "duration": 3.065}, {"text": "the two models it matches more closely against.", "start": 230.69, "duration": 3.765}, {"text": "So let's formalize this.", "start": 234.455, "duration": 1.995}, {"text": "Um, a discriminative learning [NOISE] algorithm", "start": 236.45, "duration": 5.31}, {"text": "learns P of y given x, right?", "start": 241.76, "duration": 8.625}, {"text": "Um, or uh, what it learns um,", "start": 250.385, "duration": 6.285}, {"text": "[NOISE] right?", "start": 256.67, "duration": 6.21}, {"text": "Some mapping [NOISE] from x to y directly.", "start": 262.88, "duration": 4.21}, {"text": "You know, as I learn- Or it can learn,", "start": 267.09, "duration": 1.695}, {"text": "I think Annan briefly talked about the Perceptron algorithm,", "start": 268.785, "duration": 2.425}, {"text": "it's helpful to support vector machines later.", "start": 271.21, "duration": 2.14}, {"text": "But learns a function mapping from x to the labels directly.", "start": 273.35, "duration": 3.35}, {"text": "So that's a discriminative learning algorithm.", "start": 276.7, "duration": 1.89}, {"text": "We're trying to discriminate between positive and negative classes.", "start": 278.59, "duration": 2.55}, {"text": "[NOISE] In contrast, a generative learning algorithm,", "start": 281.14, "duration": 4.18}, {"text": "[NOISE] it learns P", "start": 285.32, "duration": 9.36}, {"text": "of um, x given y.", "start": 294.68, "duration": 6.37}, {"text": "So this says, what are the features like,", "start": 301.05, "duration": 3.74}, {"text": "[NOISE] given the class, right?", "start": 304.79, "duration": 6.225}, {"text": "So um, instead P of y given x,", "start": 311.015, "duration": 2.7}, {"text": "we're gonna learn p of x given y.", "start": 313.715, "duration": 2.805}, {"text": "So in other words, given that a tumor is malignant,", "start": 316.52, "duration": 2.36}, {"text": "what are the features likely gonna be like?", "start": 318.88, "duration": 1.825}, {"text": "Or given the tumor's benign,", "start": 320.705, "duration": 1.575}, {"text": "what are the features x gonna be like?", "start": 322.28, "duration": 2.465}, {"text": "Okay? And then as- and then they'll also- generative learning algorithm,", "start": 324.745, "duration": 5.03}, {"text": "will also learn P of y.", "start": 329.775, "duration": 1.53}, {"text": "So this is a- this is also called the class prior to be this probability, I guess.", "start": 331.305, "duration": 6.18}, {"text": "It's called a class prior.", "start": 337.485, "duration": 1.44}, {"text": "It's just- when the patient walks into your office,", "start": 338.925, "duration": 2.455}, {"text": "before you've even examined them,", "start": 341.38, "duration": 1.42}, {"text": "before you've even seen them,", "start": 342.8, "duration": 1.29}, {"text": "what are the odds that their tumor is malignant versus benign, right?", "start": 344.09, "duration": 3.21}, {"text": "Before you see any features, okay?", "start": 347.3, "duration": 2.955}, {"text": "And so using Bayes' Rule,", "start": 350.255, "duration": 3.745}, {"text": "[NOISE] if you can build a model for P of x given y and for P of y,", "start": 354.0, "duration": 7.68}, {"text": "um, if- you know,", "start": 361.68, "duration": 1.69}, {"text": "if you can calculate numbers for both of these quantities then using Bayes' rule,", "start": 363.37, "duration": 3.73}, {"text": "when you have a new test example [NOISE] with features x,", "start": 367.1, "duration": 4.56}, {"text": "you can then calculate the chance of y being equal to", "start": 371.66, "duration": 2.46}, {"text": "1 as this,", "start": 374.12, "duration": 2.07}, {"text": "[NOISE] right?", "start": 376.19, "duration": 9.78}, {"text": "Where P of x", "start": 385.97, "duration": 3.28}, {"text": "by the - [NOISE] okay?", "start": 390.95, "duration": 7.56}, {"text": "[NOISE] Um, and so", "start": 398.51, "duration": 9.49}, {"text": "if you learn this term, P of x given y,", "start": 408.0, "duration": 2.82}, {"text": "then you can plug that in here, right?", "start": 410.82, "duration": 7.23}, {"text": "And if you've also learned this term P of y,", "start": 418.05, "duration": 3.435}, {"text": "you can plug that in here.", "start": 421.485, "duration": 2.31}, {"text": "Right. Um, and so P of x in the denominators,", "start": 423.795, "duration": 7.065}, {"text": "goes into denominator, okay?", "start": 430.86, "duration": 2.355}, {"text": "So if you've learned both- both of those terms", "start": 433.215, "duration": 2.235}, {"text": "in the red square and in the orange square,", "start": 435.45, "duration": 2.395}, {"text": "you could plug it into all of those terms and therefore use", "start": 437.845, "duration": 3.01}, {"text": "Bayes' rule to calculate P of y equals 1, given x.", "start": 440.855, "duration": 4.495}, {"text": "So given the new patient with features x,", "start": 445.35, "duration": 1.95}, {"text": "you could use this formula to calculate what's the chance that a tumor is malignant.", "start": 447.3, "duration": 3.815}, {"text": "If you've estimated you know", "start": 451.115, "duration": 1.89}, {"text": "these - these two quantities in the red and in the orange circles.", "start": 453.005, "duration": 5.175}, {"text": "Okay? So um, [NOISE]", "start": 458.18, "duration": 3.36}, {"text": "that's the framework we'll use to build generative learning algorithms.", "start": 461.54, "duration": 3.45}, {"text": "And in fact, today you see two examples of generative learning algorithms.", "start": 464.99, "duration": 3.87}, {"text": "One for continuous value features,", "start": 468.86, "duration": 2.175}, {"text": "which is used for things like the tumor classification and one for discrete features,", "start": 471.035, "duration": 4.99}, {"text": "which uh, you can use for building,", "start": 476.025, "duration": 1.755}, {"text": "like in email spam, for example, right?", "start": 477.78, "duration": 1.77}, {"text": "Or - or I don't know. Or If you want to download", "start": 479.55, "duration": 2.535}, {"text": "Twitter things and see how positive or negative a sentiment on Twitter is or something.", "start": 482.085, "duration": 4.535}, {"text": "right? Well we'll have a natural language processing example later.", "start": 486.62, "duration": 3.485}, {"text": "So um, let's talk about Gaussian discriminant analysis.", "start": 490.105, "duration": 7.355}, {"text": "[NOISE] GDA.", "start": 497.46, "duration": 6.75}, {"text": "Um, so uh,", "start": 504.21, "duration": 5.98}, {"text": "let's develop this model,", "start": 514.19, "duration": 2.245}, {"text": "assuming that the features x are continuous values.", "start": 516.435, "duration": 2.775}, {"text": "And when we develop um,", "start": 519.21, "duration": 2.505}, {"text": "generative learning algorithms, I'm gonna use x and Rn.", "start": 521.715, "duration": 3.735}, {"text": "So you know, I'm gonna drop the x 0 equals 1, convention.", "start": 525.45, "duration": 5.22}, {"text": "So I'm not gonna- we're not gonna need that extra x equals 1.", "start": 530.67, "duration": 4.52}, {"text": "So x is now Rn rather than Rn plus 1.", "start": 535.19, "duration": 3.795}, {"text": "And the key assumption in Gaussian discriminant analysis is,", "start": 538.985, "duration": 4.125}, {"text": "we're going to assume that P of x given y", "start": 543.11, "duration": 4.86}, {"text": "[NOISE] is distributed Gaussian, right?", "start": 547.97, "duration": 5.62}, {"text": "In other words conditioned on the tumors being malignant,", "start": 553.59, "duration": 3.264}, {"text": "the distribution of the features is Gaussian.", "start": 556.854, "duration": 2.531}, {"text": "The other features like uh,", "start": 559.385, "duration": 1.285}, {"text": "size of the- size of the tumor,", "start": 560.67, "duration": 1.8}, {"text": "the- the cell adhesion or whatever features you use to measure a tumor um,", "start": 562.47, "duration": 4.28}, {"text": "and condition on it being benign,", "start": 566.75, "duration": 1.56}, {"text": "the distribution is also Gaussian.", "start": 568.31, "duration": 2.82}, {"text": "So um, actually, how many of you are familiar with the multivariate Gaussian?", "start": 571.13, "duration": 4.275}, {"text": "Raise your hand if you are. Like half of you?", "start": 575.405, "duration": 2.43}, {"text": "One-third? No. Two-fifths? Okay. Cool. Alright. How many", "start": 577.835, "duration": 3.315}, {"text": "of you are familiar about a uni-variate,", "start": 581.15, "duration": 1.485}, {"text": "like a single dimensional Gaussian?", "start": 582.635, "duration": 1.845}, {"text": "Okay. Cool. Almost everyone. All right.", "start": 584.48, "duration": 2.115}, {"text": "Cool. So let me- let me just go through what is a multivariate Gaussian distribution.", "start": 586.595, "duration": 4.725}, {"text": "So the Gaussian is this familiar bell-shaped curve.", "start": 591.32, "duration": 2.85}, {"text": "A multivariate Gaussian is the generalization of", "start": 594.17, "duration": 2.82}, {"text": "this familiar bell-shaped curve over a 1-dimensional random", "start": 596.99, "duration": 3.42}, {"text": "variable to multiple random variables at the same time to- to- to", "start": 600.41, "duration": 3.42}, {"text": "vector value random variables rather than a uni-variate random variable.", "start": 603.83, "duration": 3.825}, {"text": "So um, if z,", "start": 607.655, "duration": 2.325}, {"text": "[NOISE] this is due to Gaussian,", "start": 609.98, "duration": 2.64}, {"text": "with some mean vector mu and some covariance matrix sigma um,", "start": 612.62, "duration": 5.995}, {"text": "so if z is in Rn then mu would be Rn as well.", "start": 618.615, "duration": 8.2}, {"text": "And sigma, the covariance matrix,", "start": 626.815, "duration": 1.685}, {"text": "will be n by n. So z is two-dimensional,", "start": 628.5, "duration": 2.34}, {"text": "mu is two-dimensional and sigma is two-dimensional.", "start": 630.84, "duration": 2.525}, {"text": "And the expected value of z is equal to um, the mean.", "start": 633.365, "duration": 6.155}, {"text": "And the um, covariance of z,", "start": 639.52, "duration": 4.92}, {"text": "[NOISE] if you're familiar with multivariate co-variances,", "start": 644.44, "duration": 3.97}, {"text": "uh, this is the formula.", "start": 648.41, "duration": 3.24}, {"text": "Right. Um, and this simplifies,", "start": 651.65, "duration": 3.085}, {"text": "we show in the lecture notes.", "start": 654.735, "duration": 1.095}, {"text": "You can get this in the lecture notes.", "start": 655.83, "duration": 1.17}, {"text": "[NOISE] So you- and uh,", "start": 657.0, "duration": 5.805}, {"text": "following sometimes semi-standard convention,", "start": 662.805, "duration": 3.03}, {"text": "I'm sometimes gonna omit the square brackets.", "start": 665.835, "duration": 2.065}, {"text": "So instead of writing the expected value of z,", "start": 667.9, "duration": 2.355}, {"text": "meaning the mean of z, sometimes I just write to this e, z right?", "start": 670.255, "duration": 3.585}, {"text": "And omit- omit the square brackets to simplify the notation a bit.", "start": 673.84, "duration": 3.67}, {"text": "Okay? And the derivation from this step to this step is given in the lecture notes.", "start": 677.51, "duration": 5.135}, {"text": "Um, and so, well,", "start": 682.645, "duration": 2.865}, {"text": "[NOISE] the probability density function for a Gaussian looks like this.", "start": 685.51, "duration": 5.25}, {"text": "[NOISE]", "start": 690.76, "duration": 5.69}, {"text": "And this is one of those formulas that, I don't know.", "start": 696.45, "duration": 4.605}, {"text": "When you're implementing these algorithms you use it over and over.", "start": 701.055, "duration": 3.09}, {"text": "But what I've seen for a lot of people is al- almost no one- well,", "start": 704.145, "duration": 4.125}, {"text": "very few people start their machine learning and memorize this formula.", "start": 708.27, "duration": 2.64}, {"text": "Just look at it every time you need it.", "start": 710.91, "duration": 1.59}, {"text": "I've used it so many times I seem to have it seared in my brain by now,", "start": 712.5, "duration": 3.3}, {"text": "but most people don't even- when you've used it enough,", "start": 715.8, "duration": 2.82}, {"text": "you- you- you end up memorizing it.", "start": 718.62, "duration": 2.085}, {"text": "But let me show you some pictures of what this looks like since I think that would,", "start": 720.705, "duration": 4.065}, {"text": "um, that might be more useful.", "start": 724.77, "duration": 3.795}, {"text": "So the Multivariate Gaussian density has two parameters; Mu and Sigma.", "start": 728.565, "duration": 4.83}, {"text": "They control the mean and the variance of this density.", "start": 733.395, "duration": 5.385}, {"text": "Okay? So this is a picture of the Gaussian density.", "start": 738.78, "duration": 4.5}, {"text": "Um, this is a two-dimensional Gaussian bump.", "start": 743.28, "duration": 2.835}, {"text": "And for now, I've set the mean parameter to 0.", "start": 746.115, "duration": 4.095}, {"text": "So Mu is a two dimensional parameter,", "start": 750.21, "duration": 1.98}, {"text": "it's uh, it's 0, 0,", "start": 752.19, "duration": 1.05}, {"text": "which is why this Gaussian bump is centered at 0.", "start": 753.24, "duration": 5.34}, {"text": "Um, and the Co-variance matrix Sigma is the identity,", "start": 758.58, "duration": 6.745}, {"text": "um, i- i- i - is the identity matrix.", "start": 765.325, "duration": 3.2}, {"text": "So uh, so you know, well,", "start": 768.525, "duration": 2.295}, {"text": "so- so you've have this standard- this is also called the standard Gaussian", "start": 770.82, "duration": 3.51}, {"text": "distribution which means 0 and covariance equals to the identity.", "start": 774.33, "duration": 3.96}, {"text": "Now, I'm gonna take the covariance matrix and shrink it, right?", "start": 778.29, "duration": 3.57}, {"text": "So take a covariance matrix and multiply it by a number less than 1.", "start": 781.86, "duration": 3.045}, {"text": "That should shrink the variance- reduce the variability of distributions.", "start": 784.905, "duration": 4.125}, {"text": "If I do that, the density um,", "start": 789.03, "duration": 3.36}, {"text": "the p- probability density function becomes taller.", "start": 792.39, "duration": 3.57}, {"text": "Uh, this- this is a probability density function.", "start": 795.96, "duration": 2.13}, {"text": "So it always integrates to 1, right?", "start": 798.09, "duration": 2.28}, {"text": "The area under the curve,", "start": 800.37, "duration": 1.14}, {"text": "you know, is- is 1.", "start": 801.51, "duration": 1.485}, {"text": "And so by reducing the covariance from the identity to 0.6 times the identity,", "start": 802.995, "duration": 5.28}, {"text": "it reduces the spread of the Gaussian density, um,", "start": 808.275, "duration": 3.555}, {"text": "but it also makes it tall as a result, because,", "start": 811.83, "duration": 2.145}, {"text": "you know, the area under the curve must integrate to 1.", "start": 813.975, "duration": 2.715}, {"text": "Now let's make it fatter.", "start": 816.69, "duration": 2.445}, {"text": "Let's make the covariance two times the identity.", "start": 819.135, "duration": 3.0}, {"text": "Then you end up with a wider distribution where the values of", "start": 822.135, "duration": 4.26}, {"text": "um- I guess the axes here, this would be the z1 and the z2 axis;", "start": 826.395, "duration": 4.665}, {"text": "the two-dimensional Gaussian density, right?", "start": 831.06, "duration": 2.01}, {"text": "Increases the variance of the density.", "start": 833.07, "duration": 2.325}, {"text": "So let's go back to a standard Gaussian,", "start": 835.395, "duration": 2.37}, {"text": "uh, covariance equal 1, 1.", "start": 837.765, "duration": 1.8}, {"text": "Now, let's try fooling around with the off-diagonal entries.", "start": 839.565, "duration": 3.825}, {"text": "Um, I'm gonna- So right now,", "start": 843.39, "duration": 1.65}, {"text": "the off diagonal entries are 0, right?", "start": 845.04, "duration": 2.73}, {"text": "So in this Gaussian density,", "start": 847.77, "duration": 1.545}, {"text": "the off-diagonal elements are 0, 0.", "start": 849.315, "duration": 2.465}, {"text": "Let's increase that to 0.5 and see what happens.", "start": 851.78, "duration": 3.0}, {"text": "So if you do that,", "start": 854.78, "duration": 1.245}, {"text": "then the Gaussian density,", "start": 856.025, "duration": 2.085}, {"text": "uh, hope you can see see the change, right?", "start": 858.11, "duration": 1.65}, {"text": "It goes from this round shape to this slightly narrower thing.", "start": 859.76, "duration": 3.295}, {"text": "Let's increase that further to 0.8, 0.8.", "start": 863.055, "duration": 2.265}, {"text": "Then the density ends up looking like that, um,", "start": 865.32, "duration": 3.66}, {"text": "where now, it's more likely that z1 now- z1 and z2 are positively correlated.", "start": 868.98, "duration": 6.12}, {"text": "Okay? So let's go through all of these plots.", "start": 875.1, "duration": 2.85}, {"text": "But now looking at contours of these Gaussian densities instead of these 3-D bumps.", "start": 877.95, "duration": 4.17}, {"text": "So uh, this is the contours of", "start": 882.12, "duration": 3.84}, {"text": "the Gaussian density when the covariance matrix", "start": 885.96, "duration": 2.715}, {"text": "is the identity matrix and I apologize the aspect ratio.", "start": 888.675, "duration": 3.09}, {"text": "These are supposed to be perfectly round circles", "start": 891.765, "duration": 2.085}, {"text": "but the aspect ratio makes this look a little bit fatter,", "start": 893.85, "duration": 2.58}, {"text": "but this is supposed to be perfectly round circles.", "start": 896.43, "duration": 2.685}, {"text": "Um, and so, uh, when, uh,", "start": 899.115, "duration": 3.045}, {"text": "the covariance matrix is the identity matrix,", "start": 902.16, "duration": 2.265}, {"text": "you know, z1 and z2 are uncorrelated.", "start": 904.425, "duration": 2.91}, {"text": "Um, uh, and the contours of the Gaussian bump,", "start": 907.335, "duration": 3.0}, {"text": "of the Gaussian density look like brown circles.", "start": 910.335, "duration": 2.49}, {"text": "And if you increase the off-diagonal,", "start": 912.825, "duration": 1.83}, {"text": "excuse me, then it looks like that.", "start": 914.655, "duration": 2.55}, {"text": "If you increase it further to 0.8,", "start": 917.205, "duration": 1.485}, {"text": "0.8, it looks like that, okay?", "start": 918.69, "duration": 2.58}, {"text": "Uh, where now, most of", "start": 921.27, "duration": 1.68}, {"text": "the probability mass- probability ma- most probably density function places value on,", "start": 922.95, "duration": 4.05}, {"text": "um, z1 and z2 being positively correlated.", "start": 927.0, "duration": 4.53}, {"text": "Um, next, let's look at, uh,", "start": 931.53, "duration": 2.505}, {"text": "what happens if we set the off-diagonal elements to negative values, right?", "start": 934.035, "duration": 5.655}, {"text": "So, um, actually what do you think will happen?", "start": 939.69, "duration": 2.64}, {"text": "Let's set the off-diagonals to negative 0.5, 0.5.", "start": 942.33, "duration": 3.97}, {"text": "Right. Oh well. People are seeing,", "start": 947.24, "duration": 2.305}, {"text": "fewer making that hand gesture. Okay, cool.", "start": 949.545, "duration": 1.89}, {"text": "Right. [LAUGHTER] Right.", "start": 951.435, "duration": 1.455}, {"text": "So- so- so as you- you endow the two random variables with negative correlation,", "start": 952.89, "duration": 3.839}, {"text": "so you end up with, um,", "start": 956.729, "duration": 1.816}, {"text": "this type of probability density function, right?", "start": 958.545, "duration": 3.605}, {"text": "Uh, and the contours, it looks like this.", "start": 962.15, "duration": 2.88}, {"text": "Okay? Whe- whereas now slanted the other way.", "start": 965.03, "duration": 2.76}, {"text": "So now z1 and z2 have a negative correlation.", "start": 967.79, "duration": 2.7}, {"text": "And that's 0.8, 0.8.", "start": 970.49, "duration": 1.59}, {"text": "Okay? All right.", "start": 972.08, "duration": 1.45}, {"text": "So- so far we've been keeping the mean vector as", "start": 973.53, "duration": 2.94}, {"text": "0 and just varying the covariance matrix.", "start": 976.47, "duration": 3.195}, {"text": "Um, oh good. Yeah?", "start": 979.665, "duration": 1.245}, {"text": "[inaudible].", "start": 980.91, "duration": 3.45}, {"text": "Uh, yes. Every covariance matrix is symmetric. Yeah.", "start": 984.36, "duration": 2.28}, {"text": "[inaudible]", "start": 986.64, "duration": 0.93}, {"text": "Uh, the true thing about", "start": 987.57, "duration": 9.66}, {"text": "the covariance matrix has interesting column vectors,", "start": 997.23, "duration": 2.43}, {"text": "that point in interesting directions.", "start": 999.66, "duration": 1.29}, {"text": "Not really.", "start": 1000.95, "duration": 1.635}, {"text": "Um, let me think.", "start": 1002.585, "duration": 2.76}, {"text": "Maybe you should- yeah- yeah- uh,", "start": 1005.345, "duration": 3.585}, {"text": "no I- I- I think the covariance matrix is always symmetric.", "start": 1008.93, "duration": 3.75}, {"text": "And so I would usually not look at single columns of the covariance matrix in isolation.", "start": 1012.68, "duration": 6.39}, {"text": "Uh, when we talk about Principal components analysis,", "start": 1019.07, "duration": 2.28}, {"text": "we talk about the Eigenvectors of the covariance matrix,", "start": 1021.35, "duration": 2.55}, {"text": "which are the principle directions in which it points but,", "start": 1023.9, "duration": 2.37}, {"text": "uh, yeah we- we- we- we'll get to that later.", "start": 1026.27, "duration": 2.29}, {"text": "[inaudible]", "start": 1028.56, "duration": 4.36}, {"text": "Uh, yeah.", "start": 1032.92, "duration": 0.42}, {"text": "So the Eigenvectors are a covariance matrix,", "start": 1033.34, "duration": 1.3}, {"text": "points in the principal axes of the ellipse.", "start": 1034.64, "duration": 2.01}, {"text": "That's defined by the contents.", "start": 1036.65, "duration": 1.44}, {"text": "Yeah. Cool. Okay. Um, so this standard Gaussian would mean 0.", "start": 1038.09, "duration": 7.08}, {"text": "So the Gaussian bump is centered at 0,", "start": 1045.17, "duration": 2.19}, {"text": "0 because mu is 0, 0.", "start": 1047.36, "duration": 2.295}, {"text": "Uh, let's move Mu around.", "start": 1049.655, "duration": 1.455}, {"text": "So I'm going to move, you know, Mu to 0,", "start": 1051.11, "duration": 2.16}, {"text": "1.5.", "start": 1053.27, "duration": 1.905}, {"text": "So that moves the Gaussian, uh,", "start": 1055.175, "duration": 1.92}, {"text": "the position of the Gaussian density right.", "start": 1057.095, "duration": 2.25}, {"text": "Now let's move it to a different location.", "start": 1059.345, "duration": 2.325}, {"text": "Move it to minus 1.5, minus 1.", "start": 1061.67, "duration": 2.865}, {"text": "And so by varying the value of Mu,", "start": 1064.535, "duration": 2.19}, {"text": "you could also shift the center of the Gaussian density around.", "start": 1066.725, "duration": 3.555}, {"text": "Okay? So I hope this gives you a sense of,", "start": 1070.28, "duration": 3.45}, {"text": "um, as you vary the parameters,", "start": 1073.73, "duration": 1.889}, {"text": "the mean and the covariance matrix of the 2D Gaussian density, um,", "start": 1075.619, "duration": 5.071}, {"text": "those are probably- probably density functions you can get as", "start": 1080.69, "duration": 2.46}, {"text": "a result of changing Mu and Sigma.", "start": 1083.15, "duration": 3.06}, {"text": "Okay? Um, any other questions about this?", "start": 1086.21, "duration": 4.21}, {"text": "Raise the screen. [NOISE] All right, cool.", "start": 1092.65, "duration": 5.0}, {"text": "Here is a GDA, right, model.", "start": 1129.07, "duration": 7.735}, {"text": "Um, and- and, uh, let's see.", "start": 1136.805, "duration": 6.045}, {"text": "So, um, remember for GDA,", "start": 1142.85, "duration": 6.24}, {"text": "we need to model P of x given y,", "start": 1149.09, "duration": 2.67}, {"text": "right? It's up here, y given x.", "start": 1151.76, "duration": 1.26}, {"text": "So I'm gonna write this separately in two separate equations P of x given y equals 0.", "start": 1153.02, "duration": 5.055}, {"text": "So what's the chance- what's the, uh,", "start": 1158.075, "duration": 1.755}, {"text": "probability density of the features if it is a benign tumor?", "start": 1159.83, "duration": 4.395}, {"text": "Um, I'm going to assume it's Gaussian.", "start": 1164.225, "duration": 3.285}, {"text": "So I'm just going to write down the formula for Gaussian.", "start": 1167.51, "duration": 2.82}, {"text": "[NOISE]", "start": 1170.33, "duration": 24.885}, {"text": "And then similarly, I'm going to assume", "start": 1195.215, "duration": 2.85}, {"text": "that if is a malignant tumor as if y is equal to 1,", "start": 1198.065, "duration": 3.93}, {"text": "that the density of the features is also Gaussian, okay?", "start": 1201.995, "duration": 9.195}, {"text": "And, um, I wanna point out a couple of things,", "start": 1211.19, "duration": 2.07}, {"text": "so the parameters of the GDA model are", "start": 1213.26, "duration": 5.34}, {"text": "mu0, mu1, and sigma.", "start": 1218.6, "duration": 6.03}, {"text": "Um, and for reasons,", "start": 1224.63, "duration": 1.41}, {"text": "we'll go into a little bit,", "start": 1226.04, "duration": 1.125}, {"text": "we'll use the same sigma for both class.", "start": 1227.165, "duration": 4.675}, {"text": "Um, but we use different means, 0 and 1, okay?", "start": 1231.94, "duration": 5.38}, {"text": "Uh, and we can come back to this later.", "start": 1237.32, "duration": 2.475}, {"text": "If you want, you could use separate parameters, you know,", "start": 1239.795, "duration": 2.895}, {"text": "sigma 0 and sigma 1,", "start": 1242.69, "duration": 1.59}, {"text": "but that's not usually done.", "start": 1244.28, "duration": 1.59}, {"text": "So we're going to assume that the two Gaussians,", "start": 1245.87, "duration": 2.31}, {"text": "for the positive and negative classes,", "start": 1248.18, "duration": 1.35}, {"text": "have the same covariance matrix but they,", "start": 1249.53, "duration": 1.79}, {"text": "they have different means.", "start": 1251.32, "duration": 1.18}, {"text": "Uh, you don't have to make this assumption,", "start": 1252.5, "duration": 1.98}, {"text": "but this is the way it's most commonly done.", "start": 1254.48, "duration": 2.01}, {"text": "And then we can talk about the reason why we tend to do that in a second.", "start": 1256.49, "duration": 3.91}, {"text": "Um, so this is a model for P of y given x.", "start": 1260.4, "duration": 4.64}, {"text": "The other thing we need to do is model P of y.", "start": 1265.04, "duration": 4.39}, {"text": "Uh, so y is just a Bernoulli random variable, right.", "start": 1269.43, "duration": 3.31}, {"text": "It takes on, you know, the values 0 or 1.", "start": 1272.74, "duration": 2.595}, {"text": "And so, I'm going to write it like this,", "start": 1275.335, "duration": 2.94}, {"text": "phi to the y times 1 minus phi to the 1 minus y, okay?", "start": 1278.275, "duration": 8.665}, {"text": "Um, and you saw this kind of notation when we talked about logistic regression,", "start": 1286.94, "duration": 5.715}, {"text": "but all this means is that, um, you know,", "start": 1292.655, "duration": 2.7}, {"text": "probability of y being equal to 1 is equal to phi, right.", "start": 1295.355, "duration": 5.22}, {"text": "Because y is either 0 or 1.", "start": 1300.575, "duration": 1.59}, {"text": "And so, um, this is the way of writing,", "start": 1302.165, "duration": 2.985}, {"text": "uh, uh, probability of y equals 1 is equal to phi, okay?", "start": 1305.15, "duration": 3.855}, {"text": "And, uh, you saw a similar explanation,", "start": 1309.005, "duration": 1.935}, {"text": "it's a notation when we're talking about,", "start": 1310.94, "duration": 2.34}, {"text": "um, logistic regression, right,", "start": 1313.28, "duration": 2.22}, {"text": "one week ago, last Monday.", "start": 1315.5, "duration": 1.965}, {"text": "And so, the last parameter is phi.", "start": 1317.465, "duration": 3.615}, {"text": "So this is Rn,", "start": 1321.08, "duration": 3.32}, {"text": "this is also Rn,", "start": 1324.4, "duration": 1.74}, {"text": "this is Rn by n and that's just a real number between 0 and 1, okay?", "start": 1326.14, "duration": 8.71}, {"text": "So, um, for any- let's see.", "start": 1345.22, "duration": 4.51}, {"text": "So if you can fit mu0,", "start": 1349.73, "duration": 1.395}, {"text": "mu1, sigma, and phi to your data,", "start": 1351.125, "duration": 3.405}, {"text": "then these parameters will define P of x given y and P of y.", "start": 1354.53, "duration": 6.435}, {"text": "And so, if at test time you have a new patient walk into your office,", "start": 1360.965, "duration": 4.86}, {"text": "and you need to compute this,", "start": 1365.825, "duration": 1.605}, {"text": "then you can compute,", "start": 1367.43, "duration": 1.305}, {"text": "right, these things in the red and the orange boxes.", "start": 1368.735, "duration": 3.045}, {"text": "Each of these is a number,", "start": 1371.78, "duration": 1.17}, {"text": "and by plugging all these numbers in the formula,", "start": 1372.95, "duration": 1.86}, {"text": "you get a number alpha P of y equals 1 given x and you can then predict,", "start": 1374.81, "duration": 3.765}, {"text": "you know, malignant or benign tumor.", "start": 1378.575, "duration": 2.745}, {"text": "Right. So let's talk about how to fit the parameters.", "start": 1381.32, "duration": 4.14}, {"text": "So you have a training set, um, as usual,", "start": 1385.46, "duration": 5.04}, {"text": "I'm gonna write the tre- well,", "start": 1390.5, "duration": 1.35}, {"text": "I'm go- let me write the training set like this xi,", "start": 1391.85, "duration": 2.955}, {"text": "yi, for i equals 1 through m, right?", "start": 1394.805, "duration": 2.625}, {"text": "This is a usual training set.", "start": 1397.43, "duration": 1.8}, {"text": "Um, and what we're going to do,", "start": 1399.23, "duration": 4.845}, {"text": "in order to fit these parameters is maximize the joint likelihood.", "start": 1404.075, "duration": 5.935}, {"text": "And in particular, um,", "start": 1412.6, "duration": 3.235}, {"text": "let me define the likelihood of", "start": 1415.835, "duration": 2.61}, {"text": "the parameters to be", "start": 1418.445, "duration": 8.415}, {"text": "equal to the product from i equals 1 through m,", "start": 1426.86, "duration": 3.63}, {"text": "up here, xi, yi, you know,", "start": 1430.49, "duration": 4.755}, {"text": "parameterized by the, um, the parameters, okay?", "start": 1435.245, "duration": 10.465}, {"text": "Um, and I'm, I'm just like dropped the parameters here, right?", "start": 1453.79, "duration": 5.44}, {"text": "To simplify the notation a little bit, okay?", "start": 1459.23, "duration": 3.18}, {"text": "And the big difference between, um,", "start": 1462.41, "duration": 4.725}, {"text": "a generative learning algorithm like this,", "start": 1467.135, "duration": 2.28}, {"text": "compared to a discriminative learning algorithm,", "start": 1469.415, "duration": 2.579}, {"text": "is that the cost function you maximize is this joint likelihood which is p of x, y.", "start": 1471.994, "duration": 9.766}, {"text": "Whereas for a discriminative learning algorithm,", "start": 1481.76, "duration": 3.73}, {"text": "we were maximizing, um,", "start": 1486.79, "duration": 2.815}, {"text": "this other thing, right.", "start": 1489.605, "duration": 4.186}, {"text": "Uh, which is sometimes also called the conditional likelihood, okay?", "start": 1500.16, "duration": 10.415}, {"text": "So the big difference between the- these two cost functions,", "start": 1510.575, "duration": 3.765}, {"text": "is that for logistic regression or linear regression and generalized linear models,", "start": 1514.34, "duration": 4.65}, {"text": "you were trying to choose parameters theta,", "start": 1518.99, "duration": 2.07}, {"text": "that maximize p of y given x.", "start": 1521.06, "duration": 3.42}, {"text": "But for generative learning algorithms,", "start": 1524.48, "duration": 1.95}, {"text": "we're gonna try to choose parameters that maximize p of x and y or p of x, y, right.", "start": 1526.43, "duration": 6.015}, {"text": "Okay?", "start": 1532.445, "duration": 1.975}, {"text": "So all right.", "start": 1536.68, "duration": 8.48}, {"text": "So if you use,", "start": 1563.08, "duration": 2.485}, {"text": "um, maximum likelihood estimation.", "start": 1565.565, "duration": 3.205}, {"text": "Um, so you choose the parameters phi, mu0, mu1,", "start": 1578.65, "duration": 6.64}, {"text": "and sigma they maximize the log likelihood, right.", "start": 1585.29, "duration": 6.36}, {"text": "Where this you define as, you know,", "start": 1591.65, "duration": 2.295}, {"text": "log of the likelihood that we defined up there.", "start": 1593.945, "duration": 4.095}, {"text": "Um, and so, uh, th- we,", "start": 1598.04, "duration": 1.935}, {"text": "we actually ask you to do this as a problem set in the next homework.", "start": 1599.975, "duration": 3.435}, {"text": "But so the way you maximize this is,", "start": 1603.41, "duration": 2.175}, {"text": "um, look at that formula for the likelihood,", "start": 1605.585, "duration": 3.42}, {"text": "take logs, take derivatives of this thing,", "start": 1609.005, "duration": 2.415}, {"text": "set the derivative equal to 0 and then solve for", "start": 1611.42, "duration": 2.13}, {"text": "the values of the parameters that maximize this whole thing.", "start": 1613.55, "duration": 2.565}, {"text": "And I'll, I'll, I'll just tell you the answers you are supposed to get.", "start": 1616.115, "duration": 2.985}, {"text": "[LAUGHTER].", "start": 1619.1, "duration": 0.9}, {"text": "But you still have to do the derivation.", "start": 1620.0, "duration": 2.95}, {"text": "Right. um, the value of phi that maximizes this is,", "start": 1625.15, "duration": 5.95}, {"text": "you know, not that surprisingly.", "start": 1631.1, "duration": 2.415}, {"text": "So, so phi is the estimate of probability of y being equal to 1, right?", "start": 1633.515, "duration": 5.475}, {"text": "So what's the chance when the next patient walks into your, uh,", "start": 1638.99, "duration": 3.21}, {"text": "doctor's office that they have a, a malignant tumor?", "start": 1642.2, "duration": 3.555}, {"text": "And so the maximum likelihood estimate for phi is, um,", "start": 1645.755, "duration": 3.33}, {"text": "it's just of all of your training examples,", "start": 1649.085, "duration": 2.235}, {"text": "what's the fraction with label y equals 1, right.", "start": 1651.32, "duration": 2.895}, {"text": "So it's the, the maximum likelihood of the, uh,", "start": 1654.215, "duration": 2.25}, {"text": "bias of a coin toss is just, well, count up the fraction of heads you got,", "start": 1656.465, "duration": 4.11}, {"text": "okay? So this, this is it.", "start": 1660.575, "duration": 1.545}, {"text": "um, and one other way to write this is,", "start": 1662.12, "duration": 2.985}, {"text": "um, sum from i equals 1 through m indicator.", "start": 1665.105, "duration": 6.235}, {"text": "Okay. Right. Um, let's see.", "start": 1674.11, "duration": 10.495}, {"text": "So as you saw the indicator notation on Wednesday, did you?", "start": 1684.605, "duration": 4.01}, {"text": "No.", "start": 1688.615, "duration": 0.665}, {"text": "Uh, did you so- do, did we talk about the indicator notation on Wednesday?", "start": 1689.28, "duration": 3.2}, {"text": "No.", "start": 1692.48, "duration": 0.21}, {"text": "Okay. Um, so, um, uh,", "start": 1692.69, "duration": 1.845}, {"text": "this notation is an indicator function, uh, where,", "start": 1694.535, "duration": 4.425}, {"text": "um, indicator yi, equals 1 is,", "start": 1698.96, "duration": 3.18}, {"text": "uh, uh, return 0 or 1 depending on whether the thing inside is true, right?", "start": 1702.14, "duration": 3.78}, {"text": "So there's an indicator notation in which an indicator of", "start": 1705.92, "duration": 3.99}, {"text": "a true statement is equal to 1 and indicator of a false statement is equal to 0.", "start": 1709.91, "duration": 5.22}, {"text": "So that's another way of writing,", "start": 1715.13, "duration": 1.795}, {"text": "writing this formula, right.", "start": 1716.925, "duration": 2.8}, {"text": "Um, and then the maximum likelihood estimate for mu0 is this, um,", "start": 1719.725, "duration": 6.285}, {"text": "I'll just write out.", "start": 1726.01, "duration": 1.75}, {"text": "Okay.", "start": 1742.86, "duration": 1.89}, {"text": "Ah, so, well, it- it actually if you,", "start": 1744.75, "duration": 3.65}, {"text": "ah, put aside the math for now,", "start": 1748.4, "duration": 1.92}, {"text": "what do you think is the maximum likelihood estimate of the mean of all of the,", "start": 1750.32, "duration": 3.24}, {"text": "ah, features for the benign tumors, right?", "start": 1753.56, "duration": 2.58}, {"text": "Well, what you do is you take", "start": 1756.14, "duration": 1.02}, {"text": "all the benign tumors in your training set and just take the average,", "start": 1757.16, "duration": 3.225}, {"text": "that seems like a very reasonable way.", "start": 1760.385, "duration": 1.5}, {"text": "Just look- look at your training set.", "start": 1761.885, "duration": 1.815}, {"text": "Look at all of the- look at all of the benign tumors, all the Os,", "start": 1763.7, "duration": 5.1}, {"text": "I guess, and you just take the mean of these, and that,", "start": 1768.8, "duration": 2.865}, {"text": "you know, seems like a pretty reasonable way to estimate Mu 0, right?", "start": 1771.665, "duration": 3.66}, {"text": "Look all of your negative examples and average their features.", "start": 1775.325, "duration": 2.565}, {"text": "So this is a way of writing out that intuition.", "start": 1777.89, "duration": 3.0}, {"text": "Um, So the denominator is sum from i equals 1 through m indicates a y_i equals, 0,", "start": 1780.89, "duration": 5.67}, {"text": "and so the denominator will count up the number of", "start": 1786.56, "duration": 3.21}, {"text": "examples that have benign tumors, right?", "start": 1789.77, "duration": 3.39}, {"text": "Because every time y_i equals 0,", "start": 1793.16, "duration": 2.385}, {"text": "you get an extra 1 in this sum,", "start": 1795.545, "duration": 3.48}, {"text": "um, ah, and so the denominator ends up being", "start": 1799.025, "duration": 3.225}, {"text": "the total number of benign tumors in your training set.", "start": 1802.25, "duration": 4.095}, {"text": "Okay? Um, and the numerator, ah,", "start": 1806.345, "duration": 3.33}, {"text": "sum for m equals 1 through m indicator is a benign tumor times x_i.", "start": 1809.675, "duration": 5.7}, {"text": "So the effect of that is, um,", "start": 1815.375, "duration": 3.165}, {"text": "whenever, a tumor is benign is 1 times the features,", "start": 1818.54, "duration": 5.114}, {"text": "whenever an example is malignant is", "start": 1823.654, "duration": 3.436}, {"text": "0 times the features and so the numerator is summing up all the features,", "start": 1827.09, "duration": 5.76}, {"text": "all the feature vectors for all of the examples that are benign.", "start": 1832.85, "duration": 3.63}, {"text": "Does that make sense?", "start": 1836.48, "duration": 1.83}, {"text": "I- I just write this out,", "start": 1838.31, "duration": 1.185}, {"text": "so this is the sum of feature vectors for,", "start": 1839.495, "duration": 7.95}, {"text": "um, for all the examples", "start": 1847.445, "duration": 3.345}, {"text": "with y equals 0 and the denominator is a number of the examples,", "start": 1850.79, "duration": 7.69}, {"text": "where y equals 0, okay?", "start": 1861.13, "duration": 3.985}, {"text": "And then if you take this ratio,", "start": 1865.115, "duration": 2.55}, {"text": "if you take this fraction,", "start": 1867.665, "duration": 1.065}, {"text": "then you're summing up all of the feature vectors for", "start": 1868.73, "duration": 2.67}, {"text": "the benign tumors divide by the total number of benign tumors in the training set,", "start": 1871.4, "duration": 4.26}, {"text": "and so that's just the mean of the feature vectors of all of the benign examples.", "start": 1875.66, "duration": 6.015}, {"text": "Okay? Um, and then,", "start": 1881.675, "duration": 8.845}, {"text": "right, maximum likelihood for Mu 1,", "start": 1898.63, "duration": 2.59}, {"text": "no surprises, is sort of kind of what you'd expect,", "start": 1901.22, "duration": 2.655}, {"text": "sum up all of the positive examples and", "start": 1903.875, "duration": 2.505}, {"text": "divide by the total number of positive examples and get the means.", "start": 1906.38, "duration": 2.76}, {"text": "So that's maximum likelihood for Mu_1,", "start": 1909.14, "duration": 3.375}, {"text": "um, and then I just write this out.", "start": 1912.515, "duration": 4.345}, {"text": "If you are familiar with covariance matrices,", "start": 1917.8, "duration": 3.94}, {"text": "this formula may not surprise you.", "start": 1921.74, "duration": 2.97}, {"text": "But if you're less familiar,", "start": 1924.71, "duration": 2.79}, {"text": "then I guess you can see the details in the homework.", "start": 1927.5, "duration": 5.98}, {"text": "Okay. Don't worry too much about that.", "start": 1939.88, "duration": 2.8}, {"text": "Ah, you can unpack the details in the lecture notes.", "start": 1942.68, "duration": 2.85}, {"text": "So we'll know how it works, okay?", "start": 1945.53, "duration": 2.58}, {"text": "But the covariance matrix, basically tries to,", "start": 1948.11, "duration": 3.735}, {"text": "you know, fit contours to the ellipse, right?", "start": 1951.845, "duration": 3.735}, {"text": "Like we saw, ah, so- so try to fill the Gaussian to both of these with", "start": 1955.58, "duration": 3.36}, {"text": "these corresponding means but you want one covariance matrix to both of these.", "start": 1958.94, "duration": 4.245}, {"text": "Okay? Um, So these are the- so- so- so the way- so the way I motivated this was,", "start": 1963.185, "duration": 6.81}, {"text": "you know, I said, well,", "start": 1969.995, "duration": 1.125}, {"text": "if you want to estimate the mean of a coin toss,", "start": 1971.12, "duration": 1.98}, {"text": "just count up the fraction of coin tosses, they came up heads,", "start": 1973.1, "duration": 2.4}, {"text": "ah, and then it seems that the mean for Mu_0 and Mu_1,", "start": 1975.5, "duration": 2.97}, {"text": "you just look at these examples and pick the mean, right?", "start": 1978.47, "duration": 2.46}, {"text": "So that- that was the intuitive explanation for how you get these formulas.", "start": 1980.93, "duration": 3.39}, {"text": "But the mathematically sound way to get", "start": 1984.32, "duration": 2.46}, {"text": "these formulas is not by this intuitive argument that I just gave,", "start": 1986.78, "duration": 4.005}, {"text": "it's instead to look at the likelihood, ah,", "start": 1990.785, "duration": 3.195}, {"text": "take logs, get the log likelihood,", "start": 1993.98, "duration": 1.65}, {"text": "take derivatives, set derivatives equal to 0,", "start": 1995.63, "duration": 2.385}, {"text": "solve for all these values and prove more formally that these", "start": 1998.015, "duration": 3.39}, {"text": "are the actual values that maximize this thing, right?", "start": 2001.405, "duration": 3.435}, {"text": "By- by the same theories as you solved,", "start": 2004.84, "duration": 1.815}, {"text": "so you can see that for yourself,", "start": 2006.655, "duration": 1.92}, {"text": "um, in the problem sets.", "start": 2008.575, "duration": 2.37}, {"text": "Okay? So- All right.", "start": 2010.945, "duration": 9.105}, {"text": "Um, finally, having fit these parameters,", "start": 2020.05, "duration": 6.495}, {"text": "um, if you want to make a prediction, right?", "start": 2026.545, "duration": 8.265}, {"text": "So given the new patient, ah,", "start": 2034.81, "duration": 2.13}, {"text": "how do you make a prediction for whether their tumor is malignant or benign?", "start": 2036.94, "duration": 4.995}, {"text": "Um, so if you want to predict the most likely class label,", "start": 2041.935, "duration": 7.845}, {"text": "ah, you choose max over y,", "start": 2049.78, "duration": 2.835}, {"text": "of p of y, given x, right?", "start": 2052.615, "duration": 4.605}, {"text": "Um, and by Bayes' rule,", "start": 2057.22, "duration": 3.06}, {"text": "this is max over y of p of x given y,", "start": 2060.28, "duration": 3.45}, {"text": "p of y divided by p of x.", "start": 2063.73, "duration": 5.475}, {"text": "Okay? Now, um, I wanna introduce one esh- well,", "start": 2069.205, "duration": 4.86}, {"text": "one- one more piece of notation which is,", "start": 2074.065, "duration": 2.25}, {"text": "ah, I wanna introduce,", "start": 2076.315, "duration": 3.525}, {"text": "actually, how- how many of you are familiar with the arg max notation?", "start": 2079.84, "duration": 3.96}, {"text": "Most of you? Like two- two-thirds? Okay, cool.", "start": 2083.8, "duration": 3.81}, {"text": "I- I- I'll go over this quickly.", "start": 2087.61, "duration": 1.545}, {"text": "So, um, this is just an example.", "start": 2089.155, "duration": 3.795}, {"text": "So the, um, let's see.", "start": 2092.95, "duration": 4.935}, {"text": "Ah, boy. All right.", "start": 2097.885, "duration": 5.19}, {"text": "So, you know, the Min over z of, uh,", "start": 2103.075, "duration": 4.455}, {"text": "z minus 5 squared is equal to 0", "start": 2107.53, "duration": 3.66}, {"text": "because the smallest possible value of z by a 5 squared is 0, right?", "start": 2111.19, "duration": 4.035}, {"text": "and the arg min over z of z minus 5 squared is equal to 5.", "start": 2115.225, "duration": 7.875}, {"text": "Okay? So the min is the smallest possible value attained by the thing inside", "start": 2123.1, "duration": 5.34}, {"text": "and the arg min is the value you need to", "start": 2128.44, "duration": 2.64}, {"text": "plug in to achieve that smallest possible value, right?", "start": 2131.08, "duration": 3.09}, {"text": "So ah, the prediction you actually want to make,", "start": 2134.17, "duration": 2.76}, {"text": "if you want to output a value for y,", "start": 2136.93, "duration": 1.635}, {"text": "you don't wanna output a probability, right?", "start": 2138.565, "duration": 2.055}, {"text": "You know what I'm saying? Well, what do I think is the value of y?", "start": 2140.62, "duration": 1.785}, {"text": "So you might choose a value of y that maximizes this,", "start": 2142.405, "duration": 2.835}, {"text": "and- so- so there's the arg max of this and this would be either 0 or 1, right?", "start": 2145.24, "duration": 3.93}, {"text": "Um, so that's equal to arg max of that,", "start": 2149.17, "duration": 2.34}, {"text": "and you notice that,", "start": 2151.51, "duration": 2.235}, {"text": "ah, this denominator is just a constant, right?", "start": 2153.745, "duration": 2.835}, {"text": "It doesn't- it doesn't- it's a p of x,", "start": 2156.58, "duration": 1.86}, {"text": "it's- y doesn't even appear in there?", "start": 2158.44, "duration": 2.04}, {"text": "It's just some positive number.", "start": 2160.48, "duration": 1.62}, {"text": "And so this is equal to,", "start": 2162.1, "duration": 2.79}, {"text": "just arg max over y,", "start": 2164.89, "duration": 2.31}, {"text": "p of x given y times p of y, okay?", "start": 2167.2, "duration": 4.26}, {"text": "So when implementing, um, ah,", "start": 2171.46, "duration": 4.44}, {"text": "when- when making predictions with Gaussian disc-", "start": 2175.9, "duration": 3.15}, {"text": "in a- with the generative learning algorithms,", "start": 2179.05, "duration": 2.7}, {"text": "sometimes to save on computation,", "start": 2181.75, "duration": 2.025}, {"text": "you don't bother to calculate the denominator,", "start": 2183.775, "duration": 2.205}, {"text": "if all you care about is to make a prediction,", "start": 2185.98, "duration": 2.52}, {"text": "but if you'd actually need a probability,", "start": 2188.5, "duration": 1.875}, {"text": "then you'd have to normalize the probability, okay?", "start": 2190.375, "duration": 3.595}, {"text": "Okay. So let's examine", "start": 2196.95, "duration": 10.42}, {"text": "what the algorithm is doing.", "start": 2207.37, "duration": 2.04}, {"text": "[NOISE].", "start": 2209.41, "duration": 7.56}, {"text": "All right. So let's look at the same dataset and compare and contrast what", "start": 2216.97, "duration": 5.16}, {"text": "a discriminative learning algorithm versus", "start": 2222.13, "duration": 2.01}, {"text": "a generative learning algorithm will do on this dataset.", "start": 2224.14, "duration": 3.54}, {"text": "Right. Um, here's example with two features X1 and X2 and positive and negative examples.", "start": 2227.68, "duration": 8.1}, {"text": "So let's start with a discriminative learning algorithm.", "start": 2235.78, "duration": 2.715}, {"text": "Um, let say you initialize the parameters randomly.", "start": 2238.495, "duration": 3.375}, {"text": "Typically, when you run a logistic regression,", "start": 2241.87, "duration": 2.025}, {"text": "I almost always initialize the parameters as 0 but- but this just, you know,", "start": 2243.895, "duration": 3.705}, {"text": "it's more interesting to start off for the purposes of visualization,", "start": 2247.6, "duration": 2.94}, {"text": "with a random line I guess.", "start": 2250.54, "duration": 1.66}, {"text": "And then if you run one iteration of gradient descent on the conditional likelihood,", "start": 2252.2, "duration": 5.125}, {"text": "um, one iteration of logistic regression moves the line there.", "start": 2257.325, "duration": 3.66}, {"text": "There's two iterations, three iterations, um,", "start": 2260.985, "duration": 3.45}, {"text": "four iterations and so on and after", "start": 2264.435, "duration": 3.37}, {"text": "about 20 iterations it will converge to that pretty decent discriminative boundary.", "start": 2267.805, "duration": 5.76}, {"text": "So that's logistic regression,", "start": 2273.565, "duration": 1.32}, {"text": "really searching for a line that separates positive and negative examples.", "start": 2274.885, "duration": 3.81}, {"text": "How about the generative learning algorithm?", "start": 2278.695, "duration": 2.85}, {"text": "What it does is the following,", "start": 2281.545, "duration": 2.37}, {"text": "which is fit with Gaussian discriminant analysis.", "start": 2283.915, "duration": 3.69}, {"text": "What we'll do, is fit Gaussians to the positive and negative examples.", "start": 2287.605, "duration": 5.505}, {"text": "Right, and just one- one technical detail, um,", "start": 2293.11, "duration": 3.45}, {"text": "I described this as if we look at the two classes separately", "start": 2296.56, "duration": 3.45}, {"text": "because we use the same covariance matrix sigma for the positive and negative classes.", "start": 2300.01, "duration": 3.795}, {"text": "We actually don't quite look at them totally separately but we do", "start": 2303.805, "duration": 3.105}, {"text": "fit two Gaussian densities to the positive and negative examples.", "start": 2306.91, "duration": 4.2}, {"text": "And then what we do is,", "start": 2311.11, "duration": 2.145}, {"text": "for each point try to decide whether this is class label using Bayes' rule,", "start": 2313.255, "duration": 5.325}, {"text": "using that formula and it turns out that this implies the following decision boundary.", "start": 2318.58, "duration": 5.235}, {"text": "Right. So points to the upper right of this decision boundary,", "start": 2323.815, "duration": 4.334}, {"text": "to that straight line I just drew,", "start": 2328.149, "duration": 1.666}, {"text": "you are closer to the negative class.", "start": 2329.815, "duration": 2.655}, {"text": "You end up classifying them as negative examples and", "start": 2332.47, "duration": 1.98}, {"text": "points to the lower left of that line,", "start": 2334.45, "duration": 2.445}, {"text": "you end there classifying as- as a positive examples.", "start": 2336.895, "duration": 3.15}, {"text": "And I've- I've also drawn in green here the decision boundary for logistic regression.", "start": 2340.045, "duration": 5.985}, {"text": "So- so- so these two algorithms actually come up", "start": 2346.03, "duration": 2.64}, {"text": "with slightly different decision boundaries.", "start": 2348.67, "duration": 3.045}, {"text": "Okay, but the way you arrive at these two decision boundaries are a little bit different.", "start": 2351.715, "duration": 4.605}, {"text": "So, um. All right,", "start": 2356.86, "duration": 6.505}, {"text": "let's go back to the- Any questions about this? Yeah.", "start": 2363.365, "duration": 4.085}, {"text": "[NOISE]", "start": 2367.45, "duration": 0.91}, {"text": "[inaudible].", "start": 2368.36, "duration": 13.311}, {"text": "Oh, sure yes, good question.", "start": 2381.671, "duration": 0.869}, {"text": "So why- why- why do we use two separate means,", "start": 2382.54, "duration": 3.12}, {"text": "mu 0 and mu 1 and a single covariance matrix sigma?", "start": 2385.66, "duration": 3.945}, {"text": "It turns out that, um-.", "start": 2389.605, "duration": 3.375}, {"text": "It turns out that if you choose to build the model this way,", "start": 2392.98, "duration": 3.45}, {"text": "the decision boundary ends up being linear and so", "start": 2396.43, "duration": 2.46}, {"text": "for a lot of problems if you want to linear decision boundary,", "start": 2398.89, "duration": 2.88}, {"text": "um, uh, um, yeah.", "start": 2401.77, "duration": 1.965}, {"text": "And it turns out you could choose to use two separate,", "start": 2403.735, "duration": 3.285}, {"text": "um, covariance matrix sigma 0 and sigma 1,", "start": 2407.02, "duration": 2.94}, {"text": "and they'll actually work okay.", "start": 2409.96, "duration": 1.29}, {"text": "Right. There's- it is actually very reasonable to do so as well,", "start": 2411.25, "duration": 3.09}, {"text": "but you double the number of parameters roughly and", "start": 2414.34, "duration": 3.54}, {"text": "you end up with a decision boundary that isn't linear anymore.", "start": 2417.88, "duration": 3.465}, {"text": "But it is actually not an unreasonable algorithm to do that as well.", "start": 2421.345, "duration": 3.625}, {"text": "Um, now, there's one-", "start": 2425.705, "duration": 4.625}, {"text": "[BACKGROUND].", "start": 2430.33, "duration": 20.85}, {"text": "Now, there's one very interesting property, um,", "start": 2451.18, "duration": 3.105}, {"text": "about Gaussian discriminant analysis and it turns out that's- ah.", "start": 2454.285, "duration": 8.235}, {"text": "Well, let's- let's compare", "start": 2462.52, "duration": 3.94}, {"text": "GDA to logistic regression and,", "start": 2469.53, "duration": 6.475}, {"text": "um, for a fixed set of parameters.", "start": 2476.005, "duration": 5.515}, {"text": "Right. So let's say you've learned some set of parameters.", "start": 2484.23, "duration": 5.425}, {"text": "Um, I'm going to do an exercise where we're going to plot,", "start": 2489.655, "duration": 5.905}, {"text": "P of Y equals 1 given X,", "start": 2498.24, "duration": 4.81}, {"text": "you're parameterized by all these things,", "start": 2503.05, "duration": 3.61}, {"text": "right, as a function of x.", "start": 2507.32, "duration": 3.59}, {"text": "So I'm gonna do this little exercise in a second,", "start": 2514.41, "duration": 3.64}, {"text": "but what this means is,", "start": 2518.05, "duration": 1.635}, {"text": "um, well, this formula,", "start": 2519.685, "duration": 2.205}, {"text": "this is equal to P of X given Y equals 1, you know,", "start": 2521.89, "duration": 6.525}, {"text": "which is parameterized by- right well,", "start": 2528.415, "duration": 3.435}, {"text": "the various parameters times p of y equals 1,", "start": 2531.85, "duration": 3.825}, {"text": "is parameterized by phi divided by P of X which depends on all the parameters, I guess.", "start": 2535.675, "duration": 7.345}, {"text": "Right. So by Bayes rule,", "start": 2546.84, "duration": 5.124}, {"text": "you know this formula is equal to", "start": 2551.964, "duration": 2.596}, {"text": "this little thing and just as we saw earlier, I guess right.", "start": 2554.56, "duration": 5.7}, {"text": "Once you have fixed all the parameters that's", "start": 2560.26, "duration": 1.89}, {"text": "just a number you compute by evaluating the Gaussian density.", "start": 2562.15, "duration": 3.93}, {"text": "Um, this is the Bernoulli probability,", "start": 2566.08, "duration": 4.559}, {"text": "so actually P of Y equals 1 parameterized by phi is just equal to phi", "start": 2570.639, "duration": 3.121}, {"text": "is that second term and you similarly calculate the denominator.", "start": 2573.76, "duration": 3.33}, {"text": "But so for every value of x,", "start": 2577.09, "duration": 2.175}, {"text": "you can compute this ratio and thus get a number for", "start": 2579.265, "duration": 4.455}, {"text": "the chance of Y being 1 given X. So I'm gonna go", "start": 2583.72, "duration": 5.01}, {"text": "through one example of", "start": 2588.73, "duration": 5.655}, {"text": "what function you'd get for P of Y equals 1 given X,", "start": 2594.385, "duration": 4.785}, {"text": "for what function you get for this if you actually plot this for,", "start": 2599.17, "duration": 3.57}, {"text": "um, different values of X.", "start": 2602.74, "duration": 2.655}, {"text": "Okay. So, um, let's see.", "start": 2605.395, "duration": 6.225}, {"text": "Let's say you have just one feature X,", "start": 2611.62, "duration": 2.46}, {"text": "so X is a- a- and let's say that you have", "start": 2614.08, "duration": 4.95}, {"text": "a few negative examples there and a few positive examples there.", "start": 2619.03, "duration": 6.315}, {"text": "Right. So it's a simple dataset.", "start": 2625.345, "duration": 4.105}, {"text": "Okay, and let's see what Gaussian discriminant analysis will do on this dataset.", "start": 2629.67, "duration": 5.86}, {"text": "Um, with just one feature so that's why all the data is parsing on 1D.", "start": 2635.53, "duration": 4.455}, {"text": "So let me map all this data to an x-axis.", "start": 2639.985, "duration": 10.075}, {"text": "I just filled this data and mapped it down.", "start": 2652.32, "duration": 3.625}, {"text": "And if you fit a Gaussian to each of these two data sets then you end up with, you know,", "start": 2655.945, "duration": 7.68}, {"text": "Gaussians as follows where this bump on the left is P of X given Y equals 0 and", "start": 2663.625, "duration": 6.795}, {"text": "this bump on the right is P of X given Y equals 1.", "start": 2670.42, "duration": 6.63}, {"text": "Right, and- and again just to check on all details that", "start": 2677.05, "duration": 2.31}, {"text": "we set the same variance to the two Gaussians,", "start": 2679.36, "duration": 3.03}, {"text": "but you know, you kinda model the Gaussian densities of what does this class 0 look like?", "start": 2682.39, "duration": 4.215}, {"text": "What does class 1 look like with two Gaussian bumps like this?", "start": 2686.605, "duration": 3.63}, {"text": "Then because the dataset is split 50-50 P of Y equals 1 is 0.5.", "start": 2690.235, "duration": 4.995}, {"text": "Right, so one half prior.", "start": 2695.23, "duration": 2.055}, {"text": "Okay. Now, let's go through that exercise I described on the left of trying to", "start": 2697.285, "duration": 5.925}, {"text": "plot P of Y equals 1 given X for different values of X.", "start": 2703.21, "duration": 6.39}, {"text": "So the vertical axis here as P of Y equals 1 given different values of X.", "start": 2709.6, "duration": 5.22}, {"text": "So, um, let's pick a point far to the left here.", "start": 2714.82, "duration": 4.95}, {"text": "Right. With this model you- if you actually", "start": 2719.77, "duration": 3.6}, {"text": "calculate this ratio you find that if you have a point here,", "start": 2723.37, "duration": 4.59}, {"text": "it almost certainly came from this Gaussian on the left.", "start": 2727.96, "duration": 4.185}, {"text": "If- if you have an unlabeled example here,", "start": 2732.145, "duration": 2.64}, {"text": "you're almost certain it came from the class 0 Gaussian", "start": 2734.785, "duration": 3.675}, {"text": "because the chance of", "start": 2738.46, "duration": 1.38}, {"text": "this Gaussian generating example all the way to left is almost 0.", "start": 2739.84, "duration": 3.69}, {"text": "Right, and so chance of P- P of Y equals 1 given X is very small.", "start": 2743.53, "duration": 3.96}, {"text": "So for a point-like that,", "start": 2747.49, "duration": 1.38}, {"text": "you end up with a point you know,", "start": 2748.87, "duration": 1.725}, {"text": "very close to 0, right.", "start": 2750.595, "duration": 2.835}, {"text": "Um, let's pick another point.", "start": 2753.43, "duration": 1.77}, {"text": "Right, how about this point, the midpoint.", "start": 2755.2, "duration": 2.415}, {"text": "Well, if you're getting example right at the midpoint,", "start": 2757.615, "duration": 1.98}, {"text": "you- you really have no idea. You really can't tell.", "start": 2759.595, "duration": 1.98}, {"text": "Did this come from the negative or the positive Gaussian?", "start": 2761.575, "duration": 2.295}, {"text": "Can't tell. Right. So this is really 50-50.", "start": 2763.87, "duration": 3.18}, {"text": "So I guess if this is 0.5 for that midpoint you", "start": 2767.05, "duration": 4.14}, {"text": "would have P of Y equals 1 given X is 0.5.", "start": 2771.19, "duration": 5.025}, {"text": "Um, then if you go to a point away to the variance,", "start": 2776.215, "duration": 2.655}, {"text": "if you get an example way here,", "start": 2778.87, "duration": 1.335}, {"text": "then you'd be pretty sure this came from the positive examples and so,", "start": 2780.205, "duration": 3.66}, {"text": "you know, you get a point like that.", "start": 2783.865, "duration": 3.165}, {"text": "Right. Now, it turns out that if you repeat this exercise", "start": 2787.03, "duration": 4.8}, {"text": "sweeping from left to right for many many points on the X axis you find that,", "start": 2791.83, "duration": 5.505}, {"text": "for points far to the left,", "start": 2797.335, "duration": 1.89}, {"text": "the chance of this coming from, um,", "start": 2799.225, "duration": 3.87}, {"text": "the Y equals 1 class is very small and as you approach this midpoint,", "start": 2803.095, "duration": 5.01}, {"text": "it increases to 0.5 and it surpasses 0.5.", "start": 2808.105, "duration": 3.975}, {"text": "And then beyond a certain point,", "start": 2812.08, "duration": 2.37}, {"text": "it becomes very very close to 1.", "start": 2814.45, "duration": 3.105}, {"text": "Right, and you do this exercise and actually just for every point, you know,", "start": 2817.555, "duration": 3.795}, {"text": "for a dense grid on the x-axis evaluate", "start": 2821.35, "duration": 3.15}, {"text": "this formula which will give you a number between 0 and 1.", "start": 2824.5, "duration": 3.645}, {"text": "Is the probability and go ahead and plot,", "start": 2828.145, "duration": 2.25}, {"text": "you know, the values you get a curve like this.", "start": 2830.395, "duration": 2.895}, {"text": "It turns out that if you connect up the dots,", "start": 2833.29, "duration": 2.94}, {"text": "um, then this is exactly a sigmoid function.", "start": 2836.23, "duration": 4.26}, {"text": "The shape of that turns out to be", "start": 2840.49, "duration": 1.77}, {"text": "exactly a shaped sigmoid function and you prove this in the problem sets as well.", "start": 2842.26, "duration": 4.8}, {"text": "Right. Um, so, um,", "start": 2847.06, "duration": 11.1}, {"text": "both logistic regression and Gaussian discriminant analysis actually end up using", "start": 2858.16, "duration": 5.565}, {"text": "a sigmoid function to calculate P of Y equals 1 given X or- or the,", "start": 2863.725, "duration": 7.17}, {"text": "the outcome ends up being a sigmoid function.", "start": 2870.895, "duration": 2.25}, {"text": "I guess the mechanics is,", "start": 2873.145, "duration": 1.125}, {"text": "you actually use this calculation rather than compute a sigmoid function.", "start": 2874.27, "duration": 3.21}, {"text": "Right. But, um, the specific choice of the parameters they end up", "start": 2877.48, "duration": 5.13}, {"text": "choosing are quite different and you saw when I was", "start": 2882.61, "duration": 2.82}, {"text": "projecting the results on the display just now in PowerPoint,", "start": 2885.43, "duration": 3.24}, {"text": "that the two algorithms actually come up with two different decision boundaries.", "start": 2888.67, "duration": 4.86}, {"text": "Right. So, um, let's discuss when", "start": 2893.53, "duration": 4.545}, {"text": "a genitive algorithm like GDA is superior and when", "start": 2898.075, "duration": 3.615}, {"text": "a distributed algorithm like logistic regression is superior.", "start": 2901.69, "duration": 3.6}, {"text": "Um, let's see if I can get rid of this.", "start": 2905.29, "duration": 7.59}, {"text": "[BACKGROUND]", "start": 2912.88, "duration": 15.29}, {"text": "All right. So GDA, Gaussian Distributed Analysis.", "start": 2928.17, "duration": 5.79}, {"text": "So the generative approach.", "start": 2933.96, "duration": 3.07}, {"text": "This assumes that x given y equals 0,", "start": 2937.46, "duration": 5.44}, {"text": "this is Gaussian, with mean Mu_0 and co variance Sigma.", "start": 2942.9, "duration": 3.975}, {"text": "It assumes x given y equals 1,", "start": 2946.875, "duration": 2.295}, {"text": "this is Gaussian with mean Mu_1 and covariance Sigma,", "start": 2949.17, "duration": 3.465}, {"text": "and y is Bernoulli with, um, parenthesis Phi.", "start": 2952.635, "duration": 8.13}, {"text": "Right. And what logistic regression does.", "start": 2960.765, "duration": 3.135}, {"text": "[NOISE] This is a discriminative algorithm,", "start": 2963.9, "duration": 6.48}, {"text": "uh, there is some [LAUGHTER] strange wind at the back, is it?", "start": 2970.38, "duration": 9.165}, {"text": "Yeah.", "start": 2979.545, "duration": 0.715}, {"text": "I see.", "start": 2980.26, "duration": 0.5}, {"text": "Okay. Cool. All right.", "start": 2980.76, "duration": 1.665}, {"text": "Yeah. Why? You know", "start": 2982.425, "duration": 3.705}, {"text": "the-there's just a scary UN report on global [LAUGHTER] warming over the weekend.", "start": 2986.13, "duration": 3.51}, {"text": "I hope we don't already have storms here, um.", "start": 2989.64, "duration": 2.835}, {"text": "Okay. It's okay. Did you guys see the UN report?", "start": 2992.475, "duration": 3.915}, {"text": "It's slightly scary actually wa- the-", "start": 2996.39, "duration": 2.16}, {"text": "the UN report on global warming but hopefully- all right.", "start": 2998.55, "duration": 2.895}, {"text": "Good. Hurricane stopped.", "start": 3001.445, "duration": 2.145}, {"text": "[LAUGHTER] Um, let's see.", "start": 3003.59, "duration": 6.36}, {"text": "Uh, so what logistic regression assumes is p of y equals 1 given x.", "start": 3009.95, "duration": 10.155}, {"text": "You know, that this is, uh, governed by logistic function.", "start": 3020.105, "duration": 2.88}, {"text": "Right. So this is really 1 over 1 plus e is a negative Theta transpose x.", "start": 3022.985, "duration": 3.735}, {"text": "We-where some details about x_0 equals 1 and so on.", "start": 3026.72, "duration": 5.97}, {"text": "Right. So just- just- okay.", "start": 3032.69, "duration": 2.775}, {"text": "So- so in other words,", "start": 3035.465, "duration": 1.215}, {"text": "uh, it's assumed that this is,", "start": 3036.68, "duration": 1.53}, {"text": "um, p of y equals 1 given x is logistic.", "start": 3038.21, "duration": 6.64}, {"text": "Okay. And the argument that I just described just now, uh,", "start": 3045.31, "duration": 5.32}, {"text": "plotting you know p of y equals 1 given x", "start": 3050.63, "duration": 3.0}, {"text": "point-by-point to really the sigmoid curve I drew on the other board.", "start": 3053.63, "duration": 4.03}, {"text": "What that illustrates.", "start": 3057.66, "duration": 1.9}, {"text": "Um, it doesn't prove it.", "start": 3059.56, "duration": 1.62}, {"text": "You prove it yourself in a homework problem.", "start": 3061.18, "duration": 1.8}, {"text": "But what that illustrates is that,", "start": 3062.98, "duration": 1.71}, {"text": "this set of assumptions", "start": 3064.69, "duration": 2.65}, {"text": "implies that p of y equals 1 given x is governed by a logistic function.", "start": 3067.41, "duration": 6.655}, {"text": "Right. But it turns out that the implication in the opposite direction is not true.", "start": 3074.065, "duration": 7.2}, {"text": "Right. So if you assume p of y equals 1", "start": 3081.265, "duration": 3.835}, {"text": "given x is governed by logistic function by- by this shape,", "start": 3085.1, "duration": 3.525}, {"text": "this does not in any way shape or form assume that x given y is Gaussian,", "start": 3088.625, "duration": 5.07}, {"text": "uh, uh, x given y equals 0 is Gaussian x given y equals 1 is Gaussian.", "start": 3093.695, "duration": 4.215}, {"text": "Right. So what this means is that GDA,", "start": 3097.91, "duration": 5.295}, {"text": "the generative learning algorithm in this case,", "start": 3103.205, "duration": 2.4}, {"text": "this makes a stronger set of assumptions and which this regression makes", "start": 3105.605, "duration": 9.405}, {"text": "a weaker set of", "start": 3115.01, "duration": 5.73}, {"text": "assumptions because you can prove these assumptions from these assumptions.", "start": 3120.74, "duration": 4.725}, {"text": "Okay. Um, and by the way as- as- uh,", "start": 3125.465, "duration": 6.405}, {"text": "as- as- uh, let's see.", "start": 3131.87, "duration": 3.435}, {"text": "And so what you see in a lot of learning algorithms is that, um,", "start": 3135.305, "duration": 4.83}, {"text": "if you make strongly modeling assumptions", "start": 3140.135, "duration": 2.505}, {"text": "and if your modeling assumptions are roughly correct,", "start": 3142.64, "duration": 2.565}, {"text": "then your model will do better because you're telling more information to the algorithm.", "start": 3145.205, "duration": 5.205}, {"text": "So if indeed x given y is Gaussian,", "start": 3150.41, "duration": 4.245}, {"text": "then GDA will do better because you're telling", "start": 3154.655, "duration": 3.345}, {"text": "the algorithm x given y is Gaussian and so it can be more efficient.", "start": 3158.0, "duration": 3.63}, {"text": "And so even if a very small dataset, um,", "start": 3161.63, "duration": 3.21}, {"text": "if these assumptions are roughly correct,", "start": 3164.84, "duration": 1.755}, {"text": "then GA will do better.", "start": 3166.595, "duration": 1.95}, {"text": "And the problem with GDA is,", "start": 3168.545, "duration": 2.265}, {"text": "if these assumptions turn out to be wrong.", "start": 3170.81, "duration": 2.13}, {"text": "So if x given y is not at all Gaussian,", "start": 3172.94, "duration": 2.505}, {"text": "then this might be a very bad set of assumptions to make.", "start": 3175.445, "duration": 2.415}, {"text": "You might be trying to fit a Gaussian density to data that is not", "start": 3177.86, "duration": 3.15}, {"text": "at all Gaussian and then GDA would do more poorly.", "start": 3181.01, "duration": 3.855}, {"text": "Okay. So here's one fun fact.", "start": 3184.865, "duration": 4.095}, {"text": "Here's another example, get to your question in a second,", "start": 3188.96, "duration": 2.265}, {"text": "which is let's say the following are true;", "start": 3191.225, "duration": 3.75}, {"text": "let's say that x given y equals 1 is Poisson with, uh,", "start": 3194.975, "duration": 7.095}, {"text": "parameter Lambda_1 and x given y equals 0 is Poisson with mean,", "start": 3202.07, "duration": 9.42}, {"text": "uh, Lambda_0, or lambda_1 not 0 and y,", "start": 3211.49, "duration": 4.19}, {"text": "as before, is Bernoulli 5x.", "start": 3215.68, "duration": 3.405}, {"text": "Right. It turns out that this set of assumptions also imply that p of y equals 1 given x.", "start": 3219.085, "duration": 8.515}, {"text": "This is logistic, okay, and you can prove this.", "start": 3227.6, "duration": 5.37}, {"text": "And this is actually true for, um,", "start": 3232.97, "duration": 1.635}, {"text": "any generalized linear model, actually where, uh,", "start": 3234.605, "duration": 3.165}, {"text": "where- where, uh, the difference between", "start": 3237.77, "duration": 2.16}, {"text": "these two distributions varies only", "start": 3239.93, "duration": 2.1}, {"text": "according to the natural parameter as a generalized name.", "start": 3242.03, "duration": 2.535}, {"text": "Excuse me, of the exponential family distribution.", "start": 3244.565, "duration": 2.265}, {"text": "Right. And so what this means is that, um,", "start": 3246.83, "duration": 4.955}, {"text": "if you don't know if your data is Gaussian or Poisson,", "start": 3251.785, "duration": 3.195}, {"text": "um, if you're using logistic regression you don't need to worry about it.", "start": 3254.98, "duration": 3.885}, {"text": "It'll work fine either way.", "start": 3258.865, "duration": 1.475}, {"text": "Right. So- so, you know, maybe, um,", "start": 3260.34, "duration": 2.3}, {"text": "you are fitting data to s- maybe a fitting, uh,", "start": 3262.64, "duration": 2.73}, {"text": "uh, a model, binary classification model to some data.", "start": 3265.37, "duration": 3.06}, {"text": "And you don't know, is a data Gaussian?", "start": 3268.43, "duration": 2.145}, {"text": "Is it Poisson? Is this some other exponential family model? Maybe you just don't know.", "start": 3270.575, "duration": 4.11}, {"text": "But if you're fitting logistic regression,", "start": 3274.685, "duration": 1.875}, {"text": "it- it'll do fine under all of those scenarios.", "start": 3276.56, "duration": 2.835}, {"text": "Right. But if your data was actually Poisson but you assumed it was Gaussian,", "start": 3279.395, "duration": 4.724}, {"text": "then your model might do quite poorly.", "start": 3284.119, "duration": 2.791}, {"text": "Okay. So the key high level principles when you take away from this is, um, uh, uh,", "start": 3286.91, "duration": 7.27}, {"text": "if you make weaker assumptions as in logistic regression,", "start": 3295.72, "duration": 4.63}, {"text": "then your algorithm will be more robust to modeling", "start": 3300.35, "duration": 2.85}, {"text": "assumptions such as accidentally assuming the data is Gaussian and it is not.", "start": 3303.2, "duration": 3.36}, {"text": "Uh, but on the flip side,", "start": 3306.56, "duration": 1.755}, {"text": "if you have a very small dataset, then, um,", "start": 3308.315, "duration": 3.06}, {"text": "using a model that makes more assumptions will actually allow you to do better", "start": 3311.375, "duration": 5.04}, {"text": "because by making more assumptions you're just", "start": 3316.415, "duration": 2.55}, {"text": "telling the algorithm more truth about the world which is,", "start": 3318.965, "duration": 2.67}, {"text": "you know, \"Hey, algorithm, the world is Gaussian,\" and if it is Gaussian,", "start": 3321.635, "duration": 3.285}, {"text": "then it will actually do- do- do better.", "start": 3324.92, "duration": 2.25}, {"text": "Okay. Your question at the back or a few questions. Go ahead.", "start": 3327.17, "duration": 2.97}, {"text": "Just from that, is there a point do you know like what sort", "start": 3330.14, "duration": 5.22}, {"text": "of data it usually has a Gaussian problem?", "start": 3335.36, "duration": 5.24}, {"text": "Oh, oh, yeah.", "start": 3340.6, "duration": 1.2}, {"text": "Practical sample without data is a Gaussian probably,", "start": 3341.8, "duration": 2.42}, {"text": "you know, it's, uh,", "start": 3344.22, "duration": 1.34}, {"text": "uh- yeah, you know,", "start": 3345.56, "duration": 1.995}, {"text": "it's a matter of degree.", "start": 3347.555, "duration": 1.155}, {"text": "Right. Most data on this universe is Gaussian [LAUGHTER] uh,", "start": 3348.71, "duration": 3.81}, {"text": "uh, uh, except at this feed data, I guess.", "start": 3352.52, "duration": 3.015}, {"text": "Yeah, but- but, um- I think it's actually a- a matter of degree.", "start": 3355.535, "duration": 3.63}, {"text": "Right. If- if you plot- actually if you take", "start": 3359.165, "duration": 2.055}, {"text": "continuous value data- no, ther- ther- there are exceptions.", "start": 3361.22, "duration": 3.21}, {"text": "You could plot it and most data that you plot, you know,", "start": 3364.43, "duration": 2.565}, {"text": "will not really be Gaussian but a lot of", "start": 3366.995, "duration": 2.265}, {"text": "it you can convince yourself is vaguely Gaussian.", "start": 3369.26, "duration": 2.34}, {"text": "So I think a lot of it is amount of degree.", "start": 3371.6, "duration": 1.515}, {"text": "I- I- I actually tell you the way I choose to use,", "start": 3373.115, "duration": 2.325}, {"text": "um, these two algorithms.", "start": 3375.44, "duration": 1.83}, {"text": "So I think that the whole world has moved toward using bigger than three datasets.", "start": 3377.27, "duration": 4.08}, {"text": "Right. Digital Civil Society which is a lot of data", "start": 3381.35, "duration": 2.385}, {"text": "and so for a lot of problems we have a lot of data,", "start": 3383.735, "duration": 3.045}, {"text": "I would probably use logistic regression.", "start": 3386.78, "duration": 2.4}, {"text": "Because with more data,", "start": 3389.18, "duration": 1.89}, {"text": "you could overcome telling the algorithm less about the world.", "start": 3391.07, "duration": 3.66}, {"text": "Right. So- so the algorithm has two sources of knowledge.", "start": 3394.73, "duration": 3.12}, {"text": "Uh, one source of knowledge is what did you tell it,", "start": 3397.85, "duration": 2.775}, {"text": "what are the assumptions you told it to make?", "start": 3400.625, "duration": 1.725}, {"text": "And the second source of knowledge is learned from", "start": 3402.35, "duration": 2.28}, {"text": "the data and in this era of big data,", "start": 3404.63, "duration": 2.58}, {"text": "we have a lot of data, you know,", "start": 3407.21, "duration": 1.89}, {"text": "there is a strong trend to use logistic regression which makes", "start": 3409.1, "duration": 2.97}, {"text": "less assumptions and just lets the algorithm", "start": 3412.07, "duration": 1.92}, {"text": "figure out whether it wants to figure out from the data.", "start": 3413.99, "duration": 2.265}, {"text": "Right. Now, one practical reason why I still use algorithms like the GDA,", "start": 3416.255, "duration": 4.905}, {"text": "general discriminant analysis, so algorithms like this,", "start": 3421.16, "duration": 2.295}, {"text": "um, uh, is that,", "start": 3423.455, "duration": 2.235}, {"text": "it's actually quite computationally efficient and so", "start": 3425.69, "duration": 2.19}, {"text": "the- there's actually one use case at Landing. AI that we're working", "start": 3427.88, "duration": 2.46}, {"text": "on where we just need to fit a ton of models and", "start": 3430.34, "duration": 2.49}, {"text": "don't have the patience to run the GC progression over and over.", "start": 3432.83, "duration": 2.925}, {"text": "And it turns out computing mean and variances of, um,", "start": 3435.755, "duration": 3.675}, {"text": "covariance matrices is very efficient and so", "start": 3439.43, "duration": 2.79}, {"text": "there's actually apart from the assumptions type of benefit,", "start": 3442.22, "duration": 3.54}, {"text": "uh, which is a general philosophical point.", "start": 3445.76, "duration": 1.905}, {"text": "We'll see again later in this course.", "start": 3447.665, "duration": 1.515}, {"text": "Right. Th- this idea about do you make strong or weak assumptions?", "start": 3449.18, "duration": 2.85}, {"text": "This is a general principle in machine learning that we'll see again in other places.", "start": 3452.03, "duration": 3.78}, {"text": "But the very concrete- the other reason I tend to use GDA these days is", "start": 3455.81, "duration": 3.915}, {"text": "less that I think I perform better from", "start": 3459.725, "duration": 1.965}, {"text": "an accuracy point of view but there's actually a very efficient algorithm.", "start": 3461.69, "duration": 2.34}, {"text": "We just compute the mean covar- covariance and we are", "start": 3464.03, "duration": 2.07}, {"text": "done and there's no iterative process needed.", "start": 3466.1, "duration": 2.4}, {"text": "So these days when I use these models, um,", "start": 3468.5, "duration": 3.195}, {"text": "is more motivated by computation and less by performance.", "start": 3471.695, "duration": 4.755}, {"text": "But this general principle is one that we'll come back to again later", "start": 3476.45, "duration": 3.27}, {"text": "when we develop more sophisticated learning algorithms. Yeah.", "start": 3479.72, "duration": 3.09}, {"text": "Uh, if the data is generated from a Gaussian but", "start": 3482.81, "duration": 4.5}, {"text": "my program synthesis are different with the assumption", "start": 3487.31, "duration": 2.7}, {"text": "that we just use the same program for performance-?", "start": 3490.01, "duration": 3.38}, {"text": "Oh, right, ah, so what happens when the co-variance matrices are different?", "start": 3493.39, "duration": 3.665}, {"text": "It turns out that, uh,", "start": 3497.055, "duration": 2.475}, {"text": "uh, trying to remember, it still ends up being a logistic function but with", "start": 3499.53, "duration": 1.95}, {"text": "a bunch of quadratic terms in the logistic function.", "start": 3501.48, "duration": 2.01}, {"text": "So it's not a linear decision boundary anymore.", "start": 3503.49, "duration": 1.755}, {"text": "You can end up with a decision boundary,", "start": 3505.245, "duration": 2.295}, {"text": "you know, that- that- that looks like this, right?", "start": 3507.54, "duration": 2.25}, {"text": "With positive and negative examples separated by", "start": 3509.79, "duration": 1.83}, {"text": "some- by some other shape from a linear decision boundary.", "start": 3511.62, "duration": 2.7}, {"text": "Uh, you- you could- you could- you could fig- actually,", "start": 3514.32, "duration": 2.43}, {"text": "I- if you're curious, I encourage you to, you know, uh, uh,", "start": 3516.75, "duration": 2.88}, {"text": "fire up Python NumPy and- and", "start": 3519.63, "duration": 2.445}, {"text": "play around their parameters and plot this for yourself, uh, questions?", "start": 3522.075, "duration": 3.075}, {"text": "Is it recommended that we use some kind of statistical global test to make", "start": 3525.15, "duration": 5.06}, {"text": "sure that the plot distribution results have a equal variance before we do GDA?", "start": 3530.21, "duration": 5.095}, {"text": "Yeah. It's recommended that you do some cyclical tests to see if it's Gaussian,", "start": 3535.305, "duration": 3.075}, {"text": "um, I can tell you what's done in practice.", "start": 3538.38, "duration": 3.735}, {"text": "I think in practice,", "start": 3542.115, "duration": 1.08}, {"text": "if you have enough data to do a cyclical test and gain conviction,", "start": 3543.195, "duration": 3.885}, {"text": "you probably have enough data to just use logistic regression,", "start": 3547.08, "duration": 3.45}, {"text": "um, uh, the- the- I- I don't know.", "start": 3550.53, "duration": 2.4}, {"text": "[LAUGHTER] Well no, that's not really fair. I don't know.", "start": 3552.93, "duration": 2.43}, {"text": "If they're very high dimensional data,", "start": 3555.36, "duration": 1.545}, {"text": "I- I think what often happens more,", "start": 3556.905, "duration": 1.8}, {"text": "is people just plot the data,", "start": 3558.705, "duration": 1.845}, {"text": "and if it looks clearly non-Gaussian,", "start": 3560.55, "duration": 2.25}, {"text": "then, you know, there will be reasons not to use GDA.", "start": 3562.8, "duration": 2.415}, {"text": "But what happens often is that um,", "start": 3565.215, "duration": 3.84}, {"text": "uh, uh, yeah sometimes you just have a very small training", "start": 3569.055, "duration": 3.105}, {"text": "set and it's just a matter of judgment, right?", "start": 3572.16, "duration": 2.49}, {"text": "Like if you have, you- if you have, uh, uh, uh,", "start": 3574.65, "duration": 2.085}, {"text": "you know, I don't know, 50 examples of healthcare records,", "start": 3576.735, "duration": 3.42}, {"text": "then you just have to ask some doctors and ask, \"Well,", "start": 3580.155, "duration": 3.165}, {"text": "do you think the distribution is rath- rath-", "start": 3583.32, "duration": 1.725}, {"text": "relatively Gaussian,\" and use domain knowledge like that.", "start": 3585.045, "duration": 2.835}, {"text": "Right? I think- by the way a- another philosophical point,", "start": 3587.88, "duration": 3.105}, {"text": "um, I think that,", "start": 3590.985, "duration": 1.89}, {"text": "uh, the machine learning world has,", "start": 3592.875, "duration": 2.625}, {"text": "frank- you know, a little bit overhyped big data, right?", "start": 3595.5, "duration": 3.27}, {"text": "And- and yes it's true that when we have more data,", "start": 3598.77, "duration": 2.25}, {"text": "it's great and I love data and a- um,", "start": 3601.02, "duration": 2.34}, {"text": "having more data pretty much never hurts and", "start": 3603.36, "duration": 1.995}, {"text": "usually the more data the better, so all that is true.", "start": 3605.355, "duration": 2.325}, {"text": "And I think we did a good job telling people that high-level message,", "start": 3607.68, "duration": 2.835}, {"text": "you know, more data almost always helps.", "start": 3610.515, "duration": 1.815}, {"text": "But, um, uh, I think a lot of the skill in machine learning", "start": 3612.33, "duration": 3.93}, {"text": "these days is getting your algorithms to work", "start": 3616.26, "duration": 2.175}, {"text": "even when you don't have a million in examples,", "start": 3618.435, "duration": 2.235}, {"text": "even you don't have a hundred million examples.", "start": 3620.67, "duration": 1.77}, {"text": "So there are lots of machine learning applications", "start": 3622.44, "duration": 1.815}, {"text": "where you just don't have a million examples, uh,", "start": 3624.255, "duration": 2.385}, {"text": "you have a hundred examples and, um,", "start": 3626.64, "duration": 2.985}, {"text": "it's then the skill in designing your learning algorithm matters much more.", "start": 3629.625, "duration": 3.375}, {"text": "Um, so if you take something like ImageNet,", "start": 3633.0, "duration": 2.415}, {"text": "mi- million in- in- images,", "start": 3635.415, "duration": 1.215}, {"text": "there are now dozens of teams,", "start": 3636.63, "duration": 1.44}, {"text": "maybe hundreds of teams, I don't know.", "start": 3638.07, "duration": 1.335}, {"text": "They can get great results.", "start": 3639.405, "duration": 1.335}, {"text": "They give a million examples, right?", "start": 3640.74, "duration": 1.86}, {"text": "and so the performance difference between teams, you know,", "start": 3642.6, "duration": 3.105}, {"text": "there are now dozens of teams that get great performance,", "start": 3645.705, "duration": 2.115}, {"text": "if a million examples, uh,", "start": 3647.82, "duration": 1.545}, {"text": "for- for- for image classification, for ImageNet.", "start": 3649.365, "duration": 2.28}, {"text": "But if you have only a hundred examples,", "start": 3651.645, "duration": 2.355}, {"text": "then the high-skilled teams will actually do much,", "start": 3654.0, "duration": 3.18}, {"text": "much, much, much better than the low skilled teams,", "start": 3657.18, "duration": 2.1}, {"text": "whereas the performance gap is smaller when you have giant data sets I think,", "start": 3659.28, "duration": 3.765}, {"text": "so and I think that it's these types of intuitions,", "start": 3663.045, "duration": 2.94}, {"text": "you know, what assumptions you use,", "start": 3665.985, "duration": 1.425}, {"text": "generative or discriminative, that actually", "start": 3667.41, "duration": 1.905}, {"text": "distinguishes the high-skilled teams and, uh, and, uh,", "start": 3669.315, "duration": 2.355}, {"text": "and the less experienced teams and drives a lot of", "start": 3671.67, "duration": 2.34}, {"text": "the performance differences when you have small data.", "start": 3674.01, "duration": 2.79}, {"text": "Oh, and if someone goes to you and says,", "start": 3676.8, "duration": 2.775}, {"text": "\"Oh you only have a hundred examples,", "start": 3679.575, "duration": 1.635}, {"text": "you'll never do anything.\"", "start": 3681.21, "duration": 1.215}, {"text": "Uh, then I don't know,", "start": 3682.425, "duration": 1.095}, {"text": "if- if there's a competitor saying that,", "start": 3683.52, "duration": 1.755}, {"text": "then I'll say, \"Great, you know,", "start": 3685.275, "duration": 1.305}, {"text": "don't do it because I can make it work.\"", "start": 3686.58, "duration": 1.8}, {"text": "Uh, well, I don't know.", "start": 3688.38, "duration": 1.41}, {"text": "Uh, but- but I think there are a lot of applications where", "start": 3689.79, "duration": 2.235}, {"text": "your skill at designing a machine learning system,", "start": 3692.025, "duration": 2.04}, {"text": "really makes a bigger difference when you have a- make- makes", "start": 3694.065, "duration": 2.415}, {"text": "a- it makes a difference from big data and small data,", "start": 3696.48, "duration": 2.58}, {"text": "but it just- this is a very clear where you don't have much data,", "start": 3699.06, "duration": 2.91}, {"text": "is the assumptions you code into the algorithm like,", "start": 3701.97, "duration": 2.37}, {"text": "is it Gaussian, is it Poisson?", "start": 3704.34, "duration": 1.26}, {"text": "That- that skill allows you to drive", "start": 3705.6, "duration": 2.715}, {"text": "much bigger performance than a lower-skill team would be able to.", "start": 3708.315, "duration": 4.395}, {"text": "All right. This is- uh- uh-", "start": 3712.71, "duration": 2.295}, {"text": "coul- could- I should still take questions from all of you. Yeah, go ahead.", "start": 3715.005, "duration": 2.995}, {"text": "Um, what's the implication when [inaudible].", "start": 3723.68, "duration": 3.67}, {"text": "Oh, sure. So does this, uh,", "start": 3727.35, "duration": 1.32}, {"text": "yes, so what's the general statement of this?", "start": 3728.67, "duration": 1.86}, {"text": "Yes, so if, uh,", "start": 3730.53, "duration": 1.56}, {"text": "x given y equals 1,", "start": 3732.09, "duration": 1.695}, {"text": "uh, it comes from an exponential family distribution,", "start": 3733.785, "duration": 2.31}, {"text": "x given y equals 0 comes to an exponential family distribution,", "start": 3736.095, "duration": 2.925}, {"text": "it's the same exponential family distribution and if they", "start": 3739.02, "duration": 2.37}, {"text": "vary only by the natural parameter of the exponential family distribution,", "start": 3741.39, "duration": 4.095}, {"text": "then this will be logistic.", "start": 3745.485, "duration": 1.695}, {"text": "Yeah. Um, I think this was once a midterm homework problem to prove this actually?", "start": 3747.18, "duration": 5.085}, {"text": "But, yeah. All right,", "start": 3752.265, "duration": 2.055}, {"text": "uh, actually let's take one last question then we move on, go ahead.", "start": 3754.32, "duration": 3.7}, {"text": "Uh, if performance [inaudible]", "start": 3762.29, "duration": 2.77}, {"text": "Oh, uh, does performance", "start": 3765.06, "duration": 1.755}, {"text": "improvement happen even as you increase the number of classes?", "start": 3766.815, "duration": 2.985}, {"text": "Uh, ye- I think so yes,", "start": 3769.8, "duration": 5.34}, {"text": "uh, and the generalization of this would be", "start": 3775.14, "duration": 1.5}, {"text": "the Softmax Regression which I didn't talk about. But yes.", "start": 3776.64, "duration": 2.76}, {"text": "I think it's a similar thing holds true for, um,", "start": 3779.4, "duration": 2.94}, {"text": "GDA for multiple- and we have so far we're going to talk about Binary Classification,", "start": 3782.34, "duration": 3.24}, {"text": "whether you have more than two classes.", "start": 3785.58, "duration": 1.5}, {"text": "But, uh, but yes, similar- similar things holds true for,", "start": 3787.08, "duration": 3.51}, {"text": "uh, like a GDA with three classes and Softmax.", "start": 3790.59, "duration": 3.81}, {"text": "Yeah. Oh yes, right. You saw Softmax the other day.", "start": 3794.4, "duration": 3.285}, {"text": "Cool. Um, and this- this theme", "start": 3797.685, "duration": 5.685}, {"text": "that when you have less data the algorithm needs to rely more on assumptions you code in.", "start": 3803.37, "duration": 5.37}, {"text": "This is a recurring theme that we'll come back to it as well.", "start": 3808.74, "duration": 2.745}, {"text": "This is one of the important principles of machine learning,", "start": 3811.485, "duration": 2.655}, {"text": "that when you have less data your skill at coding and your knowledge matters much more.", "start": 3814.14, "duration": 5.265}, {"text": "Uh, this is a theme we'll come back to you when we", "start": 3819.405, "duration": 2.175}, {"text": "talk about much more complicated learning algorithms", "start": 3821.58, "duration": 2.13}, {"text": "as well. All right.", "start": 3823.71, "duration": 5.25}, {"text": "So, uh, I want a fresh board for this.", "start": 3828.96, "duration": 4.6}, {"text": "So you've seen GDA in the context of, um,", "start": 3839.39, "duration": 4.36}, {"text": "continuous valued, uh, features x.", "start": 3843.75, "duration": 5.025}, {"text": "The last thing I want to do today, um,", "start": 3848.775, "duration": 3.825}, {"text": "is talk about one more generative learning algorithm called Naive Bayes,", "start": 3852.6, "duration": 6.165}, {"text": "um, and I'm gonna use as most of the example;", "start": 3858.765, "duration": 4.245}, {"text": "e-mail spam classification, but this- this is- this-", "start": 3863.01, "duration": 3.0}, {"text": "I guess this is our first foray into natural language processing, right?", "start": 3866.01, "duration": 3.03}, {"text": "But given in a piece of text, like given a piece of email,", "start": 3869.04, "duration": 2.145}, {"text": "can you classify this as spam or not spam?", "start": 3871.185, "duration": 2.58}, {"text": "Or, uh, other examples, uh, uh, actually several years ago, Ebay,", "start": 3873.765, "duration": 3.975}, {"text": "used to have a problem of, you know,", "start": 3877.74, "duration": 1.32}, {"text": "the- if someone's trying to sell something and you write a text description, right?", "start": 3879.06, "duration": 3.51}, {"text": "\"Hey, I have a secondhand, you know,", "start": 3882.57, "duration": 2.04}, {"text": "Roomba, I'm trying to sell it on Ebay.\"", "start": 3884.61, "duration": 2.115}, {"text": "How do you take that text that someone wrote over the description and", "start": 3886.725, "duration": 2.685}, {"text": "categorize it, is it an electronic thing or are they trying to sell a TV?", "start": 3889.41, "duration": 2.76}, {"text": "Are they trying to sell clothing?", "start": 3892.17, "duration": 1.05}, {"text": "Uh, but these- these examples of text classification problems,", "start": 3893.22, "duration": 3.12}, {"text": "we have a piece of text and you want to classify into one of", "start": 3896.34, "duration": 3.615}, {"text": "two categories for spam or not spam or one of maybe thousands of categories,", "start": 3899.955, "duration": 4.095}, {"text": "and they're trying to take a product description and classify it into one of the classes.", "start": 3904.05, "duration": 4.02}, {"text": "Um, and so the first question we will have is, um, uh,", "start": 3908.07, "duration": 6.6}, {"text": "given the e-mail problem, uh,", "start": 3914.67, "duration": 2.43}, {"text": "given the e-mail classification problem,", "start": 3917.1, "duration": 1.89}, {"text": "how do you represent it as a feature vector?", "start": 3918.99, "duration": 4.57}, {"text": "And so, um, in Naive Bayes what we're going to do is take your e-mail,", "start": 3927.41, "duration": 7.135}, {"text": "take a piece of e-mail and first map it to a feature vector X.", "start": 3934.545, "duration": 5.82}, {"text": "And we'll do so as follows,", "start": 3940.365, "duration": 1.635}, {"text": "which is first, um, let's start with a- let's start with", "start": 3942.0, "duration": 5.01}, {"text": "the English dictionary and make a list of all the words in the English dictionary, right?", "start": 3947.01, "duration": 5.19}, {"text": "So first of all there's the English dictionary as A,", "start": 3952.2, "duration": 2.16}, {"text": "second word in the English edition is aardvark.", "start": 3954.36, "duration": 2.685}, {"text": "Third word is aardwolf.", "start": 3957.045, "duration": 2.175}, {"text": "[BACKGROUND]", "start": 3959.22, "duration": 2.52}, {"text": "No, it's easy, look it up.", "start": 3961.74, "duration": 0.93}, {"text": "[NOISE] [LAUGHTER]", "start": 3962.67, "duration": 2.22}, {"text": "Um, and then, you know, uh, uh,", "start": 3964.89, "duration": 2.07}, {"text": "e- e-mail spam lot of people asking to buy stuff so that they would buy, right?", "start": 3966.96, "duration": 4.605}, {"text": "And then, um, uh,", "start": 3971.565, "duration": 1.755}, {"text": "and then the last word in my dictionary is zymurgy,", "start": 3973.32, "duration": 3.344}, {"text": "which is the technological chemistry that refers to the fermentation process in brewing.", "start": 3976.664, "duration": 6.706}, {"text": "Um, So- so- I think it is a useful way to think about it, in- in- in- practice,", "start": 3983.37, "duration": 3.99}, {"text": "what you do is not, uh, uh,", "start": 3987.36, "duration": 1.59}, {"text": "actually look at the dictionary but look at the top 10,000 words,", "start": 3988.95, "duration": 2.94}, {"text": "you know, in your training set.", "start": 3991.89, "duration": 1.14}, {"text": "Right? So maybe you have 10,000,", "start": 3993.03, "duration": 1.89}, {"text": "it's easier to think about it as if it was a dictionary but, you know,", "start": 3994.92, "duration": 3.09}, {"text": "in practice, well, you- the other thing that's- dictionary has too many words,", "start": 3998.01, "duration": 3.27}, {"text": "but where- the other way to do this is to look through your own e-mail", "start": 4001.28, "duration": 3.27}, {"text": "co-pairs and just find the top 10,000 occurring words and use that as a feature set,", "start": 4004.55, "duration": 4.56}, {"text": "and so I don't know.", "start": 4009.11, "duration": 1.48}, {"text": "Right? And your e-mails, I guess you're getting a bunch of", "start": 4010.59, "duration": 2.035}, {"text": "e-mail about- from us or maybe others about CS229.", "start": 4012.625, "duration": 2.715}, {"text": "So CS229 might appear in", "start": 4015.34, "duration": 1.38}, {"text": "your dictionary of building your e-mail spam filter for yourself,", "start": 4016.72, "duration": 2.415}, {"text": "even if it doesn't appear in the- in the official, uh,", "start": 4019.135, "duration": 3.105}, {"text": "was it like the Oxford dictionary,", "start": 4022.24, "duration": 1.97}, {"text": "just yet just- just- just- you wait,", "start": 4024.21, "duration": 1.715}, {"text": "we'll- we- we'll get CS229 there someday.", "start": 4025.925, "duration": 2.32}, {"text": "All right. Um, and so given an e-mail,", "start": 4028.245, "duration": 6.395}, {"text": "what we would like to do is then, um,", "start": 4034.64, "duration": 2.805}, {"text": "take this piece of text and represent it as a feature vector.", "start": 4037.445, "duration": 3.015}, {"text": "And so one way to do this is,", "start": 4040.46, "duration": 2.34}, {"text": "um, you can create a binary feature vector,", "start": 4042.8, "duration": 2.685}, {"text": "that puts a 1,", "start": 4045.485, "duration": 2.19}, {"text": "if a word appears in the e-mail and puts a 0 if it doesn't.", "start": 4047.675, "duration": 4.08}, {"text": "Right? So if you've gotten an e-mail, um, uh,", "start": 4051.755, "duration": 2.4}, {"text": "that asks you to, you know,", "start": 4054.155, "duration": 1.68}, {"text": "buy some stuff and then the word A appears in e-mail, you put a 1 there.", "start": 4055.835, "duration": 3.645}, {"text": "Did not try to sell aardvark or aardwolf,", "start": 4059.48, "duration": 2.13}, {"text": "so 0 there, buy and so on.", "start": 4061.61, "duration": 4.995}, {"text": "Right? So you take a- take an e-mail and turn it into a binary feature vector.", "start": 4066.605, "duration": 7.065}, {"text": "Um, and so here the feature vector is 0, 1 to the n,", "start": 4073.67, "duration": 7.59}, {"text": "because there's a n-dimensional binary feature vector,", "start": 4081.26, "duration": 3.345}, {"text": "where- where for the purpose of illustration, let's say,", "start": 4084.605, "duration": 2.7}, {"text": "n is 10,000 because you're using,", "start": 4087.305, "duration": 2.01}, {"text": "you know, take the top 10,000 words, uh,", "start": 4089.315, "duration": 1.935}, {"text": "that appear in your e-mail training set as the dictionary that you will use.", "start": 4091.25, "duration": 5.67}, {"text": "So, um.", "start": 4096.92, "duration": 12.84}, {"text": "So in other words, X_i is", "start": 4109.76, "duration": 4.515}, {"text": "indicator word i appears in the e-mail, right?", "start": 4114.275, "duration": 8.34}, {"text": "So it's either 0 or 1 depending on whether or not that word", "start": 4122.615, "duration": 2.58}, {"text": "i from this list appears in your e-mail.", "start": 4125.195, "duration": 4.515}, {"text": "Now, um, in the Naive Bayes algorithm,", "start": 4129.71, "duration": 4.98}, {"text": "we're going to build a generative learning algorithm.", "start": 4134.69, "duration": 3.555}, {"text": "Um, and so we want to", "start": 4138.245, "duration": 2.865}, {"text": "model P of x given y, right?", "start": 4141.11, "duration": 7.89}, {"text": "As well as P of y, okay?", "start": 4149.0, "duration": 2.985}, {"text": "But there are, uh,", "start": 4151.985, "duration": 3.135}, {"text": "2 to the 10,000 possible values of x, right?", "start": 4155.12, "duration": 9.885}, {"text": "Because x is a binary vector of this 10,000 dimensional.", "start": 4165.005, "duration": 3.57}, {"text": "So we try to model P of x in the straightforward way as a multinomial distribution over,", "start": 4168.575, "duration": 6.75}, {"text": "you know, 2 to the 10,000 possible outcomes.", "start": 4175.325, "duration": 3.39}, {"text": "Then you need, right, uh, uh, you need,", "start": 4178.715, "duration": 2.76}, {"text": "you know 2 to the 10,000 parameters, right?", "start": 4181.475, "duration": 3.54}, {"text": "Which is a lot, or technically,", "start": 4185.015, "duration": 2.415}, {"text": "you need 2 to 10,000 minus", "start": 4187.43, "duration": 2.01}, {"text": "1 parameter because that adds up to 1, and you can see one parameter.", "start": 4189.44, "duration": 3.12}, {"text": "But so, modeling this without additional assumptions won't- won't work,", "start": 4192.56, "duration": 4.11}, {"text": "right, because of the excessive number of parameters.", "start": 4196.67, "duration": 3.81}, {"text": "So in the Naive Bayes algorithm,", "start": 4200.48, "duration": 1.95}, {"text": "we're going to assume that X_i's are", "start": 4202.43, "duration": 6.04}, {"text": "conditionally independent given y, okay?", "start": 4208.87, "duration": 12.28}, {"text": "Uh, let me just write out what this means,", "start": 4221.15, "duration": 2.415}, {"text": "but so P of x_1 up to x_10,000 given y by the chain rule of probability,", "start": 4223.565, "duration": 9.36}, {"text": "this is equal to P of x_1 given y times P of x_2 given,", "start": 4232.925, "duration": 11.025}, {"text": "um, x_1 and y times p of x_3 given x_1,", "start": 4243.95, "duration": 6.38}, {"text": "x_2 Y up to your p of x_10,000 given, and so on, right?", "start": 4250.33, "duration": 7.44}, {"text": "So I haven't made any assumptions yet.", "start": 4257.77, "duration": 2.13}, {"text": "This is just a true statement of fact as always", "start": 4259.9, "duration": 1.95}, {"text": "true by the- by the chain rule of probability.", "start": 4261.85, "duration": 2.73}, {"text": "Um, and what we're going to assume which is what this assumption is,", "start": 4264.58, "duration": 5.47}, {"text": "is that this is equal to this first term no change the", "start": 4275.86, "duration": 5.11}, {"text": "x_2 given y p of x_3 given y and so on,", "start": 4280.97, "duration": 5.07}, {"text": "p of X_10,000 given y, okay?", "start": 4286.04, "duration": 5.4}, {"text": "So this assumption is called a", "start": 4291.44, "duration": 3.465}, {"text": "conditional independence assumption it's also sometimes called the Naive Bayes assumption.", "start": 4294.905, "duration": 4.515}, {"text": "But you're assuming that, um,", "start": 4299.42, "duration": 2.475}, {"text": "so long as you know why the chance of seeing the words,", "start": 4301.895, "duration": 4.32}, {"text": "um, aardvark in your e-mail does not", "start": 4306.215, "duration": 3.255}, {"text": "depend on whether the word \"A\" appears in your e-mail, right?", "start": 4309.47, "duration": 3.78}, {"text": "Um, and this is one of those assumptions that is definitely not", "start": 4313.25, "duration": 2.565}, {"text": "a true assumption and that is just not mathematically true assumption.", "start": 4315.815, "duration": 3.705}, {"text": "Just that sometimes your data isn't perfectly Gaussian,", "start": 4319.52, "duration": 2.37}, {"text": "but if it was Gaussian you can kind of get away with it.", "start": 4321.89, "duration": 2.625}, {"text": "So this assumption is not true,", "start": 4324.515, "duration": 2.625}, {"text": "um, in a mathematical sense,", "start": 4327.14, "duration": 1.62}, {"text": "but it may be not so horrible that you can't get away with it, right?", "start": 4328.76, "duration": 4.44}, {"text": "Um, and so- so- so it's like,", "start": 4333.2, "duration": 3.075}, {"text": "if you- if any of you are familiar with probabilistic graphical models,", "start": 4336.275, "duration": 2.775}, {"text": "if you've taken CS-228, uh,", "start": 4339.05, "duration": 2.489}, {"text": "this assumption is summarizing this picture,", "start": 4341.539, "duration": 2.701}, {"text": "and if you haven't taken CS-228 this picture won't make sense, but don't worry about it.", "start": 4344.24, "duration": 4.44}, {"text": "Um, right, that, uh,", "start": 4348.68, "duration": 4.26}, {"text": "once you know the class label is a spam or not spam whether", "start": 4352.94, "duration": 3.84}, {"text": "or not each word appears or does not appear is independent, okay?", "start": 4356.78, "duration": 3.795}, {"text": "So this is called conditional.", "start": 4360.575, "duration": 1.23}, {"text": "So the mechanics of this assumption is really just captured by this equation,", "start": 4361.805, "duration": 5.1}, {"text": "um, and you just use this equation,", "start": 4366.905, "duration": 2.325}, {"text": "that's all you need to derive Naive Bayes.", "start": 4369.23, "duration": 2.115}, {"text": "But intuition is that if I tell you whether", "start": 4371.345, "duration": 3.435}, {"text": "this piece- if I tell you that this piece of e-mail is spam then", "start": 4374.78, "duration": 3.525}, {"text": "whether the word by appears in it doesn't affect you believes that what- whether the word", "start": 4378.305, "duration": 3.645}, {"text": "mortgage or discount or whatever spammy words appear, right?", "start": 4381.95, "duration": 4.9}, {"text": "So just to summarize,", "start": 4387.73, "duration": 2.08}, {"text": "this is product from i equals 1 through n of p of X_i given y.", "start": 4389.81, "duration": 7.21}, {"text": "All right, so the parameters of this model,", "start": 4426.67, "duration": 5.275}, {"text": "um, are, I'm going to write it, Phi subscript,", "start": 4431.945, "duration": 5.805}, {"text": "um, j given y equals", "start": 4437.75, "duration": 5.58}, {"text": "1 as the probability that x_j equals 1 given y equals 1,", "start": 4443.33, "duration": 6.465}, {"text": "phi subscript j given y equals", "start": 4449.795, "duration": 2.715}, {"text": "0, and then Phi.", "start": 4452.51, "duration": 10.03}, {"text": "And just to distinguish all these Phi's from each other,", "start": 4464.89, "duration": 3.61}, {"text": "we can just call this Phi subscript y, okay?", "start": 4468.5, "duration": 3.51}, {"text": "So this parameter says,", "start": 4472.01, "duration": 2.295}, {"text": "if a spam e-mail,", "start": 4474.305, "duration": 1.635}, {"text": "if y equals 1 is spam and y equals 0 is not spam.", "start": 4475.94, "duration": 2.88}, {"text": "If it's a spam e-mail, what's the chance of word j appearing in the e-mail?", "start": 4478.82, "duration": 4.515}, {"text": "If it's not spam e-mail what's the chance of word j appearing in the e-mail.", "start": 4483.335, "duration": 3.99}, {"text": "Then also, what's the cost prior,", "start": 4487.325, "duration": 1.875}, {"text": "what's the prior probability that the next e-mail you receive in your,", "start": 4489.2, "duration": 3.09}, {"text": "uh, in your- in your inbox is spam e-mail?", "start": 4492.29, "duration": 4.57}, {"text": "And so to fit the parameters of this model,", "start": 4500.62, "duration": 5.14}, {"text": "you would s- similar to Gaussian discriminant analysis,", "start": 4505.76, "duration": 5.62}, {"text": "write down the John- joint likelihood.", "start": 4512.53, "duration": 3.34}, {"text": "So the joint likelihood of these parameters, right?", "start": 4515.87, "duration": 7.02}, {"text": "Is a product, you know,", "start": 4522.89, "duration": 5.91}, {"text": "given these parameters, right?", "start": 4528.8, "duration": 5.97}, {"text": "Similar to what we had for Gaussian discriminant analysis.", "start": 4534.77, "duration": 3.96}, {"text": "And the maximum likelihood estimates, um, if you take this,", "start": 4538.73, "duration": 4.38}, {"text": "take logs, take derivatives,", "start": 4543.11, "duration": 1.44}, {"text": "set derivatives to 0, solve for the values that maximize this,", "start": 4544.55, "duration": 3.045}, {"text": "you find that the maximum likelihood estimates of the parameters are, Phi_y,", "start": 4547.595, "duration": 4.605}, {"text": "this is pretty much what you'd expect, right?", "start": 4552.2, "duration": 8.64}, {"text": "It's just a fraction of spam e-mails and, uh,", "start": 4560.84, "duration": 3.735}, {"text": "Phi of j given y equals 1 is, um, well,", "start": 4564.575, "duration": 4.995}, {"text": "I'll write this out in indicator", "start": 4569.57, "duration": 1.32}, {"text": "function notation.", "start": 4570.89, "duration": 2.17}, {"text": "Oh, shoot, sorry.", "start": 4579.97, "duration": 2.96}, {"text": "Okay. So that's the indicator function notation of writing notes.", "start": 4603.88, "duration": 5.215}, {"text": "Look through your, uh, training set,", "start": 4609.095, "duration": 2.505}, {"text": "find all the spam e-mails and of all the spam e-mails,", "start": 4611.6, "duration": 3.165}, {"text": "i.e., examples of y equals 1 count up what fraction of them had word j in it, right?", "start": 4614.765, "duration": 5.925}, {"text": "So you estimate that the chance of word j appearing- you estimate the chance of", "start": 4620.69, "duration": 4.28}, {"text": "the word by appearing in a spam e-mail is", "start": 4624.97, "duration": 2.1}, {"text": "just we have all the spam e-mails in your training set,", "start": 4627.07, "duration": 2.82}, {"text": "what fraction of them contain the word by?", "start": 4629.89, "duration": 2.775}, {"text": "What- what fraction of them had, you know,", "start": 4632.665, "duration": 2.04}, {"text": "x_j equals 1 for say,", "start": 4634.705, "duration": 1.455}, {"text": "the word by, okay?", "start": 4636.16, "duration": 1.81}, {"text": "Um, and so it turns out that if you implement this algorithm,", "start": 4637.97, "duration": 5.085}, {"text": "it will- it will nearly work,", "start": 4643.055, "duration": 2.055}, {"text": "I guess, uh, uh,", "start": 4645.11, "duration": 1.395}, {"text": "but this is Naive Bayes for,", "start": 4646.505, "duration": 2.265}, {"text": "um, for e-mail spam classification, right?", "start": 4648.77, "duration": 3.27}, {"text": "And I mentioned, uh, one reason this,", "start": 4652.04, "duration": 3.06}, {"text": "uh, and it turns out that what one fixed to this algorithm,", "start": 4655.1, "duration": 3.3}, {"text": "which we'll talk about on Wednesday, um,", "start": 4658.4, "duration": 2.805}, {"text": "this is actually, it's actually a not too horrible spam classifier.", "start": 4661.205, "duration": 4.545}, {"text": "It turns out that if you used logistic regression", "start": 4665.75, "duration": 2.82}, {"text": "for spam classification you do better than this almost all the time.", "start": 4668.57, "duration": 3.69}, {"text": "But this is a very efficient algorithm,", "start": 4672.26, "duration": 2.1}, {"text": "because estimating these parameters is just counting,", "start": 4674.36, "duration": 2.835}, {"text": "and then computing probabilities is just multiplying a bunch of numbers.", "start": 4677.195, "duration": 3.075}, {"text": "So there's nothing iterative about this.", "start": 4680.27, "duration": 1.725}, {"text": "So you can fit this model very efficiently and", "start": 4681.995, "duration": 2.445}, {"text": "also keep on updating this model even as you get new data,", "start": 4684.44, "duration": 2.805}, {"text": "even as you get new- new- new uses hits mark or spam or whatever,", "start": 4687.245, "duration": 4.2}, {"text": "even as you get new data, you can update this model very efficiently.", "start": 4691.445, "duration": 3.045}, {"text": "Um, but it turns out that,", "start": 4694.49, "duration": 2.91}, {"text": "uh, actually, the biggest problem with this algorithm is,", "start": 4697.4, "duration": 2.805}, {"text": "what happens if, uh,", "start": 4700.205, "duration": 1.125}, {"text": "this is zero or if- if you get zeros in some of these equations, right?", "start": 4701.33, "duration": 4.5}, {"text": "But we'll come back to that when we talk about Laplace moving on Wednesday, okay?", "start": 4705.83, "duration": 6.15}, {"text": "All right, any quick questions before we wrap up? Okay, okay good.", "start": 4711.98, "duration": 6.155}, {"text": "So now you've learned about generative learning algorithms, um,", "start": 4718.135, "duration": 3.0}, {"text": "we'll come back on Wednesday and learn about", "start": 4721.135, "duration": 1.815}, {"text": "some more fine details how to make this work even better.", "start": 4722.95, "duration": 2.82}, {"text": "So let's break, I'll see you on Wednesday.", "start": 4725.77, "duration": 2.44}]