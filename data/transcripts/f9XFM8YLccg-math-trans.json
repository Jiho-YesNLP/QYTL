[{"text": "The following content is\nprovided under a Creative", "start": 0.06, "duration": 2.44}, {"text": "Commons license.", "start": 2.5, "duration": 1.519}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 4.019, "duration": 2.341}, {"text": "continue to offer high quality\neducational resources for free.", "start": 6.36, "duration": 4.37}, {"text": "To make a donation or\nview additional materials", "start": 10.73, "duration": 2.61}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 13.34, "duration": 3.877}, {"text": "at ocw.mit.edu.", "start": 17.217, "duration": 0.625}, {"text": "PROFESSOR: OK, so\ngood afternoon.", "start": 21.52, "duration": 3.74}, {"text": "Today, we will review\nprobability theory.", "start": 25.26, "duration": 5.56}, {"text": "So I will mostly focus on-- I'll\ngive you some distributions.", "start": 30.82, "duration": 5.27}, {"text": "So probabilistic distributions,\nthat will be of interest to us", "start": 36.09, "duration": 2.74}, {"text": "throughout the course.", "start": 38.83, "duration": 2.0}, {"text": "And I will talk about\nmoment-generating function", "start": 40.83, "duration": 3.78}, {"text": "a little bit.", "start": 44.61, "duration": 1.51}, {"text": "Afterwards, I will talk\nabout law of large numbers", "start": 46.12, "duration": 4.54}, {"text": "and central limit theorem.", "start": 50.66, "duration": 1.55}, {"text": "Who has heard of all\nof these topics before?", "start": 56.31, "duration": 4.37}, {"text": "OK.", "start": 60.68, "duration": 1.47}, {"text": "That's good.", "start": 62.15, "duration": 1.97}, {"text": "Then I'll try to focus\nmore on a little bit more", "start": 64.12, "duration": 2.504}, {"text": "of the advanced stuff.", "start": 66.624, "duration": 0.916}, {"text": "Then a big part of it\nwill be review for you.", "start": 70.89, "duration": 2.94}, {"text": "So first of all, just to\nagree on terminology, let's", "start": 73.83, "duration": 4.43}, {"text": "review some definitions.", "start": 78.26, "duration": 3.23}, {"text": "So a random variable\nX-- we will talk", "start": 81.49, "duration": 11.18}, {"text": "about discrete and\ncontinuous random variables.", "start": 92.67, "duration": 6.23}, {"text": "Just to set up the notation,\nI will write discrete as X", "start": 103.31, "duration": 3.93}, {"text": "and continuous random\nvariable as Y for now.", "start": 107.24, "duration": 2.89}, {"text": "So they are given by its\nprobability distribution--", "start": 110.13, "duration": 2.69}, {"text": "discrete random variable is\ngiven by its probability mass", "start": 112.82, "duration": 4.25}, {"text": "function, f sub\nX, I will denote.", "start": 117.07, "duration": 5.42}, {"text": "And continuous is given by\nprobability distribution", "start": 122.49, "duration": 4.41}, {"text": "function.", "start": 126.9, "duration": 0.499}, {"text": "I will denote by f\nsub Y. So pmf and pdf.", "start": 131.53, "duration": 6.215}, {"text": "Here, I just use a\nsubscript because I", "start": 142.21, "duration": 1.72}, {"text": "wanted to distinguish\nf sub x and f sub y.", "start": 143.93, "duration": 2.1}, {"text": "But when it's clear which random\nvariable we're talking about,", "start": 146.03, "duration": 3.11}, {"text": "I'll just say f.", "start": 149.14, "duration": 3.05}, {"text": "So what is this?", "start": 152.19, "duration": 1.55}, {"text": "A probability mass function is\na function from the sample space", "start": 153.74, "duration": 9.24}, {"text": "to non-negative reals such\nthat the sum over all points", "start": 162.98, "duration": 7.31}, {"text": "in the domain equals 1.", "start": 170.29, "duration": 4.19}, {"text": "The probability distribution\nis very similar.", "start": 174.48, "duration": 2.63}, {"text": "The function from the\nsample space non-negative", "start": 179.73, "duration": 3.16}, {"text": "reals, but now the\nintegration over the domain.", "start": 182.89, "duration": 4.61}, {"text": "So it's pretty much safe to\nconsider our sample space", "start": 191.78, "duration": 4.87}, {"text": "to be the real numbers for\ncontinuous random variables.", "start": 196.65, "duration": 3.92}, {"text": "Later in the course, you\nwill see some examples where", "start": 200.57, "duration": 3.39}, {"text": "it's not the real numbers.", "start": 203.96, "duration": 1.27}, {"text": "But for now, just consider\nit as real numbers.", "start": 205.23, "duration": 3.987}, {"text": "For example, probability\nmass function.", "start": 214.84, "duration": 4.572}, {"text": "If X takes 1 with\nprobability 1/3,", "start": 219.412, "duration": 7.398}, {"text": "minus 1 with probability 1/3,\nand 0 with probability 1/3.", "start": 226.81, "duration": 6.2}, {"text": "Then our probability mass\nfunction is f_x(1) equals", "start": 236.07, "duration": 5.394}, {"text": "f_x(-1), 1/3, just like that.", "start": 241.464, "duration": 6.906}, {"text": "An example of a\ncontinuous random variable", "start": 248.37, "duration": 3.45}, {"text": "is if-- let's say, for\nexample, if f sub Y is", "start": 251.82, "duration": 5.65}, {"text": "equal to 1 for all\ny in [0,1], then", "start": 257.47, "duration": 7.95}, {"text": "this is pdf of uniform\nrandom variable", "start": 265.42, "duration": 10.885}, {"text": "where the space is [0,1].", "start": 276.305, "duration": 3.495}, {"text": "So this random variable\njust picks one out", "start": 279.8, "duration": 2.05}, {"text": "of the three numbers\nwith equal probability.", "start": 281.85, "duration": 2.48}, {"text": "This picks one out of this,\nall the real numbers between 0", "start": 284.33, "duration": 3.12}, {"text": "and 1, with equal probability.", "start": 287.45, "duration": 4.15}, {"text": "These are just some basic stuff.", "start": 291.6, "duration": 3.356}, {"text": "You should be\nfamiliar with this,", "start": 294.956, "duration": 1.374}, {"text": "but I wrote it down just so\nthat we agree on the notation.", "start": 296.33, "duration": 4.604}, {"text": "OK.", "start": 300.934, "duration": 0.924}, {"text": "Both of the boards don't slide.", "start": 301.858, "duration": 1.495}, {"text": "That's good.", "start": 303.353, "duration": 2.958}, {"text": "A few more stuff.", "start": 306.311, "duration": 2.179}, {"text": "Expectation-- probability first.", "start": 308.49, "duration": 6.04}, {"text": "Probability of an event can be\ncomputed as probability of A", "start": 314.53, "duration": 7.562}, {"text": "is equal to either sum of all\npoints in A-- this probability", "start": 322.092, "duration": 6.108}, {"text": "mass function-- or\nintegral over the set A", "start": 328.2, "duration": 8.5}, {"text": "depending on what you're using.", "start": 336.7, "duration": 2.84}, {"text": "And expectation, or mean\nis-- expectation of X", "start": 339.54, "duration": 10.51}, {"text": "is equal to the sum over\nall x, x times that.", "start": 350.05, "duration": 5.36}, {"text": "And expectation of Y is\nthe integral over omega.", "start": 355.41, "duration": 5.7}, {"text": "Oh, sorry.", "start": 361.11, "duration": 1.47}, {"text": "Space.", "start": 362.58, "duration": 1.96}, {"text": "y times.", "start": 364.54, "duration": 0.998}, {"text": "OK.", "start": 371.016, "duration": 1.504}, {"text": "And one more basic\nconcept I'd like to review", "start": 372.52, "duration": 4.33}, {"text": "is two random variables X_1, X_2\nare independent if probability", "start": 376.85, "duration": 15.3}, {"text": "that X_1 is in A and\nX_2 is in B equals", "start": 392.15, "duration": 6.07}, {"text": "the product of the\nprobabilities, for all events A", "start": 398.22, "duration": 10.678}, {"text": "and B. OK.", "start": 408.898, "duration": 5.324}, {"text": "All agreed?", "start": 417.61, "duration": 1.96}, {"text": "So for independence, I will\ntalk about independence", "start": 419.57, "duration": 2.34}, {"text": "of several random\nvariables as well.", "start": 421.91, "duration": 2.66}, {"text": "There are two concepts\nof independence--", "start": 424.57, "duration": 4.72}, {"text": "not two, but several.", "start": 429.29, "duration": 1.47}, {"text": "The two most popular are\nmutually independent events", "start": 430.76, "duration": 6.46}, {"text": "and pairwise independent events.", "start": 437.22, "duration": 1.89}, {"text": "Can somebody tell me the\ndifference between these two", "start": 443.583, "duration": 3.477}, {"text": "for several variables?", "start": 447.06, "duration": 1.805}, {"text": "Yes?", "start": 453.23, "duration": 0.97}, {"text": "AUDIENCE: So\nusually, independent", "start": 454.2, "duration": 1.455}, {"text": "means all the random\nvariables are independent,", "start": 455.655, "duration": 2.985}, {"text": "like X_1 is independent\nwith every others.", "start": 458.64, "duration": 3.91}, {"text": "But pairwise means X_1\nand X_2 are independent,", "start": 462.55, "duration": 4.06}, {"text": "but X_1, X_2, and x_3, they\nmay not be independent.", "start": 466.61, "duration": 5.067}, {"text": "PROFESSOR: OK.", "start": 471.677, "duration": 0.583}, {"text": "Maybe-- yeah.", "start": 472.26, "duration": 2.68}, {"text": "So that's good.", "start": 474.94, "duration": 2.08}, {"text": "So let's see-- for the example\nof three random variables,", "start": 477.02, "duration": 7.4}, {"text": "it might be the case that\neach pair are independent.", "start": 484.42, "duration": 3.35}, {"text": "X_1 and X_2 X_1 is\nindependent with X_2,", "start": 487.77, "duration": 2.34}, {"text": "X_1 is independent with\nX_3, X_2 is with X_3.", "start": 490.11, "duration": 2.83}, {"text": "But altogether, it's\nnot independent.", "start": 492.94, "duration": 2.35}, {"text": "What that means is, this type\nof statement is not true.", "start": 495.29, "duration": 5.49}, {"text": "So there are say A_1, A_2, A_3\nfor which this does not hold.", "start": 500.78, "duration": 4.42}, {"text": "But that's just some\ntechnical detail.", "start": 505.2, "duration": 2.95}, {"text": "We will mostly just consider\nmutually independent events.", "start": 508.15, "duration": 2.81}, {"text": "So when we say that several\nrandom variables are", "start": 510.96, "duration": 2.0}, {"text": "independent, it just means\nwhatever collection you take,", "start": 512.96, "duration": 3.67}, {"text": "they're all independent.", "start": 516.63, "duration": 1.112}, {"text": "OK.", "start": 523.995, "duration": 0.965}, {"text": "So a little bit more fun\nstuff [? in this ?] overview.", "start": 524.96, "duration": 2.82}, {"text": "So we defined random variables.", "start": 530.64, "duration": 3.635}, {"text": "And one of the most\nuniversal random variable,", "start": 534.275, "duration": 4.785}, {"text": "or distribution, is a\nnormal distribution.", "start": 539.06, "duration": 3.25}, {"text": "It's a continuous\nrandom variable.", "start": 550.92, "duration": 3.53}, {"text": "Our continuous random variable\nhas normal distribution,", "start": 554.45, "duration": 6.71}, {"text": "is said to have normal\ndistribution, if-- N(mu,", "start": 561.16, "duration": 8.675}, {"text": "sigma)-- if the probability\ndistribution function is given", "start": 569.835, "duration": 10.545}, {"text": "as 1 over sigma\nsquare root 2 pi,", "start": 580.38, "duration": 6.44}, {"text": "e to the minus x\nminus mu squared.", "start": 586.82, "duration": 4.01}, {"text": "For all reals.", "start": 597.27, "duration": 3.924}, {"text": "OK?", "start": 601.194, "duration": 2.952}, {"text": "So mu mean over--\nthat's one of the most", "start": 604.146, "duration": 8.354}, {"text": "universal random variables--\ndistributions, the most", "start": 612.5, "duration": 4.55}, {"text": "important one as well.", "start": 617.05, "duration": 1.05}, {"text": "OK.", "start": 628.99, "duration": 0.88}, {"text": "So this distribution, how\nit looks like-- I'm sure", "start": 629.87, "duration": 3.28}, {"text": "you saw this bell curve before.", "start": 633.15, "duration": 2.893}, {"text": "It looks like this if\nit's N(0,1), let's say.", "start": 636.043, "duration": 6.308}, {"text": "And that's your y.", "start": 642.351, "duration": 3.069}, {"text": "So it's centered\naround the origin,", "start": 645.42, "duration": 2.94}, {"text": "and it's symmetrical\non the origin.", "start": 648.36, "duration": 3.73}, {"text": "So now let's look\nat our purpose.", "start": 652.09, "duration": 3.2}, {"text": "Let's think about our purpose.", "start": 655.29, "duration": 1.56}, {"text": "We want to model a financial\nproduct or a stock,", "start": 656.85, "duration": 5.09}, {"text": "the price of the stock,\nusing some random variable.", "start": 661.94, "duration": 3.41}, {"text": "The first thing you can try\nis to use normal distribution.", "start": 665.35, "duration": 3.715}, {"text": "Normal distribution\ndoesn't make sense,", "start": 669.065, "duration": 1.625}, {"text": "but we can say the price at\nday n minus the price at day n", "start": 670.69, "duration": 8.896}, {"text": "minus 1 is normal distribution.", "start": 679.586, "duration": 2.029}, {"text": "Is this a sensible definition?", "start": 685.575, "duration": 3.865}, {"text": "That's not really.", "start": 689.44, "duration": 1.197}, {"text": "So it's not a good choice.", "start": 690.637, "duration": 1.083}, {"text": "You can model it like this,\nbut it's not a good choice.", "start": 691.72, "duration": 4.09}, {"text": "There may be several\nreasons, but one reason", "start": 695.81, "duration": 2.24}, {"text": "is that it doesn't take into\naccount the order of magnitude", "start": 698.05, "duration": 2.81}, {"text": "of the price itself.", "start": 700.86, "duration": 1.25}, {"text": "So the stock-- let's say\nyou have a stock price that", "start": 702.11, "duration": 7.377}, {"text": "goes something like that.", "start": 709.487, "duration": 3.243}, {"text": "And say it was $10\nhere, and $50 here.", "start": 712.73, "duration": 5.89}, {"text": "Regardless of where\nyour position is at,", "start": 718.62, "duration": 3.27}, {"text": "it says that the increment,\nthe absolute value of increment", "start": 721.89, "duration": 4.01}, {"text": "is identically distributed at\nthis point and at this point.", "start": 725.9, "duration": 5.18}, {"text": "But if you observed\nhow it works,", "start": 731.08, "duration": 3.69}, {"text": "usually that's not\nnormally distributed.", "start": 734.77, "duration": 3.27}, {"text": "What's normally distributed\nis the percentage", "start": 738.04, "duration": 3.76}, {"text": "of how much it changes daily.", "start": 741.8, "duration": 2.81}, {"text": "So this is not a sensible\nmodel, not a good model.", "start": 744.61, "duration": 7.515}, {"text": "But still, we can use\nnormal distribution", "start": 755.91, "duration": 5.29}, {"text": "to come up with a\npretty good model.", "start": 761.2, "duration": 1.63}, {"text": "So instead, what we want\nis a relative difference", "start": 769.17, "duration": 16.96}, {"text": "to be normally distributed.", "start": 786.13, "duration": 1.762}, {"text": "That is the percent.", "start": 795.68, "duration": 1.04}, {"text": "The question is, what is\nthe distribution of price?", "start": 806.76, "duration": 6.39}, {"text": "What does the\ndistribution of price?", "start": 813.15, "duration": 1.676}, {"text": "So it's not a very\ngood explanation.", "start": 825.75, "duration": 2.91}, {"text": "Because I'm giving just\ndiscrete increments while", "start": 828.66, "duration": 4.2}, {"text": "these are continuous\nrandom variables and so on.", "start": 832.86, "duration": 2.91}, {"text": "But what I'm trying to say here\nis that normal distribution", "start": 835.77, "duration": 3.26}, {"text": "is not good enough.", "start": 839.03, "duration": 1.47}, {"text": "Instead, we want the\npercentage change", "start": 840.5, "duration": 2.86}, {"text": "to be normally distributed.", "start": 843.36, "duration": 2.09}, {"text": "And if that is the case,\nwhat will be the distribution", "start": 845.45, "duration": 5.85}, {"text": "of the random variable?", "start": 851.3, "duration": 1.766}, {"text": "In this case, what will be\nthe distribution of the price?", "start": 853.066, "duration": 2.374}, {"text": "One thing I should\nmention is, in this case,", "start": 867.42, "duration": 2.83}, {"text": "if each discrement is\nnormally distributed,", "start": 870.25, "duration": 3.98}, {"text": "then the price at\nday n will still", "start": 874.23, "duration": 5.3}, {"text": "be a normal random variable\ndistributed like that.", "start": 879.53, "duration": 4.74}, {"text": "So if there's no tendency-- if\nthe average daily increment is", "start": 887.44, "duration": 6.46}, {"text": "0, then no matter\nhow far you go,", "start": 893.9, "duration": 2.932}, {"text": "your random variable will\nbe normally distributed.", "start": 896.832, "duration": 2.083}, {"text": "But here, that will\nnot be the case.", "start": 902.23, "duration": 3.88}, {"text": "So we want to see what\nthe distribution of P_n", "start": 906.11, "duration": 2.675}, {"text": "will be in this case.", "start": 908.785, "duration": 3.196}, {"text": "OK.", "start": 911.981, "duration": 0.499}, {"text": "To do that-- let me formally\nwrite down what I want to say.", "start": 917.82, "duration": 11.48}, {"text": "What I want to say is this.", "start": 929.3, "duration": 4.708}, {"text": "I want to define a\nlog-normal distribution Y,", "start": 934.008, "duration": 12.022}, {"text": "or log-normal random variable\nY, such that log of Y", "start": 946.03, "duration": 21.244}, {"text": "is normally distributed.", "start": 967.274, "duration": 1.488}, {"text": "So to derive the probability\ndistribution of this", "start": 984.17, "duration": 2.5}, {"text": "from the normal\ndistribution, we can", "start": 986.67, "duration": 1.55}, {"text": "use the change of\nvariable formula, which", "start": 988.22, "duration": 11.79}, {"text": "says the following:\nsuppose X and Y", "start": 1000.01, "duration": 7.33}, {"text": "are random variables such\nthat-- probability of X", "start": 1007.34, "duration": 29.441}, {"text": "minus x-- for all x.", "start": 1036.781, "duration": 9.481}, {"text": "Then F of Y of y--\nthe first-- of f", "start": 1052.25, "duration": 15.968}, {"text": "sub X is equal to f sub Y of y.", "start": 1068.218, "duration": 4.491}, {"text": "h of x.", "start": 1078.198, "duration": 0.998}, {"text": "So let's try to fit\ninto this story.", "start": 1087.2, "duration": 4.73}, {"text": "We want to have a\nrandom variable Y such", "start": 1091.93, "duration": 2.99}, {"text": "that log Y is\nnormally distributed.", "start": 1094.92, "duration": 3.59}, {"text": "Here-- so you can\nput log of x here.", "start": 1098.51, "duration": 7.92}, {"text": "If Y is normally distributed,\nX will be the distribution", "start": 1106.43, "duration": 3.87}, {"text": "that we're interested in.", "start": 1110.3, "duration": 2.59}, {"text": "So using this formula, we can\nfind probability distribution", "start": 1112.89, "duration": 4.98}, {"text": "function of the log-normal\ndistribution using", "start": 1117.87, "duration": 2.78}, {"text": "the probability\ndistribution of normal.", "start": 1120.65, "duration": 3.07}, {"text": "So let's do that.", "start": 1123.72, "duration": 1.09}, {"text": "AUDIENCE: [INAUDIBLE], right?", "start": 1145.669, "duration": 4.99}, {"text": "PROFESSOR: Yes.", "start": 1150.659, "duration": 2.251}, {"text": "So it's not a good choice.", "start": 1152.91, "duration": 2.096}, {"text": "Locally, it might\nbe good choice.", "start": 1155.006, "duration": 1.374}, {"text": "But if it's taken\nover a long time,", "start": 1156.38, "duration": 3.977}, {"text": "it won't be a good choice.", "start": 1160.357, "duration": 1.083}, {"text": "Because it will also take\nnegative values, for example.", "start": 1161.44, "duration": 2.958}, {"text": "So if you just take\nthis model, what's", "start": 1168.517, "duration": 1.583}, {"text": "going to happen over\na long period of time", "start": 1170.1, "duration": 1.749}, {"text": "is it's going to hit\nthis square root of n,", "start": 1171.849, "duration": 3.881}, {"text": "negative square root of\nn line infinitely often.", "start": 1175.73, "duration": 2.36}, {"text": "And then it can\ngo up to infinity,", "start": 1182.05, "duration": 2.57}, {"text": "or it can go down to\ninfinity eventually.", "start": 1184.62, "duration": 2.85}, {"text": "So it will take negative\nvalues and positive values.", "start": 1187.47, "duration": 2.25}, {"text": "That's one reason, but\nthere are several reasons", "start": 1193.31, "duration": 2.15}, {"text": "why that's not a good choice.", "start": 1195.46, "duration": 2.51}, {"text": "If you look at a\nvery small scale,", "start": 1197.97, "duration": 1.47}, {"text": "it might be OK, because the base\nprice doesn't change that much.", "start": 1199.44, "duration": 4.17}, {"text": "So if you model\nin terms of ratio,", "start": 1203.61, "duration": 1.88}, {"text": "our if you model it\nin an absolute way,", "start": 1205.49, "duration": 2.44}, {"text": "it doesn't matter that much.", "start": 1207.93, "duration": 1.9}, {"text": "But if you want to do it a\nlittle bit more large scale,", "start": 1209.83, "duration": 4.02}, {"text": "then that's not a\nvery good choice.", "start": 1213.85, "duration": 4.04}, {"text": "Other questions?", "start": 1217.89, "duration": 2.23}, {"text": "Do you want me to\nadd some explanation?", "start": 1220.12, "duration": 1.625}, {"text": "OK.", "start": 1225.322, "duration": 0.5}, {"text": "So let me get this right.", "start": 1229.58, "duration": 3.14}, {"text": "Y. I want X to be-- yes.", "start": 1237.12, "duration": 8.32}, {"text": "I want X to be the log\nnormal distribution.", "start": 1245.44, "duration": 4.51}, {"text": "And I want Y to be\nnormal distribution", "start": 1256.95, "duration": 7.63}, {"text": "or a normal random variable.", "start": 1264.58, "duration": 2.61}, {"text": "Then the probability\nthat X is at most x", "start": 1267.19, "duration": 5.382}, {"text": "equals the probability\nthat Y is at most-- sigma.", "start": 1272.572, "duration": 11.928}, {"text": "Y is at most log x.", "start": 1284.5, "duration": 4.57}, {"text": "That's the definition of\nlog-normal distribution.", "start": 1289.07, "duration": 4.09}, {"text": "Then by using this change\nof variable formula,", "start": 1293.16, "duration": 5.97}, {"text": "probability density\nfunction of X", "start": 1299.13, "duration": 2.65}, {"text": "is equal to probability\ndensity function of Y at log", "start": 1301.78, "duration": 5.2}, {"text": "x times the differentiation\nof log x which is 1 over x.", "start": 1306.98, "duration": 7.46}, {"text": "So it becomes 1 over\nx sigma square root", "start": 1314.44, "duration": 6.02}, {"text": "2 pi, e to the minus\nlog x minus mu squared.", "start": 1320.46, "duration": 7.244}, {"text": "So log-normal\ndistribution can also", "start": 1331.61, "duration": 1.82}, {"text": "be defined as the\ndistribution which has", "start": 1333.43, "duration": 1.95}, {"text": "probability mass function this.", "start": 1335.38, "duration": 1.866}, {"text": "You can use either definition.", "start": 1342.65, "duration": 3.51}, {"text": "Let me just make sure that I\ndidn't mess up in the middle.", "start": 1346.16, "duration": 3.231}, {"text": "Yes.", "start": 1352.8, "duration": 0.98}, {"text": "And that only works\nfor x greater than 0.", "start": 1353.78, "duration": 5.407}, {"text": "Yes?", "start": 1359.187, "duration": 0.5}, {"text": "AUDIENCE: [INAUDIBLE]?", "start": 1359.687, "duration": 2.027}, {"text": "PROFESSOR: Yeah.", "start": 1361.714, "duration": 0.666}, {"text": "So all logs are natural log.", "start": 1362.38, "duration": 1.56}, {"text": "It should be ln.", "start": 1363.94, "duration": 2.231}, {"text": "Yeah.", "start": 1366.171, "duration": 0.499}, {"text": "Thank you.", "start": 1366.67, "duration": 1.65}, {"text": "OK.", "start": 1368.32, "duration": 1.49}, {"text": "So question-- what's the mean\nof this distribution here?", "start": 1369.81, "duration": 8.56}, {"text": "Yeah?", "start": 1378.37, "duration": 0.5}, {"text": "AUDIENCE: 1?", "start": 1378.87, "duration": 2.1}, {"text": "PROFESSOR: Not 1.", "start": 1380.97, "duration": 1.49}, {"text": "It might be mu.", "start": 1382.46, "duration": 2.36}, {"text": "Is it mu?", "start": 1384.82, "duration": 2.68}, {"text": "Oh, sorry.", "start": 1387.5, "duration": 0.76}, {"text": "It might be e to the mu.", "start": 1388.26, "duration": 1.59}, {"text": "Because log X, the normal\ndistribution had mean mu.", "start": 1389.85, "duration": 5.62}, {"text": "log x equals mu\nmight be the center.", "start": 1395.47, "duration": 2.16}, {"text": "If that's the case, x is e\nto the mu will be the mean.", "start": 1397.63, "duration": 3.22}, {"text": "Is that the case?", "start": 1400.85, "duration": 3.065}, {"text": "Yes?", "start": 1403.915, "duration": 0.5}, {"text": "AUDIENCE: Can you get\nthe mu minus [INAUDIBLE]?", "start": 1404.415, "duration": 3.475}, {"text": "PROFESSOR: Probably right.", "start": 1407.89, "duration": 1.87}, {"text": "I don't remember what's there.", "start": 1409.76, "duration": 1.31}, {"text": "There is a correcting factor.", "start": 1411.07, "duration": 1.42}, {"text": "I don't remember\nexactly what that is,", "start": 1412.49, "duration": 1.802}, {"text": "but I think you're right.", "start": 1414.292, "duration": 2.918}, {"text": "So one very important\nthing to remember", "start": 1417.21, "duration": 2.56}, {"text": "is log-normal\ndistribution are referred", "start": 1419.77, "duration": 3.73}, {"text": "to in terms of the\nparameters mu and sigma,", "start": 1423.5, "duration": 4.65}, {"text": "because that's the mu and\nsigma up here and here coming", "start": 1428.15, "duration": 2.36}, {"text": "from the normal distribution.", "start": 1430.51, "duration": 2.09}, {"text": "But those are not the\nmean and variance anymore,", "start": 1432.6, "duration": 4.98}, {"text": "because you skew\nthe distribution.", "start": 1437.58, "duration": 4.32}, {"text": "It's no longer centered at mu.", "start": 1441.9, "duration": 1.8}, {"text": "log X is centered at mu, but\nwhen it takes exponential,", "start": 1443.7, "duration": 3.79}, {"text": "it becomes skewed.", "start": 1447.49, "duration": 1.1}, {"text": "And we take the average,\nyou'll see that the mean", "start": 1448.59, "duration": 4.04}, {"text": "is no longer e to the mu.", "start": 1452.63, "duration": 1.3}, {"text": "So that doesn't give the mean.", "start": 1453.93, "duration": 2.435}, {"text": "That doesn't imply that\nthe mean is e to the sigma.", "start": 1456.365, "duration": 2.125}, {"text": "That doesn't imply\nthat the variance is", "start": 1458.49, "duration": 2.38}, {"text": "something like e to the sigma.", "start": 1460.87, "duration": 2.372}, {"text": "That's just totally nonsense.", "start": 1463.242, "duration": 3.798}, {"text": "Just remember-- these are just\nparameters, some parameters.", "start": 1467.04, "duration": 3.04}, {"text": "It's no longer mean or variance.", "start": 1470.08, "duration": 2.37}, {"text": "And in your homework,\none exercise,", "start": 1475.67, "duration": 4.124}, {"text": "we'll ask you to compute\nthe mean and variance", "start": 1479.794, "duration": 1.916}, {"text": "of the random variable.", "start": 1481.71, "duration": 2.78}, {"text": "But really, just try to\nhave it stick in your mind", "start": 1484.49, "duration": 4.07}, {"text": "that mu and sigma is no\nlonger mean and variance.", "start": 1488.56, "duration": 4.6}, {"text": "That's only the case for\nnormal random variables.", "start": 1493.16, "duration": 3.07}, {"text": "And the reason we are\nstill using mu and sigma", "start": 1496.23, "duration": 2.15}, {"text": "is because of this derivation.", "start": 1498.38, "duration": 2.3}, {"text": "And it's easy to\ndescribe it in those.", "start": 1500.68, "duration": 1.71}, {"text": "OK.", "start": 1505.83, "duration": 2.11}, {"text": "So the normal distribution\nand log-normal distribution", "start": 1507.94, "duration": 3.86}, {"text": "will probably be\nthe distributions", "start": 1511.8, "duration": 1.92}, {"text": "that you'll see the most\nthroughout the course.", "start": 1513.72, "duration": 2.022}, {"text": "But there are some\nother distributions", "start": 1515.742, "duration": 1.583}, {"text": "that you'll also see.", "start": 1517.325, "duration": 1.175}, {"text": "I need this.", "start": 1523.46, "duration": 1.488}, {"text": "I will not talk\nabout it in detail.", "start": 1532.884, "duration": 2.766}, {"text": "It will be some\nexercise questions.", "start": 1535.65, "duration": 2.89}, {"text": "For example, you have Poisson\ndistribution or exponential", "start": 1538.54, "duration": 6.399}, {"text": "distributions.", "start": 1544.939, "duration": 0.583}, {"text": "These are some other\ndistributions that you'll see.", "start": 1552.13, "duration": 4.42}, {"text": "And all of these-- normal,\nlog-normal, Poisson,", "start": 1556.55, "duration": 2.51}, {"text": "and exponential,\nand a lot more can", "start": 1559.06, "duration": 2.0}, {"text": "be grouped into a\nfamily of distributions", "start": 1561.06, "duration": 3.34}, {"text": "called exponential family.", "start": 1564.4, "duration": 1.398}, {"text": "So a distribution is called to\nbe in an exponential family--", "start": 1578.49, "duration": 5.536}, {"text": "A distribution belongs\nto exponential family", "start": 1584.026, "duration": 12.564}, {"text": "if there exists a theta,\na vector that parametrizes", "start": 1596.59, "duration": 14.3}, {"text": "the distribution such that\nthe probability density", "start": 1610.89, "duration": 14.63}, {"text": "function for this choice\nof parameter theta", "start": 1625.52, "duration": 5.15}, {"text": "can be written as h\nof x times c of theta", "start": 1630.67, "duration": 5.81}, {"text": "times the exponent of\nsum from i equal 1 to k--", "start": 1636.48, "duration": 6.018}, {"text": "Yes.", "start": 1655.446, "duration": 0.524}, {"text": "So here, when I write\nonly x, h should only", "start": 1655.97, "duration": 4.13}, {"text": "depend on x, not on theta.", "start": 1660.1, "duration": 3.3}, {"text": "When I write some\nfunction of theta,", "start": 1663.4, "duration": 1.69}, {"text": "it should only depend\non theta, not on x.", "start": 1665.09, "duration": 2.93}, {"text": "So h(x), t_i(x) depends only\non x and c(theta) on my value", "start": 1668.02, "duration": 13.05}, {"text": "theta, depends only on theta.", "start": 1681.07, "duration": 3.609}, {"text": "That's an abstract thing.", "start": 1684.679, "duration": 1.041}, {"text": "It's not clear why\nthis is so useful,", "start": 1685.72, "duration": 2.11}, {"text": "at least from the definition.", "start": 1687.83, "duration": 2.31}, {"text": "But you're going to talk\nabout some distribution", "start": 1690.14, "duration": 4.815}, {"text": "for an exponential\nfamily, right?", "start": 1694.955, "duration": 1.695}, {"text": "Yeah.", "start": 1696.65, "duration": 0.5}, {"text": "So you will see\nsomething about this.", "start": 1697.15, "duration": 2.69}, {"text": "But one good thing\nis, they exhibit", "start": 1699.84, "duration": 1.93}, {"text": "some good statistical\nbehavior, the things-- when", "start": 1701.77, "duration": 3.59}, {"text": "you group them into--\nall distributions", "start": 1705.36, "duration": 2.97}, {"text": "in the exponential family\nhave some nice statistical", "start": 1708.33, "duration": 3.13}, {"text": "properties, which makes it good.", "start": 1711.46, "duration": 4.13}, {"text": "That's too abstract.", "start": 1715.59, "duration": 1.68}, {"text": "Let's see how log-normal\ndistribution actually falls", "start": 1717.27, "duration": 4.87}, {"text": "into the exponential family.", "start": 1722.14, "duration": 1.491}, {"text": "AUDIENCE: So, let\nme just comment.", "start": 1727.607, "duration": 1.837}, {"text": "PROFESSOR: Yeah, sure.", "start": 1729.444, "duration": 0.916}, {"text": "AUDIENCE: The notion of\nindependent random variables,", "start": 1730.36, "duration": 3.616}, {"text": "you went over how the--\nwell, the probability density", "start": 1733.976, "duration": 4.711}, {"text": "functions of collections\nof random variables", "start": 1738.687, "duration": 1.833}, {"text": "if they're mutually\nindependent is", "start": 1740.52, "duration": 1.416}, {"text": "the product of the\nprobability densities", "start": 1741.936, "duration": 3.704}, {"text": "of the individual variables.", "start": 1745.64, "duration": 1.492}, {"text": "And so with this\nexponential family,", "start": 1747.132, "duration": 3.108}, {"text": "if you have random variables\nfrom the same exponential", "start": 1750.24, "duration": 2.445}, {"text": "family, products of this\ndensity function factor out", "start": 1752.685, "duration": 5.695}, {"text": "into a very simple form.", "start": 1758.38, "duration": 1.32}, {"text": "It doesn't get more\ncomplicated as you", "start": 1759.7, "duration": 1.66}, {"text": "look at the joint density\nof many variables,", "start": 1761.36, "duration": 3.07}, {"text": "and in fact simplifies to\nthe same exponential family.", "start": 1764.43, "duration": 3.08}, {"text": "So that's where that\nbecomes very useful.", "start": 1767.51, "duration": 2.7}, {"text": "PROFESSOR: So it's designed\nso that it factors out", "start": 1770.21, "duration": 2.095}, {"text": "when it's multiplied.", "start": 1772.305, "duration": 0.875}, {"text": "It factors out well.", "start": 1773.18, "duration": 1.464}, {"text": "OK.", "start": 1777.99, "duration": 0.66}, {"text": "So-- sorry about that.", "start": 1778.65, "duration": 4.35}, {"text": "Yeah, log-normal distribution.", "start": 1783.0, "duration": 1.96}, {"text": "So take h(x), 1 over x.", "start": 1784.96, "duration": 5.01}, {"text": "Before that, let's just rewrite\nthat in a different way.", "start": 1789.97, "duration": 2.38}, {"text": "So 1 over x sigma square\nroot 2 pi, e to the minus log", "start": 1792.35, "duration": 6.454}, {"text": "x [INAUDIBLE] squared.", "start": 1798.804, "duration": 4.626}, {"text": "Square.", "start": 1803.43, "duration": 1.1}, {"text": "Can be rewritten as 1\nover x, times 1 over sigma", "start": 1804.53, "duration": 6.016}, {"text": "squared 2 pi, e to\nthe minus log x square", "start": 1810.546, "duration": 7.669}, {"text": "over 2 sigma square plus\nmu log x over sigma square", "start": 1818.215, "duration": 12.375}, {"text": "minus mu square.", "start": 1830.59, "duration": 2.475}, {"text": "Let's write it like that.", "start": 1837.05, "duration": 1.68}, {"text": "Set up h(x) equals 1 over x.", "start": 1838.73, "duration": 3.734}, {"text": "c of theta-- sorry,\ntheta equals mu sigma.", "start": 1842.464, "duration": 8.958}, {"text": "c(theta) is equal to 1 over\nsigma square root 2 pi, e", "start": 1851.422, "duration": 4.51}, {"text": "to the minus mu square.", "start": 1855.932, "duration": 1.231}, {"text": "So you will\nparametrize this family", "start": 1861.51, "duration": 2.41}, {"text": "in terms of mu and sigma.", "start": 1863.92, "duration": 2.95}, {"text": "Your h of x here\nwill be 1 over x.", "start": 1866.87, "duration": 2.62}, {"text": "Your c(theta) will be this\nterm and the last term here,", "start": 1869.49, "duration": 4.51}, {"text": "because this\ndoesn't depend on x.", "start": 1874.0, "duration": 2.96}, {"text": "And then you have to\nfigure out what w and t is.", "start": 1876.96, "duration": 4.67}, {"text": "You can let w_1 of\nx be log x square.", "start": 1881.63, "duration": 3.34}, {"text": "t_1-- no, t_1 of x be log x\nsquare, w_1 of theta be minus 1", "start": 1889.18, "duration": 9.76}, {"text": "over 2 sigma square.", "start": 1898.94, "duration": 2.452}, {"text": "And similarly, you\ncan let t_2 equals log", "start": 1901.392, "duration": 2.688}, {"text": "x and w_2 equals mu over sigma.", "start": 1904.08, "duration": 7.324}, {"text": "It's just some technicality,\nbut at least you", "start": 1914.58, "duration": 1.99}, {"text": "can see it really fits in.", "start": 1916.57, "duration": 3.404}, {"text": "OK.", "start": 1922.69, "duration": 2.51}, {"text": "So that's all\nabout distributions", "start": 1925.2, "duration": 2.18}, {"text": "that I want to talk about.", "start": 1927.38, "duration": 2.7}, {"text": "And then let's talk\na little bit more", "start": 1930.08, "duration": 2.56}, {"text": "about more interesting\nstuff, in my opinion.", "start": 1932.64, "duration": 2.7}, {"text": "I like this stuff better.", "start": 1935.34, "duration": 1.365}, {"text": "There are two main things\nthat we're interested in.", "start": 1939.44, "duration": 3.9}, {"text": "When we have a random variable,\nat least for our purpose, what", "start": 1943.34, "duration": 7.31}, {"text": "we want to study is given\na random variable, first,", "start": 1950.65, "duration": 12.116}, {"text": "we want to study a statistics.", "start": 1962.766, "duration": 1.249}, {"text": "So we want to study this\nstatistics, whatever", "start": 1970.71, "duration": 4.116}, {"text": "that means.", "start": 1974.826, "duration": 0.972}, {"text": "And that will be represented\nby the k-th moments", "start": 1979.69, "duration": 2.877}, {"text": "of the random variable.", "start": 1982.567, "duration": 0.958}, {"text": "Where k-th moment is defined\nas expectation of X to the k.", "start": 1990.34, "duration": 5.03}, {"text": "And a good way to study\nall the moments together", "start": 2000.6, "duration": 3.4}, {"text": "in one function is a\nmoment-generating function.", "start": 2004.0, "duration": 2.855}, {"text": "So this moment-generating\nfunction", "start": 2014.3, "duration": 2.18}, {"text": "encodes all the k-th moments\nof a random variable.", "start": 2016.48, "duration": 3.86}, {"text": "So it contains all the\nstatistical information", "start": 2020.34, "duration": 2.79}, {"text": "of a random variable.", "start": 2023.13, "duration": 2.209}, {"text": "That's why\nmoment-generating function", "start": 2025.339, "duration": 1.541}, {"text": "will be interesting to us.", "start": 2026.88, "duration": 1.18}, {"text": "Because when you\nwant to study it,", "start": 2028.06, "duration": 1.99}, {"text": "you don't have to consider\neach moment separately.", "start": 2030.05, "duration": 2.71}, {"text": "It gives a unified way.", "start": 2032.76, "duration": 1.33}, {"text": "It gives a very good\nfeeling about your function.", "start": 2034.09, "duration": 3.96}, {"text": "That will be our first topic.", "start": 2038.05, "duration": 1.51}, {"text": "Our second topic will\nbe we want to study", "start": 2039.56, "duration": 2.64}, {"text": "its long-term or\nlarge-scale behavior.", "start": 2042.2, "duration": 7.94}, {"text": "So for example, assume that you\nhave a normal distribution--", "start": 2058.19, "duration": 3.009}, {"text": "one random variable with\nnormal distribution.", "start": 2061.199, "duration": 3.25}, {"text": "If we just have a\nsingle random variable,", "start": 2064.449, "duration": 4.351}, {"text": "you really have no control.", "start": 2068.8, "duration": 1.96}, {"text": "It can be anywhere.", "start": 2070.76, "duration": 1.11}, {"text": "The outcome can be anything\naccording to that distribution.", "start": 2071.87, "duration": 7.39}, {"text": "But if you have several\nindependent random variables", "start": 2079.26, "duration": 2.169}, {"text": "with the exact\nsame distribution,", "start": 2081.429, "duration": 3.111}, {"text": "if the number is super large--\nlet's say 100 million--", "start": 2084.54, "duration": 4.99}, {"text": "and you plot how many random\nvariables fall into each point", "start": 2089.53, "duration": 5.79}, {"text": "into a graph,\nyou'll know that it", "start": 2095.32, "duration": 2.83}, {"text": "has to look very\nclose to this curve.", "start": 2098.15, "duration": 3.522}, {"text": "It will be more dense\nhere, sparser there,", "start": 2101.672, "duration": 2.488}, {"text": "and sparser there.", "start": 2104.16, "duration": 2.56}, {"text": "So you don't have\nindividual control on each", "start": 2106.72, "duration": 2.33}, {"text": "of the random variables.", "start": 2109.05, "duration": 1.1}, {"text": "But when you look\nat large scale,", "start": 2110.15, "duration": 2.035}, {"text": "you know, at least with\nvery high probability,", "start": 2112.185, "duration": 4.675}, {"text": "it has to look like this curve.", "start": 2116.86, "duration": 3.13}, {"text": "Those kind of things are\nwhat we want to study.", "start": 2119.99, "duration": 2.49}, {"text": "When we look at this long-term\nbehavior or large scale", "start": 2122.48, "duration": 3.24}, {"text": "behavior, what can we say?", "start": 2125.72, "duration": 2.78}, {"text": "What kind of events\nare guaranteed", "start": 2128.5, "duration": 1.63}, {"text": "to happen with probability,\nlet's say, 99.9%?", "start": 2130.13, "duration": 4.98}, {"text": "And actually, some interesting\nthings are happening.", "start": 2135.11, "duration": 3.57}, {"text": "As you might already know, two\ntypical theorems of this type", "start": 2138.68, "duration": 6.12}, {"text": "will be, in this\ntopic will be law", "start": 2144.8, "duration": 2.05}, {"text": "of large numbers and\ncentral limit theorem.", "start": 2146.85, "duration": 6.432}, {"text": "So let's start with\nour first topic--", "start": 2162.52, "duration": 2.07}, {"text": "the moment-generating function.", "start": 2164.59, "duration": 1.385}, {"text": "The moment-generating\nfunction of a random variable", "start": 2186.31, "duration": 2.49}, {"text": "is defined as-- I\nwrite it as m sub", "start": 2188.8, "duration": 2.74}, {"text": "X. It's defined as expectation\nof e to the t times x", "start": 2191.54, "duration": 7.79}, {"text": "where t is some parameter.", "start": 2199.33, "duration": 1.76}, {"text": "t can be any real.", "start": 2201.09, "duration": 1.42}, {"text": "You have to be careful.", "start": 2207.372, "duration": 0.958}, {"text": "It doesn't always converge.", "start": 2208.33, "duration": 3.35}, {"text": "So remark: does not\nnecessarily exist.", "start": 2211.68, "duration": 6.68}, {"text": "So for example, one of the\ndistributions you already saw", "start": 2229.9, "duration": 3.06}, {"text": "does not have\nmoment-generating function.", "start": 2232.96, "duration": 2.05}, {"text": "The log-normal\ndistribution does not", "start": 2235.01, "duration": 7.091}, {"text": "have any moment-generating\nfunction.", "start": 2242.101, "duration": 1.499}, {"text": "And that's one thing\nyou have to be careful.", "start": 2250.65, "duration": 3.07}, {"text": "It's not just some\ntheoretical thing.", "start": 2253.72, "duration": 2.15}, {"text": "The statement is not\nsomething theoretical.", "start": 2258.329, "duration": 1.791}, {"text": "It actually happens for\nsome random variables", "start": 2260.12, "duration": 2.55}, {"text": "that you encounter in your life.", "start": 2262.67, "duration": 2.878}, {"text": "So be careful.", "start": 2265.548, "duration": 2.642}, {"text": "And that will actually show\nsome very interesting thing", "start": 2268.19, "duration": 6.27}, {"text": "I will later explain.", "start": 2274.46, "duration": 2.76}, {"text": "Some very interesting\nfacts arise from this fact.", "start": 2277.22, "duration": 2.576}, {"text": "Before going into\nthat, first of all,", "start": 2283.9, "duration": 2.377}, {"text": "why is it called\nmoment-generating function?", "start": 2286.277, "duration": 1.833}, {"text": "It's because if you\ntake the k-th derivative", "start": 2288.11, "duration": 6.43}, {"text": "of this function,\nthen it actually", "start": 2294.54, "duration": 11.74}, {"text": "gives the k-th moment\nof your random variable.", "start": 2306.28, "duration": 6.851}, {"text": "That's where the\nname comes from.", "start": 2313.131, "duration": 1.374}, {"text": "It's for all integers.", "start": 2323.235, "duration": 1.99}, {"text": "And that gives a\ndifferent way of writing", "start": 2338.32, "duration": 1.72}, {"text": "a moment-generating function.", "start": 2340.04, "duration": 1.208}, {"text": "Because of that, we may write\nthe moment-generating function", "start": 2351.23, "duration": 6.86}, {"text": "as the sum from k equals\n0 to infinity, t to the k,", "start": 2358.09, "duration": 6.902}, {"text": "k factorial, times\na k-th moment.", "start": 2364.992, "duration": 4.92}, {"text": "That's like the\nTaylor expansion.", "start": 2377.79, "duration": 2.679}, {"text": "Because you know\nall the derivatives,", "start": 2380.469, "duration": 1.541}, {"text": "you know what the\nfunctions would be.", "start": 2382.01, "duration": 1.541}, {"text": "Of course, only if it exists.", "start": 2383.551, "duration": 1.749}, {"text": "This might not converge.", "start": 2385.3, "duration": 1.0}, {"text": "So if moment-generating\nfunction exists,", "start": 2395.08, "duration": 3.28}, {"text": "they pretty much classify\nyour random variables.", "start": 2398.36, "duration": 2.76}, {"text": "So if two random\nvariables, X, Y,", "start": 2404.63, "duration": 4.39}, {"text": "have the same\nmoment-generating function,", "start": 2409.02, "duration": 7.1}, {"text": "then X and Y have the\nsame distribution.", "start": 2416.12, "duration": 8.715}, {"text": "I will not prove this theorem.", "start": 2430.02, "duration": 2.53}, {"text": "But it says that\nmoment-generating function,", "start": 2432.55, "duration": 2.53}, {"text": "if it exists, encodes\nreally all the information", "start": 2435.08, "duration": 4.52}, {"text": "about your random variables.", "start": 2439.6, "duration": 1.916}, {"text": "You're not losing anything.", "start": 2441.516, "duration": 1.474}, {"text": "However, be very careful when\nyou're applying this theorem.", "start": 2446.32, "duration": 4.22}, {"text": "Because remark,\nit does not imply", "start": 2450.54, "duration": 9.38}, {"text": "that all random variables\nwith identical k-th moments", "start": 2459.92, "duration": 20.82}, {"text": "for all k has the\nsame distribution.", "start": 2480.74, "duration": 6.05}, {"text": "Do you see it?", "start": 2497.418, "duration": 2.612}, {"text": "If X and Y have a\nmoment-generating function,", "start": 2500.03, "duration": 3.3}, {"text": "and they're the same, then they\nhave the same distribution.", "start": 2503.33, "duration": 5.88}, {"text": "This looks a little bit\ncontroversial to this theorem.", "start": 2509.21, "duration": 3.5}, {"text": "It says that it's not\nnecessarily the case", "start": 2512.71, "duration": 4.18}, {"text": "that two random variables, which\nhave identical moments-- so", "start": 2516.89, "duration": 4.11}, {"text": "all k-th moments are the\nsame for two variables--", "start": 2521.0, "duration": 3.75}, {"text": "even if that's the case,\nthey don't necessarily", "start": 2524.75, "duration": 1.96}, {"text": "have to have the\nsame distribution.", "start": 2526.71, "duration": 3.35}, {"text": "Which seems like it\ndoesn't make sense", "start": 2530.06, "duration": 1.954}, {"text": "if you look at this theorem.", "start": 2532.014, "duration": 1.166}, {"text": "Because moment-generating\nfunction", "start": 2533.18, "duration": 1.416}, {"text": "is defined in terms\nof the moments.", "start": 2534.596, "duration": 2.054}, {"text": "If two random variables\nhave the same moment,", "start": 2536.65, "duration": 2.092}, {"text": "we have the same\nmoment-generating function.", "start": 2538.742, "duration": 1.833}, {"text": "If they have the same\nmoment-generating function,", "start": 2540.575, "duration": 2.041}, {"text": "they have the same distribution.", "start": 2542.616, "duration": 2.354}, {"text": "There is a hole\nin this argument.", "start": 2544.97, "duration": 3.48}, {"text": "Even if they have\nthe same moments,", "start": 2548.45, "duration": 3.4}, {"text": "it doesn't necessarily\nimply that they", "start": 2551.85, "duration": 1.942}, {"text": "have the same\nmoment-generating function.", "start": 2553.792, "duration": 1.708}, {"text": "They might both not have\nmoment-generating functions.", "start": 2555.5, "duration": 4.02}, {"text": "That's the glitch.", "start": 2559.52, "duration": 3.1}, {"text": "Be careful.", "start": 2562.62, "duration": 1.42}, {"text": "So just remember that even if\nthey have the same moments,", "start": 2564.04, "duration": 3.547}, {"text": "they don't necessarily\nhave the same distribution.", "start": 2567.587, "duration": 2.083}, {"text": "And the reason is\nbecause-- one reason", "start": 2569.67, "duration": 2.07}, {"text": "is because the moment-generating\nfunction might not exist.", "start": 2571.74, "duration": 4.37}, {"text": "And if you look in\nto Wikipedia, you'll", "start": 2576.11, "duration": 1.82}, {"text": "see an example of\nwhen it happens,", "start": 2577.93, "duration": 2.92}, {"text": "of two random variables\nwhere this happens.", "start": 2580.85, "duration": 2.495}, {"text": "So that's one thing\nwe will use later.", "start": 2590.31, "duration": 3.07}, {"text": "Another thing that\nwe will use later,", "start": 2593.38, "duration": 4.28}, {"text": "it's a statement\nvery similar to that,", "start": 2597.66, "duration": 3.29}, {"text": "but it says something about a\nsequence of random variables.", "start": 2600.95, "duration": 4.87}, {"text": "So if X_1, X_2, up to X_n is\na sequence of random variables", "start": 2605.82, "duration": 13.586}, {"text": "such that the moment-generating\nfunction exists,", "start": 2619.406, "duration": 9.064}, {"text": "and it converges-- ah,\nit goes to infinity.", "start": 2628.47, "duration": 4.11}, {"text": "Tends to the\nmoment-generating function", "start": 2637.542, "duration": 5.708}, {"text": "of some random variable t.", "start": 2643.25, "duration": 2.13}, {"text": "X. For some random\nvariable X for all t.", "start": 2645.38, "duration": 7.711}, {"text": "Here, we're assuming that all\nmoment-generating function", "start": 2656.25, "duration": 2.72}, {"text": "exists.", "start": 2658.97, "duration": 1.31}, {"text": "So again, the\nsituation is, you have", "start": 2660.28, "duration": 1.77}, {"text": "a sequence of random variables.", "start": 2662.05, "duration": 2.85}, {"text": "Their moment-generating\nfunction exists.", "start": 2664.9, "duration": 2.7}, {"text": "And in each point\nt, it converges", "start": 2667.6, "duration": 4.19}, {"text": "to the value of the\nmoment-generating function", "start": 2671.79, "duration": 2.177}, {"text": "of some other random variable x.", "start": 2673.967, "duration": 1.333}, {"text": "And what should happen?", "start": 2678.27, "duration": 3.04}, {"text": "In light of this theorem,\nit should be the case", "start": 2681.31, "duration": 2.57}, {"text": "that the distribution\nof this sequence", "start": 2683.88, "duration": 3.61}, {"text": "gets closer and closer\nto the distribution", "start": 2687.49, "duration": 1.75}, {"text": "of this random variable x.", "start": 2689.24, "duration": 4.12}, {"text": "And to make it formal, to make\nthat information formal, what", "start": 2693.36, "duration": 6.86}, {"text": "we can conclude is, for\nall x, the probability", "start": 2700.22, "duration": 9.54}, {"text": "X_i is less than or equal to\nx tends to the probability", "start": 2709.76, "duration": 5.68}, {"text": "that at x.", "start": 2715.44, "duration": 1.86}, {"text": "So in this sense,\nthe distributions", "start": 2720.09, "duration": 2.9}, {"text": "of these random variables\nconverges to the distribution", "start": 2722.99, "duration": 2.95}, {"text": "of that random variable.", "start": 2725.94, "duration": 1.276}, {"text": "So it's just a technical issue.", "start": 2730.09, "duration": 2.24}, {"text": "You can just think of it as\nthese random variables converge", "start": 2732.33, "duration": 6.56}, {"text": "to that random variable.", "start": 2738.89, "duration": 2.31}, {"text": "If you take some graduate\nprobability course,", "start": 2741.2, "duration": 2.03}, {"text": "you'll see that there's\nseveral possible ways", "start": 2743.23, "duration": 3.87}, {"text": "to define convergence.", "start": 2747.1, "duration": 1.63}, {"text": "But that's just\nsome technicality.", "start": 2748.73, "duration": 2.01}, {"text": "And the spirit\nhere is just really", "start": 2750.74, "duration": 2.657}, {"text": "the sequence converges if its\nmoment-generating function", "start": 2753.397, "duration": 2.333}, {"text": "converges.", "start": 2755.73, "duration": 0.499}, {"text": "So as you can see from\nthese two theorems,", "start": 2759.79, "duration": 2.68}, {"text": "moment-generating\nfunction, if it exists,", "start": 2762.47, "duration": 1.97}, {"text": "is a really powerful\ntool that allows you", "start": 2764.44, "duration": 3.83}, {"text": "to control the distribution.", "start": 2768.27, "duration": 1.21}, {"text": "You'll see some applications\nlater in central limit theorem.", "start": 2773.06, "duration": 3.347}, {"text": "Any questions?", "start": 2776.407, "duration": 0.583}, {"text": "AUDIENCE: [INAUDIBLE]?", "start": 2781.53, "duration": 0.916}, {"text": "PROFESSOR: This one?", "start": 2788.557, "duration": 0.833}, {"text": "Why?", "start": 2792.87, "duration": 1.284}, {"text": "AUDIENCE: Because\nit starts with t,", "start": 2794.154, "duration": 1.458}, {"text": "and the right-hand side\nhas nothing general.", "start": 2795.612, "duration": 2.55}, {"text": "PROFESSOR: Ah.", "start": 2800.777, "duration": 0.583}, {"text": "Thank you.", "start": 2804.318, "duration": 2.862}, {"text": "We evaluated at zero.", "start": 2807.18, "duration": 1.17}, {"text": "Other questions?", "start": 2813.23, "duration": 1.464}, {"text": "Other corrections?", "start": 2814.694, "duration": 1.952}, {"text": "AUDIENCE: When you say the\nmoment-generating function", "start": 2816.646, "duration": 2.44}, {"text": "doesn't exist, do you mean\nthat it isn't analytic", "start": 2819.086, "duration": 2.44}, {"text": "or it doesn't converge?", "start": 2821.526, "duration": 1.484}, {"text": "PROFESSOR: It\nmight not converge.", "start": 2823.01, "duration": 1.57}, {"text": "So log-normal distribution,\nit does not converge.", "start": 2824.58, "duration": 3.55}, {"text": "So for all non-zero\nt, it does not", "start": 2828.13, "duration": 2.282}, {"text": "converge, for\nlog-normal distribution.", "start": 2830.412, "duration": 1.697}, {"text": "AUDIENCE: [INAUDIBLE]?", "start": 2832.109, "duration": 0.916}, {"text": "PROFESSOR: Here?", "start": 2836.35, "duration": 0.79}, {"text": "Yes.", "start": 2837.14, "duration": 0.5}, {"text": "Pointwise convergence implies\npointwise convergence.", "start": 2837.64, "duration": 2.182}, {"text": "No, no.", "start": 2842.42, "duration": 0.525}, {"text": "Because it's pointwise, this\nconclusion is also rather weak.", "start": 2846.76, "duration": 3.714}, {"text": "It's almost the weakest\nconvergence in distribution.", "start": 2850.474, "duration": 2.166}, {"text": "OK.", "start": 2881.024, "duration": 0.5}, {"text": "The law of large numbers.", "start": 2881.524, "duration": 10.956}, {"text": "So now we're talking about\nlarge-scale behavior.", "start": 2944.1, "duration": 2.84}, {"text": "Let X_1 up to X_n be\nindependent random variables", "start": 2946.94, "duration": 2.69}, {"text": "with identical distribution.", "start": 2949.63, "duration": 1.704}, {"text": "We don't really know\nwhat the distribution is,", "start": 2951.334, "duration": 1.916}, {"text": "but we know that\nthey're all the same.", "start": 2953.25, "duration": 2.02}, {"text": "In short, I'll just refer\nto this condition as i.i.d.", "start": 2955.27, "duration": 3.35}, {"text": "random variables later.", "start": 2958.62, "duration": 3.37}, {"text": "Independent, identically\ndistributed random variables.", "start": 2961.99, "duration": 3.058}, {"text": "And let mean be mu,\nvariance be sigma square.", "start": 2969.04, "duration": 7.49}, {"text": "Let's also define X as the\naverage of n random variables.", "start": 2984.47, "duration": 6.27}, {"text": "Then the probability that--\nX-- for all-- all positive", "start": 2994.59, "duration": 28.396}, {"text": "[INAUDIBLE].", "start": 3022.986, "duration": 0.5}, {"text": "So whenever you have identical\nindependent distributions, when", "start": 3031.59, "duration": 3.51}, {"text": "you take their average, if\nyou take a large enough number", "start": 3035.1, "duration": 3.95}, {"text": "of samples, they will be\nvery close to the mean, which", "start": 3039.05, "duration": 4.38}, {"text": "makes sense.", "start": 3043.43, "duration": 0.714}, {"text": "So what's an example of this?", "start": 3064.42, "duration": 1.85}, {"text": "Before proving it, example\nof this theorem in practice", "start": 3066.27, "duration": 7.74}, {"text": "can be seen in the casino.", "start": 3074.01, "duration": 2.595}, {"text": "So for example, if\nyou're playing blackjack", "start": 3082.53, "duration": 2.59}, {"text": "in a casino, when you're\nplaying against the casino,", "start": 3085.12, "duration": 13.77}, {"text": "you have a very\nsmall disadvantage.", "start": 3098.89, "duration": 3.81}, {"text": "If you're playing at\nthe optimal strategy,", "start": 3102.7, "duration": 9.8}, {"text": "you have-- does anybody\nknow the probability?", "start": 3112.5, "duration": 3.88}, {"text": "It's about 48%, 49%.", "start": 3116.38, "duration": 4.08}, {"text": "About 48% chance of winning.", "start": 3120.46, "duration": 4.06}, {"text": "That means if you bet $1 at\nthe beginning of each round,", "start": 3129.16, "duration": 5.18}, {"text": "the expected amount\nyou'll win is $0.48.", "start": 3134.34, "duration": 8.265}, {"text": "The expected amount that the\ncasino will win is $0.52.", "start": 3142.605, "duration": 5.455}, {"text": "But it's designed so\nthat the variance is", "start": 3148.06, "duration": 2.7}, {"text": "so big that this expectation\nis hidden, the mean is hidden.", "start": 3150.76, "duration": 6.27}, {"text": "From the player's\npoint of view, you only", "start": 3157.03, "duration": 2.36}, {"text": "have a very small sample.", "start": 3159.39, "duration": 2.0}, {"text": "So it looks like the\nmean doesn't matter,", "start": 3161.39, "duration": 3.57}, {"text": "because the variance takes\nover in a very short scale.", "start": 3164.96, "duration": 3.75}, {"text": "But from the casino's\npoint of view,", "start": 3168.71, "duration": 2.02}, {"text": "they're taking a\nvery large n there.", "start": 3170.73, "duration": 3.95}, {"text": "So for each round, let's\nsay from the casino's", "start": 3174.68, "duration": 8.04}, {"text": "point of view, it's\nlike taking, they", "start": 3182.72, "duration": 10.78}, {"text": "are taking enormous value of n.", "start": 3193.5, "duration": 7.02}, {"text": "n here.", "start": 3206.64, "duration": 1.02}, {"text": "And that means as long as they\nhave the slightest advantage,", "start": 3207.66, "duration": 4.72}, {"text": "they'll be winning money,\nand a huge amount of money.", "start": 3212.38, "duration": 2.613}, {"text": "And most games played in the\ncasinos are designed like this.", "start": 3218.24, "duration": 3.45}, {"text": "It looks like the mean\nis really close to 50%,", "start": 3221.69, "duration": 4.04}, {"text": "but it's hidden,\nbecause they designed it", "start": 3225.73, "duration": 2.11}, {"text": "so the variance is big.", "start": 3227.84, "duration": 3.16}, {"text": "But from the casino's\npoint of view,", "start": 3231.0, "duration": 2.18}, {"text": "they have enough\nplayers to play the game", "start": 3233.18, "duration": 1.83}, {"text": "so that the law of large\nnumbers just makes them money.", "start": 3235.01, "duration": 7.11}, {"text": "The moral is, don't\nplay blackjack.", "start": 3247.77, "duration": 1.76}, {"text": "Play poker.", "start": 3252.24, "duration": 3.12}, {"text": "The reason that the rule\nof law of large numbers", "start": 3255.36, "duration": 4.43}, {"text": "doesn't apply, at least\nin this sense, to poker--", "start": 3259.79, "duration": 3.22}, {"text": "can anybody explain why?", "start": 3263.01, "duration": 1.21}, {"text": "It's because poker, you're\nplaying against other players.", "start": 3267.1, "duration": 4.9}, {"text": "If you have an advantage, if\nyour skill-- if you believe", "start": 3272.0, "duration": 4.5}, {"text": "that there is skill in poker--\nif your skill is better", "start": 3276.5, "duration": 2.48}, {"text": "than the other\nplayer by, let's say,", "start": 3278.98, "duration": 2.35}, {"text": "5% chance, then you have\nan edge over that player.", "start": 3281.33, "duration": 5.68}, {"text": "So you can win money.", "start": 3287.01, "duration": 1.0}, {"text": "The only problem is that\nbecause-- poker, you're", "start": 3288.01, "duration": 5.86}, {"text": "not playing against the casino.", "start": 3293.87, "duration": 1.821}, {"text": "Don't play against casino.", "start": 3300.39, "duration": 4.38}, {"text": "But they still\nhave to make money.", "start": 3304.77, "duration": 1.76}, {"text": "So what they do instead\nis they take rake.", "start": 3306.53, "duration": 2.24}, {"text": "So for each round\nthat the players play,", "start": 3308.77, "duration": 3.58}, {"text": "they pay some fee to the casino.", "start": 3312.35, "duration": 3.39}, {"text": "And how the casino makes\nmoney at the poker table", "start": 3315.74, "duration": 4.18}, {"text": "is by accumulating those fees.", "start": 3319.92, "duration": 2.95}, {"text": "They're not taking\nchances there.", "start": 3322.87, "duration": 2.421}, {"text": "But from the player's\npoint of view,", "start": 3325.291, "duration": 1.499}, {"text": "if you're better than the other\nplayer, and the amount of edge", "start": 3326.79, "duration": 5.615}, {"text": "you have over the other\nplayer is larger than the fee", "start": 3332.405, "duration": 3.225}, {"text": "that the casino\ncharges to you, then", "start": 3335.63, "duration": 2.37}, {"text": "now you can apply law of large\nnumbers to yourself and win.", "start": 3338.0, "duration": 3.38}, {"text": "And if you take an\nexample as poker,", "start": 3345.42, "duration": 4.94}, {"text": "it looks like-- OK, I'm\nnot going to play poker.", "start": 3350.36, "duration": 4.012}, {"text": "But if it's a hedge\nfund, or if you're", "start": 3354.372, "duration": 4.948}, {"text": "doing high-frequency trading,\nthat's the moral behind it.", "start": 3359.32, "duration": 5.53}, {"text": "So that's the belief\nyou should have.", "start": 3364.85, "duration": 3.01}, {"text": "You have to believe\nthat you have an edge.", "start": 3367.86, "duration": 2.9}, {"text": "Even if you have a\ntiny edge, if you", "start": 3370.76, "duration": 2.9}, {"text": "can have enough\nnumber of trials,", "start": 3373.66, "duration": 2.74}, {"text": "if you can trade enough of times\nusing some strategy that you", "start": 3376.4, "duration": 4.6}, {"text": "believe is winning over time,\nthen law of large numbers", "start": 3381.0, "duration": 5.58}, {"text": "will take it from there and\nwill bring you money, profit.", "start": 3386.58, "duration": 4.686}, {"text": "Of course, the problem is,\nwhen the variance is big,", "start": 3394.92, "duration": 6.85}, {"text": "your belief starts to fall.", "start": 3401.77, "duration": 3.44}, {"text": "At least, that was the case for\nme when I was playing poker.", "start": 3405.21, "duration": 3.45}, {"text": "Because I believed\nthat I had an edge,", "start": 3408.66, "duration": 2.99}, {"text": "but when there is\nreally swing, it", "start": 3411.65, "duration": 3.87}, {"text": "looks like your\nexpectation is negative.", "start": 3415.52, "duration": 4.16}, {"text": "And that's when you have\nto believe in yourself.", "start": 3419.68, "duration": 2.205}, {"text": "Yeah.", "start": 3425.59, "duration": 2.1}, {"text": "That's when your\nfaith in mathematics", "start": 3427.69, "duration": 1.79}, {"text": "is being challenged.", "start": 3429.48, "duration": 2.449}, {"text": "It really happened.", "start": 3431.929, "duration": 0.791}, {"text": "I hope it doesn't happen to you.", "start": 3435.29, "duration": 2.0}, {"text": "Anyway, that's proof\nlaw of large numbers.", "start": 3437.29, "duration": 5.44}, {"text": "How do you prove it?", "start": 3442.73, "duration": 0.96}, {"text": "The proof is quite easy.", "start": 3443.69, "duration": 1.0}, {"text": "First of all, one observation--\nexpectation of X is just", "start": 3447.84, "duration": 5.1}, {"text": "expectation of 1 over\nn times sum of X_i's.", "start": 3452.94, "duration": 4.7}, {"text": "And that, by linearity,\njust becomes the sum of--", "start": 3461.4, "duration": 11.071}, {"text": "and that's mu.", "start": 3472.471, "duration": 3.412}, {"text": "OK.", "start": 3475.883, "duration": 0.5}, {"text": "That's good.", "start": 3476.383, "duration": 2.934}, {"text": "And then the variance,\nwhat's the variance of X?", "start": 3479.317, "duration": 2.293}, {"text": "That's the expectation\nof X minus mu", "start": 3484.43, "duration": 5.32}, {"text": "square, which is the expectation\nsum over all i's, minus mu", "start": 3489.75, "duration": 11.226}, {"text": "square.", "start": 3500.976, "duration": 0.5}, {"text": "I'll group them.", "start": 3504.344, "duration": 1.916}, {"text": "That's the expectation of 1 over\nn sum of X_i minus mu square.", "start": 3506.26, "duration": 7.324}, {"text": "i is from 1 to n.", "start": 3513.584, "duration": 1.996}, {"text": "What did I do wrong?", "start": 3523.57, "duration": 1.23}, {"text": "1 over n is inside the square.", "start": 3524.8, "duration": 1.81}, {"text": "So I can take it out\nand square, n square.", "start": 3526.61, "duration": 4.11}, {"text": "And then you're summing\nn terms of sigma square.", "start": 3530.72, "duration": 2.94}, {"text": "So that is equal to\nsigma square over n.", "start": 3533.66, "duration": 3.485}, {"text": "That means the\neffect of averaging", "start": 3542.45, "duration": 1.66}, {"text": "n terms does not\naffect your average,", "start": 3544.11, "duration": 4.49}, {"text": "but it affects your variance.", "start": 3548.6, "duration": 1.42}, {"text": "It divides your variance by n.", "start": 3553.51, "duration": 2.292}, {"text": "If you take larger and\nlarger n, your variance", "start": 3555.802, "duration": 3.088}, {"text": "gets smaller and smaller.", "start": 3558.89, "duration": 1.19}, {"text": "And using that, we can\nprove this statement.", "start": 3562.59, "duration": 3.38}, {"text": "There's only one thing\nyou have to notice--", "start": 3565.97, "duration": 1.87}, {"text": "that the probability\nthat x minus mu", "start": 3567.84, "duration": 2.67}, {"text": "is greater than epsilon.", "start": 3570.51, "duration": 2.11}, {"text": "When you multiply this\nby epsilon square.", "start": 3572.62, "duration": 3.22}, {"text": "This will be less than or\nequal to the variance of x.", "start": 3575.84, "duration": 5.39}, {"text": "The reason this\ninequality holds is", "start": 3581.23, "duration": 1.55}, {"text": "because variance X is defined\nas the expectation of X minus mu", "start": 3582.78, "duration": 3.51}, {"text": "square.", "start": 3586.29, "duration": 1.91}, {"text": "For all the events when you have\nX minus mu at least epsilon,", "start": 3588.2, "duration": 4.14}, {"text": "your multiplying\nfactor X square will", "start": 3592.34, "duration": 1.92}, {"text": "be at least epsilon square.", "start": 3594.26, "duration": 2.52}, {"text": "This term will be at\nleast epsilon square", "start": 3596.78, "duration": 3.57}, {"text": "when you fall into this event.", "start": 3600.35, "duration": 3.17}, {"text": "So your variance has\nto be at least that.", "start": 3603.52, "duration": 3.58}, {"text": "And this is known to\nbe sigma square over n.", "start": 3607.1, "duration": 4.871}, {"text": "So probability that\nx minus mu is greater", "start": 3611.971, "duration": 3.733}, {"text": "than epsilon is at most sigma\nsquare over n epsilon squared.", "start": 3615.704, "duration": 6.276}, {"text": "That means if you take n to go\nto infinity, that goes to zero.", "start": 3621.98, "duration": 4.16}, {"text": "So the probability that\nyou deviate from the mean", "start": 3626.14, "duration": 3.45}, {"text": "by more than epsilon goes to 0.", "start": 3629.59, "duration": 3.597}, {"text": "You can actually read out a\nlittle bit more from the proof.", "start": 3633.187, "duration": 2.458}, {"text": "It also tells a little bit\nabout the speed of convergence.", "start": 3638.69, "duration": 2.945}, {"text": "So let's say you have a random\nvariable X. Your mean is 50.", "start": 3644.26, "duration": 5.97}, {"text": "You epsilon is 0.1.", "start": 3650.23, "duration": 3.7}, {"text": "So you want to know\nthe probability", "start": 3653.93, "duration": 1.9}, {"text": "that you deviate from your\nmean by more than 0.1.", "start": 3655.83, "duration": 4.65}, {"text": "Let's say you want\nto be 99% sure.", "start": 3660.48, "duration": 5.53}, {"text": "Want to be 99% sure that X\nminus mu is less than 0.1,", "start": 3666.01, "duration": 8.802}, {"text": "or X minus 50 is less than 0.1.", "start": 3674.812, "duration": 3.308}, {"text": "In that case, what you can do\nis-- you want this to be 0.01.", "start": 3678.12, "duration": 4.94}, {"text": "It has to be 0.01.", "start": 3683.06, "duration": 3.3}, {"text": "So plug in that, plug in your\nvariance, plug in your epsilon.", "start": 3686.36, "duration": 3.44}, {"text": "That will give you\nsome bound on n.", "start": 3689.8, "duration": 2.43}, {"text": "If you have more than\nthat number of trials,", "start": 3692.23, "duration": 1.96}, {"text": "you can be 99% sure that you\ndon't deviate from your mean", "start": 3694.19, "duration": 3.923}, {"text": "by more than epsilon.", "start": 3698.113, "duration": 2.567}, {"text": "So that does give\nsome estimate, but I", "start": 3700.68, "duration": 2.02}, {"text": "should mention that this\nis a very bad estimate.", "start": 3702.7, "duration": 3.45}, {"text": "There are much more\npowerful estimates", "start": 3706.15, "duration": 1.84}, {"text": "that can be done here.", "start": 3707.99, "duration": 0.98}, {"text": "That will give the order of\nmagnitude-- I didn't really", "start": 3708.97, "duration": 1.8}, {"text": "calculate here, but it looks\nlike it's close to millions.", "start": 3710.77, "duration": 2.67}, {"text": "It has to be close to millions.", "start": 3713.44, "duration": 2.46}, {"text": "But in practice, if you use\na lot more powerful tool", "start": 3715.9, "duration": 4.46}, {"text": "of estimating it, it should\nonly be hundreds or at most", "start": 3720.36, "duration": 4.648}, {"text": "thousands.", "start": 3725.008, "duration": 0.5}, {"text": "So the tool you'll use there\nis moment-generating functions,", "start": 3733.46, "duration": 2.5}, {"text": "something similar to\nmoment-generating functions.", "start": 3735.96, "duration": 2.4}, {"text": "But I will not go into it.", "start": 3738.36, "duration": 2.052}, {"text": "Any questions?", "start": 3740.412, "duration": 0.583}, {"text": "OK.", "start": 3743.61, "duration": 1.48}, {"text": "For those who already saw\nlaw of large numbers before,", "start": 3745.09, "duration": 3.462}, {"text": "the name suggests there's\nalso something called", "start": 3748.552, "duration": 1.958}, {"text": "strong law of large numbers.", "start": 3750.51, "duration": 1.74}, {"text": "In that theorem, your\nconclusion is stronger.", "start": 3755.982, "duration": 5.398}, {"text": "So the convergence is stronger\nthan this type of convergence.", "start": 3761.38, "duration": 3.625}, {"text": "And also, the\ncondition I gave here", "start": 3767.81, "duration": 3.8}, {"text": "is a very strong condition.", "start": 3771.61, "duration": 1.97}, {"text": "The same conclusion\nis true even if you", "start": 3773.58, "duration": 2.44}, {"text": "weaken some of the conditions.", "start": 3776.02, "duration": 2.82}, {"text": "So for example, the variance\ndoes not have to exist.", "start": 3778.84, "duration": 2.74}, {"text": "It can be replaced by some\nother condition, and so on.", "start": 3781.58, "duration": 4.9}, {"text": "But here, I just want\nit to be a simple form", "start": 3786.48, "duration": 2.38}, {"text": "so that it's easy to prove.", "start": 3788.86, "duration": 2.49}, {"text": "And you at least get the\nspirit of what's happening.", "start": 3791.35, "duration": 2.924}, {"text": "Now let's move on to the next\ntopic-- central limit theorem.", "start": 3800.48, "duration": 5.66}, {"text": "So weak law of\nlarge numbers says", "start": 3851.24, "duration": 5.64}, {"text": "that if you have IID random\nvariables, 1 over n times", "start": 3856.88, "duration": 5.33}, {"text": "sum over X_i's converges to mu,\nthe mean, in some weak sense.", "start": 3862.21, "duration": 5.19}, {"text": "And the reason it happened\nwas because this had", "start": 3871.21, "duration": 2.52}, {"text": "mean mu and variance\nsigma square over n.", "start": 3873.73, "duration": 5.427}, {"text": "We've exploited the fact that\nvariance vanishes to get this.", "start": 3883.66, "duration": 6.07}, {"text": "So the question is, what\nhappens if you replace 1 over n", "start": 3889.73, "duration": 3.83}, {"text": "by 1 over square root n?", "start": 3893.56, "duration": 1.343}, {"text": "What happens if-- for\nthe random variable", "start": 3899.25, "duration": 5.34}, {"text": "1 over square root n times X_i?", "start": 3904.59, "duration": 3.71}, {"text": "The reason I'm making this\nchoice of 1 over square root n", "start": 3914.18, "duration": 2.81}, {"text": "is because if you\nmake this choice,", "start": 3916.99, "duration": 2.32}, {"text": "now the average has mean mu\nand variance sigma square just", "start": 3919.31, "duration": 7.02}, {"text": "as in X_i's.", "start": 3926.33, "duration": 2.44}, {"text": "So this is the same as X_i.", "start": 3928.77, "duration": 6.211}, {"text": "Then what should it look like?", "start": 3940.91, "duration": 3.42}, {"text": "If the random variable is the\nsame mean and same variance", "start": 3944.33, "duration": 2.4}, {"text": "as your original random\nvariable, the distribution", "start": 3946.73, "duration": 5.39}, {"text": "of this, should it look like\nthe distribution of X_i?", "start": 3952.12, "duration": 2.675}, {"text": "If mean is mu.", "start": 3960.53, "duration": 0.76}, {"text": "Thank you very much.", "start": 3961.29, "duration": 2.88}, {"text": "The case when mean is 0.", "start": 3964.17, "duration": 1.365}, {"text": "OK.", "start": 3973.16, "duration": 0.5}, {"text": "For this special case,\nwill it look like X_i,", "start": 3973.66, "duration": 3.96}, {"text": "or will it not look like X_i?", "start": 3977.62, "duration": 3.2}, {"text": "If it doesn't look like X_i,\ncan we say anything interesting", "start": 3980.82, "duration": 3.44}, {"text": "about the distribution of this?", "start": 3984.26, "duration": 3.33}, {"text": "And central limit theorem\nanswers this question.", "start": 3987.59, "duration": 3.89}, {"text": "When I first saw it, I thought\nit was really interesting.", "start": 3991.48, "duration": 3.5}, {"text": "Because normal\ndistribution comes up here.", "start": 3994.98, "duration": 2.181}, {"text": "And that's probably\none of the reasons", "start": 4000.25, "duration": 1.8}, {"text": "that normal distribution\nis so universal.", "start": 4002.05, "duration": 2.96}, {"text": "Because when you take\nmany independent events", "start": 4005.01, "duration": 5.3}, {"text": "and take the average\nin this sense,", "start": 4010.31, "duration": 2.96}, {"text": "their distribution converges\nto a normal distribution.", "start": 4013.27, "duration": 3.495}, {"text": "Yes?", "start": 4016.765, "duration": 0.5}, {"text": "AUDIENCE: How did you get\nmean equals [INAUDIBLE]?", "start": 4017.265, "duration": 2.395}, {"text": "PROFESSOR: I didn't get it.", "start": 4019.66, "duration": 1.31}, {"text": "I assumed it if X-- yeah.", "start": 4020.97, "duration": 1.708}, {"text": "So theorem: let\nX_1, X_2, to X_n be", "start": 4049.6, "duration": 11.88}, {"text": "IID random variables with mean,\nthis time, mu and variance", "start": 4061.48, "duration": 10.48}, {"text": "sigma squared.", "start": 4071.96, "duration": 3.06}, {"text": "And let X-- or Y_n.", "start": 4075.02, "duration": 4.288}, {"text": "Y_n be square root n times\n1 over n, of X_i minus mu.", "start": 4081.94, "duration": 8.083}, {"text": "Then the distribution\nof Y_n converges", "start": 4104.813, "duration": 16.267}, {"text": "to that of normal distribution\nwith mean 0 and variance sigma.", "start": 4121.08, "duration": 8.976}, {"text": "What this means-- I'll\nwrite it down again--", "start": 4135.05, "duration": 2.3}, {"text": "it means for all x,\nprobability that Y_n", "start": 4137.35, "duration": 4.44}, {"text": "is less than or\nequal to x converges", "start": 4141.79, "duration": 2.0}, {"text": "the probability that normal\ndistribution is less than", "start": 4143.79, "duration": 3.932}, {"text": "or equal to x.", "start": 4147.722, "duration": 1.188}, {"text": "What's really\ninteresting here is,", "start": 4154.14, "duration": 2.08}, {"text": "no matter what distribution\nyou had in the beginning,", "start": 4156.22, "duration": 4.12}, {"text": "if we average it\nout in this sense,", "start": 4160.34, "duration": 3.75}, {"text": "then you converge to\nthe normal distribution.", "start": 4164.09, "duration": 1.875}, {"text": "Any questions about this\nstatement, or any corrections?", "start": 4175.429, "duration": 2.291}, {"text": "Any mistakes that I made?", "start": 4180.49, "duration": 3.055}, {"text": "OK.", "start": 4183.545, "duration": 2.47}, {"text": "Here's the proof.", "start": 4186.015, "duration": 0.988}, {"text": "I will prove it when the\nmoment-generating function", "start": 4190.97, "duration": 3.43}, {"text": "exists.", "start": 4194.4, "duration": 0.5}, {"text": "So assume that the\nmoment-generating functions", "start": 4194.9, "duration": 1.916}, {"text": "exists.", "start": 4196.816, "duration": 1.194}, {"text": "So proof assuming\nm of X_i exists.", "start": 4198.01, "duration": 6.953}, {"text": "So remember that theorem.", "start": 4216.81, "duration": 3.05}, {"text": "Try to recall that\ntheorem where if you", "start": 4219.86, "duration": 2.3}, {"text": "know that the moment-generating\nfunction of Y_n's converges", "start": 4222.16, "duration": 2.97}, {"text": "to the moment-generating\nfunction of the normal, then", "start": 4225.13, "duration": 4.12}, {"text": "we have the statement.", "start": 4229.25, "duration": 0.96}, {"text": "The distribution converges.", "start": 4230.21, "duration": 1.19}, {"text": "So that's the statement\nwe're going to use.", "start": 4231.4, "duration": 2.928}, {"text": "That means our goal is to prove\nthat the moment-generating", "start": 4234.328, "duration": 2.772}, {"text": "function of these Y_n's converge\nto the moment-generating", "start": 4237.1, "duration": 5.92}, {"text": "function of the normal for\nall t, pointwise convergence.", "start": 4243.02, "duration": 8.068}, {"text": "And this part is well known.", "start": 4256.36, "duration": 3.72}, {"text": "I'll just write it down.", "start": 4260.08, "duration": 1.375}, {"text": "It's known to be e to the t\nsquare sigma square over 2.", "start": 4261.455, "duration": 4.639}, {"text": "That just can be computed.", "start": 4268.818, "duration": 2.355}, {"text": "So we want to somehow show that\nthe moment-generating function", "start": 4278.61, "duration": 2.66}, {"text": "of this Y_n converges to that.", "start": 4281.27, "duration": 4.468}, {"text": "The moment-generating\nfunction of Y_n", "start": 4285.738, "duration": 3.702}, {"text": "is equal to expectation\nof e to t Y_n.", "start": 4289.44, "duration": 6.662}, {"text": "e to the t, 1 over square\nroot n, sum of X_i minus mu.", "start": 4302.544, "duration": 7.952}, {"text": "And then because each of\nthe X_i's are independent,", "start": 4314.49, "duration": 3.19}, {"text": "this sum will split\ninto products.", "start": 4317.68, "duration": 1.723}, {"text": "Product of-- let\nme split it better.", "start": 4322.65, "duration": 11.409}, {"text": "Meets the expectation-- we\ndidn't use independent yet.", "start": 4334.059, "duration": 5.181}, {"text": "Sum becomes products of e to\nthe t, 1 over square root n, X_i", "start": 4339.24, "duration": 7.264}, {"text": "minus mu.", "start": 4346.504, "duration": 0.958}, {"text": "And then because\nthey're independent,", "start": 4354.65, "duration": 1.73}, {"text": "this product can go out.", "start": 4356.38, "duration": 1.15}, {"text": "Equal to the product from 1 to\nn expectation e to the t times", "start": 4360.925, "duration": 9.071}, {"text": "square root n--", "start": 4369.996, "duration": 0.988}, {"text": "OK.", "start": 4376.16, "duration": 0.5}, {"text": "Now they're identically\ndistributed,", "start": 4376.66, "duration": 1.499}, {"text": "so you just have to take\nthe n-th power of that.", "start": 4378.159, "duration": 2.741}, {"text": "That's equal to the\nexpectation of e", "start": 4380.9, "duration": 3.023}, {"text": "to the t over square root n,\nX_i minus mu, to the n-th power.", "start": 4383.923, "duration": 7.997}, {"text": "Now we'll do some estimation.", "start": 4391.92, "duration": 3.5}, {"text": "So use the Taylor\nexpansion of this.", "start": 4395.42, "duration": 4.03}, {"text": "What we get is expectation of 1\nplus that, t over square root n", "start": 4399.45, "duration": 10.552}, {"text": "xi minus mu, plus 1 over\n2 factorial, that squared,", "start": 4410.002, "duration": 6.988}, {"text": "t over square root n,\nxi minus mu squared,", "start": 4416.99, "duration": 6.77}, {"text": "plus 1 over 3 factorial,\nthat cubed plus so on.", "start": 4423.76, "duration": 4.988}, {"text": "Then that's equal to 1--\nAh, to the n-th power.", "start": 4435.05, "duration": 2.94}, {"text": "The linearity of\nexpectation, 1 comes out.", "start": 4442.92, "duration": 3.97}, {"text": "Second term is 0,\nbecause X_i have mean mu.", "start": 4446.89, "duration": 5.94}, {"text": "So that disappears.", "start": 4452.83, "duration": 2.19}, {"text": "This term-- we have 1 over 2,\nt squared over n, X_i minus mu", "start": 4455.02, "duration": 11.91}, {"text": "square.", "start": 4466.93, "duration": 2.44}, {"text": "X_i minus mu square, when\nyou take expectation,", "start": 4469.37, "duration": 2.22}, {"text": "that will be sigma square.", "start": 4471.59, "duration": 3.96}, {"text": "And then the terms after\nthat, because we're", "start": 4475.55, "duration": 4.17}, {"text": "only interested in\nproving that for fixed t,", "start": 4479.72, "duration": 3.13}, {"text": "this converges-- so we're only\nproving pointwise convergence.", "start": 4482.85, "duration": 3.31}, {"text": "You may consider t\nas a fixed number.", "start": 4486.16, "duration": 2.87}, {"text": "So as n goes to infinity--\nif n is really, really large,", "start": 4489.03, "duration": 3.51}, {"text": "all these terms will be\nsmaller order of magnitude", "start": 4492.54, "duration": 4.19}, {"text": "than n, 1 over n.", "start": 4496.73, "duration": 4.1}, {"text": "Something like that happens.", "start": 4500.83, "duration": 1.44}, {"text": "And that's happening\nbecause we're fixed.", "start": 4508.53, "duration": 2.72}, {"text": "For fixed t, we\nhave to prove it.", "start": 4511.25, "duration": 3.01}, {"text": "So if we're saying\nsomething uniformly about t,", "start": 4514.26, "duration": 2.032}, {"text": "that's no longer true.", "start": 4516.292, "duration": 2.098}, {"text": "Now we go back to\nthe exponential form.", "start": 4518.39, "duration": 2.67}, {"text": "So this is pretty much\njust e to that term,", "start": 4521.06, "duration": 5.48}, {"text": "1 over 2 t square\nsigma square over n", "start": 4526.54, "duration": 4.36}, {"text": "plus little o of 1 over\nn to the n-th power.", "start": 4530.9, "duration": 6.47}, {"text": "Now, that n can be\nmultiplied to cancel out.", "start": 4537.37, "duration": 5.61}, {"text": "And we see that it's e to t\nsquare sigma square over 2", "start": 4542.98, "duration": 3.66}, {"text": "plus the little o of 1.", "start": 4546.64, "duration": 1.702}, {"text": "So if you take n\nto go to infinity,", "start": 4548.342, "duration": 2.028}, {"text": "that term disappears,\nand we prove", "start": 4550.37, "duration": 5.47}, {"text": "that it converges to that.", "start": 4555.84, "duration": 1.57}, {"text": "And then by the theorem that I\nstated before, if we have this,", "start": 4560.1, "duration": 4.416}, {"text": "we know that the\ndistribution converges.", "start": 4564.516, "duration": 1.666}, {"text": "Any questions?", "start": 4569.88, "duration": 0.62}, {"text": "OK.", "start": 4573.76, "duration": 0.5}, {"text": "I'll make one final remark.", "start": 4574.26, "duration": 1.255}, {"text": "So suppose there is a random\nvariable x whose mean we do not", "start": 4589.009, "duration": 13.631}, {"text": "know, whose mean is unknown.", "start": 4602.64, "duration": 2.225}, {"text": "Our goal is to\nestimate the mean.", "start": 4613.67, "duration": 2.04}, {"text": "And one way to do that is by\ntaking many independent trials", "start": 4618.97, "duration": 3.76}, {"text": "of this random variable.", "start": 4622.73, "duration": 2.49}, {"text": "So take independent trials X_1,\nX_2, to X_n, and use 1 over--", "start": 4625.22, "duration": 16.46}, {"text": "X_1 plus...", "start": 4641.68, "duration": 0.57}, {"text": "X_n as our estimator.", "start": 4642.25, "duration": 1.315}, {"text": "Then the law of large\nnumbers says that this", "start": 4652.96, "duration": 2.03}, {"text": "will be very close to the mean.", "start": 4654.99, "duration": 1.76}, {"text": "So if you take n\nto be large enough,", "start": 4656.75, "duration": 3.09}, {"text": "you will more than likely\nhave some value which", "start": 4659.84, "duration": 2.26}, {"text": "is very close to the mean.", "start": 4662.1, "duration": 2.09}, {"text": "And then the central\nlimit theorem", "start": 4664.19, "duration": 2.86}, {"text": "tells you how the\ndistribution of this variable", "start": 4667.05, "duration": 6.48}, {"text": "is around the mean.", "start": 4673.53, "duration": 2.385}, {"text": "So we don't know what\nthe real value is,", "start": 4675.915, "duration": 2.005}, {"text": "but we know that\nthe distribution", "start": 4677.92, "duration": 2.7}, {"text": "of the value that\nwe will obtain here", "start": 4680.62, "duration": 2.36}, {"text": "is something like\nthat around the mean.", "start": 4682.98, "duration": 2.068}, {"text": "And because normal distribution\nhave very small tails,", "start": 4689.34, "duration": 7.74}, {"text": "the tail distributions\nis really small,", "start": 4697.08, "duration": 4.82}, {"text": "we will get really\nclose really fast.", "start": 4701.9, "duration": 2.05}, {"text": "And this is known as the maximum\nlikelihood estimator, is it?", "start": 4707.29, "duration": 7.097}, {"text": "OK, yeah.", "start": 4717.67, "duration": 0.64}, {"text": "For some distributions,\nit's better", "start": 4718.31, "duration": 1.67}, {"text": "to take some other estimator.", "start": 4719.98, "duration": 4.1}, {"text": "Which is quite interesting.", "start": 4724.08, "duration": 3.2}, {"text": "At least my intuition is to\ntake this for every single case,", "start": 4727.28, "duration": 2.735}, {"text": "looks like that will\nbe a good choice.", "start": 4730.015, "duration": 2.875}, {"text": "But it turns out that\nthat's not the case;", "start": 4732.89, "duration": 1.79}, {"text": "for some distributions there's\na better choice than this.", "start": 4734.68, "duration": 4.812}, {"text": "And Peter will\nlater talk about it.", "start": 4739.492, "duration": 3.718}, {"text": "If you're interested\nin, come back.", "start": 4746.34, "duration": 3.62}, {"text": "And that's it for\ntoday, any questions?", "start": 4749.96, "duration": 4.0}, {"text": "So next Tuesday we will\nhave an outside speaker,", "start": 4753.96, "duration": 3.915}, {"text": "and it will be on bonds.", "start": 4757.875, "duration": 3.381}, {"text": "and I don't think anything from\nlinear algebra will be here.", "start": 4761.256, "duration": 3.627}]