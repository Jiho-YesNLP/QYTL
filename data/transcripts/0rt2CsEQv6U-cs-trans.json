[{"text": "Okay, hey everyone. So welcome to the final week of the class.", "start": 4.34, "duration": 7.785}, {"text": "Uh, what I wanna do today,", "start": 12.125, "duration": 1.755}, {"text": "is share with you a few generalizations of,", "start": 13.88, "duration": 3.81}, {"text": "um, reinforcement learning and of MDPs.", "start": 17.69, "duration": 3.005}, {"text": "So you've learned about the basic MDP formulas of state action,", "start": 20.695, "duration": 4.965}, {"text": "state transition probability, discount factor and rewards.", "start": 25.66, "duration": 3.12}, {"text": "Um, the first thing you see today is two, you know,", "start": 28.78, "duration": 3.89}, {"text": "slight generalizations of this framework to", "start": 32.67, "duration": 2.5}, {"text": "state-action rewards and to the finite horizon MDPs.", "start": 35.17, "duration": 2.73}, {"text": "They're making it a little bit easier for you to model certain types of problems,", "start": 37.9, "duration": 4.17}, {"text": "certain types of robots,", "start": 42.07, "duration": 1.02}, {"text": "so certain types of factory automation problems will be easier to", "start": 43.09, "duration": 3.28}, {"text": "model with these two small generalizations.", "start": 46.37, "duration": 3.45}, {"text": "So we'll talk about those first, and then second,", "start": 49.82, "duration": 2.79}, {"text": "we'll talk about linear dynamical systems.", "start": 52.61, "duration": 3.485}, {"text": "Last Wednesday, you saw a fitted value iteration which was a way", "start": 56.095, "duration": 4.825}, {"text": "to solve for an MDP even when the state-space may be infinite,", "start": 60.92, "duration": 5.52}, {"text": "even when the state space is a set of", "start": 66.44, "duration": 2.25}, {"text": "real numbers known as RN so it's an infinite list of states.", "start": 68.69, "duration": 2.86}, {"text": "So a continuous set states,", "start": 71.55, "duration": 1.36}, {"text": "we use fitted value iteration in which we had to use a function approximator,", "start": 72.91, "duration": 3.985}, {"text": "right, like linear regression,", "start": 76.895, "duration": 1.32}, {"text": "to try to approximate the value function.", "start": 78.215, "duration": 2.52}, {"text": "There's one very important special case of an MDP where even", "start": 80.735, "duration": 4.305}, {"text": "if the state space is infinite or continuous real numbers,", "start": 85.04, "duration": 5.08}, {"text": "uh, there's one important special case where you can still", "start": 90.12, "duration": 3.785}, {"text": "compute the value function exactly without needing to use,", "start": 93.905, "duration": 4.115}, {"text": "you know, like a linear function approximator or to use something like", "start": 98.02, "duration": 2.71}, {"text": "linear regression in the inner loop of fitted value iteration.", "start": 100.73, "duration": 3.805}, {"text": "Um, and so you also see that today,", "start": 104.535, "duration": 2.355}, {"text": "and when you can take a robot or", "start": 106.89, "duration": 2.51}, {"text": "some factory automation task or whatever problem and model it in this framework,", "start": 109.4, "duration": 4.545}, {"text": "it turns out to be incredibly efficient because you can fit a continuous- fit", "start": 113.945, "duration": 4.12}, {"text": "a value function as a function of the states without needing to approximate anything,", "start": 118.065, "duration": 5.015}, {"text": "just compute the exact value function,", "start": 123.08, "duration": 2.11}, {"text": "uh, even though the state space is continuous.", "start": 125.19, "duration": 2.42}, {"text": "So, um, this is a framework that doesn't apply to all problems,", "start": 127.61, "duration": 3.37}, {"text": "but when it does apply,", "start": 130.98, "duration": 1.26}, {"text": "it's incredibly convenient and incredibly efficient.", "start": 132.24, "duration": 2.39}, {"text": "So you see that in the second half of today.", "start": 134.63, "duration": 3.435}, {"text": "Um, uh, yes.", "start": 138.065, "duration": 2.215}, {"text": "Uh, uh, one, one tactical uh,", "start": 140.28, "duration": 2.16}, {"text": "two, two tactical things, um, let's see,", "start": 142.44, "duration": 2.265}, {"text": "from the questions that we're getting from students,", "start": 144.705, "duration": 2.345}, {"text": "some students are asking us, uh,", "start": 147.05, "duration": 1.215}, {"text": "how is grading in CS229?", "start": 148.265, "duration": 1.605}, {"text": "Whatever I did well and this,", "start": 149.87, "duration": 1.155}, {"text": "you know, didn't do so well in that.", "start": 151.025, "duration": 1.56}, {"text": "Um, for people taking the class,", "start": 152.585, "duration": 2.175}, {"text": "pass-fail, a C minus or better is a passing grade.", "start": 154.76, "duration": 2.955}, {"text": "This is quite- I think this is standard at Stanford.", "start": 157.715, "duration": 2.57}, {"text": "Uh, and, um, I think CS229 has historically been one of the heavy workload classes.", "start": 160.285, "duration": 5.545}, {"text": "We know that people taking CS229- yeah, I see a few heads nodding.", "start": 165.83, "duration": 3.09}, {"text": "[LAUGHTER].", "start": 168.92, "duration": 1.89}, {"text": "I said, sorry, people, uh, uh,", "start": 170.81, "duration": 2.75}, {"text": "taking CS229 end up, you know,", "start": 173.56, "duration": 2.18}, {"text": "putting a lot of work in this class more,", "start": 175.74, "duration": 1.6}, {"text": "maybe frankly more than average for even Stanford classes.", "start": 177.34, "duration": 3.16}, {"text": "And so we've usually been quite nice.", "start": 180.5, "duration": 2.22}, {"text": "With respect to, to grading partly, and we acknowledge that.", "start": 182.72, "duration": 3.51}, {"text": "So I think, uh, uh, yeah I just thought that as well so don't,", "start": 186.23, "duration": 3.1}, {"text": "don't, don't, don't sweat too much.", "start": 189.33, "duration": 1.68}, {"text": "Do work hard for the final project,", "start": 191.01, "duration": 1.41}, {"text": "but just don't, don't sweat too much.", "start": 192.42, "duration": 1.95}, {"text": "Um, uh and, uh,", "start": 194.37, "duration": 2.4}, {"text": "on Wednesday after class,", "start": 196.77, "duration": 1.38}, {"text": "I had a funny question.", "start": 198.15, "duration": 1.13}, {"text": "After I talked about the fitted value iteration question,", "start": 199.28, "duration": 2.52}, {"text": "someone came up to me and said, \"Hey Andrew,", "start": 201.8, "duration": 1.59}, {"text": "um, you know, this algorithm you,", "start": 203.39, "duration": 2.115}, {"text": "you just taught us, does it actually work?", "start": 205.505, "duration": 1.975}, {"text": "Like it- does it actually work on autonomous helicopter?\"", "start": 207.48, "duration": 2.53}, {"text": "And the answer is yes.", "start": 210.01, "duration": 1.195}, {"text": "Uh, the algorithms I'm teaching,", "start": 211.205, "duration": 2.22}, {"text": "you know, if you, uh, uh,", "start": 213.425, "duration": 2.065}, {"text": "the fitted evaluation as you learned last week,", "start": 215.49, "duration": 2.12}, {"text": "it will work on a finite autonomous helicopter at low speeds.", "start": 217.61, "duration": 2.58}, {"text": "So if you fly very high speeds, very dynamic maneuvers,", "start": 220.19, "duration": 3.15}, {"text": "crazy bang, flipping upside down you,", "start": 223.34, "duration": 1.83}, {"text": "you need a bit more than that,", "start": 225.17, "duration": 1.47}, {"text": "but for flying a helicopter at low speeds the,", "start": 226.64, "duration": 2.5}, {"text": "the exact algorithm that you learned, uh,", "start": 229.14, "duration": 2.02}, {"text": "last Wednesday as well as any of the algorithms you'll learn today including, uh, LQR.", "start": 231.16, "duration": 5.65}, {"text": "You know, if you actually ever need to fly an autonomous helicopter for real,", "start": 236.81, "duration": 3.42}, {"text": "these algorithms will actually work.", "start": 240.23, "duration": 1.37}, {"text": "These simply will work quite well for flying a helicopter at low speeds,", "start": 241.6, "duration": 3.865}, {"text": "maybe not at very,", "start": 245.465, "duration": 1.035}, {"text": "very high speeds and crazy dynamic maneuvers.", "start": 246.5, "duration": 2.49}, {"text": "But at those low speeds these algorithms,", "start": 248.99, "duration": 1.605}, {"text": "pretty much as I'm presenting them,", "start": 250.595, "duration": 1.68}, {"text": "will work. So, um, okay.", "start": 252.275, "duration": 4.025}, {"text": "So the first generalization to the MDP framework that I want to describe is,", "start": 256.3, "duration": 7.66}, {"text": "um, state-action rewards. Um, and so, um,", "start": 263.96, "duration": 16.27}, {"text": "so far we've had the rewards be a function", "start": 280.23, "duration": 2.749}, {"text": "mapping from the states to the set of real numbers,", "start": 282.979, "duration": 4.196}, {"text": "and with state action rewards- um,", "start": 287.175, "duration": 4.155}, {"text": "this is a slight modification to the MDP formalism.", "start": 291.33, "duration": 3.785}, {"text": "Where now, the reward function R is", "start": 295.115, "duration": 3.375}, {"text": "a function mapping from states and actions to the rewards.", "start": 298.49, "duration": 4.44}, {"text": "Um, and so, you know,", "start": 302.93, "duration": 1.425}, {"text": "in an MDP you start from the state S0,", "start": 304.355, "duration": 2.745}, {"text": "you take an action a0,", "start": 307.1, "duration": 1.845}, {"text": "then based on that, you get S1, take an action a1,", "start": 308.945, "duration": 3.105}, {"text": "take a state S2, uh,", "start": 312.05, "duration": 2.415}, {"text": "get to a state S2, take an action a2 and so on.", "start": 314.465, "duration": 2.89}, {"text": "And with the state-action rewards,", "start": 317.355, "duration": 2.13}, {"text": "the total payoff is written like this.", "start": 319.485, "duration": 8.46}, {"text": "Um, and this is, uh, this, this,", "start": 327.945, "duration": 2.565}, {"text": "this allows you to model that different actions may have different costs.", "start": 330.51, "duration": 4.76}, {"text": "Uh, for example, in the little robot wandering around the maze example,", "start": 335.27, "duration": 4.56}, {"text": "um, maybe it's more costly for the robot to move than to stay still.", "start": 339.83, "duration": 4.47}, {"text": "And so, uh, if you have an action for the robot to stay", "start": 344.3, "duration": 2.7}, {"text": "still the reward can be, you know, 0,", "start": 347.0, "duration": 2.97}, {"text": "for staying still and a slight negative reward for moving because you're burning,", "start": 349.97, "duration": 3.41}, {"text": "uh, because, because you're using electricity.", "start": 353.38, "duration": 2.585}, {"text": "Um, right, uh, and so in that case,", "start": 355.965, "duration": 7.875}, {"text": "uh, Bellman's equation becomes this, V star equals,", "start": 367.12, "duration": 5.729}, {"text": "right. Um, where now,", "start": 385.61, "duration": 5.81}, {"text": "you still break down the value of a state as the sum of the immediate reward plus the,", "start": 395.92, "duration": 7.35}, {"text": "you know, expected future rewards.", "start": 403.27, "duration": 2.94}, {"text": "But now, the immediate reward you get depends on", "start": 406.82, "duration": 3.66}, {"text": "the action that you take in the current state, right?", "start": 410.48, "duration": 3.33}, {"text": "So this is a- and so this is Bellman's equations.", "start": 413.81, "duration": 3.16}, {"text": "And if, uh, and notice that previously,", "start": 416.97, "duration": 2.909}, {"text": "um, we had the max kind of over here,", "start": 419.879, "duration": 3.406}, {"text": "but now you need to choose the action,", "start": 423.285, "duration": 2.2}, {"text": "a, that maximizes your immediate reward plus your discounted future reward,", "start": 425.485, "duration": 4.355}, {"text": "which is why the max kind of moved right.", "start": 429.84, "duration": 2.68}, {"text": "If you look at the equation,", "start": 432.52, "duration": 1.205}, {"text": "uh, if you look at this equation,", "start": 433.725, "duration": 1.315}, {"text": "I guess the max had to move outside because now the immediate reward you get,", "start": 435.04, "duration": 3.97}, {"text": "uh, depends on the action you choose at this step in time as well.", "start": 439.01, "duration": 3.63}, {"text": "So these models that different actions,", "start": 442.64, "duration": 2.41}, {"text": "um, may have different costs. Yeah.", "start": 445.05, "duration": 2.16}, {"text": "[inaudible]", "start": 447.21, "duration": 6.51}, {"text": "Uh, yes. Uh, yes.", "start": 453.72, "duration": 1.38}, {"text": "Yes, this max applies to the entire expression, right, yeah.", "start": 455.1, "duration": 5.19}, {"text": "[inaudible]", "start": 460.29, "duration": 11.7}, {"text": "Uh, let's see. So in this formulation,", "start": 471.99, "duration": 2.22}, {"text": "reward is determined based on the state and action,", "start": 474.21, "duration": 3.15}, {"text": "yes that is correct.", "start": 477.36, "duration": 1.05}, {"text": "So, um, in this formulation,", "start": 478.41, "duration": 2.61}, {"text": "the reward depends on the, uh,", "start": 481.02, "duration": 2.035}, {"text": "current state and the current action but on the next state you get to.", "start": 483.055, "duration": 4.175}, {"text": "Right, um, oh, and by the way, there, there are multiple variations of formulations of MDPs,", "start": 490.18, "duration": 4.81}, {"text": "but this is, um,", "start": 494.99, "duration": 1.25}, {"text": "one convenient one. I guess.", "start": 496.24, "duration": 1.55}, {"text": "The model that different costs and I think the- and,", "start": 497.79, "duration": 2.885}, {"text": "and actually- and you find in a helicopter a common, um,", "start": 500.675, "duration": 4.05}, {"text": "formulation of this would be to say that yanking aggressive on the control stick, uh,", "start": 504.725, "duration": 5.37}, {"text": "should be assigned a higher costs because yanking", "start": 510.095, "duration": 3.135}, {"text": "the control stick aggressively causes your helicopter to jerk around more,", "start": 513.23, "duration": 3.69}, {"text": "and so maybe you want to penalize that by setting", "start": 516.92, "duration": 2.91}, {"text": "reward function that penalizes very aggressive maneuvers.", "start": 519.83, "duration": 4.62}, {"text": "The, the- this gives you the, uh, as a,", "start": 524.45, "duration": 3.13}, {"text": "as a problem designer, um,", "start": 527.58, "duration": 2.1}, {"text": "uh, sort of more flexibility.", "start": 529.68, "duration": 2.98}, {"text": "Um, and then, uh,", "start": 532.66, "duration": 2.6}, {"text": "uh, and then finally- so let me just write this on top.", "start": 535.26, "duration": 3.015}, {"text": "In this formulation, um,", "start": 538.275, "duration": 2.235}, {"text": "the optimal action- so,", "start": 540.51, "duration": 3.13}, {"text": "uh, right, so in order to compute the value function,", "start": 543.64, "duration": 3.31}, {"text": "you can still use value iteration, right, which is still,", "start": 546.95, "duration": 7.98}, {"text": "you know V of S gets updated as basically the right-hand side from Bellman's equations.", "start": 554.93, "duration": 6.335}, {"text": "So, um, value iteration works just fine for the state-action reward formulation as well.", "start": 561.265, "duration": 5.605}, {"text": "And, uh, if you apply value iteration until V converges to V star,", "start": 566.87, "duration": 5.74}, {"text": "then the optimal action is,", "start": 572.61, "duration": 3.605}, {"text": "um, is, is just the argmax of that thing, right?", "start": 576.215, "duration": 6.235}, {"text": "So, so, pi star is just the,", "start": 590.09, "duration": 2.8}, {"text": "uh, argmax of this thing.", "start": 592.89, "duration": 1.755}, {"text": "Where now, when you're given state,", "start": 594.645, "duration": 2.385}, {"text": "you wanna choose the action that maximizes", "start": 597.03, "duration": 2.18}, {"text": "your immediate reward plus your expected future rewards.", "start": 599.21, "duration": 2.91}, {"text": "Okay. Yeah, so I think just maybe another example, um,", "start": 602.12, "duration": 7.81}, {"text": "if you want to use an MDP to, um, um,", "start": 609.93, "duration": 3.82}, {"text": "plan a shortest route for robot to,", "start": 613.75, "duration": 2.825}, {"text": "say drive from here in Stanford,", "start": 616.575, "duration": 1.925}, {"text": "to drive up to San Francisco, right?", "start": 618.5, "duration": 2.545}, {"text": "Then, if it cost different amounts to drive", "start": 621.045, "duration": 2.855}, {"text": "on different road segments because of traffic or because of the, uh,", "start": 623.9, "duration": 3.03}, {"text": "speed limit on different road, then this allows you to say that, well driving", "start": 626.93, "duration": 3.96}, {"text": "this distance on this road costs this much in terms of fuel consumption or in terms of,", "start": 630.89, "duration": 4.73}, {"text": "uh, time and so on, right?", "start": 635.62, "duration": 1.69}, {"text": "So the state action rewards. Or, or in factory maintenance,", "start": 637.31, "duration": 8.78}, {"text": "uh, if you send in a team to maintain", "start": 646.09, "duration": 2.98}, {"text": "a machine that has a certain cost versus if you do nothing that has a different cost.", "start": 649.07, "duration": 3.75}, {"text": "But then the machine breaks down that has yet another cost depending on your actions.", "start": 652.82, "duration": 3.69}, {"text": "Okay, so that's the first generalization.", "start": 656.51, "duration": 4.48}, {"text": "Um, the second generalization is the finite horizon MDP.", "start": 660.99, "duration": 9.72}, {"text": "Um, and in a finite horizon MDP, um,", "start": 670.71, "duration": 5.92}, {"text": "we're going to replace the discount factor, Gamma,", "start": 679.8, "duration": 4.78}, {"text": "with a horizon time,", "start": 684.58, "duration": 3.01}, {"text": "T. Uh, and- and we'll- we'll just forget about the discount vector.", "start": 688.56, "duration": 6.145}, {"text": "And in a finite horizon, um, MDP,", "start": 694.705, "duration": 3.435}, {"text": "the MDP will run for,", "start": 698.14, "duration": 4.625}, {"text": "um, a fi- a finite number of T steps.", "start": 702.765, "duration": 3.085}, {"text": "You start with state S_0,", "start": 705.85, "duration": 1.095}, {"text": "take an action a_0, get to S_1, take action a_1,", "start": 706.945, "duration": 3.81}, {"text": "get to state S_T take an action a_T,", "start": 710.755, "duration": 3.015}, {"text": "at time step T and then the world ends,", "start": 713.77, "duration": 2.76}, {"text": "and then we're done, right?", "start": 716.53, "duration": 1.5}, {"text": "So the payoff is", "start": 718.03, "duration": 1.77}, {"text": "this finite sum and-", "start": 719.8, "duration": 8.28}, {"text": "and there's just a full stop at the end of that.", "start": 728.08, "duration": 2.7}, {"text": "Um, you can also apply discounting but usually when you have a finite horizon MDP,", "start": 730.78, "duration": 4.875}, {"text": "maybe there's no need to apply discounting,", "start": 735.655, "duration": 2.37}, {"text": "and so, um, this model is a problem where there are,", "start": 738.025, "duration": 4.35}, {"text": "you know, T time steps and then the world ends after that, right?", "start": 742.375, "duration": 3.645}, {"text": "Wo- world end sounds a bit dire.", "start": 746.02, "duration": 2.355}, {"text": "But, uh, um, yeah,", "start": 748.375, "duration": 1.62}, {"text": "if you're flying an airplane or you're flying a helicopter,", "start": 749.995, "duration": 2.295}, {"text": "and you know you only have fuel,", "start": 752.29, "duration": 2.235}, {"text": "you know, for 30 minutes, right?", "start": 754.525, "duration": 1.635}, {"text": "Uh, er, or an RC helicopter,", "start": 756.16, "duration": 1.875}, {"text": "let say you have 20, 30 minutes of fuel,", "start": 758.035, "duration": 1.95}, {"text": "then you know that you're going to run this thing", "start": 759.985, "duration": 2.415}, {"text": "for 30 minutes and then you're done and so", "start": 762.4, "duration": 2.4}, {"text": "the goal is to accumulate as many rewards as possible up until you,", "start": 764.8, "duration": 4.98}, {"text": "you know, run out of fuel and then you have to land, right?", "start": 769.78, "duration": 2.85}, {"text": "So that'll be an example of a finite horizon MDP.", "start": 772.63, "duration": 3.525}, {"text": "Now, um, and- and- and the goal is to maximize this payoff,", "start": 776.155, "duration": 7.245}, {"text": "um, or the expected payoff over these T time steps, okay?", "start": 783.4, "duration": 7.92}, {"text": "Now, one interesting, uh,", "start": 791.32, "duration": 3.615}, {"text": "property of a finite horizon- of, of,", "start": 794.935, "duration": 3.125}, {"text": "of a finite horizon MDP is that the action you take,", "start": 798.06, "duration": 3.97}, {"text": "um, may depend on what time it is on the clock, right?", "start": 802.03, "duration": 3.06}, {"text": "So there's a clock marching from,", "start": 805.09, "duration": 2.04}, {"text": "you know, timestep 0 to timestep T whereupon,", "start": 807.13, "duration": 2.58}, {"text": "right, the world ends whe- whereupon that's all the rewards the MDP is trying to collect.", "start": 809.71, "duration": 5.13}, {"text": "And one interesting effect of this is that,", "start": 814.84, "duration": 2.625}, {"text": "um, this pen isn't that great, is that,", "start": 817.465, "duration": 4.44}, {"text": "um, the optimal action may depend on what,", "start": 821.905, "duration": 7.98}, {"text": "uh, what the time is on the clock.", "start": 829.885, "duration": 2.34}, {"text": "So, uh, let's say your robot is running around this maze and there's", "start": 832.225, "duration": 4.26}, {"text": "a small plus 1 reward here and a much larger plus 10 reward there,", "start": 836.485, "duration": 6.885}, {"text": "and, um, let's say your robot is here, right,", "start": 843.37, "duration": 5.415}, {"text": "then the optimal action for whether you go left or go", "start": 848.785, "duration": 3.135}, {"text": "right will depend on how much time you have left on the clock.", "start": 851.92, "duration": 3.27}, {"text": "If you have only, you know,", "start": 855.19, "duration": 1.41}, {"text": "two or three times steps left on the clock,", "start": 856.6, "duration": 1.935}, {"text": "it's better to just rush and get the plus 1.", "start": 858.535, "duration": 2.73}, {"text": "But we still have, you know,", "start": 861.265, "duration": 1.53}, {"text": "10, 20 ticks left on the clock,", "start": 862.795, "duration": 1.785}, {"text": "then you should just go and get the plus 10 reward, right?", "start": 864.58, "duration": 4.35}, {"text": "And so in this example,", "start": 868.93, "duration": 2.205}, {"text": "Pi star of S, um,", "start": 871.135, "duration": 3.39}, {"text": "it's not well defined because well,", "start": 874.525, "duration": 2.43}, {"text": "the- the optimal action to take when your robot is here in this stage,", "start": 876.955, "duration": 3.975}, {"text": "should you go left or should you go right?", "start": 880.93, "duration": 1.62}, {"text": "Um, it actually depends on what time it is on the clock,", "start": 882.55, "duration": 4.155}, {"text": "and so Pi star in this example, um,", "start": 886.705, "duration": 3.225}, {"text": "should be written as the Pi star subscript T of S,", "start": 889.93, "duration": 4.29}, {"text": "uh, because the optimal action,", "start": 894.22, "duration": 2.775}, {"text": "um, depends on what time T it is.", "start": 896.995, "duration": 2.805}, {"text": "The technical term for this is that this is a non-stationary- non-stationary policy.", "start": 899.8, "duration": 10.335}, {"text": "Um, a non-stationary means,", "start": 910.135, "duration": 2.025}, {"text": "uh, it depends on the time,", "start": 912.16, "duration": 1.41}, {"text": "a- as it changes over time, right.", "start": 913.57, "duration": 7.815}, {"text": "Whereas in contrast, up until now, we've been seeing,", "start": 921.385, "duration": 4.29}, {"text": "you know, Pi star of S is the optimal policy before we- before this formula, right,", "start": 925.675, "duration": 4.785}, {"text": "we just said Pi star of S,", "start": 930.46, "duration": 1.425}, {"text": "and that was a stationary policy and stationary means, uh,", "start": 931.885, "duration": 6.165}, {"text": "there's no change over time, okay?", "start": 938.05, "duration": 8.49}, {"text": "So one- one- one thing that, um,", "start": 946.54, "duration": 1.695}, {"text": "I didn't quite prove but that was implicit was", "start": 948.235, "duration": 2.595}, {"text": "that the optimal action you take in the original formulation,", "start": 950.83, "duration": 3.54}, {"text": "uh, is the same action,", "start": 954.37, "duration": 2.384}, {"text": "right, no matter what time it is in the MDP.", "start": 956.754, "duration": 2.991}, {"text": "So in the original formulation that you saw last week,", "start": 959.745, "duration": 3.465}, {"text": "the optimal policy was stationary,", "start": 963.21, "duration": 2.22}, {"text": "meaning that the optimal policy is the same policy,", "start": 965.43, "duration": 2.615}, {"text": "you know, no matter what time it is,", "start": 968.045, "duration": 1.505}, {"text": "it doesn't change over time.", "start": 969.55, "duration": 1.335}, {"text": "Whereas in the final horizon MDP setting, um,", "start": 970.885, "duration": 3.6}, {"text": "the optimal policy, you know,", "start": 974.485, "duration": 1.475}, {"text": "the optimal action changes over time and so this is a non-stationary policy.", "start": 975.96, "duration": 3.91}, {"text": "So stationary versus non-stationary just means,", "start": 979.87, "duration": 2.04}, {"text": "does it change over time or does it not change over time?", "start": 981.91, "duration": 2.55}, {"text": "Okay? So, um, right.", "start": 984.46, "duration": 7.095}, {"text": "If you're using a non-stationary policy anyway, uh,", "start": 991.555, "duration": 4.995}, {"text": "you can also build an MDP with", "start": 996.55, "duration": 2.58}, {"text": "non-stationary transition probabilities or non-stationary rewards- non-stationary.", "start": 999.13, "duration": 6.12}, {"text": "Um, actually so maybe here's an example.", "start": 1005.25, "duration": 9.96}, {"text": "Um, let's say you're driving from campus from", "start": 1015.21, "duration": 2.67}, {"text": "Palo Alto to San Francisco and we know that rush,", "start": 1017.88, "duration": 3.9}, {"text": "hour, is that- what like 5:00 PM or 6:00 PM or something, right?", "start": 1021.78, "duration": 3.3}, {"text": "And- and- and maybe- maybe the weather forecasts", "start": 1025.08, "duration": 1.86}, {"text": "even says it's going to rain at 6:00 PM or something, right?", "start": 1026.94, "duration": 2.13}, {"text": "But so you know that the dynamics of how you", "start": 1029.07, "duration": 2.85}, {"text": "drive your car from here to San Francisco will change over time,", "start": 1031.92, "duration": 3.51}, {"text": "uh, as in the time it takes, you know,", "start": 1035.43, "duration": 2.25}, {"text": "to drive on a certain segment of the road,", "start": 1037.68, "duration": 1.875}, {"text": "is a function of time and if you want to build an MDP to solve for,", "start": 1039.555, "duration": 4.56}, {"text": "um, the best way to drive from here to San Francisco say,", "start": 1044.115, "duration": 3.195}, {"text": "then the state transitions, um,", "start": 1047.31, "duration": 4.02}, {"text": "so S_t plus 1 is drawn from state transition probabilities indexed by the state at", "start": 1051.33, "duration": 6.27}, {"text": "time T and the action at time T. And", "start": 1057.6, "duration": 3.21}, {"text": "if these state transition probabilities change over time,", "start": 1060.81, "duration": 3.54}, {"text": "then, um, if you index it by the time t,", "start": 1064.35, "duration": 3.87}, {"text": "this would be an example of a non-stationary,", "start": 1068.22, "duration": 3.12}, {"text": "um, of a non-stationary state transition probabilities, okay?", "start": 1071.34, "duration": 3.66}, {"text": "Um, al- al- alternatively,", "start": 1075.0, "duration": 2.22}, {"text": "if you want non-stationary rewards,", "start": 1077.22, "duration": 2.64}, {"text": "then you can have R_t T of S_a, uh,", "start": 1079.86, "duration": 5.49}, {"text": "is the reward you get for taking a certain action, um,", "start": 1085.35, "duration": 3.33}, {"text": "uh, you know, o- o- for being at a certain state at a certain time, okay?", "start": 1088.68, "duration": 3.975}, {"text": "Um, so all of these are different variations of- of- of MDPs, um,", "start": 1092.655, "duration": 5.475}, {"text": "and so maybe just a few examples of when you want a, ah,", "start": 1098.13, "duration": 3.84}, {"text": "finite horizon MDP or use,", "start": 1101.97, "duration": 2.7}, {"text": "um, non-stationary, uh, state transitions.", "start": 1104.67, "duration": 3.375}, {"text": "Uh, so let's see.", "start": 1108.045, "duration": 2.97}, {"text": "Um, if you're flying an airplane, right?", "start": 1111.015, "duration": 3.855}, {"text": "For- for- for some airplanes, uh,", "start": 1114.87, "duration": 1.65}, {"text": "something like for commercial- for very large commercial airplanes,", "start": 1116.52, "duration": 2.94}, {"text": "uh, sometimes over a third of the weight of the airplane comes from the fuel, right?", "start": 1119.46, "duration": 4.71}, {"text": "So actually, if you take on a large commercial airplane, you know, when you take off, uh,", "start": 1124.17, "duration": 4.035}, {"text": "from, uh, SFO and you fly", "start": 1128.205, "duration": 2.415}, {"text": "to- I don't know- I don't know where you guys fly to, I don't know.", "start": 1130.62, "duration": 2.31}, {"text": "Fly to- fly to London or something.", "start": 1132.93, "duration": 2.295}, {"text": "Right, direct flight from here to London.", "start": 1135.225, "duration": 1.485}, {"text": "Uh, by the time the plane lands,", "start": 1136.71, "duration": 1.95}, {"text": "you- you get a much lighter airplane than when you took off, um,", "start": 1138.66, "duration": 2.43}, {"text": "because, uh, maybe sometimes- maybe like a third of the weight disappears,", "start": 1141.09, "duration": 4.23}, {"text": "you know, because of burning fuel.", "start": 1145.32, "duration": 1.71}, {"text": "And so the, the dynamics, the, um,", "start": 1147.03, "duration": 3.045}, {"text": "how the airplane feels between takeoff and landing is", "start": 1150.075, "duration": 3.045}, {"text": "actually different because the weight is dramatically different,", "start": 1153.12, "duration": 3.045}, {"text": "and so, um, uh,", "start": 1156.165, "duration": 1.575}, {"text": "this would be one example of", "start": 1157.74, "duration": 1.59}, {"text": "where the state transition probability changes in a pretty predictable way, right?", "start": 1159.33, "duration": 4.05}, {"text": "Um, or- uh, right.", "start": 1163.38, "duration": 2.34}, {"text": "I already mentioned, um,", "start": 1165.72, "duration": 1.11}, {"text": "uh, weather forecasts, right.", "start": 1166.83, "duration": 4.83}, {"text": "Where, uh, weather forecasts or traffic forecasts if you're driving here or, uh, yeah,", "start": 1171.66, "duration": 4.755}, {"text": "drivi- yeah, if you're driving over different types of terrain over time,", "start": 1176.415, "duration": 3.345}, {"text": "then you know that it's gonna rain tomorrow.", "start": 1179.76, "duration": 2.19}, {"text": "Uh, we are gonna know it's gonna rain tonight and the ground will turn muddy,", "start": 1181.95, "duration": 2.88}, {"text": "you know, then all- all the traffic will turn bad.", "start": 1184.83, "duration": 2.76}, {"text": "Um, uh, and then- and then, I don't know, industrial automation.", "start": 1187.59, "duration": 7.41}, {"text": "Um, some of my friends", "start": 1195.0, "duration": 3.405}, {"text": "work on industrial automation and I think that maybe one example, um,", "start": 1198.405, "duration": 4.455}, {"text": "if you run a factory 24 hours a day,", "start": 1202.86, "duration": 2.655}, {"text": "then the cost of labor, you know,", "start": 1205.515, "duration": 2.61}, {"text": "getting people to come into the factory to do some work at noon is actually easier,", "start": 1208.125, "duration": 5.34}, {"text": "right, and less costly than getting someone to show up", "start": 1213.465, "duration": 2.445}, {"text": "in the factory to do some work at 3:00 AM, right?", "start": 1215.91, "duration": 2.385}, {"text": "And so depending on, um, uh,", "start": 1218.295, "duration": 2.22}, {"text": "really labor availability over time,", "start": 1220.515, "duration": 2.355}, {"text": "the cost of taking different actions, uh,", "start": 1222.87, "duration": 2.355}, {"text": "and the cost of, um,", "start": 1225.225, "duration": 1.8}, {"text": "and the likelihood of transitioning into", "start": 1227.025, "duration": 1.545}, {"text": "different state transition probabilities can vary over the 24-hour clock as well, right?", "start": 1228.57, "duration": 3.87}, {"text": "And so these are other examples of when, um, uh,", "start": 1232.44, "duration": 4.65}, {"text": "uh, you can have a non-stationary policy and non-stationary state transitions, okay?", "start": 1237.09, "duration": 7.17}, {"text": "Now, um, let's talk about how you would actually solve for a finite horizon MDP,", "start": 1244.26, "duration": 6.675}, {"text": "and I think for the sake of simplicity, uh,", "start": 1250.935, "duration": 2.715}, {"text": "for the most part, I'm going to not bother with non-stationary transitions and rewards.", "start": 1253.65, "duration": 4.2}, {"text": "Fo- for the most part, I'll focus on- for the most part,", "start": 1257.85, "duration": 2.7}, {"text": "I just going to forget about, you know,", "start": 1260.55, "duration": 1.83}, {"text": "the fact that this could be varying.", "start": 1262.38, "duration": 2.31}, {"text": "Um, I- I- I'll mention it briefly but I- I wanna focus on the finite horizon aspect.", "start": 1264.69, "duration": 6.87}, {"text": "So-", "start": 1271.56, "duration": 2.41}, {"text": "so let me define,", "start": 1282.26, "duration": 2.665}, {"text": "um, the optimal value function.", "start": 1284.925, "duration": 5.845}, {"text": "Um, so this is the optimal value function for", "start": 1322.16, "duration": 4.54}, {"text": "time t for starting at state S. So this is the,", "start": 1326.7, "duration": 3.54}, {"text": "um, ah, expected total payoff.", "start": 1330.24, "duration": 3.52}, {"text": "Starting in state S at time t,", "start": 1339.44, "duration": 6.43}, {"text": "and if you execute,", "start": 1345.87, "duration": 3.19}, {"text": "you know, the best possible policy, okay?", "start": 1350.15, "duration": 3.025}, {"text": "So now the, um,", "start": 1353.175, "duration": 1.515}, {"text": "optimal value function depends on what time it is,", "start": 1354.69, "duration": 3.45}, {"text": "uh, because, if, if you look at, I don't know,", "start": 1358.14, "duration": 1.71}, {"text": "that example of the plus 1 reward on the left and the plus 10 reward on the right,", "start": 1359.85, "duration": 4.515}, {"text": "depending on how much time you have left on the clock,", "start": 1364.365, "duration": 2.76}, {"text": "the amount of rewards you can accumulate can be quite different, right?", "start": 1367.125, "duration": 3.165}, {"text": "If you have more time, you have more than, you know,", "start": 1370.29, "duration": 1.65}, {"text": "you can- more time to get to the plus 10 reward,", "start": 1371.94, "duration": 2.07}, {"text": "in the, the plus 1 and plus 10 reward that I drew- uh,", "start": 1374.01, "duration": 3.435}, {"text": "example that I drew just now.", "start": 1377.445, "duration": 1.92}, {"text": "And so, um, in this example,", "start": 1379.365, "duration": 4.185}, {"text": "value iteration, um, becomes the following.", "start": 1383.55, "duration": 6.51}, {"text": "It actually becomes a dynamic programming algorithm,", "start": 1390.06, "duration": 3.765}, {"text": "uh, which you'll see in a second, okay?", "start": 1393.825, "duration": 6.39}, {"text": "Which is that- let's see, um,", "start": 1400.215, "duration": 3.495}, {"text": "[NOISE] all right, I'm gonna need three lines, let me do this here.", "start": 1403.71, "duration": 4.1}, {"text": "[NOISE]", "start": 1407.81, "duration": 13.59}, {"text": "All right, which is that V star of", "start": 1421.4, "duration": 4.48}, {"text": "t of S is equal to max over A,", "start": 1425.88, "duration": 5.955}, {"text": "R of S, A plus-", "start": 1431.835, "duration": 4.135}, {"text": "okay? Um, and, uh,", "start": 1449.24, "duration": 3.34}, {"text": "actually, this is a question for you.", "start": 1452.58, "duration": 2.04}, {"text": "So there's, there's one missing thing here, right?", "start": 1454.62, "duration": 4.32}, {"text": "So we're saying that the optimal value,", "start": 1458.94, "duration": 3.675}, {"text": "you can get when you start off in state as at time t is the max over all actions of", "start": 1462.615, "duration": 5.655}, {"text": "the immediate reward plus sum of", "start": 1468.27, "duration": 1.95}, {"text": "s prime state transition reward S prime times V star of S prime,", "start": 1470.22, "duration": 3.9}, {"text": "and then what should go in that box.", "start": 1474.12, "duration": 2.695}, {"text": "T plus 1. Okay. Cool, awesome, great.", "start": 1476.815, "duration": 2.465}, {"text": "Right?", "start": 1479.28, "duration": 6.53}, {"text": "And then pi star of S is just,", "start": 1485.81, "duration": 5.615}, {"text": "you know, argmax of a, right?", "start": 1491.425, "duration": 3.245}, {"text": "Of the- of the same thing,", "start": 1494.67, "duration": 1.215}, {"text": "of this whole expression up on top.", "start": 1495.885, "duration": 3.18}, {"text": "Um, and so this formula defines Vt as a function of Vt plus 1.", "start": 1499.065, "duration": 7.38}, {"text": "So this is like, um [NOISE] this is like the iterative step, right?", "start": 1506.445, "duration": 2.895}, {"text": "Given V10, you can compute V9,", "start": 1509.34, "duration": 2.16}, {"text": "given V9, you can compute V8,", "start": 1511.5, "duration": 1.32}, {"text": "given V8, you can compute V7.", "start": 1512.82, "duration": 2.145}, {"text": "Um, and so to start this off,", "start": 1514.965, "duration": 2.28}, {"text": "there's just one last thing we need to define,", "start": 1517.245, "duration": 3.18}, {"text": "which is V capital T at the finite step,", "start": 1520.425, "duration": 3.81}, {"text": "uh, at the final step when the clock is about to run out.", "start": 1524.235, "duration": 3.105}, {"text": "Um, all you get to do is choose the action a,", "start": 1527.34, "duration": 5.53}, {"text": "that maximizes the immediate reward, and then,", "start": 1537.38, "duration": 3.37}, {"text": "and then, and then there's no sum after that, right?", "start": 1540.75, "duration": 2.61}, {"text": "So, um, if you start off at state S at the final time step t,", "start": 1543.36, "duration": 5.22}, {"text": "then you get to take an action and you get an immediate reward,", "start": 1548.58, "duration": 3.78}, {"text": "and then there is no next state because the world just ends right after", "start": 1552.36, "duration": 3.18}, {"text": "that step which is why the optimal value at", "start": 1555.54, "duration": 3.39}, {"text": "time t is just max over a of", "start": 1558.93, "duration": 2.97}, {"text": "the immediate reward because what happens after that doesn't matter, okay?", "start": 1561.9, "duration": 3.945}, {"text": "So this is a dynamic programming algorithm in which this,", "start": 1565.845, "duration": 5.235}, {"text": "um, uh, uh, algorithm- this step on top defines,", "start": 1571.08, "duration": 4.815}, {"text": "you know, allows you to compute V star of t,", "start": 1575.895, "duration": 2.715}, {"text": "[NOISE] and then the inductive step or the n plus 1 step, I guess, is,", "start": 1578.61, "duration": 4.065}, {"text": "uh, if you then having computed V star of t for every state S, right?", "start": 1582.675, "duration": 4.59}, {"text": "So, you know, you compute this for every state S. Having done this,", "start": 1587.265, "duration": 2.955}, {"text": "you can then compute V star T minus 1 using this,", "start": 1590.22, "duration": 3.705}, {"text": "um, inductive step, then V star t minus 2 and so on down to V star of 0.", "start": 1593.925, "duration": 7.485}, {"text": "So you compute this at every state,", "start": 1601.41, "duration": 1.65}, {"text": "and then based on this,", "start": 1603.06, "duration": 1.59}, {"text": "you can compute- oh sorry,", "start": 1604.65, "duration": 1.5}, {"text": "2 pi star of t right?", "start": 1606.15, "duration": 1.845}, {"text": "Compute the optimal policy,", "start": 1607.995, "duration": 1.95}, {"text": "the non-stationary policy for every state as", "start": 1609.945, "duration": 3.345}, {"text": "a function of both the state and the time, okay?", "start": 1613.29, "duration": 4.63}, {"text": "Um, all right, cool.", "start": 1618.44, "duration": 4.165}, {"text": "And, and I think, uh,", "start": 1622.605, "duration": 1.455}, {"text": "again, I don't want to dwell on this,", "start": 1624.06, "duration": 1.59}, {"text": "but if you want to work with", "start": 1625.65, "duration": 2.46}, {"text": "non-stationary- state transition probabilities or non-stationary rewards,", "start": 1628.11, "duration": 3.72}, {"text": "then this algorithm hardly changes in that you can just add you know,  if,", "start": 1631.83, "duration": 8.45}, {"text": "if your rewards and state transceiver is indexed by time as well,", "start": 1640.28, "duration": 3.33}, {"text": "then this is just a very small modification to this algorithm.", "start": 1643.61, "duration": 2.955}, {"text": "And it turns out that once you're using a finite horizon", "start": 1646.565, "duration": 3.27}, {"text": "MDP making the rewards and state transition rewards as non-stationary is,", "start": 1649.835, "duration": 5.23}, {"text": "is just a small tweak, right?", "start": 1655.065, "duration": 1.77}, {"text": "So you could yeah, yeah.", "start": 1656.835, "duration": 2.085}, {"text": "[inaudible] Uh, can you say that again.", "start": 1658.92, "duration": 8.79}, {"text": "In which form will it disappear? The attributes [inaudible].", "start": 1667.71, "duration": 1.8}, {"text": "This one? Oh a non-stationary.", "start": 1669.51, "duration": 4.23}, {"text": "So in the end you get a policy pi star subscript T of S.", "start": 1673.74, "duration": 4.71}, {"text": "[inaudible].", "start": 1678.45, "duration": 9.99}, {"text": "I'm sorry. This one?", "start": 1688.44, "duration": 2.23}, {"text": "This one. Oh,  I see, sure yes.", "start": 1690.95, "duration": 3.22}, {"text": "Pi star, this is a non-stationary policy.", "start": 1694.17, "duration": 2.07}, {"text": "Yes so that's why I like yeah, yeah.", "start": 1696.24, "duration": 1.47}, {"text": "Sorry, yeah, yeah. So this- the optimal policy will be a non-stationary policy.", "start": 1697.71, "duration": 4.08}, {"text": "Yes. uh, I, I, think, uh, yes, I think,", "start": 1701.79, "duration": 2.49}, {"text": "uh, I was using pi star to, to not,", "start": 1704.28, "duration": 2.31}, {"text": "not to denote that it has to be", "start": 1706.59, "duration": 2.01}, {"text": "a fixed function type, but yes, [inaudible] . Thank you.", "start": 1708.6, "duration": 2.85}, {"text": "Yeah. Right. If you", "start": 1711.45, "duration": 8.04}, {"text": "take big T to infinity can it just become the usual value iteration?", "start": 1719.49, "duration": 4.575}, {"text": "So the- let me think.", "start": 1724.065, "duration": 2.7}, {"text": "So there are two things with that.", "start": 1726.765, "duration": 3.385}, {"text": "So the two frameworks are closely related right,", "start": 1730.52, "duration": 3.07}, {"text": "you can kind of see relationship between the valuation.", "start": 1733.59, "duration": 2.88}, {"text": "One problem with taking this framework to big T to infinity", "start": 1736.47, "duration": 4.38}, {"text": "is that the values become unbounded, right?", "start": 1740.85, "duration": 4.665}, {"text": "Yeah and that's actually one of the reasons why we", "start": 1745.515, "duration": 3.315}, {"text": "use a discount factor when you have an infinite horizon MDP,", "start": 1748.83, "duration": 3.24}, {"text": "when the MDP just goes on forever.", "start": 1752.07, "duration": 1.845}, {"text": "One of the things that discount factor does is it makes sure", "start": 1753.915, "duration": 3.075}, {"text": "that the value function doesn't grow without bound, right?", "start": 1756.99, "duration": 5.085}, {"text": "And in fact, you know, if,", "start": 1762.075, "duration": 1.44}, {"text": "if the rewards are bounded by- right,", "start": 1763.515, "duration": 4.815}, {"text": "by some R max then when you use discounting then V,", "start": 1768.33, "duration": 4.47}, {"text": "you know, is bounded by I guess R max over 1 minus Gamma, right?", "start": 1772.8, "duration": 4.995}, {"text": "By the sum of a geometric sequence.", "start": 1777.795, "duration": 2.205}, {"text": "And so but, but in a finite h orizon repeat because you only add up t rewards it,", "start": 1780.0, "duration": 3.93}, {"text": "it can't get bigger than T times R max. Yeah.", "start": 1783.93, "duration": 3.87}, {"text": "[inaudible].", "start": 1787.8, "duration": 30.12}, {"text": "Let me think. So I think that, boy.", "start": 1817.92, "duration": 3.66}, {"text": "So I think, you know, what you find is that- let's see.", "start": 1821.58, "duration": 8.61}, {"text": "Um, actually let me just draw a 1D grid just to make life simpler, right?", "start": 1830.19, "duration": 5.67}, {"text": "So let's say there's a plus 1 reward there and a plus 10 reward there.", "start": 1835.86, "duration": 5.64}, {"text": "If you look at the optimal value function,", "start": 1841.5, "duration": 2.73}, {"text": "um, depending on what time it is.", "start": 1844.23, "duration": 2.055}, {"text": "If you have two times- and let's say that the dynamics are deterministic, right?", "start": 1846.285, "duration": 5.4}, {"text": "Uh, so there's no noise then if you have two times steps left,", "start": 1851.685, "duration": 3.695}, {"text": "then I guess V star would be,", "start": 1855.38, "duration": 3.25}, {"text": "you know, 10, 10, 10, 1, 1, 1, 0, 0, 0, right?", "start": 1858.63, "duration": 6.265}, {"text": "And so, uh, depending on where you are,", "start": 1864.895, "duration": 3.155}, {"text": "I guess if you're, uh, uh, uh yeah.", "start": 1868.05, "duration": 3.825}, {"text": "Actually, in fact I guess if you're here and there's nothing you can do right,", "start": 1871.875, "duration": 2.775}, {"text": "you can't get either reward in time.", "start": 1874.65, "duration": 1.98}, {"text": "Uh, but depending on whether you're here or here or here", "start": 1876.63, "duration": 3.39}, {"text": "the optimal action will change when we compute with this pi star. This makes sense?", "start": 1880.02, "duration": 4.38}, {"text": "Yeah, that's fine.", "start": 1884.4, "duration": 1.305}, {"text": "Okay, well yeah. Maybe I do, do encourage you there.", "start": 1885.705, "duration": 3.525}, {"text": "If this- if you actually build", "start": 1889.23, "duration": 3.645}, {"text": "a little grid simulator and use these equations to compute pi star and V star,", "start": 1892.875, "duration": 4.125}, {"text": "you will see that the optimal policy when you have lots of time will be this.", "start": 1897.0, "duration": 6.015}, {"text": "Wherever you are go for the 10 rewards,", "start": 1903.015, "duration": 2.915}, {"text": "but when the clock runs down then the optimal policy", "start": 1905.93, "duration": 3.12}, {"text": "will end up being a mixer, go left and go right.", "start": 1909.05, "duration": 4.45}, {"text": "All right, cool. Hope that was okay.", "start": 1915.49, "duration": 3.34}, {"text": "Yeah [NOISE].", "start": 1918.83, "duration": 5.49}, {"text": "All right. So, um-", "start": 1924.32, "duration": 6.96}, {"text": "[NOISE]", "start": 1931.28, "duration": 11.55}, {"text": "So the last thing I want to share with you today is,", "start": 1942.83, "duration": 3.315}, {"text": "uh, Linear Quadratic Regulation.", "start": 1946.145, "duration": 3.895}, {"text": "And as I was saying at the start, um,", "start": 1955.09, "duration": 3.595}, {"text": "LQR applies only in a relatively small set of problems.", "start": 1958.685, "duration": 4.95}, {"text": "But whenever it applies,", "start": 1963.635, "duration": 1.545}, {"text": "this is a great algorithm,", "start": 1965.18, "duration": 1.29}, {"text": "and I just, you know, use it whenever, right,", "start": 1966.47, "duration": 2.04}, {"text": "it seems reasonable to apply because it's, uh, uh,", "start": 1968.51, "duration": 2.535}, {"text": "is very efficient, and sometimes gives very good control policies.", "start": 1971.045, "duration": 4.74}, {"text": "And, um, let's see.", "start": 1975.785, "duration": 5.135}, {"text": "And so LQR, um,", "start": 1980.92, "duration": 2.94}, {"text": "applies in the following setting.", "start": 1983.86, "duration": 2.355}, {"text": "So let's see.", "start": 1986.215, "duration": 1.53}, {"text": "In order to specify an MDP,", "start": 1987.745, "duration": 2.085}, {"text": "we need to specify the states actions,", "start": 1989.83, "duration": 3.125}, {"text": "the state transition release.", "start": 1992.955, "duration": 2.165}, {"text": "I'm going to use a finite horizon formulation so capital T and rewards.", "start": 1995.12, "duration": 4.905}, {"text": "This, this also works with", "start": 2000.025, "duration": 2.175}, {"text": "the discounted MDP formalism but this would be a little bit easier,", "start": 2002.2, "duration": 3.3}, {"text": "a little bit more convenient to develop with the finite horizon setting,", "start": 2005.5, "duration": 2.97}, {"text": "so let me just use that today.", "start": 2008.47, "duration": 2.235}, {"text": "And LQR applies under a specific set of circumstances,", "start": 2010.705, "duration": 5.115}, {"text": "which is that, this set of states is an RN.", "start": 2015.82, "duration": 6.69}, {"text": "Set of actions is in RD and so to specify the state transition probabilities,", "start": 2023.16, "duration": 8.47}, {"text": "we need to tell you what's the distribution of the next state given the previous state.", "start": 2031.63, "duration": 4.575}, {"text": "So to specify the state transition probabilities,", "start": 2036.205, "duration": 2.565}, {"text": "I'm gonna say that the way S t plus 1 evolves is going to be as a linear function.", "start": 2038.77, "duration": 7.6}, {"text": "Some matrix A times S t plus some matrix B times A t plus some noise and", "start": 2050.46, "duration": 7.12}, {"text": "sorry there's a little bit of notation overloading again, sorry", "start": 2057.58, "duration": 3.21}, {"text": "about that, A is both the set of actions as well as this matrix A,", "start": 2060.79, "duration": 3.45}, {"text": "right so there's two separate things but same symbol.", "start": 2064.24, "duration": 3.675}, {"text": "I think, I think that the field of- a lot", "start": 2067.915, "duration": 3.225}, {"text": "the ideas from LQR came from traditional controls.", "start": 2071.14, "duration": 3.045}, {"text": "It's from, uh, what- from,", "start": 2074.185, "duration": 2.415}, {"text": "um, I guess from EE and Mechanical Engineering.", "start": 2076.6, "duration": 2.37}, {"text": "A lot of ideas from reinforcement learning came from computer science.", "start": 2078.97, "duration": 3.42}, {"text": "So these two literatures kind of evolved,", "start": 2082.39, "duration": 2.295}, {"text": "and then when the literatures merge,", "start": 2084.685, "duration": 2.205}, {"text": "you end up with clashing notations.", "start": 2086.89, "duration": 1.62}, {"text": "So CS people use A to denote the set of actions and the,", "start": 2088.51, "duration": 4.935}, {"text": "the set of mechanical engineering and EE people use A to denote", "start": 2093.445, "duration": 3.915}, {"text": "this matrix and when we merge", "start": 2097.36, "duration": 2.88}, {"text": "these two literatures the notation ends up being overloaded, right?", "start": 2100.24, "duration": 4.15}, {"text": "Okay. Oh and then, uh,", "start": 2105.3, "duration": 3.145}, {"text": "it turns out one thing we'll see later is that", "start": 2108.445, "duration": 3.105}, {"text": "this noise term it will- we'll see later is actually not super important.", "start": 2111.55, "duration": 4.26}, {"text": "But for now, let's just assume that the noise term is distributed Gaussian", "start": 2115.81, "duration": 4.604}, {"text": "with some mean 0 and some covariance sigma subscript w, okay?", "start": 2120.414, "duration": 5.311}, {"text": "But we'll see later that the noise will,", "start": 2125.725, "duration": 2.13}, {"text": "will be less important than you think.", "start": 2127.855, "duration": 3.205}, {"text": "Right. And so this matrix A is going to be R n by n. And this matrix B is going to be R n", "start": 2133.83, "duration": 8.71}, {"text": "by d where n and d are respectively", "start": 2142.54, "duration": 3.78}, {"text": "the dimensions of the state space and the dimension of the action space.", "start": 2146.32, "duration": 4.305}, {"text": "So for driving a car, for example,", "start": 2150.625, "duration": 3.09}, {"text": "we saw last time that maybe the state space is six dimensional.", "start": 2153.715, "duration": 3.165}, {"text": "So if you're driving a car where the state-space is XY theta x dot y dot theta", "start": 2156.88, "duration": 7.23}, {"text": "dot and the action space is steering controls so maybe A is two-dimensional right,", "start": 2164.11, "duration": 8.31}, {"text": "acceleration and steering, right.", "start": 2172.42, "duration": 3.13}, {"text": "Okay. So let's see.", "start": 2176.58, "duration": 2.47}, {"text": "So to specify an MDP we need to specify this five tuple, right?", "start": 2179.05, "duration": 3.93}, {"text": "So we specify three of the elements.", "start": 2182.98, "duration": 3.57}, {"text": "The fourth one, t is just some number,", "start": 2186.55, "duration": 2.7}, {"text": "right, so that's easy.", "start": 2189.25, "duration": 1.485}, {"text": "And then the final assumption we need to apply LQR,", "start": 2190.735, "duration": 3.72}, {"text": "is that the reward function has the following form.", "start": 2194.455, "duration": 4.215}, {"text": "That the reward is negative of", "start": 2198.67, "duration": 3.345}, {"text": "s transpose U s plus a transpose V a,", "start": 2202.015, "duration": 8.845}, {"text": "where U is n by n,", "start": 2211.17, "duration": 5.725}, {"text": "V is d by", "start": 2216.895, "duration": 5.565}, {"text": "d and U V are a positive semi-definite.", "start": 2222.46, "duration": 4.83}, {"text": "Okay? So these are matrices being bigger than zero, so positive semi-definite.", "start": 2227.29, "duration": 4.6}, {"text": "Okay. So the fact that U and V are positive semi-definite", "start": 2236.43, "duration": 8.115}, {"text": "that implies that S T U s is greater than or equal to 0 and excuse me,", "start": 2244.545, "duration": 7.44}, {"text": "S transpose U s sorry,", "start": 2251.985, "duration": 1.29}, {"text": "a transpose V a is also greater than or equal to 0.", "start": 2253.275, "duration": 5.955}, {"text": "Okay? So here's one example.", "start": 2259.23, "duration": 6.13}, {"text": "If you want to fly an autonomous helicopter and if you want,", "start": 2268.19, "duration": 6.1}, {"text": "you know, the state, the state vector to be close to 0.", "start": 2274.29, "duration": 5.35}, {"text": "So the state vector captures position,", "start": 2279.64, "duration": 1.68}, {"text": "orientation, velocity, angular velocity.", "start": 2281.32, "duration": 1.905}, {"text": "If you want a helicopter to just hover in place,", "start": 2283.225, "duration": 2.535}, {"text": "then maybe you want the state to be regulated or to, to,", "start": 2285.76, "duration": 3.93}, {"text": "to be controlled near some zero position and so if", "start": 2289.69, "duration": 5.91}, {"text": "you choose U equals the identity matrix,", "start": 2295.6, "duration": 6.075}, {"text": "and V also equal to the identity matrix,", "start": 2301.675, "duration": 4.214}, {"text": "this, this would be different dimensions, right?", "start": 2305.889, "duration": 2.101}, {"text": "This would be an n by n identity matrix, this would be a d by d ide- identity matrix.", "start": 2307.99, "duration": 4.455}, {"text": "Then R of s a ends up equal to negative norm of s squared plus norm of a squared.", "start": 2312.445, "duration": 9.175}, {"text": "Okay. And so this allows you to,", "start": 2322.2, "duration": 6.08}, {"text": "this allows you to specify the reward function that penalizes, you know,", "start": 2329.28, "duration": 4.81}, {"text": "we have a quadratic cost function,", "start": 2334.09, "duration": 1.845}, {"text": "the state deviating from 0 or, if you want,", "start": 2335.935, "duration": 3.135}, {"text": "the actions deviating from 0,", "start": 2339.07, "duration": 1.77}, {"text": "thus penalizing very large jerky motions on the control sticks or we set V equal to 0,", "start": 2340.84, "duration": 5.76}, {"text": "then this second term goes away.", "start": 2346.6, "duration": 1.605}, {"text": "Okay? So these are some of the cost functions you can", "start": 2348.205, "duration": 2.715}, {"text": "specify in terms of a quadratic cost function.", "start": 2350.92, "duration": 5.445}, {"text": "Okay. Now again, you know,", "start": 2356.365, "duration": 3.765}, {"text": "just so that you can see the generalization, um,", "start": 2360.13, "duration": 4.51}, {"text": "if you want non-stationary dynamics,", "start": 2365.7, "duration": 3.49}, {"text": "this model is quite simple to change where you can", "start": 2369.19, "duration": 4.59}, {"text": "say the matrices A and B depend on the time t. You can also say these,", "start": 2373.78, "duration": 5.565}, {"text": "you know, the matrices U and V depend on the time t. So if you have", "start": 2379.345, "duration": 5.955}, {"text": "non-stationary state transition probabilities or", "start": 2385.3, "duration": 2.16}, {"text": "non-stationary cost function that's how you would modify this.", "start": 2387.46, "duration": 5.16}, {"text": "But I won't, I won't use this generalization for today, okay?", "start": 2392.62, "duration": 5.5}, {"text": "Now, so the two key assumptions", "start": 2402.12, "duration": 20.47}, {"text": "of the LQR framework are that first,", "start": 2422.59, "duration": 3.825}, {"text": "the state transition dynamics,", "start": 2426.415, "duration": 2.085}, {"text": "the way your states change,", "start": 2428.5, "duration": 1.44}, {"text": "is as a linear function of the previous state and action plus some noise,", "start": 2429.94, "duration": 5.355}, {"text": "and second, that the reward function is a,", "start": 2435.295, "duration": 3.42}, {"text": "you know, quadratic cost function, right?", "start": 2438.715, "duration": 1.845}, {"text": "So these are the two key assumptions.", "start": 2440.56, "duration": 2.49}, {"text": "And so first, you know, where,", "start": 2443.05, "duration": 3.96}, {"text": "where do you get the matrices A and B.", "start": 2447.01, "duration": 8.085}, {"text": "One thing that we talked about on Wednesday already was-", "start": 2455.095, "duration": 4.035}, {"text": "so again this will actually work if you are trying to", "start": 2459.13, "duration": 2.43}, {"text": "apply LQR to fly an autonomous helicopter.", "start": 2461.56, "duration": 2.295}, {"text": "This would work for helicopter flying at low speeds.", "start": 2463.855, "duration": 2.58}, {"text": "Which is if you fly the helicopter around, [NOISE] you know,", "start": 2466.435, "duration": 7.275}, {"text": "start with some state S_0, take an action A_0, um,", "start": 2473.71, "duration": 3.4}, {"text": "get to state S_1,", "start": 2477.11, "duration": 2.315}, {"text": "do this until you get to S t, right?", "start": 2479.425, "duration": 3.525}, {"text": "And then this was the first trial,", "start": 2482.95, "duration": 2.384}, {"text": "and then you do this m times.", "start": 2485.334, "duration": 3.356}, {"text": "So we talked about this on Wednesday.", "start": 2490.44, "duration": 2.605}, {"text": "So fly the helicopter through m trajectory so t time steps each and then we know that", "start": 2493.045, "duration": 7.455}, {"text": "we want, S t plus 1 is approximately A S t plus B A t and so you can minimize,", "start": 2500.5, "duration": 8.53}, {"text": "okay. So we want", "start": 2520.53, "duration": 10.93}, {"text": "the left and the right hand side to be close to each other.", "start": 2531.46, "duration": 2.28}, {"text": "So you can, you know,", "start": 2533.74, "duration": 1.44}, {"text": "minimize the squared difference between the left hand side and the right hand side in a,", "start": 2535.18, "duration": 6.675}, {"text": "in a procedure a lot like linear regression in order to fit matrices A and B.", "start": 2541.855, "duration": 4.985}, {"text": "So if you actually fly a helicopter", "start": 2546.84, "duration": 2.19}, {"text": "around and collect this type of data and fit this model to it,", "start": 2549.03, "duration": 4.575}, {"text": "this will work, you know,", "start": 2553.605, "duration": 1.335}, {"text": "this is actually a good pretty reasonable model", "start": 2554.94, "duration": 2.295}, {"text": "for the dynamics of a helicopter at low speeds.", "start": 2557.235, "duration": 3.0}, {"text": "Okay? So this is one way to do it.", "start": 2560.235, "duration": 2.765}, {"text": "Um, so let's see.", "start": 2577.27, "duration": 2.2}, {"text": "Method 1 is to learn it, right?", "start": 2579.47, "duration": 2.76}, {"text": "A second method is to linearize a non-linear model.", "start": 2582.23, "duration": 9.93}, {"text": "[NOISE] So um,", "start": 2592.16, "duration": 9.84}, {"text": "let me just describe the ideas at a,", "start": 2602.0, "duration": 2.46}, {"text": "at a high level,", "start": 2604.46, "duration": 2.145}, {"text": "um, which is let's say that- and,", "start": 2606.605, "duration": 2.685}, {"text": "and I think for this it might be,", "start": 2609.29, "duration": 1.65}, {"text": "um, useful to think of the inverted pendulum, right?", "start": 2610.94, "duration": 3.75}, {"text": "So that was a, you know, so imagine you have a,", "start": 2614.69, "duration": 2.52}, {"text": "a, a inverted pendulum.", "start": 2617.21, "duration": 1.725}, {"text": "That was that, right? You have a pole and you're trying", "start": 2618.935, "duration": 1.785}, {"text": "to- you have a long vertical pole and you're trying to keep the pole balanced.", "start": 2620.72, "duration": 3.36}, {"text": "Um, so for an inverted pendulum like this,", "start": 2624.08, "duration": 2.685}, {"text": "if you download an open source physics simulator or if you have a friend, you know, from,", "start": 2626.765, "duration": 4.98}, {"text": "from the wi- with a physics degree to help", "start": 2631.745, "duration": 2.535}, {"text": "you derive the Newtonian mechanics equations for this.", "start": 2634.28, "duration": 2.76}, {"text": "Um, ah, [NOISE] let's see.", "start": 2637.04, "duration": 3.69}, {"text": "I, I actually tried to work through the,", "start": 2640.73, "duration": 1.845}, {"text": "the physics equations in the inverted pendulum one.", "start": 2642.575, "duration": 1.965}, {"text": "These are pretty complicated.", "start": 2644.54, "duration": 1.05}, {"text": "But I don't know [LAUGHTER].", "start": 2645.59, "duration": 2.82}, {"text": "Um, but you might have", "start": 2648.41, "duration": 1.26}, {"text": "a [NOISE] function that tells", "start": 2649.67, "duration": 5.31}, {"text": "you that if the state is a certain position orientation with the pole velocity,", "start": 2654.98, "duration": 5.37}, {"text": "angular velocity and you as-,", "start": 2660.35, "duration": 2.49}, {"text": "um, ah, what is it?", "start": 2662.84, "duration": 1.29}, {"text": "Um, apply a certain acceleration,", "start": 2664.13, "duration": 1.965}, {"text": "the actions accelerate left or accelerate right,", "start": 2666.095, "duration": 2.205}, {"text": "then, you know, one-tenth of a second later,", "start": 2668.3, "duration": 2.43}, {"text": "the state will get to this, right?", "start": 2670.73, "duration": 1.35}, {"text": "So, so, your physics friend can help you derive this equation.", "start": 2672.08, "duration": 4.305}, {"text": "Um, and, an- and then maybe plus noise, right?", "start": 2676.385, "duration": 3.78}, {"text": "Le- let me just ignore the noise for now.", "start": 2680.165, "duration": 1.545}, {"text": "Um, and so what you have is a function [NOISE],", "start": 2681.71, "duration": 25.43}, {"text": "right? Then maps from the state,", "start": 2707.14, "duration": 2.19}, {"text": "um, x x. Theta Theta.", "start": 2709.33, "duration": 2.055}, {"text": "That's a position of the cart and the angle of the pole and", "start": 2711.385, "duration": 3.345}, {"text": "the velocities and angular velocities that maps from the current state at time t,", "start": 2714.73, "duration": 4.095}, {"text": "oh, excuse me, comma [NOISE] at, right?", "start": 2718.825, "duration": 5.485}, {"text": "Maps from the, I guess,", "start": 2724.31, "duration": 1.5}, {"text": "current state vector to the next state vector,", "start": 2725.81, "duration": 3.21}, {"text": "um, as a function of the current state and current action.", "start": 2729.02, "duration": 2.385}, {"text": "Okay? So, um, here's what linearization means and,", "start": 2731.405, "duration": 6.605}, {"text": "I'm going to use a 1D example.", "start": 2738.01, "duration": 2.34}, {"text": "So because I can only draw on a flat board, right?", "start": 2740.35, "duration": 3.09}, {"text": "I can't, you know, because, because of the two-dimensional nature of the whiteboard,", "start": 2743.44, "duration": 3.255}, {"text": "um, I'm just going to use a- let's,", "start": 2746.695, "duration": 2.86}, {"text": "let's suppose that you have St plus 1 equals f of", "start": 2749.555, "duration": 3.39}, {"text": "St. And let me just forget- let me just ignore the action for now.", "start": 2752.945, "duration": 3.435}, {"text": "So I have one input and one output so I can draw this more easily on the whiteboard.", "start": 2756.38, "duration": 4.77}, {"text": "Um, so we have some function like this.", "start": 2761.15, "duration": 6.66}, {"text": "So the x-axis is St,", "start": 2767.81, "duration": 2.535}, {"text": "and y-axis is St plus 1,", "start": 2770.345, "duration": 3.135}, {"text": "and this is the function f, right?", "start": 2773.48, "duration": 2.49}, {"text": "We'll plug in back the, um, action later.", "start": 2775.97, "duration": 2.76}, {"text": "What the linearization process does is, um,", "start": 2778.73, "duration": 3.09}, {"text": "you pick a point and I'm going to call this point St over bar,", "start": 2781.82, "duration": 6.225}, {"text": "and we're going to, you know,", "start": 2788.045, "duration": 7.095}, {"text": "take the derivative of f and fill a straight line.", "start": 2795.14, "duration": 2.49}, {"text": "So we're not drawing a straight line very well.", "start": 2797.63, "duration": 2.385}, {"text": "Let's take the tangent straight line at this point St-bar,", "start": 2800.015, "duration": 5.52}, {"text": "and uh, what, ah,", "start": 2805.535, "duration": 2.175}, {"text": "and, and we're going to use this straight line.", "start": 2807.71, "duration": 3.31}, {"text": "Let's draw line green.", "start": 2813.28, "duration": 3.38}, {"text": "And we're going to use a green straight line to approximate the function f. Okay.", "start": 2818.08, "duration": 7.285}, {"text": "And so if you look at the equation for the green straight line, um,", "start": 2825.365, "duration": 4.005}, {"text": "the green straight line is a function mapping from St to St plus 1.", "start": 2829.37, "duration": 4.785}, {"text": "And S-bar is the point around which you're linearizing the function.", "start": 2834.155, "duration": 4.455}, {"text": "So S-bar, um, is a constant.", "start": 2838.61, "duration": 2.985}, {"text": "And this function is actually defined by St plus 1, um,", "start": 2841.595, "duration": 4.695}, {"text": "is approximately [NOISE] the derivative of the function at S-bar times", "start": 2846.29, "duration": 6.695}, {"text": "St minus S-bar plus f of S-bar t. Okay.", "start": 2852.985, "duration": 7.53}, {"text": "Um, and so, ah, so S-bar t is a constant, right?", "start": 2860.515, "duration": 8.53}, {"text": "And this equation expresses St plus 1 as a linear function of St.", "start": 2869.045, "duration": 7.05}, {"text": "So think of S-bar t as a fixed number, right?", "start": 2876.095, "duration": 2.715}, {"text": "It doesn't vary. So given some fixed S-bar, um,", "start": 2878.81, "duration": 4.53}, {"text": "this equation here- this is actually the equation of the green straight line,", "start": 2883.34, "duration": 3.725}, {"text": "which is it says, you know, if,", "start": 2887.065, "duration": 1.68}, {"text": "if you use a green straight line to approximate the function f,", "start": 2888.745, "duration": 2.295}, {"text": "just tells you what is St plus 1 as a function of St, and this is a, you know,", "start": 2891.04, "duration": 5.2}, {"text": "linear and affine relationship between St plus 1 and St, okay?", "start": 2896.24, "duration": 5.565}, {"text": "Um, so that's how you would linearize a function.", "start": 2901.805, "duration": 5.94}, {"text": "Um, and, and in the more general case", "start": 2907.745, "duration": 2.61}, {"text": "where, um, [NOISE].", "start": 2910.355, "duration": 10.215}, {"text": "And in a more general case where, um,", "start": 2920.57, "duration": 2.4}, {"text": "St plus 1 is actually a function of, you know,", "start": 2922.97, "duration": 3.69}, {"text": "putting this back in where both St and at, um,", "start": 2926.66, "duration": 5.745}, {"text": "the formula becomes, um, let me see.", "start": 2932.405, "duration": 7.795}, {"text": "Um, well, I'll write out the formula in a second.", "start": 2940.54, "duration": 5.26}, {"text": "Ah, but in this example,", "start": 2945.8, "duration": 2.655}, {"text": "S-bar t is usually chosen to be a typical value", "start": 2948.455, "duration": 3.945}, {"text": "[NOISE] for S, right? And so in particular,", "start": 2952.4, "duration": 7.785}, {"text": "if you expect your helicopter to be doing a pretty good job hovering near the state 0,", "start": 2960.185, "duration": 5.385}, {"text": "then, uh, it'll be pretty reasonable to choose S-bar t to be the vector of all 0s .", "start": 2965.57, "duration": 5.175}, {"text": "Because if you look at how good is the green line as an approximation of the blue line,", "start": 2970.745, "duration": 4.935}, {"text": "right, in a small region like this, you know,", "start": 2975.68, "duration": 2.34}, {"text": "the green line is actually pretty close to the blue line.", "start": 2978.02, "duration": 2.58}, {"text": "And so if you choose S-bar to", "start": 2980.6, "duration": 2.82}, {"text": "be the place where you expect your helicopter to spend most of its time,", "start": 2983.42, "duration": 4.095}, {"text": "then the green line is not too bad in approximation to the true function to the physics.", "start": 2987.515, "duration": 5.115}, {"text": "Oh, excuse me, or if you expect for the inverted pendulum,", "start": 2992.63, "duration": 2.58}, {"text": "if you expect that your inverted pendulum will spend most of", "start": 2995.21, "duration": 2.58}, {"text": "its time with the pole upright and the velocity not too large,", "start": 2997.79, "duration": 3.015}, {"text": "then you choose S-bar to be maybe the 0 vector.", "start": 3000.805, "duration": 3.18}, {"text": "Um, and so long as your pendulum-", "start": 3003.985, "duration": 2.07}, {"text": "your inverted pendulum is spending most of its time kind of,", "start": 3006.055, "duration": 3.39}, {"text": "you know, close to the 0 state,", "start": 3009.445, "duration": 2.025}, {"text": "then the green line is not to get an approximation for the blue line, right?", "start": 3011.47, "duration": 3.78}, {"text": "So this is an approximation,", "start": 3015.25, "duration": 1.515}, {"text": "but you try to choose,", "start": 3016.765, "duration": 1.245}, {"text": "um ah, be- because- I mean in,", "start": 3018.01, "duration": 2.595}, {"text": "in this little region it's actually not that bad that an approximation,", "start": 3020.605, "duration": 3.33}, {"text": "ah, it's only when you go really far away, right,", "start": 3023.935, "duration": 2.385}, {"text": "there there's a huge gap between the linear approximation,", "start": 3026.32, "duration": 2.895}, {"text": "um, and the true function f, okay?", "start": 3029.215, "duration": 4.29}, {"text": "Um, all right.", "start": 3033.505, "duration": 3.765}, {"text": "And so, um,", "start": 3037.27, "duration": 2.34}, {"text": "in the more general case where f is a function of both the state and the action,", "start": 3039.61, "duration": 5.025}, {"text": "then what you have to do is, ah,", "start": 3044.635, "duration": 2.01}, {"text": "the input now becomes St,", "start": 3046.645, "duration": 2.88}, {"text": "At because f maps from St,", "start": 3049.525, "duration": 2.7}, {"text": "At to St plus 1.", "start": 3052.225, "duration": 2.595}, {"text": "And then instead of choosing S-bar t,", "start": 3054.82, "duration": 2.43}, {"text": "you're choosing S-bar t,", "start": 3057.25, "duration": 1.785}, {"text": "a-bar t, which is a typical state and action,", "start": 3059.035, "duration": 3.36}, {"text": "ah, around which you linearize the function.", "start": 3062.395, "duration": 2.49}, {"text": "Or let me just write down the formula for that.", "start": 3064.885, "duration": 3.015}, {"text": "[NOISE]", "start": 3067.9, "duration": 21.3}, {"text": "Um, in which you would say,", "start": 3089.2, "duration": 2.355}, {"text": "if you linearize f around a point given by S-bar t,", "start": 3091.555, "duration": 7.155}, {"text": "a-bar t kind of the typical values,", "start": 3098.71, "duration": 2.775}, {"text": "then the formula you have is St plus 1 is given by f of S-bar t,", "start": 3101.485, "duration": 7.47}, {"text": "a-bar t plus the gradient with respect to S, [NOISE]", "start": 3108.955, "duration": 10.234}, {"text": "transpose S_t minus S_t bar.", "start": 3119.92, "duration": 2.73}, {"text": "[NOISE]", "start": 3122.65, "duration": 13.17}, {"text": "Okay. So this is the generalization of the 1D function we measured just now,", "start": 3135.82, "duration": 5.415}, {"text": "or we wrote down just now,", "start": 3141.235, "duration": 1.08}, {"text": "which says that, you know,", "start": 3142.315, "duration": 1.275}, {"text": "the next state is approximately this point around you, which you linearize,", "start": 3143.59, "duration": 4.319}, {"text": "plus the gradient with respect to S times how much the state differs from", "start": 3147.909, "duration": 4.051}, {"text": "the linearization point plus the gradient respect to", "start": 3151.96, "duration": 2.94}, {"text": "the actions times how much the actions vary from a-bar, okay?", "start": 3154.9, "duration": 5.535}, {"text": "And this kind of generalizes", "start": 3160.435, "duration": 2.384}, {"text": "that equation you wrote. [NOISE].", "start": 3162.819, "duration": 10.501}, {"text": "So, um, so this equation expresses St plus 1 as a,", "start": 3173.32, "duration": 7.77}, {"text": "ah, linear function or technically an affine function of", "start": 3181.09, "duration": 3.06}, {"text": "the previous state and the previous action, right?", "start": 3184.15, "duration": 4.41}, {"text": "With some matrices in between.", "start": 3188.56, "duration": 1.665}, {"text": "And from this, you know,", "start": 3190.225, "duration": 1.56}, {"text": "after some algebraic munging,", "start": 3191.785, "duration": 2.34}, {"text": "you can re-express this as St plus 1 equals ASt plus Bat.", "start": 3194.125, "duration": 6.855}, {"text": "Um, and, and just- there- there's just one other little detail which is,", "start": 3200.98, "duration": 4.38}, {"text": "um, you might need to redefine St to add an intercept term.", "start": 3205.36, "duration": 4.66}, {"text": "Right. And because this is, is a affine function", "start": 3211.8, "duration": 3.16}, {"text": "with an intercept term rather than the linear function.", "start": 3214.96, "duration": 2.295}, {"text": "But so from this formula,", "start": 3217.255, "duration": 2.475}, {"text": "you know, with a little bit of algebraic munging,", "start": 3219.73, "duration": 2.22}, {"text": "you should be able to figure out whether the matrix is a and b, ah, ah,", "start": 3221.95, "duration": 3.44}, {"text": "but you might need to add an intercept term to the S,", "start": 3225.39, "duration": 2.07}, {"text": "but this is just an affine function to kind of rewrite", "start": 3227.46, "duration": 2.16}, {"text": "in terms of matrices a and b, okay?", "start": 3229.62, "duration": 2.19}, {"text": "Um, all right.", "start": 3231.81, "duration": 6.75}, {"text": "So right,", "start": 3238.56, "duration": 2.29}, {"text": "I hope that makes sense, right?", "start": 3240.85, "duration": 1.14}, {"text": "That this thing, this linearization thing", "start": 3241.99, "duration": 2.715}, {"text": "expresses St plus 1 as a linear function of St and at, right?", "start": 3244.705, "duration": 4.035}, {"text": "This is just a linear- is just- the wa- way St plus 1 varies,", "start": 3248.74, "duration": 3.18}, {"text": "you know, is just some matrix times St,", "start": 3251.92, "duration": 1.635}, {"text": "some matrix times at, um, and that's why with some munging,", "start": 3253.555, "duration": 3.675}, {"text": "you can get into this formula for some matrix a and b, okay?", "start": 3257.23, "duration": 3.345}, {"text": "Um, but because there are some constants floating around as well, like this,", "start": 3260.575, "duration": 4.29}, {"text": "you might need an extra intercept term to multiply to a to give you that extra constant.", "start": 3264.865, "duration": 4.185}, {"text": "[NOISE]", "start": 3269.05, "duration": 11.01}, {"text": "That's where we are. Um, we now have that for", "start": 3280.06, "duration": 4.8}, {"text": "these MDPs either by learning a linear model with the matrices A and B,", "start": 3284.86, "duration": 5.055}, {"text": "um, or by taking a nonlinear model and linearizing it.", "start": 3289.915, "duration": 3.9}, {"text": "Like you just saw, you can model- hopefully model an MDP as a,", "start": 3293.815, "duration": 5.04}, {"text": "um, [NOISE] linear dynamical system, meaning this, you know,", "start": 3298.855, "duration": 3.285}, {"text": "S_T plus 1 is this linear function or the previous state and action,", "start": 3302.14, "duration": 3.87}, {"text": "as well as hopefully with a quadratic reward function or the- really,", "start": 3306.01, "duration": 4.335}, {"text": "the- er, right, in the form that we saw just now.", "start": 3310.345, "duration": 4.5}, {"text": "Um, so let me just summarize the problem we want to solve.", "start": 3314.845, "duration": 7.455}, {"text": "a_ST, oops sorry, sorry.", "start": 3322.3, "duration": 3.72}, {"text": "S_t plus 1 equals A S_T plus B_at plus w_t,", "start": 3326.02, "duration": 6.045}, {"text": "so this is a noise term, um,", "start": 3332.065, "duration": 4.005}, {"text": "and then R of S, a", "start": 3336.07, "duration": 5.73}, {"text": "equals negative S transpose U_S plus a transpose V_a.", "start": 3341.8, "duration": 6.06}, {"text": "All right. And this is a finite horizon MDP.", "start": 3347.86, "duration": 3.3}, {"text": "And so the total payoff is R of S_0,", "start": 3351.16, "duration": 5.07}, {"text": "a_0 plus dot dot dot plus R S_T.", "start": 3356.23, "duration": 9.39}, {"text": "Okay. [NOISE] So let's", "start": 3365.62, "duration": 9.09}, {"text": "figure out a dynamic programming algorithm for this.", "start": 3374.71, "duration": 2.28}, {"text": "[NOISE] The remarkable problem,", "start": 3376.99, "duration": 5.415}, {"text": "the- the remarkable property of LQR, um,", "start": 3382.405, "duration": 4.44}, {"text": "and what makes this so useful is that if you are", "start": 3386.845, "duration": 3.555}, {"text": "willing to model your MDP using those sets of equations,", "start": 3390.4, "duration": 3.824}, {"text": "then the value function is a quadratic function, right?", "start": 3394.224, "duration": 4.006}, {"text": "Um, and so let me show you what I mean.", "start": 3398.23, "duration": 1.965}, {"text": "And so if your- if your model,", "start": 3400.195, "duration": 1.665}, {"text": "if your MDP can be modeled as this type of linear dynamical system,", "start": 3401.86, "duration": 3.165}, {"text": "with a quadratic cost function, uh,", "start": 3405.025, "duration": 2.34}, {"text": "then it turns out that V star is", "start": 3407.365, "duration": 1.92}, {"text": "a quadratic function and so you can compute V star exactly, right?", "start": 3409.285, "duration": 4.575}, {"text": "Um, so let me show you what I mean.", "start": 3413.86, "duration": 2.82}, {"text": "We're going to develop", "start": 3416.68, "duration": 1.59}, {"text": "a dynamic programming algorithm to compute the optimal value function V star.", "start": 3418.27, "duration": 8.895}, {"text": "Similar to, uh, what we did a bit earlier", "start": 3427.165, "duration": 3.765}, {"text": "today with the finite horizon MDP with a finite set of states,", "start": 3430.93, "duration": 3.54}, {"text": "let's start with the final time step and we will work backwards.", "start": 3434.47, "duration": 4.155}, {"text": "So, um, V star t of S_T is equal", "start": 3438.625, "duration": 5.835}, {"text": "to max over a_T of R of S_T , a_T.", "start": 3444.46, "duration": 6.24}, {"text": "Um, this is max over", "start": 3450.7, "duration": 3.21}, {"text": "a_T over negative, right?", "start": 3453.91, "duration": 12.855}, {"text": "Um, but this is always greater than or equal to 0 because V is positive semi-definite.", "start": 3466.765, "duration": 7.53}, {"text": "And so the optimal action is actually to just choose the action 0, um,", "start": 3474.295, "duration": 5.595}, {"text": "and so the max over this is equal to the negative S_T transpose U S_T because,", "start": 3479.89, "duration": 6.63}, {"text": "because V is a positive semi-definite matrix.", "start": 3486.52, "duration": 2.19}, {"text": "This thing is always greater than 0.", "start": 3488.71, "duration": 1.83}, {"text": "And then- and so this tells us also that Pi star of the final action is the argmax.", "start": 3490.54, "duration": 7.29}, {"text": "So the optimal action is to choose, you know,", "start": 3497.83, "duration": 2.925}, {"text": "the vector of 0 actions at the last time step, okay?", "start": 3500.755, "duration": 3.57}, {"text": "So this is the base case for the dynamic programming step of,", "start": 3504.325, "duration": 5.13}, {"text": "um, value iteration where, uh,", "start": 3509.455, "duration": 3.36}, {"text": "the optimal value at the last time step", "start": 3512.815, "duration": 2.925}, {"text": "is just choose the action that maximizes the immediate reward,", "start": 3515.74, "duration": 3.27}, {"text": "uh, which means maximize this, right?", "start": 3519.01, "duration": 2.91}, {"text": "And this is maximized by choosing the action 0 at the last time step, okay?", "start": 3521.92, "duration": 5.77}, {"text": "Now, these blue pens keep, let's see if this is any better, ooh, okay.", "start": 3528.0, "duration": 20.39}, {"text": "Now, the key step to the dynamic programming implementation is the following,", "start": 3549.12, "duration": 5.875}, {"text": "which is suppose that V star t", "start": 3554.995, "duration": 5.745}, {"text": "plus 1 S_t plus 1 is equal to a quadratic function.", "start": 3560.74, "duration": 6.61}, {"text": "Right.", "start": 3576.3, "duration": 1.57}, {"text": "Um, okay.", "start": 3577.87, "duration": 11.655}, {"text": "So in the- uh-huh.", "start": 3589.525, "duration": 1.965}, {"text": "[inaudible].", "start": 3591.49, "duration": 12.72}, {"text": "Yes. It's true that this term is also greater than 0 without the minus sign.", "start": 3604.21, "duration": 3.51}, {"text": "Without the minus sign, that term is positive and so,", "start": 3607.72, "duration": 4.17}, {"text": "but you only get to maximize with respect to 80 right?", "start": 3611.89, "duration": 4.53}, {"text": "So, so the best you could do for this term is set it to 0.", "start": 3616.42, "duration": 2.955}, {"text": "Thank you. All right, cool, tank you. All right.", "start": 3619.375, "duration": 7.545}, {"text": "Now, for the inductive case, um,", "start": 3626.92, "duration": 2.94}, {"text": "we want to go from V_t plus 1- V_star t plus 1 to", "start": 3629.86, "duration": 4.74}, {"text": "computing V star t, right? And the key observation that makes LQR work is,", "start": 3634.6, "duration": 7.575}, {"text": "um, let's suppose V star t plus 1,", "start": 3642.175, "duration": 3.315}, {"text": "the optimal value function at the next time step,", "start": 3645.49, "duration": 2.7}, {"text": "let's suppose is a quadratic function.", "start": 3648.19, "duration": 1.95}, {"text": "So in particular, let's suppose V star t plus 1 is this,", "start": 3650.14, "duration": 3.9}, {"text": "you know, quadratic function, uh,", "start": 3654.04, "duration": 2.775}, {"text": "parameterized by some matrix capital Phi t plus 1 which is an n", "start": 3656.815, "duration": 6.075}, {"text": "by n matrix and some constant offset Psi which is a real number.", "start": 3662.89, "duration": 5.94}, {"text": "Um, what we will be able to show is that if you do one step of dynamic programming, uh,", "start": 3668.83, "duration": 7.02}, {"text": "if this is true for V star plus 1 that V_t after one step as you go from V star plus", "start": 3675.85, "duration": 5.7}, {"text": "1 to V_t that the optimal value function", "start": 3681.55, "duration": 3.3}, {"text": "V_t is also going to be a quadratic function with a very similar form,", "start": 3684.85, "duration": 3.75}, {"text": "right, with I guess t plus 1 replaced by t, right?", "start": 3688.6, "duration": 4.635}, {"text": "Um, and so in the dynamic programming step, um,", "start": 3693.235, "duration": 6.915}, {"text": "we are going to update V_t S_t", "start": 3700.15, "duration": 3.88}, {"text": "equals max of A_t R of S_t, a_T plus.", "start": 3706.59, "duration": 8.41}, {"text": "And then, you know,", "start": 3715.0, "duration": 1.98}, {"text": "I- I think you remember, right, previously,", "start": 3716.98, "duration": 2.52}, {"text": "um, I'm going to write this in green,", "start": 3719.5, "duration": 3.225}, {"text": "previously we had sum of S prime or actually St plus", "start": 3722.725, "duration": 4.695}, {"text": "1 I guess to be S_t a_t S_t plus 1,", "start": 3727.42, "duration": 5.235}, {"text": "V star t plus 1 St plus 1.", "start": 3732.655, "duration": 4.32}, {"text": "So that's what we had previously where we", "start": 3736.975, "duration": 2.085}, {"text": "had a discrete state space and we were summing over it.", "start": 3739.06, "duration": 2.565}, {"text": "But now that we have a continuous state space,", "start": 3741.625, "duration": 2.115}, {"text": "this formula becomes expected value with respect to S_t plus 1 drawn from", "start": 3743.74, "duration": 5.745}, {"text": "the state transition probabilities [NOISE] ,", "start": 3749.485, "duration": 6.915}, {"text": "uh, V star t plus 1 S_t plus 1 [NOISE].", "start": 3756.4, "duration": 5.7}, {"text": "Uh, yeah. Okay.", "start": 3762.1, "duration": 5.14}, {"text": "[NOISE] So the optimal value when the clock is at time t is choose the", "start": 3775.28, "duration": 5.56}, {"text": "action a that maximizes immediate reward plus the expected value of, you know,", "start": 3780.84, "duration": 4.785}, {"text": "your future rewards when the clock has now ticked from time t to time t plus 1,", "start": 3785.625, "duration": 5.76}, {"text": "you're going to state S_t plus 1 at time t plus 1, right?", "start": 3791.385, "duration": 6.685}, {"text": "So, um, let's see.", "start": 3798.56, "duration": 6.8}, {"text": "So, ah, this is a pretty beefy piece of algebra to do.", "start": 3806.87, "duration": 8.08}, {"text": "Um, I think I feel like showing this full result is, I don't know,", "start": 3814.95, "duration": 5.34}, {"text": "is like at the level of complexity of a, you know,", "start": 3820.29, "duration": 2.52}, {"text": "typical CS 229 homework problem which is quite hard [LAUGHTER].", "start": 3822.81, "duration": 5.97}, {"text": "But let me just show the outline of how you do this derivation and why,", "start": 3828.78, "duration": 3.435}, {"text": "you know, why this inductive step works.", "start": 3832.215, "duration": 1.845}, {"text": "Well, but I think you- but,", "start": 3834.06, "duration": 1.08}, {"text": "but if you want you could work through the algebra details yourself at home.", "start": 3835.14, "duration": 4.17}, {"text": "Um, which is that- let me do this on the next board.", "start": 3839.31, "duration": 4.96}, {"text": "So V star_t of S_t is equal to max over a_t", "start": 3863.87, "duration": 6.895}, {"text": "of the immediate reward, right?", "start": 3870.765, "duration": 6.735}, {"text": "So that's the immediate reward.", "start": 3877.5, "duration": 1.485}, {"text": "And then plus the expected value with respect to S_t plus 1,", "start": 3878.985, "duration": 6.465}, {"text": "is drawn from a Gaussian with", "start": 3885.45, "duration": 3.72}, {"text": "mean AS_t plus Ba_t and covariance Sigma w. Ah,", "start": 3889.17, "duration": 7.905}, {"text": "so remember S_t plus 1 is equal to AS_t plus Ba_t plus W_t,", "start": 3897.075, "duration": 7.29}, {"text": "where W_t is Gaussian with mean 0 and covariance Sigma w. Right?", "start": 3904.365, "duration": 6.495}, {"text": "So ah, if you choose an action a_t,", "start": 3910.86, "duration": 2.595}, {"text": "then this is the distribution of the next state at time t plus 1.", "start": 3913.455, "duration": 4.575}, {"text": "Um, and then expected value of", "start": 3918.03, "duration": 3.66}, {"text": "[NOISE]", "start": 3921.69, "duration": 10.17}, {"text": "this quadratic term.", "start": 3931.86, "duration": 1.29}, {"text": "Um, because this quadratic term here,", "start": 3933.15, "duration": 3.51}, {"text": "kind of the inductive case was what we showed was", "start": 3936.66, "duration": 2.76}, {"text": "V star for the-", "start": 3939.42, "duration": 4.97}, {"text": "for the next time step, right?", "start": 3944.39, "duration": 5.22}, {"text": "So it turns out that, um, let's see.", "start": 3949.61, "duration": 7.165}, {"text": "So this is a quadratic function,", "start": 3956.775, "duration": 2.565}, {"text": "and this expectation is the expected value of", "start": 3959.34, "duration": 3.99}, {"text": "a quadratic function with respect to s drawn from a Gaussian, right?", "start": 3963.33, "duration": 4.88}, {"text": "With a certain mean and certain variance.", "start": 3968.21, "duration": 2.23}, {"text": "So it turns out that, um,", "start": 3970.44, "duration": 2.04}, {"text": "the expected value of this thing, right?", "start": 3972.48, "duration": 4.26}, {"text": "Well, this whole thing that I just circled.", "start": 3976.74, "duration": 2.205}, {"text": "This thing simplifies into, er,", "start": 3978.945, "duration": 2.67}, {"text": "a big quadratic function", "start": 3981.615, "duration": 2.625}, {"text": "[NOISE]", "start": 3984.24, "duration": 9.3}, {"text": "of the action a_t, right?", "start": 3993.54, "duration": 5.19}, {"text": "Um, and then, ah,", "start": 3998.73, "duration": 10.215}, {"text": "and so in order to, you know,", "start": 4008.945, "duration": 2.505}, {"text": "derive the argmax or to derive V star of S,", "start": 4011.45, "duration": 2.955}, {"text": "you would derive this big quadratic function.", "start": 4014.405, "duration": 2.895}, {"text": "Um, take derivatives with respect to a_t,", "start": 4017.3, "duration": 5.46}, {"text": "ah, set to 0, right?", "start": 4022.76, "duration": 5.535}, {"text": "And solve for a_t.", "start": 4028.295, "duration": 5.365}, {"text": "Okay? And if you go through all that algebra,", "start": 4033.79, "duration": 4.375}, {"text": "then you actually- then you end up with the formula for a_t as follows.", "start": 4038.165, "duration": 5.275}, {"text": "Um,", "start": 4043.69, "duration": 2.21}, {"text": "okay? And um,", "start": 4059.59, "duration": 2.71}, {"text": "I'm gonna use, I'm gonna do- I'm gonna take that big matrix and denote that L_t.", "start": 4062.3, "duration": 7.005}, {"text": "Okay? Um, and so this shows also that pi star", "start": 4069.305, "duration": 6.51}, {"text": "at time t of S_t is equal to L_t times S_t.", "start": 4075.815, "duration": 7.58}, {"text": "Okay? So, um,", "start": 4083.395, "duration": 10.885}, {"text": "[NOISE] one to- to take away from this is that,", "start": 4094.28, "duration": 6.57}, {"text": "under the assumptions we have, right?", "start": 4100.85, "duration": 2.025}, {"text": "Linear dynamical system with quadratic cost function.", "start": 4102.875, "duration": 2.445}, {"text": "Ah, the optimal action is", "start": 4105.32, "duration": 5.64}, {"text": "a linear function", "start": 4110.96, "duration": 2.109}, {"text": "of the state S_t.", "start": 4117.28, "duration": 6.37}, {"text": "Right? And, ah, this is not a claim that is made through functional approximation.", "start": 4123.65, "duration": 6.27}, {"text": "Ah, what I'm- I'm not saying that you could fit", "start": 4129.92, "duration": 3.0}, {"text": "a straight line t optimal action and if you fit a straight line,", "start": 4132.92, "duration": 3.375}, {"text": "that you get this linear function.", "start": 4136.295, "duration": 1.8}, {"text": "Right? That's not what we're saying.", "start": 4138.095, "duration": 1.41}, {"text": "We're saying that, um, of all the functions,", "start": 4139.505, "duration": 2.985}, {"text": "anyone could possibly come up within the world,", "start": 4142.49, "duration": 1.965}, {"text": "linear or non-linear, the best function,", "start": 4144.455, "duration": 2.4}, {"text": "the best action is linear.", "start": 4146.855, "duration": 1.59}, {"text": "So there is no approximation here.", "start": 4148.445, "duration": 2.13}, {"text": "Right? So it's just that, you know,", "start": 4150.575, "duration": 1.545}, {"text": "it's just a fact that if you have linear dynamical system,", "start": 4152.12, "duration": 2.67}, {"text": "the best possible action at any state is going to be a linear function um,", "start": 4154.79, "duration": 4.83}, {"text": "ah, of of that state.", "start": 4159.62, "duration": 1.905}, {"text": "Right? So there's no there's- we haven't approximated anything.", "start": 4161.525, "duration": 3.18}, {"text": "Right?", "start": 4164.705, "duration": 1.465}, {"text": "Um,", "start": 4166.27, "duration": 4.31}, {"text": "[NOISE]", "start": 4171.174, "duration": 8.696}, {"text": "let me see.", "start": 4179.87, "duration": 1.6}, {"text": "Yeah, all right. Let me,", "start": 4181.75, "duration": 2.185}, {"text": "let me, let me write this here.", "start": 4183.935, "duration": 1.485}, {"text": "Um, and then the other step is that ah,", "start": 4185.42, "duration": 3.21}, {"text": "if you take the optimal action and plug it into the definition of V star,", "start": 4188.63, "duration": 5.565}, {"text": "then by simplifying which again is quite a lot of algebra, but without the simplifying,", "start": 4194.195, "duration": 5.82}, {"text": "you end up with this equation.", "start": 4200.015, "duration": 2.695}, {"text": "Um, where again I'll- I'll just write out the formula as is, you know. [NOISE]", "start": 4208.09, "duration": 44.24}, {"text": "Okay.", "start": 4252.33, "duration": 6.01}, {"text": "Okay.", "start": 4258.34, "duration": 6.015}, {"text": "All right. [BACKGROUND] Um,", "start": 4264.355, "duration": 16.445}, {"text": "so to summarize the whole algorithm,", "start": 4280.8, "duration": 2.55}, {"text": "right, let's, let's put everything together.", "start": 4283.35, "duration": 1.89}, {"text": "And, and so- sorry.", "start": 4285.24, "duration": 1.875}, {"text": "And so what these two equations do is they allow you to go from V star T", "start": 4287.115, "duration": 3.975}, {"text": "plus 1 which is defined in terms of Phi T plus 1 and Psi T plus 1.", "start": 4291.09, "duration": 4.335}, {"text": "And it allows you to recursively go back to", "start": 4295.425, "duration": 3.015}, {"text": "figure out what is V star T using these two equations.", "start": 4298.44, "duration": 3.465}, {"text": "Right. So Phi T depends on Phi T plus 1,", "start": 4301.905, "duration": 3.165}, {"text": "Psi T depends on Phi T plus 1 and Psi T plus 1.", "start": 4305.07, "duration": 2.76}, {"text": "Uh, and this Sigma w,", "start": 4307.83, "duration": 2.13}, {"text": "this is the covariance of w_t.", "start": 4309.96, "duration": 5.34}, {"text": "Right. This, this Sigma subscript w. This is not a summation over w,", "start": 4315.3, "duration": 3.36}, {"text": "this is a Sigma matrix subscripted by w. That was a covariance matrix for", "start": 4318.66, "duration": 3.99}, {"text": "the noise terms you are adding on every step in our linear dynamical system.", "start": 4322.65, "duration": 5.43}, {"text": "Okay. And, and this are trace operators, some of the diagonals.", "start": 4328.08, "duration": 3.36}, {"text": "Okay? So just to summarize.", "start": 4331.44, "duration": 3.66}, {"text": "[NOISE] Um, here's the algorithm.", "start": 4335.1, "duration": 5.73}, {"text": "You initialize Phi T to be", "start": 4340.83, "duration": 6.945}, {"text": "equal to negative u and Psi T equals 0.", "start": 4347.775, "duration": 7.275}, {"text": "Um, and so, you know,", "start": 4355.05, "duration": 2.505}, {"text": "that's just taking this equation and mapping it there.", "start": 4357.555, "duration": 5.01}, {"text": "Right? So the final time step, ah,", "start": 4362.565, "duration": 3.435}, {"text": "that those two, oh, sorry,", "start": 4366.0, "duration": 2.64}, {"text": "it should be capital T. Right.", "start": 4368.64, "duration": 4.425}, {"text": "So that, um, those two equations for Phi and Psi,", "start": 4373.065, "duration": 4.365}, {"text": "it defines V star of capital T. And then you would,", "start": 4377.43, "duration": 6.135}, {"text": "um, you know, recursively calculate, um,", "start": 4383.565, "duration": 7.665}, {"text": "Phi T and Psi T using Phi T plus 1 and Psi T plus 1.", "start": 4391.23, "duration": 9.66}, {"text": "So you go from,", "start": 4400.89, "duration": 1.575}, {"text": "you know, for T equals T minus 1,", "start": 4402.465, "duration": 3.075}, {"text": "T minus 2 and so on and go back when count down from,", "start": 4405.54, "duration": 4.14}, {"text": "right, T minus 1 to T minus 2 and so on down to 0.", "start": 4409.68, "duration": 3.51}, {"text": "Um, calculate L_t as above.", "start": 4413.19, "duration": 10.56}, {"text": "Right. and L_t was a formula I guess we had over there, um,", "start": 4423.75, "duration": 5.01}, {"text": "saying how the optimal action is", "start": 4428.76, "duration": 2.73}, {"text": "a function of the current state depending on A, and B, and Phi.", "start": 4431.49, "duration": 3.21}, {"text": "Ah, and then finally,", "start": 4434.7, "duration": 3.975}, {"text": "Pi star of S_t equals L_t of S_t.", "start": 4438.675, "duration": 9.645}, {"text": "Okay? Um, and this algorithm,", "start": 4448.32, "duration": 4.755}, {"text": "the remarkable thing, what one really cool thing about", "start": 4453.075, "duration": 2.235}, {"text": "LQR is that there is no approximation anywhere.", "start": 4455.31, "duration": 3.945}, {"text": "Right? You, you might need to, um,", "start": 4459.255, "duration": 2.385}, {"text": "make some approximation steps in order to approximate", "start": 4461.64, "duration": 3.54}, {"text": "a helicopter as a linear dynamical system by, you know,", "start": 4465.18, "duration": 3.525}, {"text": "fitting matrices A and B to data or by taking a nonlinear thing and linearizing it,", "start": 4468.705, "duration": 5.415}, {"text": "and you might need to just restrict- constrict,", "start": 4474.12, "duration": 2.16}, {"text": "you know, restrict your choice of possible reward functions.", "start": 4476.28, "duration": 3.09}, {"text": "Reward function is quadratic.", "start": 4479.37, "duration": 1.59}, {"text": "But once you've made those assumptions,", "start": 4480.96, "duration": 2.294}, {"text": "none of this is approximate,", "start": 4483.254, "duration": 1.246}, {"text": "everything is exact. Right. Question?", "start": 4484.5, "duration": 1.98}, {"text": "[inaudible]", "start": 4486.48, "duration": 8.31}, {"text": "Yes, that's right. Yep, yeah.", "start": 4494.79, "duration": 1.515}, {"text": "So the approximation step needed are, ah, ah,", "start": 4496.305, "duration": 3.015}, {"text": "getting your MDP into the form of a linear dynamical system with quadratic reward.", "start": 4499.32, "duration": 4.38}, {"text": "So that is approximate.", "start": 4503.7, "duration": 1.29}, {"text": "But once you specify the MTP like that,", "start": 4504.99, "duration": 1.785}, {"text": "all of these calculations were exact, right?", "start": 4506.775, "duration": 2.145}, {"text": "So, so we're not approximating the value function or quadratic function,", "start": 4508.92, "duration": 3.93}, {"text": "is that the value function is a quadratic function and you're computing it exactly.", "start": 4512.85, "duration": 4.335}, {"text": "And the optimal policy is a linear function and you", "start": 4517.185, "duration": 2.475}, {"text": "just computing, computing that exactly.", "start": 4519.66, "duration": 2.97}, {"text": "Okay. Um, I want to mention- before we wrap up,", "start": 4522.63, "duration": 5.67}, {"text": "I want to mention one,", "start": 4528.3, "duration": 1.125}, {"text": "one unusual fun fact about LQR and this is very specific to LQR.", "start": 4529.425, "duration": 4.455}, {"text": "Uh, and, and, and it's convenient,", "start": 4533.88, "duration": 3.135}, {"text": "uh, but, but, er,", "start": 4537.015, "duration": 1.62}, {"text": "let me say what the fact is and just be careful that this doesn't give you", "start": 4538.635, "duration": 2.805}, {"text": "the wrong intuition because it doesn't apply to anything other than LQR,", "start": 4541.44, "duration": 3.36}, {"text": "which is that if you look at where, um,", "start": 4544.8, "duration": 4.14}, {"text": "so first, if you look at the formula for L, ah, let me see.", "start": 4548.94, "duration": 5.5}, {"text": "Move this around. [NOISE] All right.", "start": 4555.26, "duration": 3.73}, {"text": "If you look at the formula for L_t,", "start": 4558.99, "duration": 1.8}, {"text": "you need to compute, I mean the,", "start": 4560.79, "duration": 1.905}, {"text": "you know, the goal of doing all this work is to find the optimal policy.", "start": 4562.695, "duration": 4.245}, {"text": "Right? So you want to find L_t so that you can compute the optimal policy.", "start": 4566.94, "duration": 3.54}, {"text": "You notice that L_t, um,", "start": 4570.48, "duration": 2.655}, {"text": "just depends on Phi but not Psi.", "start": 4573.135, "duration": 9.12}, {"text": "Right? Um, so, you know,", "start": 4582.255, "duration": 2.565}, {"text": "and, and maybe it's gonna make sense.", "start": 4584.82, "duration": 1.8}, {"text": "You're going to- when you take an action,", "start": 4586.62, "duration": 2.16}, {"text": "you get to some new state and", "start": 4588.78, "duration": 1.71}, {"text": "your future payoffs is a quadratic function plus a constant.", "start": 4590.49, "duration": 2.79}, {"text": "It doesn't matter what that constant is.", "start": 4593.28, "duration": 1.62}, {"text": "Right? And so in order to compute the optimal action, in order to compute L_t,", "start": 4594.9, "duration": 4.725}, {"text": "you need to, you need to know Phi or actually Phi T", "start": 4599.625, "duration": 3.405}, {"text": "plus 1 but you don't need to know what is Psi T plus 1.", "start": 4603.03, "duration": 3.645}, {"text": "Right. Now, if you", "start": 4606.675, "duration": 8.175}, {"text": "look at the way we do the dynamic programming,", "start": 4614.85, "duration": 4.095}, {"text": "the backwards recursion, um,", "start": 4618.945, "duration": 3.51}, {"text": "what if you implement a piece of code that doesn't involve it to compute Psi, right?", "start": 4622.455, "duration": 5.955}, {"text": "So these are the two equations you use,", "start": 4628.41, "duration": 1.785}, {"text": "update Phi and Psi.", "start": 4630.195, "duration": 1.44}, {"text": "But whether, you know,", "start": 4631.635, "duration": 1.41}, {"text": "let's say you delete this line of code.", "start": 4633.045, "duration": 2.25}, {"text": "Just don't bother to compute it and just don't", "start": 4635.295, "duration": 2.745}, {"text": "bother to compute that and don't bother to compute that.", "start": 4638.04, "duration": 3.09}, {"text": "Right? So you notice that Phi depends on Phi T plus 1,", "start": 4641.13, "duration": 4.38}, {"text": "but it doesn't depend on Psi.", "start": 4645.51, "duration": 2.07}, {"text": "Uh, and so you can implement the whole thing and compute the optimal policy and", "start": 4647.58, "duration": 4.14}, {"text": "compute the optimal actions without ever computing Psi.", "start": 4651.72, "duration": 4.38}, {"text": "Right. Now the funny thing about this is that the only place that Sigma w", "start": 4656.1, "duration": 9.0}, {"text": "appears is that it", "start": 4665.1, "duration": 5.85}, {"text": "affects only Psi T. Right?", "start": 4670.95, "duration": 7.995}, {"text": "So, you know, if,", "start": 4678.945, "duration": 1.86}, {"text": "if we do what I've just cross out in orange and just don't bother to compute", "start": 4680.805, "duration": 3.705}, {"text": "Psi T. Then the whole algorithm doesn't even use Sigma w. Right.", "start": 4684.51, "duration": 5.805}, {"text": "So one very interesting property of the LQR, um, ah,", "start": 4690.315, "duration": 4.935}, {"text": "of this formalism is that the optimal policy does not depend on Sigma w. Right.", "start": 4695.25, "duration": 6.615}, {"text": "Um, and I think,", "start": 4701.865, "duration": 1.755}, {"text": "ah, maybe this is a, ah,", "start": 4703.62, "duration": 2.34}, {"text": "so V star depends on Sigma w,", "start": 4705.96, "duration": 7.59}, {"text": "because if the noise is very large,", "start": 4713.55, "duration": 1.71}, {"text": "if there's a huge gust of wind blowing a helicopter all over the place,", "start": 4715.26, "duration": 2.805}, {"text": "then the value would be worse.", "start": 4718.065, "duration": 1.905}, {"text": "But Pi star and L_t, uh,", "start": 4719.97, "duration": 3.27}, {"text": "do not depend on the Sigma w. Okay.", "start": 4723.24, "duration": 10.245}, {"text": "Um, so this is a property that is very specific to LQR,", "start": 4733.485, "duration": 3.659}, {"text": "don't, don't, don't overgeneralize it to other reinforcement learning algorithms.", "start": 4737.144, "duration": 3.691}, {"text": "But this, um, I think the intuition to, ah,", "start": 4740.835, "duration": 4.455}, {"text": "um, take from this is first,", "start": 4745.29, "duration": 1.89}, {"text": "if you are actually applying this system,", "start": 4747.18, "duration": 1.59}, {"text": "you know, don't bother to, don't,", "start": 4748.77, "duration": 1.92}, {"text": "don't- I say don't,", "start": 4750.69, "duration": 1.125}, {"text": "don't try to hard to estimate Sigma w, because you,", "start": 4751.815, "duration": 2.445}, {"text": "you don't actually need to use it, uh,", "start": 4754.26, "duration": 1.875}, {"text": "which is why when we're fitting a linear model,", "start": 4756.135, "duration": 2.655}, {"text": "I didn't talk too much about how you actually estimate Sigma w. Because in LQR system,", "start": 4758.79, "duration": 3.99}, {"text": "it literally doesn't matter in", "start": 4762.78, "duration": 2.61}, {"text": "a mathematical sense in terms of what does the optimal policy you compute.", "start": 4765.39, "duration": 3.39}, {"text": "And the second, the maybe slightly more useful intuition to take away from this,", "start": 4768.78, "duration": 4.05}, {"text": "is that, ah, for a lot of MDPs,", "start": 4772.83, "duration": 2.4}, {"text": "if you're building a robot,", "start": 4775.23, "duration": 1.365}, {"text": "you know, ah, um, remember to add some noise to your system", "start": 4776.595, "duration": 4.23}, {"text": "but the exact noise you add doesn't matter as much as one might think.", "start": 4780.825, "duration": 4.43}, {"text": "So what I've seen in,", "start": 4785.255, "duration": 1.065}, {"text": "in working on a lot of robots,", "start": 4786.32, "duration": 1.2}, {"text": "a lot of MDPs is, you know,", "start": 4787.52, "duration": 1.575}, {"text": "do add some noise to the system and make sure your learning algorithm is robust to noise.", "start": 4789.095, "duration": 4.29}, {"text": "And the form of the noise you add, it does matter.", "start": 4793.385, "duration": 2.565}, {"text": "I don't say it doesn't matter at all.", "start": 4795.95, "duration": 1.35}, {"text": "I mean, in, in LQR, it doesn't matter at all.", "start": 4797.3, "duration": 2.37}, {"text": "For other MDPs, it does matter.", "start": 4799.67, "duration": 1.985}, {"text": "But I think the fact that you've remembered to add some noise is often", "start": 4801.655, "duration": 3.5}, {"text": "in practice more important than the exact details of,", "start": 4805.155, "duration": 3.63}, {"text": "you know, is the noise 10%  higher or is the noise 10% lower.", "start": 4808.785, "duration": 2.895}, {"text": "If, if the noise is 100% higher or lower,", "start": 4811.68, "duration": 2.13}, {"text": "that will often make a big difference, but, ah,", "start": 4813.81, "duration": 2.16}, {"text": "but, but when I'm, you know,", "start": 4815.97, "duration": 1.185}, {"text": "training a model of our helicopter or something,", "start": 4817.155, "duration": 1.755}, {"text": "the noise is something that, you know,", "start": 4818.91, "duration": 1.605}, {"text": "I pay a little bit of attention to but I pay much more attention to", "start": 4820.515, "duration": 3.015}, {"text": "making sure that the matrices A and B are accurate than,", "start": 4823.53, "duration": 3.465}, {"text": "and then, you know, a little bit sloppiness in the", "start": 4826.995, "duration": 3.015}, {"text": "act of using your noise model is something that an MDP can probably survive,", "start": 4830.01, "duration": 3.12}, {"text": "that your policy can survive.", "start": 4833.13, "duration": 1.29}, {"text": "Okay. Let's take one last question. Yes.", "start": 4834.42, "duration": 1.71}, {"text": "[inaudible].", "start": 4836.13, "duration": 4.44}, {"text": "Oh V? Uh, ah, oh I see.", "start": 4840.57, "duration": 4.69}, {"text": "Sorry, yes. Let me see my notes.", "start": 4845.59, "duration": 2.95}, {"text": "Oh V. That was,", "start": 4848.54, "duration": 1.18}, {"text": "ah, this is a V. Yes, thanks, yeah.", "start": 4849.72, "duration": 3.35}, {"text": "Okay, cool. Thanks everyone.", "start": 4855.47, "duration": 2.245}, {"text": "Let's break and I'll see you for the final lecture on [NOISE] Wednesday.", "start": 4857.715, "duration": 3.405}, {"text": "Thanks everyone", "start": 4861.12, "duration": 1.63}]