[{"text": "Hey guys. Um, let's get started.", "start": 3.23, "duration": 3.59}, {"text": "So over the last several weeks,", "start": 6.82, "duration": 2.44}, {"text": "you've learned a lot about many different learning algorithms from linear regression,", "start": 9.26, "duration": 4.55}, {"text": "to logistic regression, to generalizing models,", "start": 13.81, "duration": 2.19}, {"text": "generative algorithms like GDA and Naive Bayes to most recently support-vector machines.", "start": 16.0, "duration": 5.6}, {"text": "Um, what I'd like to do today is to start talking", "start": 21.6, "duration": 3.55}, {"text": "about advice for applying learning algorithms.", "start": 25.15, "duration": 3.81}, {"text": "To teach a little bit about the theory behind, um,", "start": 28.96, "duration": 2.61}, {"text": "how to make good decisions of what to do,", "start": 31.57, "duration": 2.97}, {"text": "how to actually apply these algorithms.", "start": 34.54, "duration": 2.235}, {"text": "And so today, um,", "start": 36.775, "duration": 1.875}, {"text": "I wanna discuss bias and variance.", "start": 38.65, "duration": 2.79}, {"text": "Um, and it turns out, you know, I've,", "start": 41.44, "duration": 2.49}, {"text": "I've built quite a lot of machine learning systems, um,", "start": 43.93, "duration": 2.755}, {"text": "and it turns out that bias and variance is one of those concepts.", "start": 46.685, "duration": 3.235}, {"text": "It's, sort of, easy to understand,", "start": 49.92, "duration": 2.315}, {"text": "but hard to master.", "start": 52.235, "duration": 1.275}, {"text": "Uh, uh, what does it- lots of those,", "start": 53.51, "duration": 2.52}, {"text": "was it all these board games or sometimes, uh, uh,", "start": 56.03, "duration": 2.52}, {"text": "smartphone games, say easy to learn, hard to master or something like that?", "start": 58.55, "duration": 3.39}, {"text": "So bias and variance is actually one of those things,", "start": 61.94, "duration": 1.56}, {"text": "where I've had PhD students that worked with me for several years and then graduated,", "start": 63.5, "duration": 5.4}, {"text": "and worked in the industry for a couple years after that.", "start": 68.9, "duration": 2.52}, {"text": "And, and they actually tell me that, you know,", "start": 71.42, "duration": 2.1}, {"text": "when they took, um,", "start": 73.52, "duration": 1.525}, {"text": "machine learning at Stanford,", "start": 75.045, "duration": 1.62}, {"text": "they learned bias and variance,", "start": 76.665, "duration": 1.735}, {"text": "but as they progressed for many years", "start": 78.4, "duration": 2.23}, {"text": "their understanding of bias and variance continues to deepen.", "start": 80.63, "duration": 2.97}, {"text": "So I'm gonna try to accelerate your learning,", "start": 83.6, "duration": 1.74}, {"text": "um, uh, uh, of,", "start": 85.34, "duration": 2.375}, {"text": "of bias and variance because I find that people that understand this concept,", "start": 87.715, "duration": 5.365}, {"text": "um, are much more efficient in", "start": 93.08, "duration": 3.03}, {"text": "terms of how you develop learning algorithms and make your algorithms work.", "start": 96.11, "duration": 2.835}, {"text": "So we'll talk about this today,", "start": 98.945, "duration": 1.3}, {"text": "and it'll be a recurring theme that'll come up again a", "start": 100.245, "duration": 2.375}, {"text": "few times in the next several weeks as well.", "start": 102.62, "duration": 2.69}, {"text": "Um, then we'll discuss regularization, um,", "start": 105.31, "duration": 3.2}, {"text": "uh, and talk about, um,", "start": 108.51, "duration": 1.96}, {"text": "how to reduce variance in learning algorithms,", "start": 110.47, "duration": 2.86}, {"text": "talk about train, dev, test splits, uh,", "start": 113.33, "duration": 2.375}, {"text": "and then also talk about a few model selection and cross-validation algorithms.", "start": 115.705, "duration": 6.28}, {"text": "Um, oh, let's see, reminders for today.", "start": 121.985, "duration": 3.66}, {"text": "Uh, Problem Set 1 is due tonight,", "start": 125.645, "duration": 2.75}, {"text": "uh, uh, 11:59 P.M.", "start": 128.395, "duration": 2.195}, {"text": "Uh, and, uh, if you are not yet ready to submit it today,", "start": 130.59, "duration": 4.31}, {"text": "uh, late submissions are accepted until Saturday evening.", "start": 134.9, "duration": 3.12}, {"text": "Saturday 11:59 P.M, with the details of late submissions, uh,", "start": 138.02, "duration": 4.51}, {"text": "written according to the late day policy written on the course website. So,", "start": 142.53, "duration": 3.045}, {"text": "so I definitely encourage you to submit your homework on time today.", "start": 145.575, "duration": 3.225}, {"text": "If for some reason you're not able to the late submission,", "start": 148.8, "duration": 3.385}, {"text": "which we don't encourage anyone to take advantage of,", "start": 152.185, "duration": 2.575}, {"text": "but it is written, uh, on the course website.", "start": 154.76, "duration": 3.065}, {"text": "And Problem Set 2 will be released, uh shortly.", "start": 157.825, "duration": 4.43}, {"text": "Actually I think, uh, it was already posted online,", "start": 162.255, "duration": 2.255}, {"text": "um, uh, and is due two weeks from now.", "start": 164.51, "duration": 3.0}, {"text": "Um, yeah.", "start": 167.51, "duration": 2.22}, {"text": "Right. And so, uh, okay.", "start": 169.73, "duration": 2.815}, {"text": "So, um, and, and what I'm going to do today is talk about the conceptual aspects of this.", "start": 172.545, "duration": 6.545}, {"text": "Uh, and if you want to see even more math between these so the conceptual concepts,", "start": 179.09, "duration": 4.965}, {"text": "uh, at this Friday's discussion section,", "start": 184.055, "duration": 2.265}, {"text": "we'll be covering, um,", "start": 186.32, "duration": 1.725}, {"text": "some of the the, uh,", "start": 188.045, "duration": 1.48}, {"text": "mathematical aspects of learning theories such as error decomposition,", "start": 189.525, "duration": 3.665}, {"text": "uniform convergence, and VC dimension.", "start": 193.19, "duration": 2.28}, {"text": "You know, one, one interesting thing I've learned is, um,", "start": 195.47, "duration": 2.275}, {"text": "really watching the evolution of machine learning over many years is that,", "start": 197.745, "duration": 2.99}, {"text": "that machine learning as a discipline has actually", "start": 200.735, "duration": 2.175}, {"text": "become less mathematical over the years, right?", "start": 202.91, "duration": 2.79}, {"text": "Um, uh, so I remember when, um, you know,", "start": 205.7, "duration": 4.18}, {"text": "machine learning people used to worry about, uh,", "start": 209.88, "duration": 2.66}, {"text": "computing the normal equations,", "start": 212.54, "duration": 1.41}, {"text": "like x transpose x inverse equals x transpose y.", "start": 213.95, "duration": 2.235}, {"text": "How numerically stable is your numerical solver for solving", "start": 216.185, "duration": 3.315}, {"text": "the normal equations of inverting a matrix for solving linear equations.", "start": 219.5, "duration": 3.3}, {"text": "But because, um, numerical linear algebra has made tremendous rise,", "start": 222.8, "duration": 4.575}, {"text": "now we just call linear- linear algebra routine.", "start": 227.375, "duration": 3.285}, {"text": "To invert a matrix to solve linear system equations", "start": 230.66, "duration": 2.13}, {"text": "not worry about whether it's numerically stable or not.", "start": 232.79, "duration": 3.29}, {"text": "But once upon a time a lot of my friends in", "start": 236.08, "duration": 2.05}, {"text": "machine learning were reading text books on, uh,", "start": 238.13, "duration": 2.79}, {"text": "numerical optimization to figure out if your formula", "start": 240.92, "duration": 3.81}, {"text": "for inverting a matrix or really solving linear system equations was numerically stable.", "start": 244.73, "duration": 3.99}, {"text": "And so one of the trends I've seen is that,", "start": 248.72, "duration": 2.73}, {"text": "uh, I think, um,", "start": 251.45, "duration": 2.975}, {"text": "three or four years ago,", "start": 254.425, "duration": 1.415}, {"text": "to understand bias and variance,", "start": 255.84, "duration": 1.48}, {"text": "there was a certain mathematical theory that was crucial to understanding that.", "start": 257.32, "duration": 3.985}, {"text": "And so I used to teach that in CS229,", "start": 261.305, "duration": 2.755}, {"text": "but we decided, um,", "start": 264.06, "duration": 1.36}, {"text": "that we're constantly trying to improve this class, right?", "start": 265.42, "duration": 2.245}, {"text": "But I decided that, uh, that, uh,", "start": 267.665, "duration": 2.805}, {"text": "mathematical theory is actually less crucial today.", "start": 270.47, "duration": 2.985}, {"text": "If your main goal is to make these algorithms work.", "start": 273.455, "duration": 1.755}, {"text": "So we still teach it.", "start": 275.21, "duration": 1.23}, {"text": "But we're doing it in the Friday discussion section,", "start": 276.44, "duration": 1.92}, {"text": "and that leaves more time for the main lecture here to talk", "start": 278.36, "duration": 3.03}, {"text": "more about the conceptual thing that I think will help you build learning algorithms,", "start": 281.39, "duration": 3.3}, {"text": "as well as for the newer topics like, um,", "start": 284.69, "duration": 2.06}, {"text": "what- we'll talk about random forest,", "start": 286.75, "duration": 2.295}, {"text": "decision trees of random forests in neural networks next week.", "start": 289.045, "duration": 2.64}, {"text": "So here we go.", "start": 291.685, "duration": 1.955}, {"text": "Okay. So let's talk about bias and variance.", "start": 293.64, "duration": 3.335}, {"text": "Um, let's say you", "start": 296.975, "duration": 1.695}, {"text": "have this dataset. [NOISE]", "start": 298.67, "duration": 9.31}, {"text": "Um, I'm gonna draw the same dataset three times.", "start": 307.98, "duration": 2.24}, {"text": "[NOISE]", "start": 310.22, "duration": 8.49}, {"text": "Okay. So, um, let's say", "start": 318.71, "duration": 4.8}, {"text": "you have a housing price prediction problem where", "start": 323.51, "duration": 2.46}, {"text": "this is the size of the house and this is the price of the house.", "start": 325.97, "duration": 2.895}, {"text": "Um, it looks like if you fit a straight line to this data,", "start": 328.865, "duration": 5.91}, {"text": "maybe it's not too bad, right?", "start": 334.775, "duration": 4.65}, {"text": "But it looks like this dataset seems to go", "start": 339.425, "duration": 3.255}, {"text": "up and then curve downward a little bit, right?", "start": 342.68, "duration": 2.725}, {"text": "And so [NOISE] maybe this is a slightly better model if you fit a, let me see.", "start": 345.405, "duration": 5.075}, {"text": "So this if you fit a linear function,", "start": 350.48, "duration": 1.875}, {"text": "um, Theta 0 plus Theta 1x.", "start": 352.355, "duration": 3.0}, {"text": "Uh, but if you fit a quadratic model,", "start": 355.355, "duration": 3.445}, {"text": "maybe this actually fits to the dataset a little bit better.", "start": 358.8, "duration": 3.1}, {"text": "Um, or you could actually fit a high order polynomial.", "start": 364.66, "duration": 4.405}, {"text": "This is one, two, three, four, five, six examples.", "start": 369.065, "duration": 2.89}, {"text": "So if you fit a fifth order polynomial,", "start": 371.955, "duration": 3.445}, {"text": "let's say the 5x to the 5th,", "start": 377.84, "duration": 3.1}, {"text": "then, um, you can actually fit a function that passes through all the points perfectly.", "start": 380.94, "duration": 7.22}, {"text": "But that doesn't seem like a great model for this data either.", "start": 388.16, "duration": 5.175}, {"text": "And so, um, to name", "start": 393.335, "duration": 2.805}, {"text": "this phenomenon, the function assuming the one in the middle is what we like,", "start": 396.14, "duration": 4.44}, {"text": "um, fitting a quadratic function is maybe pretty good.", "start": 400.58, "duration": 5.115}, {"text": "Let's call it just right.", "start": 405.695, "duration": 1.44}, {"text": "Whereas, um, this, uh,", "start": 407.135, "duration": 3.515}, {"text": "example on the left,", "start": 410.65, "duration": 1.925}, {"text": "it underfits the data,", "start": 412.575, "duration": 6.21}, {"text": "um, as in, it is not capturing the trend that is maybe semi-evident in the data.", "start": 418.785, "duration": 5.915}, {"text": "And we say this algorithm has high bias.", "start": 424.7, "duration": 3.4}, {"text": "And the term bias, um,", "start": 428.32, "duration": 3.15}, {"text": "the term bias has,", "start": 431.47, "duration": 1.505}, {"text": "has actually multiple meanings in the English language.", "start": 432.975, "duration": 2.915}, {"text": "We, as a society,", "start": 435.89, "duration": 1.53}, {"text": "want to to avoid racial bias,", "start": 437.42, "duration": 2.61}, {"text": "and gender bias, and discrimination against people's orientation, and things like that.", "start": 440.03, "duration": 4.17}, {"text": "So, uh, the term bias in machine learning has a completely separate meaning.", "start": 444.2, "duration": 4.425}, {"text": "Um, and it just means that, uh, and,", "start": 448.625, "duration": 2.205}, {"text": "and it just means that, um, uh,", "start": 450.83, "duration": 2.93}, {"text": "this learning algorithm had", "start": 453.76, "duration": 1.45}, {"text": "very strong preconceptions that the data could be fit by linear functions.", "start": 455.21, "duration": 4.8}, {"text": "This album had a very strong bias or", "start": 460.01, "duration": 2.07}, {"text": "the very strong preconception that", "start": 462.08, "duration": 1.68}, {"text": "the relationship between pricing and house- size of house is linear,", "start": 463.76, "duration": 3.0}, {"text": "and this bias turns out not to be true.", "start": 466.76, "duration": 2.5}, {"text": "So this is actually a different sense of bias than, than the,", "start": 469.26, "duration": 2.99}, {"text": "than the other types of undesirable bias we want to avoid in society or which,", "start": 472.25, "duration": 4.665}, {"text": "which interestingly comes up in machine learning as well in other contexts, right?", "start": 476.915, "duration": 3.495}, {"text": "We want our learning algorithms to avoid those different biases, there's a different use of the term.", "start": 480.41, "duration": 4.395}, {"text": "And in contrast and just cut off on the right,", "start": 484.805, "duration": 2.82}, {"text": "we say that this is overfitting, um, the data.", "start": 487.625, "duration": 5.535}, {"text": "And this algorithm has high variance.", "start": 493.16, "duration": 5.53}, {"text": "Um, and the term high variance comes from this intuition that,", "start": 498.69, "duration": 4.45}, {"text": "um, you happen to get these five examples,", "start": 503.14, "duration": 3.51}, {"text": "but if, you know,", "start": 506.65, "duration": 1.56}, {"text": "a friend of yours was to collect data from, uh, see here, six,", "start": 508.21, "duration": 4.2}, {"text": "six examples and a friend of yours was to", "start": 512.41, "duration": 2.13}, {"text": "collect a slightly different set of six examples, right?", "start": 514.54, "duration": 3.84}, {"text": "So if a friend of yours were to rerun the collected slightly different, um, uh,", "start": 518.38, "duration": 4.595}, {"text": "set of housings- houses, you know, right?", "start": 522.975, "duration": 4.845}, {"text": "Then this algorithm will fit some totally other varying function", "start": 527.82, "duration": 4.36}, {"text": "on this and so the- your predictions will have very high variance.", "start": 532.18, "duration": 4.65}, {"text": "If you think of this as averaging over different random draws of the data.", "start": 536.83, "duration": 3.535}, {"text": "So, so the, the variations if,", "start": 540.365, "duration": 2.74}, {"text": "if a friend of yours does the same experiment and they just", "start": 543.105, "duration": 2.225}, {"text": "get a slightly different dataset just due to random noise,", "start": 545.33, "duration": 2.4}, {"text": "then this algorithm fitting a fifth-order", "start": 547.73, "duration": 2.19}, {"text": "polynomial results in a totally different result.", "start": 549.92, "duration": 2.865}, {"text": "So that's-, uh, so we say that this algorithm has", "start": 552.785, "duration": 3.105}, {"text": "a very high variance, there's a lot of variability in the predictions this algorithm will make, okay?", "start": 555.89, "duration": 4.98}, {"text": "Um, so one of the things we'll need to do is,", "start": 560.87, "duration": 4.235}, {"text": "um, identify if your learning algorithm.", "start": 565.105, "duration": 3.535}, {"text": "Oh, so when you train a learning algorithm.", "start": 568.64, "duration": 2.01}, {"text": "It almost never works the first time.", "start": 570.65, "duration": 1.845}, {"text": "And so when I'm developing learning algorithms,", "start": 572.495, "duration": 2.925}, {"text": "my standard work flow is often to train an algorithm-,", "start": 575.42, "duration": 4.01}, {"text": "uh, often train up something quick and dirty,", "start": 579.43, "duration": 2.23}, {"text": "and then try to understand if the algorithm has a problem of high bias or high variance,", "start": 581.66, "duration": 5.01}, {"text": "if it's underfitting it or overfitting the data,", "start": 586.67, "duration": 1.905}, {"text": "and I use that insight to decide how to improve the learning algorithm.", "start": 588.575, "duration": 4.365}, {"text": "And I will say a lot more about,", "start": 592.94, "duration": 1.995}, {"text": "um, how to improve the learning algorithm.", "start": 594.935, "duration": 2.055}, {"text": "We have a menu of tools that we'll talk about in the next couple of weeks,", "start": 596.99, "duration": 3.56}, {"text": "about how to reduce bias or reduce variance of,", "start": 600.55, "duration": 4.0}, {"text": "uh, of, of your learning algorithms.", "start": 604.55, "duration": 2.41}, {"text": "Um, I should have mentioned that the problems of bias and variance,", "start": 606.96, "duration": 5.73}, {"text": "um, also hold true for classification problems.", "start": 612.69, "duration": 5.47}, {"text": "Uh, so, [NOISE], right.", "start": 618.16, "duration": 9.635}, {"text": "So let's say that's a binary classification problem.", "start": 627.795, "duration": 3.365}, {"text": "Um, if you fit a, uh,", "start": 631.16, "duration": 2.775}, {"text": "logistic regression model to this,", "start": 633.935, "duration": 2.595}, {"text": "you know, straight line fit to the data.", "start": 636.53, "duration": 3.18}, {"text": "Maybe that's not great, right?", "start": 639.71, "duration": 2.33}, {"text": "Um, if you fit a logistic regression model, um,", "start": 642.04, "duration": 3.615}, {"text": "with a few nonlinear features;", "start": 645.655, "duration": 3.285}, {"text": "so you have features x_1 and x_2.", "start": 648.94, "duration": 2.505}, {"text": "Um, if instead of using x_1 and x_2 as features,", "start": 651.445, "duration": 3.015}, {"text": "you use additional features x_1 squared,", "start": 654.46, "duration": 2.205}, {"text": "x_2 squared, x_1 times x_2, x_1,", "start": 656.665, "duration": 2.655}, {"text": "qx_2 and this is Phi of x, right?", "start": 659.32, "duration": 4.1}, {"text": "And you can have a small set of features you choose by hand.", "start": 663.42, "duration": 3.52}, {"text": "excuse me, probably more features in this or using SVM kernel and using SVM for this problem.", "start": 666.94, "duration": 6.115}, {"text": "Then, um, if you, let's see,", "start": 673.055, "duration": 3.265}, {"text": "if you have too many features,", "start": 676.32, "duration": 2.2}, {"text": "then you might actually have a learning algorithm that", "start": 678.52, "duration": 2.65}, {"text": "fits a decision boundary that looks like that.", "start": 681.17, "duration": 3.64}, {"text": "Right? And this learning algorithm actually gets", "start": 686.67, "duration": 3.46}, {"text": "perfect performance on the training set but this overfits.", "start": 690.13, "duration": 4.81}, {"text": "Um, excuse me, I meant to make the colors consistent,", "start": 695.28, "duration": 3.94}, {"text": "sorry I meant to use red.", "start": 699.22, "duration": 1.11}, {"text": "But you- you get what I mean.", "start": 700.33, "duration": 1.5}, {"text": "Um, and there's only if you choose somewhere in-between,", "start": 701.83, "duration": 6.27}, {"text": "you know, that you get something that,", "start": 708.1, "duration": 2.79}, {"text": "that seems to be a much better fit to the data.", "start": 710.89, "duration": 2.415}, {"text": "The green line seems to be a pretty good way of separating", "start": 713.305, "duration": 2.505}, {"text": "the positive and negative examples that they're sort of just right.", "start": 715.81, "duration": 2.805}, {"text": "So, uh, similar to,", "start": 718.615, "duration": 1.995}, {"text": "I guess I messed up the colors slightly before,", "start": 720.61, "duration": 1.665}, {"text": "kind of but similar to these colors here,", "start": 722.275, "duration": 1.875}, {"text": "the blue line underfits because it's not", "start": 724.15, "duration": 1.92}, {"text": "capturing trends that are pretty apparently in the data.", "start": 726.07, "duration": 2.61}, {"text": "The orange line overfits.", "start": 728.68, "duration": 2.22}, {"text": "It's just much too complicated a hypothesis whereas the green line,", "start": 730.9, "duration": 3.15}, {"text": "um, is just right, okay?", "start": 734.05, "duration": 3.075}, {"text": "So it turns out that, um,", "start": 737.125, "duration": 4.755}, {"text": "in the error of GPU computing ability to train models with a lot of features,", "start": 741.88, "duration": 7.17}, {"text": "um, one of the- by building a big enough model,", "start": 749.05, "duration": 4.665}, {"text": "uh, so take a support vector machine.", "start": 753.715, "duration": 1.71}, {"text": "If you add enough features to it,", "start": 755.425, "duration": 1.845}, {"text": "if you have a high enough dimensional feature space,", "start": 757.27, "duration": 2.865}, {"text": "um, or if you, um,", "start": 760.135, "duration": 2.175}, {"text": "take a linear regression model, logistic", "start": 762.31, "duration": 1.8}, {"text": "regression model and just add enough features to it,", "start": 764.11, "duration": 2.13}, {"text": "you can often, um, overfit the data.", "start": 766.24, "duration": 3.825}, {"text": "And it turns out that, um,", "start": 770.065, "duration": 2.76}, {"text": "one of the most effective ways to prevent overfitting, um, is regularization.", "start": 772.825, "duration": 6.21}, {"text": "So let me describe what that is and,", "start": 779.035, "duration": 2.67}, {"text": "um, excuse me, just finding my notes, reworking today's lecture.", "start": 781.705, "duration": 7.725}, {"text": "So this is new things I have not presented.", "start": 789.43, "duration": 3.37}, {"text": "Um, so all that. Okay, cool.", "start": 803.28, "duration": 4.645}, {"text": "And, um, regularization is the- it'll be one of those techniques that,", "start": 807.925, "duration": 5.07}, {"text": "um, won't take that long to explain.", "start": 812.995, "duration": 2.34}, {"text": "It'll sound deceptively simple but is one of the techniques that I use most often.", "start": 815.335, "duration": 5.22}, {"text": "I, I, I feel like I use regularization in many, many models.", "start": 820.555, "duration": 3.105}, {"text": "So, so just because it doesn't that sound that", "start": 823.66, "duration": 1.935}, {"text": "complicated or maybe won't even take that long to explain today,", "start": 825.595, "duration": 2.97}, {"text": "don't underestimate how widely used it is.", "start": 828.565, "duration": 2.325}, {"text": "It's used in- it's not used in", "start": 830.89, "duration": 2.415}, {"text": "every single machine learning model but it's used very, very often.", "start": 833.305, "duration": 2.805}, {"text": "Um, so here's the idea,", "start": 836.11, "duration": 11.68}, {"text": "um, which is- let's take linear regression.", "start": 847.97, "duration": 5.51}, {"text": "Right. So that's the optimization objective for linear regression.", "start": 866.25, "duration": 5.41}, {"text": "Um, if you want to add regularization, uh,", "start": 871.66, "duration": 3.315}, {"text": "you just add one extra term here,", "start": 874.975, "duration": 5.235}, {"text": "uh, Lambda, uh, times norm of,", "start": 880.21, "duration": 4.155}, {"text": "uh, Theta squared, right?", "start": 884.365, "duration": 2.61}, {"text": "Sometimes you write Lambda over two to make some of the derivations come out easier.", "start": 886.975, "duration": 4.905}, {"text": "And what this does is it takes your cost function for logistic regression,", "start": 891.88, "duration": 5.925}, {"text": "uh, which you try to minimize, try to minimize the square", "start": 897.805, "duration": 2.265}, {"text": "error fit to the data and you're creating", "start": 900.07, "duration": 3.09}, {"text": "an incentive term for the algorithm to make the parameter's Thetas, uh, smaller, okay?", "start": 903.16, "duration": 5.97}, {"text": "So this is called a regularization term.", "start": 909.13, "duration": 2.8}, {"text": "And it turns out that, um,", "start": 917.04, "duration": 3.52}, {"text": "let's take the linear regression overfitting example, right.", "start": 920.56, "duration": 6.15}, {"text": "So you know if you set Lambda equal to0,", "start": 926.71, "duration": 4.455}, {"text": "then it's just linear regression over the fifth order polynomial features.", "start": 931.165, "duration": 4.11}, {"text": "Uh, it turns out that as you increase Lambda,", "start": 935.275, "duration": 4.545}, {"text": "you know, Lambda to some intermediate value,", "start": 939.82, "duration": 1.935}, {"text": "uh, depending on the scales of data.", "start": 941.755, "duration": 1.185}, {"text": "Let's say you said Lambda equals 1.", "start": 942.94, "duration": 1.875}, {"text": "Then, when you solve for this minimization problem,", "start": 944.815, "duration": 3.905}, {"text": "or this augmented problem for the value of Theta, um,", "start": 948.72, "duration": 2.865}, {"text": "this term penalizes the parameters being too big and it turns out that you", "start": 951.585, "duration": 6.075}, {"text": "end up with a fit that looks a little bit better, right?", "start": 957.66, "duration": 5.65}, {"text": "It maybe it looks like that, okay?", "start": 963.31, "duration": 2.7}, {"text": "Um, and by preventing the parameters Theta from being too big,", "start": 966.01, "duration": 3.765}, {"text": "you make it harder for the learning algorithm to overfit the data and it turns out", "start": 969.775, "duration": 5.055}, {"text": "fitting a very high order polynomial like that may result in", "start": 974.83, "duration": 2.55}, {"text": "value of states that is very large, right?", "start": 977.38, "duration": 2.82}, {"text": "Um, and, and then if you set Lambda to be too large,", "start": 980.2, "duration": 5.985}, {"text": "then you actually end up,", "start": 986.185, "duration": 2.025}, {"text": "um, in an underfitting regime, okay?", "start": 988.21, "duration": 4.185}, {"text": "So there'll usually be some optimal value of Lambda where if Lambda equals 0,", "start": 992.395, "duration": 5.445}, {"text": "you're not using any regularization.", "start": 997.84, "duration": 1.71}, {"text": "You're so- maybe overfitting.", "start": 999.55, "duration": 1.875}, {"text": "Um, if Lambda is way too big,", "start": 1001.425, "duration": 2.13}, {"text": "then you're forcing all the parameters to be too close to 0.", "start": 1003.555, "duration": 4.305}, {"text": "Um, in fact actually, if you think about it,", "start": 1007.86, "duration": 1.77}, {"text": "if Lambda was equal to 10 to the 100 or some ridiculously large number,", "start": 1009.63, "duration": 4.65}, {"text": "then you're really forcing all the Thetas to be 0, right?", "start": 1014.28, "duration": 4.38}, {"text": "If all the Thetas is 0, then you know then you're kinda fitting the straight line, right?", "start": 1018.66, "duration": 4.56}, {"text": "So that's if Lambda equals, uh, 10 to the 100.", "start": 1023.22, "duration": 3.255}, {"text": "And so- and this is a very simple function which is the function 0, right?", "start": 1026.475, "duration": 4.545}, {"text": "And, and this function h of Theta,", "start": 1031.02, "duration": 2.1}, {"text": "x equals 0, right, approximately 0.", "start": 1033.12, "duration": 4.545}, {"text": "It is a very simple function which you get if you set Lambda very large.", "start": 1037.665, "duration": 4.38}, {"text": "And by doubling Lambda between, you know,", "start": 1042.045, "duration": 2.73}, {"text": "a far too large value like", "start": 1044.775, "duration": 1.695}, {"text": "10 to the 100 compared to a far too small value like Lambda 0, you, you,", "start": 1046.47, "duration": 4.29}, {"text": "you smooth the interpolate between this much too simple function", "start": 1050.76, "duration": 3.33}, {"text": "of h equals 0 and a much too complex function, okay?", "start": 1054.09, "duration": 3.81}, {"text": "Um, so there is,", "start": 1057.9, "duration": 6.27}, {"text": "um- so that's pretty, uh,", "start": 1064.17, "duration": 3.615}, {"text": "it, it, it, um,", "start": 1067.785, "duration": 1.725}, {"text": "it- so that's pretty much it for regularization in", "start": 1069.51, "duration": 3.48}, {"text": "terms of what you need to implement but you feel", "start": 1072.99, "duration": 1.65}, {"text": "like your learning algorithm may be overfitting,", "start": 1074.64, "duration": 2.205}, {"text": "um, add this to your model and solve this optimization problem,", "start": 1076.845, "duration": 5.1}, {"text": "um, and it will help relieve overfitting.", "start": 1081.945, "duration": 3.255}, {"text": "Um, more generally, if you are, um, let's see.", "start": 1085.2, "duration": 10.245}, {"text": "More generally if you have a, uh,", "start": 1095.445, "duration": 3.9}, {"text": "say logistic regression problem where this is your cost function.", "start": 1099.345, "duration": 5.845}, {"text": "Then to add regularization,", "start": 1112.12, "duration": 4.03}, {"text": "I guess instead of min this is a max, right?", "start": 1116.15, "duration": 2.61}, {"text": "If you're applying logistic regression, uh,", "start": 1118.76, "duration": 2.235}, {"text": "then this was the original cost function, um,", "start": 1120.995, "duration": 2.715}, {"text": "then you can have minus [NOISE] Lambda or Lambda over 2,", "start": 1123.71, "duration": 4.875}, {"text": "right, it just depends on scaling of Lambda times", "start": 1128.585, "duration": 2.595}, {"text": "the norm of Theta squared and there's a minus here because for logistic regression,", "start": 1131.18, "duration": 3.66}, {"text": "we're maximizing rather than minimizing.", "start": 1134.84, "duration": 1.875}, {"text": "Or this could be argmax at any of the generalized linear model family as well.", "start": 1136.715, "duration": 3.78}, {"text": "But by subtracting Lambda times the norm of Theta squared,", "start": 1140.495, "duration": 3.555}, {"text": "this allows you to also regularize", "start": 1144.05, "duration": 1.68}, {"text": "the classification algorithm such as logistic regression.", "start": 1145.73, "duration": 3.345}, {"text": "Okay? [NOISE] Um, it turns out that, uh,", "start": 1149.075, "duration": 5.61}, {"text": "and- and I- I- I- wan- I make an analogy that, uh,", "start": 1154.685, "duration": 3.54}, {"text": "where all the math details are true,", "start": 1158.225, "duration": 2.805}, {"text": "but we don't wanna talk about all the math details.", "start": 1161.03, "duration": 2.625}, {"text": "It turns out that, um,", "start": 1163.655, "duration": 1.845}, {"text": "one of the reasons the support vector machine doesn't", "start": 1165.5, "duration": 2.505}, {"text": "overfit too badly even though it has,", "start": 1168.005, "duration": 2.58}, {"text": "you know, been working in infinite like,", "start": 1170.585, "duration": 2.55}, {"text": "you know, infinite dimensional feature space, right?", "start": 1173.135, "duration": 2.625}, {"text": "So- so why- why doesn't a support vector machine just overfit like crazy?", "start": 1175.76, "duration": 3.975}, {"text": "We showed, uh, on Monday that by using kernels,", "start": 1179.735, "duration": 3.615}, {"text": "it's sort of using infinite dimensional feature space, right?", "start": 1183.35, "duration": 3.03}, {"text": "So why doesn't it always fit these crazy complicated functions,", "start": 1186.38, "duration": 3.45}, {"text": "it just overfits the dataset like crazy?", "start": 1189.83, "duration": 2.858}, {"text": "It turns out and the theory is complicated.", "start": 1192.688, "duration": 1.492}, {"text": "It turns out that, um,", "start": 1194.18, "duration": 2.55}, {"text": "[NOISE] you know, the optimization", "start": 1196.73, "duration": 3.495}, {"text": "objective of the support vector machine was to minimize the norm of w squared.", "start": 1200.225, "duration": 3.855}, {"text": "Uh, this turns out to, uh,", "start": 1204.08, "duration": 2.01}, {"text": "correspond to maximizing the margin,", "start": 1206.09, "duration": 2.01}, {"text": "the geometric margin SVM,", "start": 1208.1, "duration": 1.62}, {"text": "and it's actually possible to prove that, um,", "start": 1209.72, "duration": 2.565}, {"text": "this has a similar effect as that, right?", "start": 1212.285, "duration": 4.02}, {"text": "That this is why the support vector machine despite working in", "start": 1216.305, "duration": 2.835}, {"text": "infinite dimensional feature space sometimes, um,", "start": 1219.14, "duration": 2.97}, {"text": "by forcing the parameters to be small is difficult for", "start": 1222.11, "duration": 2.91}, {"text": "the support vector machine to overfit the data too much.", "start": 1225.02, "duration": 3.51}, {"text": "Okay? The theory to actually show this is quite complicated.", "start": 1228.53, "duration": 3.6}, {"text": "Uh, um, uh, yeah, er,", "start": 1232.13, "duration": 2.34}, {"text": "uh, it's actually very- yeah,", "start": 1234.47, "duration": 1.935}, {"text": "is to show that the cost of cost Phi is where this is- where norm of", "start": 1236.405, "duration": 3.225}, {"text": "w small cannot be too complicated, complicating can overfit basically.", "start": 1239.63, "duration": 3.375}, {"text": "Um, but that's why, uh, SVMs can", "start": 1243.005, "duration": 2.115}, {"text": "work in- can work in infinite dimensional feature spaces. Yeah?", "start": 1245.12, "duration": 3.3}, {"text": "[inaudible]", "start": 1248.42, "duration": 7.88}, {"text": "Oh, sure. Do you ever regularized per elements of parameters?", "start": 1256.3, "duration": 2.995}, {"text": "Um, not really.", "start": 1259.295, "duration": 1.545}, {"text": "Uh, and the problem with that is, um,", "start": 1260.84, "duration": 3.24}, {"text": "you know, let me give one- let me give one more specific example,", "start": 1264.08, "duration": 2.79}, {"text": "then come back to that, right?", "start": 1266.87, "duration": 1.515}, {"text": "So it turns out that, um,", "start": 1268.385, "duration": 2.46}, {"text": "uh, so we talked about Naive Bayes as a text classification algorithm.", "start": 1270.845, "duration": 6.045}, {"text": "[NOISE] It turns out that,", "start": 1276.89, "duration": 3.0}, {"text": "um, let's see if the text classification algorithm problem,", "start": 1279.89, "duration": 2.61}, {"text": "you know, classify spam, non-spam,", "start": 1282.5, "duration": 2.13}, {"text": "or classified it to a sentiment,", "start": 1284.63, "duration": 1.56}, {"text": "possible negative sentiment of a tweet or something.", "start": 1286.19, "duration": 2.1}, {"text": "[NOISE] Let's say you have 100 examples,", "start": 1288.29, "duration": 2.445}, {"text": "but you have [NOISE] 10,000 dimensional features, right?", "start": 1290.735, "duration": 4.575}, {"text": "So let's say your features are these, you know,", "start": 1295.31, "duration": 2.415}, {"text": "take the dictionary A, aardvark and so on.", "start": 1297.725, "duration": 3.435}, {"text": "So 101, right.", "start": 1301.16, "duration": 2.43}, {"text": "So let's say you construct a feature like this, um,", "start": 1303.59, "duration": 2.595}, {"text": "it turns out that if you fit logistic regression to this type of data,", "start": 1306.185, "duration": 5.055}, {"text": "where you have 10,000 parameters and 100 examples,", "start": 1311.24, "duration": 3.33}, {"text": "this will badly- this will probably overfit the data,", "start": 1314.57, "duration": 3.36}, {"text": "um, because you have, uh,", "start": 1317.93, "duration": 3.3}, {"text": "uh, but it turns out that if you use logistic regression with regularization,", "start": 1321.23, "duration": 4.395}, {"text": "this is actually a pretty good algorithm for text classification, um,", "start": 1325.625, "duration": 3.345}, {"text": "and this will usually i- in terms of performance accuracy, um, yeah,", "start": 1328.97, "duration": 4.335}, {"text": "because this is logistic regression,", "start": 1333.305, "duration": 1.365}, {"text": "you need to implement gradient descent or something to solve local value parameters.", "start": 1334.67, "duration": 3.945}, {"text": "But logistic regression with regularization for text classification,", "start": 1338.615, "duration": 3.93}, {"text": "will usually perform- outperform Naive Bayes", "start": 1342.545, "duration": 2.715}, {"text": "o- on a- on a classification accuracy standpoint.", "start": 1345.26, "duration": 3.345}, {"text": "Uh, without regularization, logistic regression will badly overfit this data, right?", "start": 1348.605, "duration": 5.01}, {"text": "Um, and- and to- to explain a bit more, um, you know,", "start": 1353.615, "duration": 5.01}, {"text": "imagine that you have a three-dimensional subspace where you have two examples.", "start": 1358.625, "duration": 6.84}, {"text": "Then all you can do is fit a straight line,", "start": 1365.465, "duration": 2.385}, {"text": "right, for the hyper-plane to separate these two examples.", "start": 1367.85, "duration": 3.06}, {"text": "But so one rule of thumb for,", "start": 1370.91, "duration": 2.55}, {"text": "um, logistic regression is that,", "start": 1373.46, "duration": 2.16}, {"text": "if you do not use regularization,", "start": 1375.62, "duration": 2.43}, {"text": "it's nice if the number of examples is at least on", "start": 1378.05, "duration": 2.88}, {"text": "the order of the number of parameters you want to fit, right?", "start": 1380.93, "duration": 2.97}, {"text": "So this is if you're not using regularization.", "start": 1383.9, "duration": 1.995}, {"text": "It's nice if- in fact,", "start": 1385.895, "duration": 1.38}, {"text": "I- I personally think that, uh,", "start": 1387.275, "duration": 1.35}, {"text": "I tend to use the duration only if the number of", "start": 1388.625, "duration": 3.375}, {"text": "examples can be maybe 10x bigger than the number of examples,", "start": 1392.0, "duration": 4.35}, {"text": "uh, because that's what you need to have enough information", "start": 1396.35, "duration": 2.61}, {"text": "to fit good choices for all these parameters,", "start": 1398.96, "duration": 2.58}, {"text": "um, but that's if you're not using regularization.", "start": 1401.54, "duration": 2.745}, {"text": "But if you are using regularization, then, um,", "start": 1404.285, "duration": 2.79}, {"text": "you can fit, you know,", "start": 1407.075, "duration": 1.44}, {"text": "even 10,000 parameters, right?", "start": 1408.515, "duration": 2.61}, {"text": "Even with only 100 examples,", "start": 1411.125, "duration": 1.695}, {"text": "and this will be a pretty decent,", "start": 1412.82, "duration": 1.905}, {"text": "um, text classification algorithm.", "start": 1414.725, "duration": 2.1}, {"text": "Okay? Um, the question you have just now: why don't we regularize per parameter, right?", "start": 1416.825, "duration": 5.925}, {"text": "So why don't we, uh, let's see.", "start": 1422.75, "duration": 3.33}, {"text": "I guess instead of Lambda [NOISE] norm of Theta squared,", "start": 1426.08, "duration": 4.245}, {"text": "it would be a sum over j Lambda j,", "start": 1430.325, "duration": 3.75}, {"text": "you know, Theta j squared, right?", "start": 1434.075, "duration": 2.76}, {"text": "Um, the reason we don't do this is because you then end up with,", "start": 1436.835, "duration": 3.6}, {"text": "if you have 10,000 parameters here,", "start": 1440.435, "duration": 2.04}, {"text": "you end up with another 10,000 parameters here,", "start": 1442.475, "duration": 2.955}, {"text": "and so choosing all these 10,000 Lambdas is", "start": 1445.43, "duration": 2.715}, {"text": "as difficult as just choosing all these parameters in the first place.", "start": 1448.145, "duration": 2.97}, {"text": "So we don't have good weights to do this.", "start": 1451.115, "duration": 1.485}, {"text": "Whereas, when you talk about cross-validation, multiple regression a little bit,", "start": 1452.6, "duration": 3.24}, {"text": "we'll talk about how to choose maybe one parameter Lambda,", "start": 1455.84, "duration": 3.15}, {"text": "but that- those techniques won't work for choosing from", "start": 1458.99, "duration": 2.295}, {"text": "10,000 parameters Lambda j. You've got a question?", "start": 1461.285, "duration": 2.865}, {"text": "[inaudible]", "start": 1464.15, "duration": 14.97}, {"text": "You're absolutely right. Yes. Thank you. Um, yes.", "start": 1479.12, "duration": 2.52}, {"text": "So in order to make sure that the different Lambdas on the similar scale, uh,", "start": 1481.64, "duration": 3.405}, {"text": "a common pre-processing step we're using learning algorithms is, uh,", "start": 1485.045, "duration": 4.04}, {"text": "take your different features, um,", "start": 1489.085, "duration": 1.84}, {"text": "so for text classification of all the features is 01,", "start": 1490.925, "duration": 2.655}, {"text": "you can just leave the features alone.", "start": 1493.58, "duration": 1.65}, {"text": "But if a housing classification,", "start": 1495.23, "duration": 2.325}, {"text": "if feature one is the size of house which I guess ranges from,", "start": 1497.555, "duration": 3.3}, {"text": "I don't know, 100 to,", "start": 1500.855, "duration": 1.395}, {"text": "uh, how big are the biggest houses?", "start": 1502.25, "duration": 1.755}, {"text": "I don't know, like whatever.", "start": 1504.005, "duration": 1.56}, {"text": "Let's say houses go from, I don't know,", "start": 1505.565, "duration": 1.995}, {"text": "five inches square feet to 10,000 square feet.", "start": 1507.56, "duration": 2.1}, {"text": "Ten thousand square feet is really really big for a house, I guess.", "start": 1509.66, "duration": 2.565}, {"text": "But the numb- feature x2 is the number of", "start": 1512.225, "duration": 2.625}, {"text": "bedrooms which probably ranges from like, I don't know,", "start": 1514.85, "duration": 2.61}, {"text": "one to- I guess there's some houses with a ton of bedrooms,", "start": 1517.46, "duration": 2.76}, {"text": "but I would say most houses have at most five bedrooms,", "start": 1520.22, "duration": 2.19}, {"text": "I don't know, right?", "start": 1522.41, "duration": 1.425}, {"text": "Then these features are on very different scales and, uh,", "start": 1523.835, "duration": 2.925}, {"text": "normalizing them to all be on a similar scale,", "start": 1526.76, "duration": 2.94}, {"text": "so subtract out the mean and divide it by the standard deviation.", "start": 1529.7, "duration": 3.33}, {"text": "So scale all of these things to be between,", "start": 1533.03, "duration": 2.445}, {"text": "you know, 01 over 2 minus 1,", "start": 1535.475, "duration": 2.28}, {"text": "um, to 1, would- would- would be", "start": 1537.755, "duration": 2.805}, {"text": "a good pre-processing step before applying these methods.", "start": 1540.56, "duration": 2.565}, {"text": "Um, it turns out that this will make gradient descent run faster as well,", "start": 1543.125, "duration": 3.24}, {"text": "as a common pre-processing step to scale", "start": 1546.365, "duration": 1.83}, {"text": "each individual feature to be on a similar range of values.", "start": 1548.195, "duration": 3.66}, {"text": "All right. Yeah? At the back?", "start": 1551.855, "duration": 3.115}, {"text": "Uh, can we quickly go back to some more support vector machine model like NLG?", "start": 1559.54, "duration": 11.255}, {"text": "So it's actually both, so just to", "start": 1570.795, "duration": 3.225}, {"text": "repeat it, why-why don't support vector machines suffer too badly,", "start": 1574.02, "duration": 3.18}, {"text": "is it because it's small numbers for vectors or is it because of", "start": 1577.2, "duration": 2.46}, {"text": "minimizing the penalty W. Um,", "start": 1579.66, "duration": 2.805}, {"text": "I would say the formal argument relies more on the latter.", "start": 1582.465, "duration": 2.79}, {"text": "So it turns out that if you look all the class- if you're looking", "start": 1585.255, "duration": 2.805}, {"text": "at all the class of functions separate the data with a large margin, ah,", "start": 1588.06, "duration": 3.135}, {"text": "that class has low complexity formalized by", "start": 1591.195, "duration": 2.91}, {"text": "low VC dimension which you'll learn about in", "start": 1594.105, "duration": 2.115}, {"text": "Friday's discussion section if you want to come to that.", "start": 1596.22, "duration": 2.31}, {"text": "And so, it turns out that the class of all functions that separate the data of", "start": 1598.53, "duration": 3.48}, {"text": "a large margin is", "start": 1602.01, "duration": 1.29}, {"text": "a relatively simple class of functions by- and by simple class functions,", "start": 1603.3, "duration": 3.18}, {"text": "I mean, it has low VC dimension.", "start": 1606.48, "duration": 1.335}, {"text": "We should talk about this Friday.", "start": 1607.815, "duration": 1.035}, {"text": "Um, and thus any function within that class of functions,", "start": 1608.85, "duration": 3.93}, {"text": "uh, is not too likely to over-fit.", "start": 1612.78, "duration": 2.19}, {"text": "So, um, it is convenient", "start": 1614.97, "duration": 2.219}, {"text": "the support vector machine has a relatively low number of support vectors.", "start": 1617.189, "duration": 3.061}, {"text": "But, um, uh, you could imagine", "start": 1620.25, "duration": 2.655}, {"text": "other algorithms of a very large number of support vectors,", "start": 1622.905, "duration": 3.135}, {"text": "uh, but smallest to large margin is still a low complexity class that will move with it.", "start": 1626.04, "duration": 4.68}, {"text": "Alright, next one question.", "start": 1630.72, "duration": 2.71}, {"text": "I'm sorry say that again.", "start": 1640.34, "duration": 1.84}, {"text": "[inaudible]", "start": 1642.18, "duration": 12.54}, {"text": "Oh, sure yes. So is it possible that so yes.", "start": 1654.72, "duration": 2.94}, {"text": "So one of the- so yes.", "start": 1657.66, "duration": 2.46}, {"text": "So in general, models that have", "start": 1660.12, "duration": 2.34}, {"text": "high bias tend to underfit and models have high variance tend to overfit.", "start": 1662.46, "duration": 3.105}, {"text": "Um, we use these terms over-fit high variance,", "start": 1665.565, "duration": 3.51}, {"text": "underfit high bias not quite and they have very similar meanings.", "start": 1669.075, "duration": 3.855}, {"text": "Right, at their first approximation assume they, they mean the same thing.", "start": 1672.93, "duration": 2.58}, {"text": "One thing we'll see later,", "start": 1675.51, "duration": 1.29}, {"text": "uh, two weeks from now is,", "start": 1676.8, "duration": 1.365}, {"text": "uh, we'll talk about algorithms with high bias and high variance.", "start": 1678.165, "duration": 3.705}, {"text": "So, uh, this is, uh,", "start": 1681.87, "duration": 2.955}, {"text": "and actually one way to think of high bias and high variance,", "start": 1684.825, "duration": 2.865}, {"text": "we will talk about this later,", "start": 1687.69, "duration": 1.32}, {"text": "is if you have a dataset that looks like this,", "start": 1689.01, "duration": 3.04}, {"text": "uh, and if somehow your classifier has very high complexity,", "start": 1692.3, "duration": 7.9}, {"text": "there is a very, very complicated function.", "start": 1700.2, "duration": 2.565}, {"text": "But for some reason it's still not fitting your data well right,", "start": 1702.765, "duration": 3.165}, {"text": "so that would be one way to have high bias and high variance which does", "start": 1705.93, "duration": 2.58}, {"text": "happen. All right. Cool.", "start": 1708.51, "duration": 4.59}, {"text": "[NOISE].", "start": 1713.1, "duration": 14.9}, {"text": "So to wrap up the discussion on regularization, um,", "start": 1728.0, "duration": 6.19}, {"text": "there's one- so mechanically the way you implement", "start": 1735.67, "duration": 4.76}, {"text": "regularization is by adding that penalty on the norm of the parameters,", "start": 1740.43, "duration": 4.35}, {"text": "uh, so that's what you actually implement.", "start": 1744.78, "duration": 2.265}, {"text": "It turns out that, um,", "start": 1747.045, "duration": 1.89}, {"text": "there's another way to think about regularization.", "start": 1748.935, "duration": 2.385}, {"text": "So you remember when we talked about the new, uh,", "start": 1751.32, "duration": 2.4}, {"text": "linear regression we talked about minimizing squared error and then later on we saw that", "start": 1753.72, "duration": 4.83}, {"text": "linear regression was maximum likelihood estimation", "start": 1758.55, "duration": 2.895}, {"text": "on a certain generalized linear model using,", "start": 1761.445, "duration": 2.295}, {"text": "uh, using, using, using a Gaussian distribution as", "start": 1763.74, "duration": 2.85}, {"text": "the choice for the exponential family as a member of the exponential family.", "start": 1766.59, "duration": 4.395}, {"text": "It turns out that, um,", "start": 1770.985, "duration": 1.605}, {"text": "there's a similar point of view you can", "start": 1772.59, "duration": 2.25}, {"text": "take on the regularization algorithm that we just saw.", "start": 1774.84, "duration": 2.865}, {"text": "Which is, let's say S is the training set.", "start": 1777.705, "duration": 3.825}, {"text": "[NOISE].", "start": 1781.53, "duration": 5.52}, {"text": "Right. So, um, given a training set,", "start": 1787.05, "duration": 7.11}, {"text": "um, you want to find the most likely value of Theta, right?", "start": 1794.16, "duration": 7.335}, {"text": "Um, and so by Bayes rule P of Theta given S is P of S given Theta", "start": 1801.495, "duration": 9.674}, {"text": "times P of Theta divided by P of S. And", "start": 1811.169, "duration": 6.181}, {"text": "so if you want to pick the value of", "start": 1817.35, "duration": 2.7}, {"text": "Theta that's the most likely value of Theta given the data you saw,", "start": 1820.05, "duration": 4.92}, {"text": "then because the denominator is just a constant,", "start": 1824.97, "duration": 3.06}, {"text": "this is arg max over Theta of P of S given Theta times P of Theta.", "start": 1828.03, "duration": 8.17}, {"text": "Um, and so if you're using", "start": 1840.56, "duration": 5.74}, {"text": "logistic regression then the first term is this.", "start": 1846.3, "duration": 7.36}, {"text": "Right and in the second term is P of Theta,", "start": 1856.13, "duration": 5.83}, {"text": "um, where this is the,", "start": 1861.96, "duration": 2.055}, {"text": "you know, logistic regression model say,", "start": 1864.015, "duration": 3.175}, {"text": "right or any generalized linear model.", "start": 1868.52, "duration": 4.04}, {"text": "And it turns out that, um,", "start": 1872.99, "duration": 3.52}, {"text": "if you assume P of Theta is Gaussian.", "start": 1876.51, "duration": 4.705}, {"text": "So if you assume P of Theta is follow Theta.", "start": 1881.215, "duration": 5.045}, {"text": "The prior probability on Theta is Gaussian with mean 0 and,", "start": 1886.26, "duration": 6.99}, {"text": "uh, some variance tau squared i.", "start": 1893.25, "duration": 4.275}, {"text": "So in other words P of Theta is,", "start": 1897.525, "duration": 3.36}, {"text": "you know, 1 over root 2 pi, um,", "start": 1900.885, "duration": 3.015}, {"text": "I guess this would be the determinant of tau squared i, right,", "start": 1903.9, "duration": 5.16}, {"text": "e to the negative, um,", "start": 1909.06, "duration": 2.595}, {"text": "Theta transpose, uh, tau squared i inverse.", "start": 1911.655, "duration": 6.655}, {"text": "Right. So the Gaussian probability as follows.", "start": 1919.82, "duration": 3.535}, {"text": "It turns out that if, um,", "start": 1923.355, "duration": 2.385}, {"text": "this is your prior distribution for Theta and you plug this in", "start": 1925.74, "duration": 3.51}, {"text": "here and you take logs computer maps and so on,", "start": 1929.25, "duration": 4.77}, {"text": "then you end up with exactly the regularization technique that we found just now.", "start": 1934.02, "duration": 4.56}, {"text": "Okay. Um, and so in everything we've been doing so", "start": 1938.58, "duration": 4.95}, {"text": "far we've been taking a, um, frequentist interpretation.", "start": 1943.53, "duration": 6.345}, {"text": "I guess the two main schools of statistics are", "start": 1949.875, "duration": 3.525}, {"text": "the frequentist school of statistic and the Bayesian school of statistic,", "start": 1953.4, "duration": 7.905}, {"text": "um, and there used to be some titanic academic debates about which is the right one,", "start": 1961.305, "duration": 6.315}, {"text": "but I think, uh,", "start": 1967.62, "duration": 1.62}, {"text": "statisticians have gotten together and kind of made", "start": 1969.24, "duration": 3.03}, {"text": "peace and then go freely between these two more and more these days.", "start": 1972.27, "duration": 3.27}, {"text": "Maybe not now all the time but, uh,", "start": 1975.54, "duration": 1.86}, {"text": "but then the frequency score statistic.", "start": 1977.4, "duration": 2.175}, {"text": "We say that there is some data and we want to find, um,", "start": 1979.575, "duration": 4.44}, {"text": "the value of Theta that makes the data", "start": 1984.015, "duration": 5.175}, {"text": "as likely as possible and that's where we got maximum likelihood estimation right.", "start": 1989.19, "duration": 5.46}, {"text": "And in the frequentist school of statistics,", "start": 1994.65, "duration": 2.31}, {"text": "we view them as being some true value of Theta out in the world that is unknown.", "start": 1996.96, "duration": 4.935}, {"text": "Um, and so there is some true value of Theta that generated", "start": 2001.895, "duration": 2.565}, {"text": "all these housing prices and our goal is to estimate this true parameter.", "start": 2004.46, "duration": 5.13}, {"text": "In the Bayesian school of statistics we say that Theta is unknown.", "start": 2009.59, "duration": 4.38}, {"text": "But before you see even any data you already have some prior beliefs about how", "start": 2013.97, "duration": 5.52}, {"text": "housing prices are generated out in the world and", "start": 2019.49, "duration": 2.67}, {"text": "your prior beliefs are captured in a probability distribution,", "start": 2022.16, "duration": 3.525}, {"text": "uh, denoted by P of Theta,", "start": 2025.685, "duration": 2.085}, {"text": "so this is called the Gaussian prior.", "start": 2027.77, "duration": 2.77}, {"text": "And, um, we say that,", "start": 2032.35, "duration": 3.325}, {"text": "um, and- and if you look at this Gaussian prior.", "start": 2035.675, "duration": 2.655}, {"text": "[NOISE].", "start": 2038.33, "duration": 1.92}, {"text": "Excuse me. It's quite reasonable.", "start": 2040.25, "duration": 1.995}, {"text": "It's saying that before you've seen any data", "start": 2042.245, "duration": 2.265}, {"text": "on average I think the parameters of theta have mean", "start": 2044.51, "duration": 2.52}, {"text": "0 because I don't know if each Theta is positive or", "start": 2047.03, "duration": 2.4}, {"text": "negative so giving the mean 0 seems reasonable.", "start": 2049.43, "duration": 2.88}, {"text": "And most things in the world are Gaussians and we just", "start": 2052.31, "duration": 2.22}, {"text": "assume that my prior on Theta is Gaussian.", "start": 2054.53, "duration": 2.43}, {"text": "So you know, we could debate that this is, uh,", "start": 2056.96, "duration": 2.61}, {"text": "the right assumption but it's not totally unreasonable, right?", "start": 2059.57, "duration": 3.435}, {"text": "But they say well, for actually I think, you know,", "start": 2063.005, "duration": 2.685}, {"text": "for the next linear regression problem I'm", "start": 2065.69, "duration": 2.58}, {"text": "gonna work on next week and I have no idea what I'm going to work on,", "start": 2068.27, "duration": 3.645}, {"text": "where I'm going to apply linear regression that next week.", "start": 2071.915, "duration": 2.13}, {"text": "It is actually not too bad an assumption to say,", "start": 2074.045, "duration": 2.19}, {"text": "you know, my prior is Gaussian.", "start": 2076.235, "duration": 2.7}, {"text": "And in the Bayesian view of the world,", "start": 2078.935, "duration": 2.775}, {"text": "our goal is, um,", "start": 2081.71, "duration": 2.04}, {"text": "to find the value of Theta that is most likely after,", "start": 2083.75, "duration": 8.91}, {"text": "um, we have seen the data.", "start": 2092.66, "duration": 2.355}, {"text": "Okay. And so this is called map estimation.", "start": 2095.015, "duration": 4.665}, {"text": "Which stands for the maximum a posteriori estimation.", "start": 2099.68, "duration": 7.92}, {"text": "So this is actually the map estimator I guess the arg max of this, right?", "start": 2107.6, "duration": 5.44}, {"text": "Uh, as the map or the maximum a posteriori estimates of Theta which means,", "start": 2114.1, "duration": 5.305}, {"text": "look at the data, compute", "start": 2119.405, "duration": 1.455}, {"text": "the Bayesian posterior distribution of Theta", "start": 2120.86, "duration": 1.8}, {"text": "and pick the value of Theta that's most likely.", "start": 2122.66, "duration": 2.055}, {"text": "Okay. And so one of the things you do in the problem set that was just released, um,", "start": 2124.715, "duration": 6.225}, {"text": "is-is actually show this equivalence as well", "start": 2130.94, "duration": 3.2}, {"text": "as plugged in a different prior", "start": 2134.14, "duration": 3.375}, {"text": "for theta other than the Gaussian prior you experiment with,", "start": 2137.515, "duration": 2.76}, {"text": "uh, whether P of Theta is the Laplace prior and to", "start": 2140.275, "duration": 2.985}, {"text": "find a derive a different map as mean algorithm.", "start": 2143.26, "duration": 3.61}, {"text": "Okay. Um, all right, good. Yeah, question?", "start": 2146.87, "duration": 5.12}, {"text": "[inaudible].", "start": 2151.99, "duration": 13.51}, {"text": "Sorry can you say that again?", "start": 2165.5, "duration": 0.93}, {"text": "[inaudible]", "start": 2166.43, "duration": 12.09}, {"text": "Uh, yes.", "start": 2178.52, "duration": 1.11}, {"text": "[inaudible]", "start": 2179.63, "duration": 5.34}, {"text": "Oh I see, yes, can difference between", "start": 2184.97, "duration": 1.53}, {"text": "these two be seen as regularized versus non-regularized?", "start": 2186.5, "duration": 2.16}, {"text": "Yes. So, so, um,", "start": 2188.66, "duration": 1.59}, {"text": "MLU here corresponds to the origin of regularization,", "start": 2190.25, "duration": 3.99}, {"text": "uh, and this procedure here corresponds to adding regularization.", "start": 2194.24, "duration": 4.905}, {"text": "Um, it turns out that frequency statistic- statisticians can also use", "start": 2199.145, "duration": 4.095}, {"text": "regularization it just that they don't try to", "start": 2203.24, "duration": 1.77}, {"text": "justify it through a Bayesian prior they just say,", "start": 2205.01, "duration": 2.265}, {"text": "so if you're a frequentist statistic.", "start": 2207.275, "duration": 1.59}, {"text": "If you're a frequentist statistician your job is to wake up and come up with", "start": 2208.865, "duration": 4.005}, {"text": "an algorithm to estimate this you know true value of theta that exists out in the world,", "start": 2212.87, "duration": 4.07}, {"text": "and you can come up any procedure you want and to inspire your procedure,", "start": 2216.94, "duration": 3.0}, {"text": "you can add a regularization term.", "start": 2219.94, "duration": 1.695}, {"text": "I think there's a lot of these debates between frequentists and Bayesians are more philosophical.", "start": 2221.635, "duration": 4.065}, {"text": "I think there's a machine learning person, as an engineer.", "start": 2225.7, "duration": 2.82}, {"text": "I don't really you know,", "start": 2228.52, "duration": 1.595}, {"text": "I think the philosophical debates are lovely but I just- I,", "start": 2230.115, "duration": 2.81}, {"text": "I just like my stuff to work.", "start": 2232.925, "duration": 1.455}, {"text": "So, so, so frequentists can also infer regularization.", "start": 2234.38, "duration": 4.71}, {"text": "It just that they say this is part of the algorithm", "start": 2239.09, "duration": 2.67}, {"text": "they invented rather than derived from a Bayesian prior.", "start": 2241.76, "duration": 4.0}, {"text": "All right, cool. So, um,", "start": 2248.29, "duration": 4.78}, {"text": "[NOISE] all right.", "start": 2253.07, "duration": 8.18}, {"text": "Let's talk about um, so in,", "start": 2261.25, "duration": 3.555}, {"text": "in our discussion on regularization and choosing the degree of polynomial,", "start": 2264.805, "duration": 6.565}, {"text": "um, uh, all right.", "start": 2271.37, "duration": 5.05}, {"text": "So let's see, let's say I plot a chart where on the horizontal axis I plot,", "start": 2277.93, "duration": 5.47}, {"text": "um, [NOISE] model complexity.", "start": 2283.4, "duration": 5.505}, {"text": "So how complicated is your model?", "start": 2288.905, "duration": 1.605}, {"text": "So for example, uh, to the right of this curve could be a very high degree polynomial.", "start": 2290.51, "duration": 5.1}, {"text": "[NOISE] Right.", "start": 2295.61, "duration": 7.29}, {"text": "Um, and what you find is that as you", "start": 2302.9, "duration": 4.44}, {"text": "increase model complexity your training error- if you do not regularize, right?", "start": 2307.34, "duration": 7.65}, {"text": "So if, if you fit a", "start": 2314.99, "duration": 1.755}, {"text": "linear function, cosine function,", "start": 2316.745, "duration": 1.275}, {"text": "cubic function and so on.", "start": 2318.02, "duration": 1.335}, {"text": "You find that the higher the degree of", "start": 2319.355, "duration": 1.935}, {"text": "your polynomial the better your training error because you know,", "start": 2321.29, "duration": 2.82}, {"text": "a fifth-order polynomial always fits the data better than a fourth-order polynomial.", "start": 2324.11, "duration": 4.695}, {"text": "If you, if you do not regularize.", "start": 2328.805, "duration": 2.46}, {"text": "But what we saw with the original picture was that the ability of the algorithm", "start": 2331.265, "duration": 8.045}, {"text": "[NOISE] to generalize kind of goes down and then starts to go back up, right?", "start": 2339.31, "duration": 8.485}, {"text": "And so if you were to have a separate test set and evaluate", "start": 2347.795, "duration": 3.615}, {"text": "your classifier on a set of data that the algorithm hasn't seen so far,", "start": 2351.41, "duration": 4.215}, {"text": "so measure how well the algorithm generalizes to a different novel set of data,", "start": 2355.625, "duration": 3.87}, {"text": "then if you fit a linear function then this underfits [NOISE].", "start": 2359.495, "duration": 7.395}, {"text": "If you fit a fifth-order polynomial this overfits,", "start": 2366.89, "duration": 3.51}, {"text": "[NOISE] and there is somewhere in", "start": 2370.4, "duration": 2.79}, {"text": "between right, that is just right.", "start": 2373.19, "duration": 9.48}, {"text": "Okay? And um, this curve is true for regularization as well.", "start": 2382.67, "duration": 6.03}, {"text": "So say you apply linear regression", "start": 2388.7, "duration": 2.954}, {"text": "with 10,000 features to a very small training example.", "start": 2391.654, "duration": 4.556}, {"text": "If lambda was much too big then they will um, underfit.", "start": 2397.45, "duration": 9.295}, {"text": "If [NOISE] lambda was 0 so,", "start": 2406.745, "duration": 5.145}, {"text": "you're not regularizing at all then they will overfit,", "start": 2411.89, "duration": 2.76}, {"text": "and there will be some intermediate value of", "start": 2414.65, "duration": 2.61}, {"text": "lambda that is not too big and not too small that you know,", "start": 2417.26, "duration": 2.955}, {"text": "balances overfitting and underfitting.", "start": 2420.215, "duration": 2.625}, {"text": "Okay. So, um, what I'd like to do next is describe uh, a mechanistic.", "start": 2422.84, "duration": 5.595}, {"text": "A few different mechanistic procedures for trying to find this point in the middle,", "start": 2428.435, "duration": 5.145}, {"text": "right? And so [NOISE]", "start": 2433.58, "duration": 24.39}, {"text": "um, so given a data set", "start": 2457.97, "duration": 4.38}, {"text": "[NOISE]", "start": 2462.35, "duration": 8.72}, {"text": "what we'll often do is um,", "start": 2471.07, "duration": 2.28}, {"text": "take your data set and split it into different subsets,", "start": 2473.35, "duration": 3.24}, {"text": "uh, and a, a,", "start": 2476.59, "duration": 1.41}, {"text": "a good hygiene is to take a data in the trained set- train, dev and test sets, um,", "start": 2478.0, "duration": 4.59}, {"text": "So if you have say,", "start": 2482.59, "duration": 2.22}, {"text": "10,000 examples, all right,", "start": 2484.81, "duration": 4.645}, {"text": "and you're trying to carry out this model selection problem.", "start": 2489.455, "duration": 3.375}, {"text": "So for example, let's say you're trying to decide what", "start": 2492.83, "duration": 3.48}, {"text": "order polynomial you want to fit, [NOISE] right.", "start": 2496.31, "duration": 7.11}, {"text": "Or you're trying to choose the value of lambda, um,", "start": 2503.42, "duration": 5.835}, {"text": "or you're trying to choose the value of tau,", "start": 2509.255, "duration": 2.79}, {"text": "that was the bandwidth parameter in uh,", "start": 2512.045, "duration": 2.625}, {"text": "locally weighted regression that you saw in the problem set- that we saw with,", "start": 2514.67, "duration": 3.66}, {"text": "uh, locally weighted regression, all right?", "start": 2518.33, "duration": 1.56}, {"text": "So, um, or you're trying to choose a value C in a support vector machine.", "start": 2519.89, "duration": 5.04}, {"text": "So remember, the SVM objective was actually this, right.", "start": 2524.93, "duration": 6.78}, {"text": "With the you know, subject to some other things but for", "start": 2531.71, "duration": 2.73}, {"text": "the O unknown soft margin that we talked about on Wednesday- uh, talked about on Monday.", "start": 2534.44, "duration": 5.16}, {"text": "You're trying to minimize the normal W and then there was this additional parameter C", "start": 2539.6, "duration": 4.86}, {"text": "that trains off how much you insist on", "start": 2544.46, "duration": 2.13}, {"text": "classifying every training example perfectly. All right.", "start": 2546.59, "duration": 3.225}, {"text": "So whether you're trying to make- which of these decisions you are trying to make,", "start": 2549.815, "duration": 4.38}, {"text": "um, how do you,", "start": 2554.195, "duration": 2.355}, {"text": "uh, you know, choose a polynomial size or choose lambda or choose", "start": 2556.55, "duration": 4.02}, {"text": "tau or choose parameter C which also has this bias-variance trade-off.", "start": 2560.57, "duration": 4.035}, {"text": "There'll be some values of C that are too large and some values of C that are too small.", "start": 2564.605, "duration": 3.105}, {"text": "[NOISE]", "start": 2567.71, "duration": 11.67}, {"text": "So here's one thing you can do which is um,", "start": 2579.38, "duration": 2.37}, {"text": "uh, let's see, so", "start": 2581.75, "duration": 6.33}, {"text": "split your training data S into a subset which I'm gonna call the uh,", "start": 2588.08, "duration": 6.75}, {"text": "raw training set as subscript train,", "start": 2594.83, "duration": 2.97}, {"text": "um, and then some subset which we wanna call S subscript dev.", "start": 2597.8, "duration": 4.245}, {"text": "And dev stands for uh, development,", "start": 2602.045, "duration": 2.715}, {"text": "[NOISE] um, and then later we'll talk about [NOISE] a separate test set.", "start": 2604.76, "duration": 7.56}, {"text": "And so what you can do is train each model,", "start": 2612.32, "duration": 6.645}, {"text": "and by model I mean, um,", "start": 2618.965, "duration": 2.94}, {"text": "option [NOISE] for the degree of polynomial", "start": 2621.905, "duration": 7.875}, {"text": "[NOISE] on S train.", "start": 2629.78, "duration": 8.68}, {"text": "Um, so you're evaluating a menu of models, right?", "start": 2638.56, "duration": 4.66}, {"text": "So let's say, this is model 1,", "start": 2643.22, "duration": 2.265}, {"text": "model 2, and so on up to model 5, up to some number.", "start": 2645.485, "duration": 4.215}, {"text": "They can train each of these models, uh,", "start": 2649.7, "duration": 2.55}, {"text": "on the first subset of your data [NOISE] and then get some hypothesis.", "start": 2652.25, "duration": 7.33}, {"text": "Let's call it h_i, [NOISE] um, and then,", "start": 2660.13, "duration": 5.17}, {"text": "[NOISE] measure the error", "start": 2665.3, "duration": 6.135}, {"text": "on S dev which is a second subset of your data called the development set.", "start": 2671.435, "duration": 5.34}, {"text": "And pick the one- [NOISE]", "start": 2676.775, "duration": 11.97}, {"text": "Okay. So rather than- and- and- uh,", "start": 2688.745, "duration": 4.32}, {"text": "I wanna contrast this with an alternative procedure, right?", "start": 2693.065, "duration": 4.695}, {"text": "So the two sets of the da- two subsets of the data,", "start": 2697.76, "duration": 2.25}, {"text": "some test set data, training set, and development set.", "start": 2700.01, "duration": 2.985}, {"text": "And uh, after training, uh,", "start": 2702.995, "duration": 3.15}, {"text": "first order polynomial, second order polynomial,", "start": 2706.145, "duration": 1.695}, {"text": "third order polynomial on the training set,", "start": 2707.84, "duration": 1.65}, {"text": "we evaluate all of these different models on", "start": 2709.49, "duration": 2.34}, {"text": "the separate held-out developments sets and then pick", "start": 2711.83, "duration": 2.58}, {"text": "the one with the lowest error on the development set.", "start": 2714.41, "duration": 2.94}, {"text": "Okay, um, but the one thing to not do would be to evaluate", "start": 2717.35, "duration": 4.395}, {"text": "all these algorithms instead on the training set and then pick", "start": 2721.745, "duration": 3.105}, {"text": "the one with the lowest error on the training set, right.", "start": 2724.85, "duration": 3.24}, {"text": "Why not- wha- what goes wrong when you do that? Yeah.", "start": 2728.09, "duration": 8.49}, {"text": "[BACKGROUND] [inaudible]", "start": 2736.58, "duration": 2.25}, {"text": "Yeah, right, you just over-fit.", "start": 2738.83, "duration": 1.155}, {"text": "How- why- why will you over-fit?", "start": 2739.985, "duration": 1.365}, {"text": "[BACKGROUND] Parts of the error,", "start": 2741.35, "duration": 5.91}, {"text": "what you want to remain so don't want- [inaudible]", "start": 2747.26, "duration": 3.39}, {"text": "Yeah. Yep, cool, right. So if you use this procedure,", "start": 2750.65, "duration": 2.775}, {"text": "you'll always end up picking the fifth order polynomial, right.", "start": 2753.425, "duration": 3.0}, {"text": "Because the more complex algorithm will always do better on the training set.", "start": 2756.425, "duration": 3.975}, {"text": "So if you do this, this will always cause you to say,", "start": 2760.4, "duration": 2.505}, {"text": "let's use the fifth order polynomial or the- or the highest possible order polynomial.", "start": 2762.905, "duration": 3.66}, {"text": "So this won't help you realize in the housing price prediction example to", "start": 2766.565, "duration": 3.975}, {"text": "the second order polynomial is the benefit of the data, right. Does it make sense?", "start": 2770.54, "duration": 4.095}, {"text": "Um, and that's why for this procedure,", "start": 2774.635, "duration": 3.255}, {"text": "um, if you evaluate your, uh,", "start": 2777.89, "duration": 2.58}, {"text": "model's error on a separate development set", "start": 2780.47, "duration": 3.855}, {"text": "that the algorithm did not see during training,", "start": 2784.325, "duration": 2.19}, {"text": "this allows you to hopefully pick a model that neither over-fits nor underfits.", "start": 2786.515, "duration": 4.455}, {"text": "And in this example, hopefully,", "start": 2790.97, "duration": 1.68}, {"text": "you find that uh, there will be the second-order polynomial,", "start": 2792.65, "duration": 3.015}, {"text": "right, that the one that's just right in between that actually does", "start": 2795.665, "duration": 2.475}, {"text": "best on your development set, okay.", "start": 2798.14, "duration": 3.24}, {"text": "Um, all right.", "start": 2801.38, "duration": 4.77}, {"text": "Now, uh, and then, um,", "start": 2806.15, "duration": 3.87}, {"text": "you know if- if you are, uh,", "start": 2810.02, "duration": 2.57}, {"text": "if- if you are publishing an academic paper on machine learning um,", "start": 2812.59, "duration": 4.12}, {"text": "then, this procedure has looked at", "start": 2816.71, "duration": 2.25}, {"text": "the training set as well as the development set, right.", "start": 2818.96, "duration": 2.64}, {"text": "So this- this procedure,", "start": 2821.6, "duration": 1.395}, {"text": "this piece of code is,", "start": 2822.995, "duration": 1.875}, {"text": "you know, is two in these decisions.", "start": 2824.87, "duration": 2.16}, {"text": "Uh, it's two in the parameters, the training set,", "start": 2827.03, "duration": 2.475}, {"text": "and it's two in the decision on the degree of polynomial to the dev set.", "start": 2829.505, "duration": 4.965}, {"text": "And so if you want to know,", "start": 2834.47, "duration": 1.89}, {"text": "if you want to publish a paper that say, oh,", "start": 2836.36, "duration": 1.695}, {"text": "my algorithm achieves 90% accuracy on this dataset um,", "start": 2838.055, "duration": 4.155}, {"text": "it's not valid to report the results on", "start": 2842.21, "duration": 2.79}, {"text": "the dev set because the algorithm has already been optimized to the dev set.", "start": 2845.0, "duration": 4.02}, {"text": "In particular, information about what's the most- um,", "start": 2849.02, "duration": 3.42}, {"text": "what's the best uh, degree of polynomial to choose", "start": 2852.44, "duration": 2.37}, {"text": "was derived from the dev set from the development set.", "start": 2854.81, "duration": 2.7}, {"text": "And so if you're publishing a paper or you want to report an unbiased result, um,", "start": 2857.51, "duration": 6.16}, {"text": "evaluate the algorithm on a separate test set,", "start": 2865.18, "duration": 6.25}, {"text": "S test and report that error, okay.", "start": 2871.43, "duration": 8.715}, {"text": "And so if you're publishing a paper,", "start": 2880.145, "duration": 1.515}, {"text": "it's considered good hygiene to um, uh,", "start": 2881.66, "duration": 3.3}, {"text": "report the error on a completely separate test set that you did", "start": 2884.96, "duration": 3.78}, {"text": "not in any way shape or form look at during the development of your model,", "start": 2888.74, "duration": 4.05}, {"text": "during the training procedure, okay.", "start": 2892.79, "duration": 2.655}, {"text": "Clear with things? Oh, yeah.", "start": 2895.445, "duration": 3.075}, {"text": "Are dev and test [inaudible] is uh,", "start": 2898.52, "duration": 3.27}, {"text": "generally different by much?", "start": 2901.79, "duration": 1.95}, {"text": "Um, a dev and test set's error isn't strictly different by much.", "start": 2903.74, "duration": 2.895}, {"text": "It depends on the size of- it depends on the size of the dataset.", "start": 2906.635, "duration": 3.24}, {"text": "Um, uh, and so it turns out that um,", "start": 2909.875, "duration": 4.95}, {"text": "actu- let- let- actually let me- let me give an example, actually.", "start": 2914.825, "duration": 4.155}, {"text": "So let's say you're trying to fit a degree of polynomial, right.", "start": 2918.98, "duration": 5.04}, {"text": "Um, and you want to choose uh,", "start": 2924.02, "duration": 3.42}, {"text": "uh, write the dev error.", "start": 2927.44, "duration": 2.82}, {"text": "So we can fill the first, second,", "start": 2930.26, "duration": 1.875}, {"text": "third, fourth, fifth degree polynomial.", "start": 2932.135, "duration": 2.82}, {"text": "And so um, after fitting all of these,", "start": 2934.955, "duration": 3.21}, {"text": "lets say that the square error right,", "start": 2938.165, "duration": 2.565}, {"text": "to use round numbers is 10, um,", "start": 2940.73, "duration": 3.855}, {"text": "5.1, 5.0, 4.9, you know,", "start": 2944.585, "duration": 7.35}, {"text": "um, 7, 10, and so on, okay.", "start": 2951.935, "duration": 4.78}, {"text": "Just to- just to use round numbers for illustrative purposes.", "start": 2956.715, "duration": 3.205}, {"text": "If you're using the dev error to pick the best hypothesis,", "start": 2959.92, "duration": 7.59}, {"text": "to pick the best hot spot,", "start": 2967.51, "duration": 1.17}, {"text": "you would say that uh,", "start": 2968.68, "duration": 1.745}, {"text": "using the fifth order polynomial gets you 4.9 squared error, right.", "start": 2970.425, "duration": 6.695}, {"text": "But did you really earn that 4.9 square error or did you just get lucky?", "start": 2977.12, "duration": 5.91}, {"text": "Because there is some noise and so maybe", "start": 2983.03, "duration": 3.06}, {"text": "all of these actually have error that close to 5.0.", "start": 2986.09, "duration": 3.435}, {"text": "But some are just higher, some are just lower,", "start": 2989.525, "duration": 2.25}, {"text": "and you just got a little bit lucky that on the dev set this did better.", "start": 2991.775, "duration": 3.84}, {"text": "Which is why, if you look at your dev set error,", "start": 2995.615, "duration": 2.805}, {"text": "your dev set error is a biased estimate, right.", "start": 2998.42, "duration": 3.18}, {"text": "And so where's your very large test set?", "start": 3001.6, "duration": 2.61}, {"text": "If it's a very large test set,", "start": 3004.21, "duration": 1.47}, {"text": "maybe the true numbers are 10, 5, 5,", "start": 3005.68, "duration": 3.24}, {"text": "5, 7, 10 are your actual expected squared errors.", "start": 3008.92, "duration": 5.49}, {"text": "It's just that um,", "start": 3014.41, "duration": 1.65}, {"text": "because of a little bit of noise you got lucky and reported 4.9.", "start": 3016.06, "duration": 3.645}, {"text": "And so this would be a bad thing to do in an academic paper, right.", "start": 3019.705, "duration": 2.685}, {"text": "Because it's uh, what you earned was an error of 5.0 you didn't earn an error of 4.9.", "start": 3022.39, "duration": 4.92}, {"text": "It's just that- because you're over-fitting a little bit in the dev set.", "start": 3027.31, "duration": 4.56}, {"text": "Um, you chose the thing that looked best for the dev set,", "start": 3031.87, "duration": 2.73}, {"text": "but your algorithm didn't actually achieve that error, it's just because of noise, okay.", "start": 3034.6, "duration": 4.335}, {"text": "So- so um, now in- in so- so it's considered a good practice to report um,", "start": 3038.935, "duration": 7.285}, {"text": "uh, uh, so reporting on the dev error isn't- isn't-", "start": 3046.53, "duration": 3.355}, {"text": "isn't really a valid unbiased procedure.", "start": 3049.885, "duration": 2.91}, {"text": "And- and uh, um, yeah. Do you have a question?", "start": 3052.795, "duration": 4.065}, {"text": "[BACKGROUND]", "start": 3056.86, "duration": 31.14}, {"text": "[inaudible]", "start": 3088.0, "duration": 0.1}, {"text": "Yeah. Ye- so- so one of the just to we say,", "start": 3088.1, "duration": 2.45}, {"text": "I- I yes, you're right.", "start": 3090.55, "duration": 1.26}, {"text": "One of the problems with some of the machine learning benchmarks that people worked on", "start": 3091.81, "duration": 3.6}, {"text": "for a long time is this is unavoidable mental over-fitting.", "start": 3095.41, "duration": 3.69}, {"text": "The people'd gotten to use the dataset and everyone's working the", "start": 3099.1, "duration": 2.31}, {"text": "same trying to publish the best numbers from the same test set.", "start": 3101.41, "duration": 2.64}, {"text": "So the academic committee on machine learning does have some amount of over-fitting uh,", "start": 3104.05, "duration": 4.95}, {"text": "to the standard benchmarks that people have worked on for a long time.", "start": 3109.0, "duration": 2.805}, {"text": "And this is an unfortunate result uh,", "start": 3111.805, "duration": 1.995}, {"text": "when the test is very- very large,", "start": 3113.8, "duration": 1.68}, {"text": "the amounts of over-fitting is probably smaller,", "start": 3115.48, "duration": 2.52}, {"text": "but when the test set is not big enough then the over-fitting result can cause um,", "start": 3118.0, "duration": 4.38}, {"text": "sometimes even research papers to uh,", "start": 3122.38, "duration": 1.785}, {"text": "to publish results that are uh,", "start": 3124.165, "duration": 1.725}, {"text": "probably over-fit to the data set, right.", "start": 3125.89, "duration": 2.4}, {"text": "um, uh, and so I think there is actually uh,", "start": 3128.29, "duration": 2.625}, {"text": "one standard academic benchmark because there's a dataset called CIFAR, it's quite small.", "start": 3130.915, "duration": 3.885}, {"text": "It's actually the very same research paper uh, uh,", "start": 3134.8, "duration": 2.85}, {"text": "analyzing um, results on CIFAR uh,", "start": 3137.65, "duration": 3.09}, {"text": "arguing that some fraction of their progress that was made was actually perhaps uh,", "start": 3140.74, "duration": 5.235}, {"text": "researchers uni- unintentionally over-fitting to this dataset.", "start": 3145.975, "duration": 3.585}, {"text": "Okay. Oh and by the way um,", "start": 3149.56, "duration": 4.455}, {"text": "one thing I do when I'm building you know,", "start": 3154.015, "duration": 2.085}, {"text": "production machine learning systems.", "start": 3156.1, "duration": 1.74}, {"text": "So when I'm- when I'm shipping a product, right.", "start": 3157.84, "duration": 2.19}, {"text": "I just don't build a speech recognition system and just make it work.", "start": 3160.03, "duration": 2.505}, {"text": "I just wanna, and not- and if I'm", "start": 3162.535, "duration": 2.055}, {"text": "not trying to publish a paper, I'm not trying to make some claim.", "start": 3164.59, "duration": 2.505}, {"text": "Sometimes I don't bother to have a test set, right.", "start": 3167.095, "duration": 2.865}, {"text": "So and uh, and it means I don't know the true error of the system sometimes uh,", "start": 3169.96, "duration": 4.515}, {"text": "but I'm very conscious of that.", "start": 3174.475, "duration": 1.47}, {"text": "If I don't have a lot of data,", "start": 3175.945, "duration": 1.425}, {"text": "sometimes I'm may decide to just not have", "start": 3177.37, "duration": 2.01}, {"text": "a test set and it means I just don't try to report the test set number.", "start": 3179.38, "duration": 3.225}, {"text": "I can report that dev set number which I know is", "start": 3182.605, "duration": 2.325}, {"text": "biased and I just don't report the test set number.", "start": 3184.93, "duration": 2.835}, {"text": "Don't do this when you're publishing your academic paper, right.", "start": 3187.765, "duration": 2.565}, {"text": "This is not good if you're publishing a paper or making claims on", "start": 3190.33, "duration": 2.82}, {"text": "the outside but all we're doing is building a product and not writing a paper out,", "start": 3193.15, "duration": 3.87}, {"text": "this is- this is actually okay. Uh, yeah.", "start": 3197.02, "duration": 2.64}, {"text": "[inaudible]", "start": 3199.66, "duration": 10.66}, {"text": "Yeah. Okay, good. Uh, that's- lemme, lemme get to that.", "start": 3210.32, "duration": 2.88}, {"text": "Good. So, um, the next topic about setting up the train dev test split is,", "start": 3213.2, "duration": 5.475}, {"text": "how do you decide how much data should go into each of these three subsets?", "start": 3218.675, "duration": 3.555}, {"text": "Um, so uh, uh,", "start": 3222.23, "duration": 3.209}, {"text": "I can tell- so, so let me just tell you", "start": 3225.439, "duration": 1.591}, {"text": "the historical perspective and then a modern perspective.", "start": 3227.03, "duration": 2.625}, {"text": "Um, historically, the rule of thumb was you take a training set, right?", "start": 3229.655, "duration": 5.04}, {"text": "Take your training set S and then you would send- here,", "start": 3234.695, "duration": 3.285}, {"text": "one rule of thumb that you see a lot of people referring to is, uh,", "start": 3237.98, "duration": 3.255}, {"text": "70% training, right?", "start": 3241.235, "duration": 3.405}, {"text": "30% test.", "start": 3244.64, "duration": 1.17}, {"text": "[NOISE] This is one common rule of thumb that you just hear a lot.", "start": 3245.81, "duration": 4.86}, {"text": "Uh, or maybe you have- if you- if you don't have a dev set,", "start": 3250.67, "duration": 3.39}, {"text": "if- if- if you're not doing model selection,", "start": 3254.06, "duration": 1.8}, {"text": "if you just- if you've already picked the model and now you're revising.", "start": 3255.86, "duration": 2.97}, {"text": "Or maybe you have people use,", "start": 3258.83, "duration": 2.58}, {"text": "you know, 60% train,", "start": 3261.41, "duration": 1.71}, {"text": "20% dev, 20% test.", "start": 3263.12, "duration": 4.86}, {"text": "Right? So these are rules of thumb that people use to give.", "start": 3267.98, "duration": 3.465}, {"text": "Um, and these are decent rules of thumb when you don't have a massive dataset.", "start": 3271.445, "duration": 5.925}, {"text": "So you may have 100- 100 examples,", "start": 3277.37, "duration": 2.55}, {"text": "maybe you have 1,000 examples,", "start": 3279.92, "duration": 1.995}, {"text": "maybe several thousand examples,", "start": 3281.915, "duration": 1.965}, {"text": "I think these rules of thumb are perfectly fine.", "start": 3283.88, "duration": 2.25}, {"text": "[NOISE] Um, what I'm seeing is that as you move to machine learning problems with really,", "start": 3286.13, "duration": 4.95}, {"text": "really giant datasets, the percentage of data you send to dev and test are shrinking.", "start": 3291.08, "duration": 6.24}, {"text": "Right? And, and so, here's what I mean.", "start": 3297.32, "duration": 1.92}, {"text": "Um, let's say you have 10 million examples.", "start": 3299.24, "duration": 4.98}, {"text": "Um, you know, yeah,", "start": 3304.22, "duration": 1.56}, {"text": "decent size, not giant but like a reasonable size.", "start": 3305.78, "duration": 2.865}, {"text": "Um, so le- let's,", "start": 3308.645, "duration": 2.16}, {"text": "let's take this- this is actually", "start": 3310.805, "duration": 1.305}, {"text": "a pretty good rule of thumb if you have a small dataset.", "start": 3312.11, "duration": 1.74}, {"text": "If you have a thou- if you have, you know, 5 million examples,", "start": 3313.85, "duration": 1.83}, {"text": "this is a perfectly fine rule of thumb to use.", "start": 3315.68, "duration": 1.83}, {"text": "Um, but if you have 10 million examples,", "start": 3317.51, "duration": 2.339}, {"text": "then, you know, you have 6 million,", "start": 3319.849, "duration": 3.241}, {"text": "[NOISE] 2 million, [NOISE] 2 million,", "start": 3323.09, "duration": 5.94}, {"text": "right, train, dev, test.", "start": 3329.03, "duration": 2.94}, {"text": "[NOISE] And the question is,", "start": 3331.97, "duration": 2.985}, {"text": "do you really need 2 million examples to", "start": 3334.955, "duration": 2.805}, {"text": "estimate the performance of your final classifier?", "start": 3337.76, "duration": 2.73}, {"text": "Uh, sometimes you do if you're working on online advertising,", "start": 3340.49, "duration": 4.395}, {"text": "you know, which I have done,", "start": 3344.885, "duration": 1.335}, {"text": "and you're trying to increase your ad click-through rate by 0.1%,", "start": 3346.22, "duration": 4.845}, {"text": "because it turns out increasing ad click-through rate by 0.1%,", "start": 3351.065, "duration": 2.82}, {"text": "which I've done multiple times,", "start": 3353.885, "duration": 1.35}, {"text": "uh, turns out to be very lucrative.", "start": 3355.235, "duration": 1.575}, {"text": "[LAUGHTER] Uh, uh, then you actually need a very large dataset to measure these very,", "start": 3356.81, "duration": 4.08}, {"text": "very small improvements because to- to- to increasing ad click-through rate by 0.1,", "start": 3360.89, "duration": 4.26}, {"text": "you might have actually a lot of projects.", "start": 3365.15, "duration": 1.29}, {"text": "You might have 10 projects,", "start": 3366.44, "duration": 1.335}, {"text": "each of which increases ad click-through rate by 0.01%, right?", "start": 3367.775, "duration": 3.825}, {"text": "And so to measure these very different- small differences in,", "start": 3371.6, "duration": 3.795}, {"text": "algorithm one does 0.01% better than algorithm b", "start": 3375.395, "duration": 3.3}, {"text": "by- so you need a lot of data to tease out that very small difference.", "start": 3378.695, "duration": 3.105}, {"text": "So if you're in the business of teasing out these very small differences,", "start": 3381.8, "duration": 3.495}, {"text": "you actually need very large test sets.", "start": 3385.295, "duration": 2.235}, {"text": "But if you're comparing different algorithms and one algorithm is, you know,", "start": 3387.53, "duration": 5.73}, {"text": "2% better or even 1% better than the other algorithm,", "start": 3393.26, "duration": 3.78}, {"text": "then with 1,000 examples maybe, right,", "start": 3397.04, "duration": 3.51}, {"text": "1,000 examples may be enough for you to distinguish", "start": 3400.55, "duration": 2.49}, {"text": "between these much larger differences.", "start": 3403.04, "duration": 2.79}, {"text": "Um, so my recommendation for choosing", "start": 3405.83, "duration": 3.09}, {"text": "the dev and test sets is choose them to be big enough,", "start": 3408.92, "duration": 3.705}, {"text": "um, that you have", "start": 3412.625, "duration": 2.595}, {"text": "enough data to make meaningful comparisons between different algorithms.", "start": 3415.22, "duration": 4.23}, {"text": "Uh, and if you suspect your algorithms will vary in performance by 0.01%,", "start": 3419.45, "duration": 4.59}, {"text": "you just need a lot of data to distinguish that, right?", "start": 3424.04, "duration": 3.36}, {"text": "So, so if you have 100 examples,", "start": 3427.4, "duration": 2.205}, {"text": "then, you know, if,", "start": 3429.605, "duration": 1.38}, {"text": "if one algorithm has 90% accuracy and one algorithm has 90.01% accuracy,", "start": 3430.985, "duration": 6.99}, {"text": "then unless you have at least 1,000 examples and maybe 10,000 or more,", "start": 3437.975, "duration": 3.87}, {"text": "you just can't see this very small difference, right?", "start": 3441.845, "duration": 2.145}, {"text": "If you have 100 examples,", "start": 3443.99, "duration": 1.11}, {"text": "you just can't measure this very small difference.", "start": 3445.1, "duration": 2.52}, {"text": "So my, my advice is, uh,", "start": 3447.62, "duration": 1.635}, {"text": "choose your dev and test sets to be big enough that, um,", "start": 3449.255, "duration": 3.705}, {"text": "uh, you could see the differences in the performance of algorithms that you,", "start": 3452.96, "duration": 4.17}, {"text": "uh, tha- that you roughly expect to see.", "start": 3457.13, "duration": 2.355}, {"text": "Um, and then you don't need to make your dev and test sets much larger than that.", "start": 3459.485, "duration": 3.495}, {"text": "And I would usually then just put the data.", "start": 3462.98, "duration": 2.55}, {"text": "You don't need the dev and sets back in the training set.", "start": 3465.53, "duration": 2.695}, {"text": "So when you're working with very large datasets, say, you know,", "start": 3468.225, "duration": 3.265}, {"text": "a million or 10 million or 100 million examples,", "start": 3471.49, "duration": 2.67}, {"text": "what you see is that the percentage of data", "start": 3474.16, "duration": 2.13}, {"text": "that goes into dev and test tends to be much smaller.", "start": 3476.29, "duration": 2.82}, {"text": "So it might be, um,", "start": 3479.11, "duration": 1.29}, {"text": "uh, so you see for example,", "start": 3480.4, "duration": 2.27}, {"text": "maybe 90% train,", "start": 3482.67, "duration": 2.21}, {"text": "you know, 5% dev,", "start": 3484.88, "duration": 1.41}, {"text": "and 5% test, right?", "start": 3486.29, "duration": 1.335}, {"text": "Or, or, or even smaller,", "start": 3487.625, "duration": 1.425}, {"text": "or even 1%, 1% depending on how much data you really need.", "start": 3489.05, "duration": 3.21}, {"text": "To measure to the level of accuracy you", "start": 3492.26, "duration": 2.4}, {"text": "need the differences in the performance of your algorithms.", "start": 3494.66, "duration": 2.62}, {"text": "Okay? Cool. All right,", "start": 3497.28, "duration": 4.855}, {"text": "um, just to give this whole procedure a name,", "start": 3502.135, "duration": 3.045}, {"text": "um, what we just did here between the train and dev set,", "start": 3505.18, "duration": 4.64}, {"text": "this procedure that we have is called hold-out cross validation.", "start": 3509.82, "duration": 3.68}, {"text": "[NOISE] And sometimes,", "start": 3513.5, "duration": 8.114}, {"text": "to distinguish this from other cross validation procedures we'll talk about in a minute,", "start": 3521.614, "duration": 4.126}, {"text": "sometimes this is called simple hold-out cross validation.", "start": 3525.74, "duration": 2.985}, {"text": "We'll talk about some other hold-out cross validation procedures in a second.", "start": 3528.725, "duration": 3.06}, {"text": "Um, and, uh, uh,", "start": 3531.785, "duration": 2.94}, {"text": "and the dev set,", "start": 3534.725, "duration": 3.085}, {"text": "um, is sometimes also called the cross validation set.", "start": 3541.51, "duration": 5.06}, {"text": "Okay, right? Uh, so sometimes you- people use- sometimes,", "start": 3549.64, "duration": 4.42}, {"text": "you hear people say, you know,", "start": 3554.06, "duration": 1.59}, {"text": "we're gonna use a cross validation set.", "start": 3555.65, "duration": 1.74}, {"text": "That means roughly the same thing as a, as a dev set.", "start": 3557.39, "duration": 2.925}, {"text": "Okay? So in the normal workflow of developing a learning algorithm,", "start": 3560.315, "duration": 6.045}, {"text": "uh, when you're given the dataset,", "start": 3566.36, "duration": 1.545}, {"text": "I would split it into a training set and a dev set.", "start": 3567.905, "duration": 2.64}, {"text": "Oh, and I used to say cross-validation set,", "start": 3570.545, "duration": 2.52}, {"text": "but cross-validation is just a mouthful.", "start": 3573.065, "duration": 2.025}, {"text": "So I think just motivated by the reducing number of syllables,", "start": 3575.09, "duration": 2.79}, {"text": "because you're using this classifier so often,", "start": 3577.88, "duration": 1.725}, {"text": "more and more people just call it the dev set,", "start": 3579.605, "duration": 1.815}, {"text": "but it means roughly the same thing.", "start": 3581.42, "duration": 1.35}, {"text": "Right? So, so when I'm,", "start": 3582.77, "duration": 1.77}, {"text": "uh, building a machine learning system,", "start": 3584.54, "duration": 1.71}, {"text": "I'll often take the dataset,", "start": 3586.25, "duration": 1.71}, {"text": "split into train and dev,", "start": 3587.96, "duration": 1.74}, {"text": "and if you need a test set,", "start": 3589.7, "duration": 1.11}, {"text": "then also a test set,", "start": 3590.81, "duration": 1.515}, {"text": "um, and then, uh,", "start": 3592.325, "duration": 1.92}, {"text": "keep on fitting the parameters to the training set and, uh,", "start": 3594.245, "duration": 3.615}, {"text": "evaluating the performance of your algorithm on", "start": 3597.86, "duration": 2.04}, {"text": "the dev set and using that to come up with new features,", "start": 3599.9, "duration": 3.09}, {"text": "choose the model size,", "start": 3602.99, "duration": 1.11}, {"text": "choose the regularization parameter Lambda, um,", "start": 3604.1, "duration": 2.355}, {"text": "really try out lots of different things and spend, you know,", "start": 3606.455, "duration": 3.105}, {"text": "several days or weeks, uh,", "start": 3609.56, "duration": 1.77}, {"text": "to optimize the performance on the dev set.", "start": 3611.33, "duration": 2.445}, {"text": "Um, and then, uh,", "start": 3613.775, "duration": 1.86}, {"text": "when you want to know how well is your algorithm performing,", "start": 3615.635, "duration": 3.18}, {"text": "to then evaluate the model on the test set.", "start": 3618.815, "duration": 3.435}, {"text": "Right? And- and the thing to be careful not to do is to make", "start": 3622.25, "duration": 3.33}, {"text": "any decisions about your model using the test set, right?", "start": 3625.58, "duration": 3.645}, {"text": "Because then- then your scientific data to the test set is no longer an unbiased estimate.", "start": 3629.225, "duration": 4.5}, {"text": "Uh, one- so- and- and o- one thing that is actually okay to do is, um,", "start": 3633.725, "duration": 5.145}, {"text": "if you have a team that's working on the problem,", "start": 3638.87, "duration": 2.475}, {"text": "if every week they measure the performance on", "start": 3641.345, "duration": 2.985}, {"text": "the test set and report out on a chart, right?", "start": 3644.33, "duration": 3.42}, {"text": "You know, uh, the, the performance on the test set, that's actually okay.", "start": 3647.75, "duration": 3.33}, {"text": "You can evaluate the model multiple times on the test set.", "start": 3651.08, "duration": 2.58}, {"text": "You can actually give out a weekly report, saying, this week,", "start": 3653.66, "duration": 2.325}, {"text": "for our online advertising system,", "start": 3655.985, "duration": 1.905}, {"text": "we have this result on the test set.", "start": 3657.89, "duration": 1.71}, {"text": "One week later, we have this result on test set,", "start": 3659.6, "duration": 1.8}, {"text": "one week later, this result on the test set.", "start": 3661.4, "duration": 1.26}, {"text": "It's actually okay to evaluate your algorithm repeatedly on the test set.", "start": 3662.66, "duration": 3.36}, {"text": "Uh, what's not okay is to use", "start": 3666.02, "duration": 2.1}, {"text": "those evaluations to make any decisions about your learning algorithm.", "start": 3668.12, "duration": 3.0}, {"text": "So for example, if one day you notice that your model is", "start": 3671.12, "duration": 3.24}, {"text": "doing worse this week than last week on the test set,", "start": 3674.36, "duration": 3.36}, {"text": "if you use that to revert back to an older model,", "start": 3677.72, "duration": 2.88}, {"text": "then you've just made a decision that's based on the test set,", "start": 3680.6, "duration": 2.58}, {"text": "and- and your test set is no longer biased.", "start": 3683.18, "duration": 2.01}, {"text": "But if all you do is report out the result but not make", "start": 3685.19, "duration": 2.88}, {"text": "any decisions based on the test set", "start": 3688.07, "duration": 1.62}, {"text": "performance such as whether to revert to an earlier model,", "start": 3689.69, "duration": 2.895}, {"text": "then you can- I- I- it's actually legitimate it's actually okay to keep on,", "start": 3692.585, "duration": 4.395}, {"text": "you know, use, uh,", "start": 3696.98, "duration": 1.305}, {"text": "use the same test set to track your- your team's performance over time.", "start": 3698.285, "duration": 3.54}, {"text": "Okay. All right.", "start": 3701.825, "duration": 3.75}, {"text": "Good. Um, so when you have very large data sets,", "start": 3705.575, "duration": 5.43}, {"text": "this is the procedure if you're developing for defining the train dev", "start": 3711.005, "duration": 5.445}, {"text": "and test sets and this procedure can be used to choose the model of polynomial.", "start": 3716.45, "duration": 5.775}, {"text": "It can also be used to choose the regularization parameter Lambda or or", "start": 3722.225, "duration": 3.615}, {"text": "the parameter C or- or- or the parameter tau from now locally weighted regression.", "start": 3725.84, "duration": 4.5}, {"text": "Um, now, whenever you have a very small data-set, right?", "start": 3730.34, "duration": 7.98}, {"text": "Um, [NOISE] So it turns out that,", "start": 3738.32, "duration": 6.975}, {"text": "uh, so I'm gonna leave out the test set for now.", "start": 3745.295, "duration": 2.22}, {"text": "Le- let's just assume there is some separate test set.", "start": 3747.515, "duration": 1.77}, {"text": "I'm not gonna worry about that for now.", "start": 3749.285, "duration": 1.545}, {"text": "Um, but let's say you have 100 examples, right?", "start": 3750.83, "duration": 5.87}, {"text": "Um, if you're going to split this into, you know,", "start": 3756.7, "duration": 3.135}, {"text": "70 in the training set in S subscript train and 30 in S dev.", "start": 3759.835, "duration": 6.48}, {"text": "Then you're training your algorithm on 70 examples instead of 100 examples.", "start": 3766.315, "duration": 4.595}, {"text": "And so I've actually worked on a few healthcare problems.", "start": 3770.91, "duration": 2.945}, {"text": "Oh, actually, mo- most of my PhD students, uh,", "start": 3773.855, "duration": 1.86}, {"text": "including Annan, work,", "start": 3775.715, "duration": 2.265}, {"text": "doing a lot of work on, uh,", "start": 3777.98, "duration": 1.185}, {"text": "machine learning applied to health care.", "start": 3779.165, "duration": 1.425}, {"text": "And so we actually worked on a few data-sets in healthcare where, you know,", "start": 3780.59, "duration": 4.035}, {"text": "every training example corresponded to some patient that sometimes that, uh, you know,", "start": 3784.625, "duration": 5.145}, {"text": "unfortunate disease or- or- or if every- if you're working- or if, um,", "start": 3789.77, "duration": 4.335}, {"text": "every example corresponded to injecting a patient with a drug and seeing what happens to", "start": 3794.105, "duration": 5.085}, {"text": "the patient right sometimes there's literally a lot", "start": 3799.19, "duration": 2.64}, {"text": "of blood and pain that goes into collecting every example.", "start": 3801.83, "duration": 2.955}, {"text": "And if you have 100 examples to hold out 30 of them,", "start": 3804.785, "duration": 4.605}, {"text": "um, for the purpose of model selection using only 70 examples and 100 examples.", "start": 3809.39, "duration": 4.74}, {"text": "It seems like you're wasting a lot of data that was collected through a lot of,", "start": 3814.13, "duration": 3.855}, {"text": "you know, literal pain, right?", "start": 3817.985, "duration": 1.74}, {"text": "Um, so is there a way to say do", "start": 3819.725, "duration": 3.365}, {"text": "model selection such as choose the degree of polynomial without,", "start": 3823.09, "duration": 3.57}, {"text": "\"Slightly wasting so much of the data.\"", "start": 3826.66, "duration": 3.43}, {"text": "There is a procedure that you should use only if you have a small data-set,", "start": 3832.66, "duration": 5.125}, {"text": "only if you're worried about the size of- oh,", "start": 3837.785, "duration": 1.71}, {"text": "and the other disadvantage of this is,", "start": 3839.495, "duration": 2.1}, {"text": "you evaluate your model only on 30 examples,", "start": 3841.595, "duration": 2.685}, {"text": "and that seems really small.", "start": 3844.28, "duration": 1.17}, {"text": "Right? You know can you- can you just find more data to evaluate your models as well.", "start": 3845.45, "duration": 4.32}, {"text": "So there's a procedure that you should use [NOISE] only if you have a small data-set,", "start": 3849.77, "duration": 6.105}, {"text": "uh, called k-fold cross-validation, or k-fold CV.", "start": 3855.875, "duration": 3.45}, {"text": "And this is, uh, in contrast to simple cross validation.", "start": 3859.325, "duration": 3.825}, {"text": "Um, but this is", "start": 3863.15, "duration": 1.95}, {"text": "the idea which is- let's say this is your training set S, so you have, you know,", "start": 3865.1, "duration": 4.305}, {"text": "X 1, Y 1 down to X say 100, Y 100.", "start": 3869.405, "duration": 7.305}, {"text": "[NOISE] What we're going to do is take the training set and,", "start": 3876.71, "duration": 4.74}, {"text": "uh, divide it into k pieces.", "start": 3881.45, "duration": 2.955}, {"text": "Um, so for the purpose of illustration, I'm gonna use k equals 5.", "start": 3884.405, "duration": 3.435}, {"text": "When I'm, just to make the- the writing on the board sane.", "start": 3887.84, "duration": 3.48}, {"text": "Uh, k equals 10, uh, is typical.", "start": 3891.32, "duration": 3.94}, {"text": "I guess, uh, for illustration.", "start": 3896.05, "duration": 3.07}, {"text": "[NOISE] But so what you do is, um,", "start": 3899.12, "duration": 4.485}, {"text": "take your data-set and divide it into five different subsets of- in this example,", "start": 3903.605, "duration": 6.15}, {"text": "you would have 20 examples.", "start": 3909.755, "duration": 1.77}, {"text": "100- 100 examples divided into five subsets,", "start": 3911.525, "duration": 2.265}, {"text": "so there are 20 examples in each subset.", "start": 3913.79, "duration": 2.175}, {"text": "And, um, what you do is,", "start": 3915.965, "duration": 3.39}, {"text": "for i equals 1 to k train i.e,", "start": 3919.355, "duration": 6.525}, {"text": "fit parameters on k minus 1 pieces.", "start": 3925.88, "duration": 9.34}, {"text": "And then test on the", "start": 3935.95, "duration": 5.95}, {"text": "remaining [NOISE] one piece,", "start": 3941.9, "duration": 7.69}, {"text": "and then you average.", "start": 3950.56, "duration": 2.23}, {"text": "Right? So in other words, uh,", "start": 3952.79, "duration": 2.01}, {"text": "when k is equals 5, we're going to loop through five times.", "start": 3954.8, "duration": 3.675}, {"text": "In the first iteration,", "start": 3958.475, "duration": 1.635}, {"text": "we're going to train on these and test on the last one fifth of the data. Um,", "start": 3960.11, "duration": 6.225}, {"text": "so we'll hold out the last one fifth of the data,", "start": 3966.335, "duration": 2.76}, {"text": "train on the rest and test on that.", "start": 3969.095, "duration": 1.695}, {"text": "And then in the second iteration, through this for loop,", "start": 3970.79, "duration": 3.105}, {"text": "we'll train on pieces 1 ,2, 3 and 5 and test on piece number 4,", "start": 3973.895, "duration": 6.3}, {"text": "and we get the number.", "start": 3980.195, "duration": 1.755}, {"text": "Um, and then you hold out this third piece,", "start": 3981.95, "duration": 3.24}, {"text": "train on the others, test on this, and so on.", "start": 3985.19, "duration": 2.37}, {"text": "So you do it five times,", "start": 3987.56, "duration": 1.11}, {"text": "where on each time,", "start": 3988.67, "duration": 1.245}, {"text": "you leave out one fifth of the data,", "start": 3989.915, "duration": 2.205}, {"text": "train on the remaining four-fifths and you evaluate the model on that final one fifth.", "start": 3992.12, "duration": 5.445}, {"text": "Okay? And so, um,", "start": 3997.565, "duration": 2.025}, {"text": "if you're trying to choose a degree of polynomial,", "start": 3999.59, "duration": 2.145}, {"text": "what you would do is- I guess for,", "start": 4001.735, "duration": 3.495}, {"text": "you know, D equals 1 through 5.", "start": 4005.23, "duration": 3.46}, {"text": "Right? So you do this procedure for a first order polynomial, uh,", "start": 4014.97, "duration": 5.83}, {"text": "fit- you fit a linear regression model five times,", "start": 4020.8, "duration": 2.565}, {"text": "each time on four-fifths of the model and test on the remaining one fifth,", "start": 4023.365, "duration": 2.85}, {"text": "and you repeat this whole procedure for the quadratic function.", "start": 4026.215, "duration": 2.775}, {"text": "Repeat this whole procedure for the cubic function, and so on.", "start": 4028.99, "duration": 3.63}, {"text": "And after doing this for every order polynomial from 1, 2, 3, 4, 5 you would then, uh,", "start": 4032.62, "duration": 6.105}, {"text": "pick the degree of polynomial that, um,", "start": 4038.725, "duration": 3.465}, {"text": "sorry and then for each of these models,", "start": 4042.19, "duration": 2.19}, {"text": "you then average the five S's you have for- for S error.", "start": 4044.38, "duration": 3.3}, {"text": "Okay? And then after doing this,", "start": 4047.68, "duration": 2.685}, {"text": "you would pick the degree of polynomial that did", "start": 4050.365, "duration": 3.105}, {"text": "best according to this- according to this metric.", "start": 4053.47, "duration": 3.54}, {"text": "Right? And then maybe you'll find that the second-order polynomial does best.", "start": 4057.01, "duration": 4.485}, {"text": "Um, and now you actually end up with, uh, five classifiers.", "start": 4061.495, "duration": 8.04}, {"text": "Right? Because you have five classifiers,", "start": 4069.535, "duration": 1.815}, {"text": "each one fits on four-fifths of the data,", "start": 4071.35, "duration": 2.745}, {"text": "uh, and then, uh,", "start": 4074.095, "duration": 1.485}, {"text": "and- and there's a- there's a final optional step,", "start": 4075.58, "duration": 3.24}, {"text": "which is to refit the model on all 100% of the data.", "start": 4078.82, "duration": 8.11}, {"text": "Right? So if you want,", "start": 4086.94, "duration": 2.35}, {"text": "you can keep five classifiers around and output their predictions,", "start": 4089.29, "duration": 3.48}, {"text": "but then you're keeping five classifiers around this.", "start": 4092.77, "duration": 2.67}, {"text": "Uh- uh maybe a bit more common to- now that you've chosen to use", "start": 4095.44, "duration": 3.0}, {"text": "a second-order polynomial to just refit the model once on all 100% of the data.", "start": 4098.44, "duration": 5.085}, {"text": "Okay? Um, and so the advantage of,", "start": 4103.525, "duration": 4.04}, {"text": "uh, k-fold cross validation is that,", "start": 4107.565, "duration": 2.52}, {"text": "instead of leaving out 30% of your data for your dev set on each iteration,", "start": 4110.085, "duration": 5.325}, {"text": "you're only leaving out 1 over k of your data.", "start": 4115.41, "duration": 3.4}, {"text": "I use k equals 5 for illustration, but in practice,", "start": 4118.81, "duration": 3.225}, {"text": "k equals 10 is by far the most common choice that we use.", "start": 4122.035, "duration": 3.6}, {"text": "I've sometimes seen people use k equals 20,", "start": 4125.635, "duration": 2.73}, {"text": "but quite rarely, but,", "start": 4128.365, "duration": 1.605}, {"text": "um, uh, if you use k equals 10,", "start": 4129.97, "duration": 1.82}, {"text": "then on each iteration,", "start": 4131.79, "duration": 1.559}, {"text": "you're leaving out just one tenth of the data.", "start": 4133.349, "duration": 2.206}, {"text": "10% of the data rather than 30% of the data.", "start": 4135.555, "duration": 2.865}, {"text": "Okay? Um, and so this procedure compared to simple cross-validation,", "start": 4138.42, "duration": 6.835}, {"text": "it makes more efficient use of the data,", "start": 4145.255, "duration": 2.31}, {"text": "because you're holding out you know only 10% of the data on each iteration.", "start": 4147.565, "duration": 3.945}, {"text": "Uh, the disadvantage of this is computationally very expensive,", "start": 4151.51, "duration": 3.3}, {"text": "that you're now fitting each model 10 times instead of just once.", "start": 4154.81, "duration": 4.665}, {"text": "Okay? But- but- but when you have a small data-set,", "start": 4159.475, "duration": 2.37}, {"text": "this- this is actually a better procedure than simple cross validation.", "start": 4161.845, "duration": 3.51}, {"text": "If you don't mind the computational expense of fitting each model 10 times.", "start": 4165.355, "duration": 3.135}, {"text": "This- this- this actually lets you get away with holding on this data.", "start": 4168.49, "duration": 3.24}, {"text": "[NOISE]", "start": 4171.73, "duration": 10.5}, {"text": "And then, um,", "start": 4182.23, "duration": 1.35}, {"text": "there's one even more extreme version of this,", "start": 4183.58, "duration": 2.865}, {"text": "which you should use, if you have very very small datasets.", "start": 4186.445, "duration": 2.67}, {"text": "So sometimes you might have an even smaller dataset.", "start": 4189.115, "duration": 2.865}, {"text": "You know, if you're doing a class project with 20 examples this-", "start": 4191.98, "duration": 2.88}, {"text": "that's- that's small even by today's machine learning standards.", "start": 4194.86, "duration": 3.345}, {"text": "So, uh, there's- there's an extreme version of k-fold cross-validation,", "start": 4198.205, "duration": 4.455}, {"text": "called leave-one-out cross validation,", "start": 4202.66, "duration": 2.98}, {"text": "which is if you set k equals m. Right?", "start": 4206.79, "duration": 4.03}, {"text": "So in other words, here's your training set, maybe 20 examples.", "start": 4210.82, "duration": 5.265}, {"text": "So you're gonna divide this into as many pieces as you have training examples.", "start": 4216.085, "duration": 5.37}, {"text": "And what you do is leave out one example, train on the other 19,", "start": 4221.455, "duration": 5.205}, {"text": "and test on the one example you held out.", "start": 4226.66, "duration": 2.355}, {"text": "And then leave out the second example,", "start": 4229.015, "duration": 2.085}, {"text": "train on the other 19 and test to the one example you held out,", "start": 4231.1, "duration": 2.82}, {"text": "and do that 20 times,", "start": 4233.92, "duration": 1.59}, {"text": "and then you average this over the 20 outcomes to", "start": 4235.51, "duration": 2.31}, {"text": "evaluate how good different orders of polynomial are.", "start": 4237.82, "duration": 3.03}, {"text": "Um, the huge downside of this is just is completely very- very expensive,", "start": 4240.85, "duration": 4.424}, {"text": "because now you need to change your algorithm m times.", "start": 4245.274, "duration": 2.746}, {"text": "So you, kind of,", "start": 4248.02, "duration": 1.29}, {"text": "never do this unless m is really small.", "start": 4249.31, "duration": 2.37}, {"text": "Uh, I personally have- I pretty much never use this procedure unless m is 100 or less.", "start": 4251.68, "duration": 5.88}, {"text": "I guess, you- your- if your model isn't too complicated,", "start": 4257.56, "duration": 2.85}, {"text": "you can afford to fit a linear regression model 100 times, like it's not too bad.", "start": 4260.41, "duration": 3.525}, {"text": "Right? So- so if- if- if m is,", "start": 4263.935, "duration": 2.175}, {"text": "uh, less than 100,", "start": 4266.11, "duration": 1.71}, {"text": "you could consider this procedure.", "start": 4267.82, "duration": 1.56}, {"text": "But- but if m is 1000,", "start": 4269.38, "duration": 1.74}, {"text": "fitting a linear model- fitting a model 1000 times,", "start": 4271.12, "duration": 2.4}, {"text": "it seems like a lot of work, then you usually use k-fold cross-validation instead.", "start": 4273.52, "duration": 3.33}, {"text": "Uh, but if you do have 20 examples,", "start": 4276.85, "duration": 2.444}, {"text": "then you know, I- I would then- then- if you have 20 examples,", "start": 4279.294, "duration": 3.646}, {"text": "I would probably use this procedure and somewhere between 20 and", "start": 4282.94, "duration": 5.01}, {"text": "50s maybe when I switch over from leave-one-out to k-fold cross-validation. Okay? Yeah.", "start": 4287.95, "duration": 6.7}, {"text": "In 10- fold cross validation should we use [inaudible] k times to go.", "start": 4298.89, "duration": 9.37}, {"text": "Yeah. So, um, right.", "start": 4308.26, "duration": 2.64}, {"text": "So since you have k estimates,", "start": 4310.9, "duration": 1.98}, {"text": "say 10- 10 estimates,", "start": 4312.88, "duration": 1.17}, {"text": "we're using 10-fold cross-validation.", "start": 4314.05, "duration": 1.995}, {"text": "Can you measure the variance on those 10 estimates?", "start": 4316.045, "duration": 2.955}, {"text": "Um, it turns out that those 10 estimates are", "start": 4319.0, "duration": 2.67}, {"text": "correlated because each of the 10 classifiers,", "start": 4321.67, "duration": 3.15}, {"text": "eight-eight-eight-eight out of nine of the sets of data they trained on overlap.", "start": 4324.82, "duration": 4.575}, {"text": "So, um, there were some very interesting results,", "start": 4329.395, "duration": 3.765}, {"text": "uh-uh there's some research papers written by Michael Kearns,", "start": 4333.16, "duration": 2.61}, {"text": "actually, um, it's like a long time ago, uh,", "start": 4335.77, "duration": 2.97}, {"text": "trying to understand how correlated are these 10 estimates.", "start": 4338.74, "duration": 4.035}, {"text": "And from a theoretical point of view,", "start": 4342.775, "duration": 2.37}, {"text": "the- we- the- as far as I know,", "start": 4345.145, "duration": 2.295}, {"text": "the latest error result shows that this is not a worse estimate in training error,", "start": 4347.44, "duration": 4.02}, {"text": "but note- but- but maybe it's showing us in", "start": 4351.46, "duration": 2.43}, {"text": "practice is not- you could measure it, but, uh,", "start": 4353.89, "duration": 4.755}, {"text": "we don't really trust that estimate of variance,", "start": 4358.645, "duration": 2.325}, {"text": "because we think all 10 estimates are highly correlated,", "start": 4360.97, "duration": 2.924}, {"text": "or- or at least somewhat correlated.", "start": 4363.894, "duration": 2.326}, {"text": "Yeah. Go ahead", "start": 4366.22, "duration": 2.34}, {"text": "[inaudible]", "start": 4368.56, "duration": 2.4}, {"text": "Whether we're gonna find using k-fold cross-validation in deep learning?", "start": 4370.96, "duration": 6.045}, {"text": "Um, if you have a very small training set, then maybe yes.", "start": 4377.005, "duration": 4.445}, {"text": "But deep learning algorithms depend on the details.", "start": 4381.45, "duration": 2.64}, {"text": "Right? Sometimes it takes so long to train,", "start": 4384.09, "duration": 1.77}, {"text": "that training- training- training on a neural network 20 times,", "start": 4385.86, "duration": 3.165}, {"text": "you know, seems like a pain unless- unless you have enough data.", "start": 4389.025, "duration": 3.585}, {"text": "Unless, um, your neural network was quite small.", "start": 4392.61, "duration": 2.885}, {"text": "Right? Um, so it's rarely done with a deep learning algorithm.", "start": 4395.495, "duration": 5.405}, {"text": "But if you have so- frank- frankly if you have so little data,", "start": 4400.9, "duration": 2.58}, {"text": "if you have 20 training examples,", "start": 4403.48, "duration": 1.619}, {"text": "uh - uh, you know there are", "start": 4405.099, "duration": 2.536}, {"text": "other techniques that you probably need to use to boost performance.", "start": 4407.635, "duration": 3.525}, {"text": "Such as transfer learning,", "start": 4411.16, "duration": 1.47}, {"text": "or just more heterogeneity of input features,", "start": 4412.63, "duration": 3.36}, {"text": "or something else. Right? Um, yeah.", "start": 4415.99, "duration": 2.61}, {"text": "[inaudible]", "start": 4418.6, "duration": 5.31}, {"text": "Sorry say again.", "start": 4423.91, "duration": 2.85}, {"text": "[inaudible].", "start": 4426.76, "duration": 4.56}, {"text": "Sorry thank you for asking that,", "start": 4431.32, "duration": 2.715}, {"text": "uh, this average set no.", "start": 4434.035, "duration": 1.335}, {"text": "Um, I meant, um,", "start": 4435.37, "duration": 2.055}, {"text": "averaging the test errors.", "start": 4437.425, "duration": 1.785}, {"text": "So, uh, here, you will have trained 10 classifiers and,", "start": 4439.21, "duration": 5.315}, {"text": "you know, when you evaluate it on the left at 110 for the data,", "start": 4444.525, "duration": 2.895}, {"text": "you get it wrong- you get a number.", "start": 4447.42, "duration": 1.86}, {"text": "Right? So you're looping 10 times, hold at one part, train on the others.", "start": 4449.28, "duration": 4.16}, {"text": "Test on this part you left out.", "start": 4453.44, "duration": 1.685}, {"text": "And so that will give you a number,", "start": 4455.125, "duration": 1.395}, {"text": "and they go say oh when you test on test on this part you left out,", "start": 4456.52, "duration": 2.595}, {"text": "the squared error was 5.0 and then you do it again,", "start": 4459.115, "duration": 3.045}, {"text": "squared error was 5.7, squared error was 2.8.", "start": 4462.16, "duration": 2.925}, {"text": "So by average I meant average those numbers.", "start": 4465.085, "duration": 3.54}, {"text": "And the average of those numbers is your estimate of the error of a,", "start": 4468.625, "duration": 5.295}, {"text": "you know, third order polynomial for this problem.", "start": 4473.92, "duration": 3.615}, {"text": "So this is an averaging the set of real numbers that you got from", "start": 4477.535, "duration": 4.11}, {"text": "this- so- so this loop gives you k real numbers, uh,", "start": 4481.645, "duration": 3.945}, {"text": "and so this is averaging those k real numbers to estimate for this outer loop,", "start": 4485.59, "duration": 5.655}, {"text": "how good a classifier with that degree polynomial is.", "start": 4491.245, "duration": 3.375}, {"text": "Okay? Wow, actually, a lot of questions,", "start": 4494.62, "duration": 1.74}, {"text": "there's one thing I want to cover,  go ahead and this last two go ahead.", "start": 4496.36, "duration": 2.04}, {"text": "[inaudible]", "start": 4498.4, "duration": 11.595}, {"text": "I see. Sure. Yes, using something other than", "start": 4509.995, "duration": 2.415}, {"text": "F1 score would it just mean other than average?", "start": 4512.41, "duration": 2.91}, {"text": "Uh, yes, it would.", "start": 4515.32, "duration": 1.71}, {"text": "Having F1 score is complicated.", "start": 4517.03, "duration": 2.01}, {"text": "Yes. Uh, I think,", "start": 4519.04, "duration": 1.86}, {"text": "I think we'll talk- actually, um,", "start": 4520.9, "duration": 1.815}, {"text": "so this week, Friday, we'll talk about learning theory.", "start": 4522.715, "duration": 2.145}, {"text": "Next week- next Friday we're talking more about performance evaluation metrics.", "start": 4524.86, "duration": 3.54}, {"text": "So actually we will talk about F1 score?", "start": 4528.4, "duration": 2.01}, {"text": "Uh, Mitch, one last question?", "start": 4530.41, "duration": 1.92}, {"text": "How do you sample the data in the, in these sets?", "start": 4532.33, "duration": 2.01}, {"text": "Oh, sure. How do you sample the data in these sets?", "start": 4534.34, "duration": 2.685}, {"text": "Um, so for the purposes of this class,", "start": 4537.025, "duration": 1.92}, {"text": "assuming all your data comes through the same distribution,", "start": 4538.945, "duration": 2.43}, {"text": "uh I, I, I,", "start": 4541.375, "duration": 1.305}, {"text": "I would usually randomly shuffle.", "start": 4542.68, "duration": 1.92}, {"text": "Uh, again, in the era of machine learning and big data,", "start": 4544.6, "duration": 3.615}, {"text": "there's one other interesting trend is which,", "start": 4548.215, "duration": 2.52}, {"text": "which wasn't true 10 years ago which is we're", "start": 4550.735, "duration": 1.815}, {"text": "increasingly trying to train and test on different sets.", "start": 4552.55, "duration": 2.505}, {"text": "Uh, uh, we're trying to,", "start": 4555.055, "duration": 1.23}, {"text": "you know, train on data, uh,", "start": 4556.285, "duration": 1.59}, {"text": "collect it in one context and apply it to a totally different context.", "start": 4557.875, "duration": 4.125}, {"text": "Uh, such as, um,", "start": 4562.0, "duration": 1.215}, {"text": "we're trying to, uh,", "start": 4563.215, "duration": 1.195}, {"text": "uh, you know, train on,", "start": 4564.41, "duration": 1.13}, {"text": "on speech collected on your cellphone because you have all that data,", "start": 4565.54, "duration": 3.93}, {"text": "and trying to apply it to a, um, uh,", "start": 4569.47, "duration": 3.15}, {"text": "uh to a smart speaker where it was collected on a different microphone,", "start": 4572.62, "duration": 3.84}, {"text": "in your cellphone or something.", "start": 4576.46, "duration": 1.08}, {"text": "So, uh, if you are doing that and the way", "start": 4577.54, "duration": 2.91}, {"text": "you set your train dev test split is a bit more complicated.", "start": 4580.45, "duration": 3.24}, {"text": "Um, I wasn't going to talk about it in this class.", "start": 4583.69, "duration": 2.475}, {"text": "If you want to learn more, uh, ah,", "start": 4586.165, "duration": 1.775}, {"text": "I think at the start of the class,", "start": 4587.94, "duration": 1.29}, {"text": "I mentioned I was working on this book, Machine Learning Yearning.", "start": 4589.23, "duration": 2.805}, {"text": "So that book is finished.", "start": 4592.035, "duration": 1.56}, {"text": "And if you go to this website,", "start": 4593.595, "duration": 1.59}, {"text": "you can get a copy of it for free.", "start": 4595.185, "duration": 2.295}, {"text": "Uh, uh, that talks about that.", "start": 4597.48, "duration": 1.93}, {"text": "Uh, and I also talk about this more in CS230 which,", "start": 4599.41, "duration": 3.33}, {"text": "which goes more into the big data.", "start": 4602.74, "duration": 1.755}, {"text": "But you can, you can go,", "start": 4604.495, "duration": 1.065}, {"text": "go and learn machine- you can also read all about it in,", "start": 4605.56, "duration": 2.685}, {"text": "in Machine Learning Yearning.", "start": 4608.245, "duration": 1.365}, {"text": "Um, if the train and test sets are a different distribution.", "start": 4609.61, "duration": 2.88}, {"text": "Uh, yeah, but random shuffling would be a good default if you think you're training", "start": 4612.49, "duration": 3.96}, {"text": "dev test on two different, right? All right.", "start": 4616.45, "duration": 4.215}, {"text": "Just one last thing I want to cover real quick which is, um, feature selection.", "start": 4620.665, "duration": 9.085}, {"text": "And so, um, so let me just describe what- so sometimes you have a lot of features.", "start": 4635.61, "duration": 10.27}, {"text": "Um, so, so actually let's take text classification.", "start": 4645.88, "duration": 2.82}, {"text": "You might have 10,000 features corresponding to 10,000 words,", "start": 4648.7, "duration": 3.525}, {"text": "but you might suspect that a lot of the features are not important, right?", "start": 4652.225, "duration": 4.625}, {"text": "You know the word the,", "start": 4656.85, "duration": 1.17}, {"text": "whether the word the is called a stop word,", "start": 4658.02, "duration": 2.265}, {"text": "whether the word the appears in e-mail or not,", "start": 4660.285, "duration": 2.04}, {"text": "doesn't really tell you if it's spam or not spam because the word the,", "start": 4662.325, "duration": 2.94}, {"text": "a, of, you know,", "start": 4665.265, "duration": 1.345}, {"text": "these are called stop words.", "start": 4666.61, "duration": 1.11}, {"text": "They don't tell you much about the content of the email.", "start": 4667.72, "duration": 3.135}, {"text": "Um, but so if a lot of features, uh,", "start": 4670.855, "duration": 2.895}, {"text": "sometimes one way to reduce overfitting is to try to", "start": 4673.75, "duration": 4.38}, {"text": "find a small subset of the features that are most useful for your task, right?", "start": 4678.13, "duration": 4.5}, {"text": "And so, um, this takes judgment.", "start": 4682.63, "duration": 2.46}, {"text": "There are some problems like computer vision where you have", "start": 4685.09, "duration": 2.94}, {"text": "a lot of features corresponding to there being a lot of pixels in every image.", "start": 4688.03, "duration": 3.39}, {"text": "But probably, every pixel is somewhat relevant.", "start": 4691.42, "duration": 3.3}, {"text": "So you don't want to select a subset of pixels for most computer vision tasks.", "start": 4694.72, "duration": 3.84}, {"text": "But there are some other problems where you might have lot of features when you suspect", "start": 4698.56, "duration": 5.16}, {"text": "the way to prevent overfitting is to find", "start": 4703.72, "duration": 1.95}, {"text": "a small subset of the most relevant features for your task.", "start": 4705.67, "duration": 3.54}, {"text": "Um, so feature selection is a special case of model selection", "start": 4709.21, "duration": 4.439}, {"text": "that applies to when you suspect that even though you have 10,000 features,", "start": 4713.649, "duration": 4.696}, {"text": "maybe only 50 of them are highly relevant, right?", "start": 4718.345, "duration": 3.45}, {"text": "And so, um, uh one example,", "start": 4721.795, "duration": 2.19}, {"text": "if you are measuring a lot of things going on in a truck,", "start": 4723.985, "duration": 3.48}, {"text": "uh, in order to figure out if the truck is about to break down, right.", "start": 4727.465, "duration": 4.005}, {"text": "You, you might, uh, for, for preventive maintenance,", "start": 4731.47, "duration": 2.325}, {"text": "you might measure hundreds of variables or many hundreds of variables,", "start": 4733.795, "duration": 3.735}, {"text": "but you might secretly suspect that there are only a few things that,", "start": 4737.53, "duration": 3.195}, {"text": "you know, predict when this truck is about to go down,", "start": 4740.725, "duration": 2.325}, {"text": "so you can do preventive maintenance.", "start": 4743.05, "duration": 1.08}, {"text": "So if you suspect that's the case,", "start": 4744.13, "duration": 1.59}, {"text": "then feature selection would be a reasonable approach to try, right?", "start": 4745.72, "duration": 3.96}, {"text": "And so, um, here's the- I'll,", "start": 4749.68, "duration": 2.355}, {"text": "I'll just write out one algorithm, uh,", "start": 4752.035, "duration": 2.355}, {"text": "which is start with- this is script f equals the empty set of features.", "start": 4754.39, "duration": 8.35}, {"text": "And then you repeatedly try adding each feature i", "start": 4765.57, "duration": 10.72}, {"text": "to f and see", "start": 4776.29, "duration": 6.63}, {"text": "which single feature addition", "start": 4782.92, "duration": 4.09}, {"text": "most improves the dev set performance, right?", "start": 4791.28, "duration": 10.975}, {"text": "And then Step 2 is go ahead and connect to add", "start": 4802.255, "duration": 3.435}, {"text": "that feature to f, okay?", "start": 4805.69, "duration": 11.77}, {"text": "So let me illustrate this with pictures.", "start": 4818.52, "duration": 4.0}, {"text": "So let's say you have, um, five features,", "start": 4822.52, "duration": 4.83}, {"text": "x1 through x5, and in practice it's usually more like x1 through x500 or x1 through 10,000,", "start": 4827.35, "duration": 5.565}, {"text": "but I'll just use 5.", "start": 4832.915, "duration": 1.455}, {"text": "So start off with an empty set of features and,", "start": 4834.37, "duration": 2.94}, {"text": "you know, train a linear classifier with no feature.", "start": 4837.31, "duration": 2.58}, {"text": "So the model is, um,", "start": 4839.89, "duration": 1.56}, {"text": "h of x equals theta 0, right?", "start": 4841.45, "duration": 2.325}, {"text": "With no features.", "start": 4843.775, "duration": 1.035}, {"text": "Uh, so this won't be a very good model.", "start": 4844.81, "duration": 1.56}, {"text": "But see how well this does on your dev set.", "start": 4846.37, "duration": 1.995}, {"text": "Uh, so this way you average the ys, right?", "start": 4848.365, "duration": 2.145}, {"text": "So it's not for your model.", "start": 4850.51, "duration": 1.305}, {"text": "Next- so this is step one.", "start": 4851.815, "duration": 2.25}, {"text": "In the second iteration,", "start": 4854.065, "duration": 1.994}, {"text": "you would then take each of these features and add it to the empty sets.", "start": 4856.059, "duration": 4.111}, {"text": "You can try the empty set plus x1,", "start": 4860.17, "duration": 2.52}, {"text": "empty set plus x2,", "start": 4862.69, "duration": 2.085}, {"text": "empty set plus x5.", "start": 4864.775, "duration": 3.345}, {"text": "And for each of these, you would fit a corresponding model.", "start": 4868.12, "duration": 2.49}, {"text": "So for this one you fit h of x equals theta 0 plus theta 1 x5.", "start": 4870.61, "duration": 5.67}, {"text": "So try adding one feature to your model,", "start": 4876.28, "duration": 2.58}, {"text": "and see which model best improves your performance on the dev set, right?", "start": 4878.86, "duration": 4.62}, {"text": "And let's say you find that adding feature two is the best choice.", "start": 4883.48, "duration": 4.155}, {"text": "So now, what we'll do is set the set of features to be x2.", "start": 4887.635, "duration": 6.57}, {"text": "For the next step, you would then consider starting of x2 and adding x1,", "start": 4894.205, "duration": 7.8}, {"text": "or x3, or x4, or x5.", "start": 4902.005, "duration": 8.115}, {"text": "So if your model is already using the feature x2,", "start": 4910.12, "duration": 3.36}, {"text": "what's the other feature,", "start": 4913.48, "duration": 1.59}, {"text": "what additional feature most helps your algorithm?", "start": 4915.07, "duration": 2.64}, {"text": "Um, and let's say it is x4, right?", "start": 4917.71, "duration": 2.565}, {"text": "So you fit three or four models, see which one does best.", "start": 4920.275, "duration": 2.25}, {"text": "And now you would commit to using the features x2 and x4.", "start": 4922.525, "duration": 5.94}, {"text": "Um, and you kind of keep on doing this,", "start": 4928.465, "duration": 3.075}, {"text": "keep on adding features greedily,", "start": 4931.54, "duration": 1.845}, {"text": "keep on adding features one at a time to see which single feature addition,", "start": 4933.385, "duration": 4.305}, {"text": "um, helps improve your algorithm the most.", "start": 4937.69, "duration": 2.55}, {"text": "Um, and, and, and you can keep iterating", "start": 4940.24, "duration": 2.835}, {"text": "until adding more features now hurts performance.", "start": 4943.075, "duration": 2.745}, {"text": "Uh, and then pick what- whichever feature subset", "start": 4945.82, "duration": 2.82}, {"text": "allows you to have the best possible performance of dev set, okay?", "start": 4948.64, "duration": 3.375}, {"text": "So this is a special case of model selection called forward search.", "start": 4952.015, "duration": 3.315}, {"text": "It's called forward search because we started with a empty set of features,", "start": 4955.33, "duration": 2.37}, {"text": "and adding features one at a time.", "start": 4957.7, "duration": 2.13}, {"text": "There's a procedure called backwards search which we'll read about that.", "start": 4959.83, "duration": 2.82}, {"text": "We install all the features and remove features one at a time.", "start": 4962.65, "duration": 2.76}, {"text": "But this would be a reasonable,", "start": 4965.41, "duration": 1.635}, {"text": "uh, uh feature selection algorithm.", "start": 4967.045, "duration": 1.98}, {"text": "The disadvantage of this is it is quite computationally expensive, uh,", "start": 4969.025, "duration": 3.405}, {"text": "but this can help you select a decent set of features, okay?", "start": 4972.43, "duration": 3.87}, {"text": "Um, so we're running a little bit late, uh, let's break.", "start": 4976.3, "duration": 3.075}, {"text": "Oh, so I think, uh,", "start": 4979.375, "duration": 1.17}, {"text": "I was meant to be on the road next week but, uh,", "start": 4980.545, "duration": 2.76}, {"text": "because [inaudible] is still unable to teach,", "start": 4983.305, "duration": 3.195}, {"text": "I think we will have, uh,", "start": 4986.5, "duration": 1.23}, {"text": "Rafael, uh, uh, teach, uh,", "start": 4987.73, "duration": 2.775}, {"text": "decision trees next week,", "start": 4990.505, "duration": 1.845}, {"text": "and then also Kian will talk about neural networks next week, okay?", "start": 4992.35, "duration": 4.605}, {"text": "So let's break for today, um, and,", "start": 4996.955, "duration": 2.145}, {"text": "and maybe we'll see some of you at the Friday discussion session.", "start": 4999.1, "duration": 3.22}]