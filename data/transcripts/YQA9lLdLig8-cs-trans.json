[{"text": "Hey everyone. Um, let's get started.", "start": 3.41, "duration": 6.04}, {"text": "So, um, let's see, the plan for the [NOISE] day is,", "start": 9.45, "duration": 5.505}, {"text": "uh, we'll go over the rest of ICA, independent component analysis.", "start": 14.955, "duration": 4.485}, {"text": "In particular, talking about CDFs,", "start": 19.44, "duration": 2.58}, {"text": "cumulative distribution functions [NOISE].", "start": 22.02, "duration": 3.42}, {"text": "And then, um, actually, uh,", "start": 25.44, "duration": 2.805}, {"text": "let's do that later.", "start": 28.245, "duration": 1.185}, {"text": "[NOISE].", "start": 29.43, "duration": 9.12}, {"text": "All right. So the plan is we'll go over,", "start": 38.55, "duration": 3.915}, {"text": "uh, the rest of ICA,", "start": 42.465, "duration": 1.305}, {"text": "independent component analysis, and we'll talk a bit about CDFs,", "start": 43.77, "duration": 3.87}, {"text": "um, cumulative distribution functions [NOISE],", "start": 47.64, "duration": 2.61}, {"text": "and then derive the ICA model.", "start": 50.25, "duration": 2.515}, {"text": "And uh, in the second half of today,", "start": 52.765, "duration": 2.255}, {"text": "we'll start on the final of the,", "start": 55.02, "duration": 3.21}, {"text": "um, interesting, four major topics of the class, which is reinforcement learning.", "start": 58.23, "duration": 7.16}, {"text": "We'll talk about MDPs, or Markov decision processes, okay?", "start": 65.39, "duration": 2.785}, {"text": "So to recap briefly,", "start": 68.175, "duration": 2.715}, {"text": "um, we had- you remember the overlapping voices demo.", "start": 70.89, "duration": 4.85}, {"text": "So we said that in the ICA problem,", "start": 75.74, "duration": 3.0}, {"text": "independent component analysis problem,", "start": 78.74, "duration": 1.77}, {"text": "we'll assume we have sources S,", "start": 80.51, "duration": 2.215}, {"text": "which are RN if you have N speakers.", "start": 82.725, "duration": 4.125}, {"text": "So for example, if this is speaker one's audio,", "start": 86.85, "duration": 3.15}, {"text": "then at time T [NOISE], um, S, you know,", "start": 90.0, "duration": 3.555}, {"text": "superscript parentheses T subscript 1 is the [NOISE]", "start": 93.555, "duration": 3.375}, {"text": "sound emitted by speaker one at time T.", "start": 96.93, "duration": 3.56}, {"text": "Sorry, I don't have the- that's interesting.", "start": 100.49, "duration": 2.61}, {"text": "All right [NOISE]. Just let me go back over a little bit.", "start": 103.1, "duration": 2.935}, {"text": "Um, and, uh, yeah, we're,", "start": 106.035, "duration": 2.13}, {"text": "we're using sometimes I to index training examples,", "start": 108.165, "duration": 3.105}, {"text": "and so the training examples sweep over time, um,", "start": 111.27, "duration": 3.305}, {"text": "and sometimes usually I use I,", "start": 114.575, "duration": 1.845}, {"text": "sometimes I use T, I guess in the case where, um,", "start": 116.42, "duration": 3.06}, {"text": "the, uh, uh, the different examples", "start": 119.48, "duration": 2.85}, {"text": "come from different points in time in your recording.", "start": 122.33, "duration": 3.075}, {"text": "And what your microphones record is XI equals A of SI.", "start": 125.405, "duration": 4.65}, {"text": "So just for now,", "start": 130.055, "duration": 1.23}, {"text": "let's say you have two speakers and two microphones, in which case,", "start": 131.285, "duration": 3.435}, {"text": "A will be a 2 by 2 matrix, and in the homework problem,", "start": 134.72, "duration": 2.85}, {"text": "we have five speakers and five microphones", "start": 137.57, "duration": 1.62}, {"text": "in which case A will be a 5 by 5 matrix.", "start": 139.19, "duration": 2.46}, {"text": "We'll talk later, um, about what happens", "start": 141.65, "duration": 2.37}, {"text": "if the number of speakers and microphones is not the same [NOISE].", "start": 144.02, "duration": 3.67}, {"text": "And the goal is to find the matrix W, uh,", "start": 147.69, "duration": 2.385}, {"text": "which should hopefully be A inverse, um,", "start": 150.075, "duration": 4.29}, {"text": "so that SI is W times X recovered the original sources.", "start": 154.365, "duration": 4.94}, {"text": "Uh, and we're going to use these W1 up through", "start": 159.305, "duration": 3.555}, {"text": "WN to represent the rows of this matrix W. Yeah.", "start": 162.86, "duration": 3.84}, {"text": "[inaudible].", "start": 166.7, "duration": 2.88}, {"text": "Uh, oh yes you're right.", "start": 169.58, "duration": 3.9}, {"text": "Thank you. Right. Okay. Thank you.", "start": 173.48, "duration": 4.995}, {"text": "Okay. [NOISE] So, um,", "start": 178.475, "duration": 2.955}, {"text": "[NOISE] last time we had [NOISE]. All right.", "start": 181.43, "duration": 6.055}, {"text": "So remember this is a picture of the Cocktail party problem.", "start": 187.485, "duration": 3.37}, {"text": "And, uh, last time I showed these pictures about,", "start": 190.855, "duration": 3.175}, {"text": "you know, why, why, why is ICA even possible, right?", "start": 194.03, "duration": 3.045}, {"text": "Given two overlapping, um,", "start": 197.075, "duration": 2.43}, {"text": "voices, how is it even possible to separate them out?", "start": 199.505, "duration": 3.135}, {"text": "How is there enough information to know,", "start": 202.64, "duration": 2.315}, {"text": "um, uh, you know, what are the two overlapping voices?", "start": 204.955, "duration": 2.945}, {"text": "And so one picture [NOISE] we saw was this one,", "start": 207.9, "duration": 2.65}, {"text": "where if S1 and S2 are uniform between minus 1 and plus 1,", "start": 210.55, "duration": 4.15}, {"text": "then the distribution of data will look like this [NOISE].", "start": 214.7, "duration": 2.34}, {"text": "If you pass this data through the mixing matrix A, then your observations,", "start": 217.04, "duration": 6.265}, {"text": "now the axes have changed to X1 and X2, may look like this,", "start": 223.305, "duration": 3.465}, {"text": "and your job is to find an unmixing matrix W that", "start": 226.77, "duration": 3.47}, {"text": "maps this data back to the square, okay?", "start": 230.24, "duration": 3.815}, {"text": "Now, this example is possible because the examples- because the,", "start": 234.055, "duration": 6.325}, {"text": "uh, sources S1 and S2,", "start": 240.38, "duration": 1.74}, {"text": "were distributed uniformly between minus 1 and plus 1.", "start": 242.12, "duration": 3.585}, {"text": "Um, it turns out human voices, you know,", "start": 245.705, "duration": 2.145}, {"text": "the recordings per moment in time are not", "start": 247.85, "duration": 2.28}, {"text": "distributed uniform between minus 1 and plus 1.", "start": 250.13, "duration": 2.73}, {"text": "And it turns out that, um, uh,", "start": 252.86, "duration": 2.205}, {"text": "if the data was Gaussian,", "start": 255.065, "duration": 2.49}, {"text": "then ICA is actually not possible.", "start": 257.555, "duration": 2.535}, {"text": "Here's what I mean [NOISE].", "start": 260.09, "duration": 1.88}, {"text": "Let's say that, uh- so,", "start": 261.97, "duration": 1.855}, {"text": "so the uniform distribution is a highly non-Gaussian [NOISE] distribution, right?", "start": 263.825, "duration": 3.015}, {"text": "Uniform B minus 1 plus 1, you know,", "start": 266.84, "duration": 1.815}, {"text": "this is non-Gaussian and that,", "start": 268.655, "duration": 1.455}, {"text": "that makes ICA possible [NOISE].", "start": 270.11, "duration": 1.86}, {"text": "Um, what if [NOISE] S1 and S2 came from Gaussian densities, right?", "start": 271.97, "duration": 6.28}, {"text": "Um, if that were the case,", "start": 278.25, "duration": 1.61}, {"text": "then this distribution S1 and S2 would be rotationally symmetric.", "start": 279.86, "duration": 5.75}, {"text": "And so, um, there would be a rotational ambiguity, right?", "start": 285.61, "duration": 4.16}, {"text": "Any axis could be S1 and S2 [NOISE].", "start": 289.77, "duration": 2.04}, {"text": "You can't map, you know,", "start": 291.81, "duration": 1.99}, {"text": "this type of parallelogram back to this square, right?", "start": 293.8, "duration": 2.96}, {"text": "So, so [NOISE] you can't sort of I think in this parallelogram,", "start": 296.76, "duration": 3.015}, {"text": "um, you can sort of lead off,", "start": 299.775, "duration": 4.68}, {"text": "you know, that may be one axis should look like that.", "start": 304.455, "duration": 2.295}, {"text": "So I'm drawing with a mouse, not doing very well.", "start": 306.75, "duration": 2.22}, {"text": "Well, second axis should maybe look like that, right?", "start": 308.97, "duration": 3.995}, {"text": "And by, by inverting that you can get the data back to the square.", "start": 312.965, "duration": 3.795}, {"text": "But in the case of if the data look like this,", "start": 316.76, "duration": 3.135}, {"text": "then [NOISE] you actually don't know, um,", "start": 319.895, "duration": 2.9}, {"text": "because [NOISE] maybe this should be S1 [NOISE] and that should be S2, right?", "start": 322.795, "duration": 6.485}, {"text": "But so this is rotational ambiguity,", "start": 329.28, "duration": 1.91}, {"text": "because the Gaussian distribution is, um,", "start": 331.19, "duration": 2.52}, {"text": "rotationally symmetric, if S1 and S2 are standard Gaussians,", "start": 333.71, "duration": 3.584}, {"text": "then, then, [NOISE] then this distribution is rotationally symmetric,", "start": 337.294, "duration": 2.806}, {"text": "and you don't have enough information to recover", "start": 340.1, "duration": 2.37}, {"text": "the directions that correspond to the original sources, okay?", "start": 342.47, "duration": 3.475}, {"text": "So it turns out that, um,", "start": 345.945, "duration": 2.545}, {"text": "there is some ambiguity in the output of ICA.", "start": 348.49, "duration": 3.92}, {"text": "In particular, last time we talked about,", "start": 352.41, "duration": 2.13}, {"text": "uh, two sources of ambiguity.", "start": 354.54, "duration": 1.635}, {"text": "Um, you don't know which is speaker one and which is speaker two, right?", "start": 356.175, "duration": 2.985}, {"text": "You don't know which one to number speaker one and which one to number speaker two,", "start": 359.16, "duration": 3.16}, {"text": "and you might take this data and flip it horizontally, uh, reflect this,", "start": 362.32, "duration": 5.21}, {"text": "you know, on, on,", "start": 367.53, "duration": 1.32}, {"text": "on the neg- S1 goes to negative S1 [NOISE],", "start": 368.85, "duration": 2.37}, {"text": "or reflect this, uh, on a vertical axis.", "start": 371.22, "duration": 2.805}, {"text": "We don't know if it's positive S2 and negative S2.", "start": 374.025, "duration": 2.315}, {"text": "And in the case of this example,", "start": 376.34, "duration": 2.369}, {"text": "where S1 is, uh, uniform minus 1 plus 1.", "start": 378.709, "duration": 2.356}, {"text": "Those are the only sources of ambiguity.", "start": 381.065, "duration": 2.435}, {"text": "Um, but if the data was Gaussian there would be additional rotational ambiguity", "start": 383.5, "duration": 4.45}, {"text": "which makes [NOISE] it in part- whi- which actually makes it", "start": 387.95, "duration": 1.83}, {"text": "impossible to separate out the sources, okay?", "start": 389.78, "duration": 2.97}, {"text": "So [NOISE] it turns out that, um, all right. cool [NOISE].", "start": 392.75, "duration": 11.47}, {"text": "So it turns out that the Gaussian density is the only distribution,", "start": 404.22, "duration": 4.8}, {"text": "um, that is rotationally symmetric.", "start": 409.02, "duration": 3.23}, {"text": "Uh, if, if, if S1 and S2 are", "start": 412.25, "duration": 2.85}, {"text": "independent and if the distribution is rotationally symmetric [NOISE],", "start": 415.1, "duration": 3.75}, {"text": "meaning that the distribution has sort of circular contours,", "start": 418.85, "duration": 4.24}, {"text": "uh, then it, then it, then it must be a Gaussian density [NOISE].", "start": 423.09, "duration": 3.06}, {"text": "And so, there is a theorem, uh,", "start": 426.15, "duration": 2.28}, {"text": "which I'll just state it formally,", "start": 428.43, "duration": 1.725}, {"text": "that ICA is possible only if your data is non Gaussian, right?", "start": 430.155, "duration": 3.695}, {"text": "But, but so long as your data is non-Gaussian,", "start": 433.85, "duration": 2.115}, {"text": "then it is possible to recover the independent sources, okay?", "start": 435.965, "duration": 3.505}, {"text": "I'm just stating [NOISE] that informally.", "start": 439.47, "duration": 2.04}, {"text": "Um, so let's [NOISE] let's see [NOISE].", "start": 441.51, "duration": 7.11}, {"text": "So what I would like to do is, um,", "start": 448.62, "duration": 2.145}, {"text": "develop [NOISE] the ICA algorithm assuming that the data is non-Gaussian, okay?", "start": 450.765, "duration": 8.365}, {"text": "Now, um, [NOISE] in order to, uh,", "start": 459.13, "duration": 5.19}, {"text": "develop the ICA model,", "start": 464.32, "duration": 1.68}, {"text": "we need to figure out what is the density of S, right?", "start": 466.0, "duration": 5.115}, {"text": "And I'm going to use P subscript S,", "start": 471.115, "duration": 2.29}, {"text": "you know, of, uh, the, the, uh,", "start": 473.405, "duration": 2.045}, {"text": "of the random variable S to represent the,", "start": 475.45, "duration": 2.37}, {"text": "um, density of S. Um,", "start": 477.82, "duration": 2.64}, {"text": "an equivalent way to represent the probability of", "start": 480.46, "duration": 2.94}, {"text": "the density of continuous random variables [NOISE] is via CDF,", "start": 483.4, "duration": 3.67}, {"text": "which stands for cumulative", "start": 487.07, "duration": 2.74}, {"text": "uh, distribution functions [NOISE].", "start": 489.81, "duration": 7.41}, {"text": "And the, uh, the cumulative distribution function of a, uh,", "start": 497.22, "duration": 4.14}, {"text": "random variable F of S in probability,", "start": 501.36, "duration": 2.89}, {"text": "is defined as the chance that the random [NOISE] variable is less than that value.", "start": 504.25, "duration": 4.455}, {"text": "So I guess, um, notations have been inconsistent, sorry,", "start": 508.705, "duration": 3.225}, {"text": "but this is capital S I'm using to denote the random variable,", "start": 511.93, "duration": 4.485}, {"text": "and this is some constant.", "start": 516.415, "duration": 3.345}, {"text": "Right, and, uh, it's that same constant as that lowercase s, okay?", "start": 519.76, "duration": 5.335}, {"text": "Um, and so for example,", "start": 525.095, "duration": 2.325}, {"text": "if this is the PDF,", "start": 527.42, "duration": 5.145}, {"text": "of a random variable S,", "start": 532.565, "duration": 2.13}, {"text": "maybe of a Gaussian, right?", "start": 534.695, "duration": 2.755}, {"text": "The CDF is a function that um, [NOISE]", "start": 538.33, "duration": 15.02}, {"text": "increases from 0 to 1 where,", "start": 553.35, "duration": 4.515}, {"text": "um, the height of a CDF at a certain point is the probability.", "start": 557.865, "duration": 8.33}, {"text": "So if you take the curves at the same point, right?", "start": 566.195, "duration": 3.855}, {"text": "So the height of a CDF at a certain point lowercase s,", "start": 570.05, "duration": 6.205}, {"text": "is the probability that the random variable", "start": 576.255, "duration": 2.995}, {"text": "takes [NOISE] on a value equal to this value or lower,", "start": 579.25, "duration": 2.91}, {"text": "which means that the height of this function is equal to, um, you know,", "start": 582.16, "duration": 4.5}, {"text": "the probability mass, the area under the curve of your PDF,", "start": 586.66, "duration": 3.82}, {"text": "um, [NOISE] over to the left of that point, okay?", "start": 590.48, "duration": 2.575}, {"text": "So that's, uh, I don't know, sometimes this-", "start": 593.055, "duration": 2.61}, {"text": "some probability and statistics courses teach this concept and some don't I guess,", "start": 595.665, "duration": 3.825}, {"text": "but there's- so there's a mapping between the PDFs and the CDFs of a function,", "start": 599.49, "duration": 5.14}, {"text": "of a, of a continuous random variable.", "start": 604.63, "duration": 2.56}, {"text": "Um, and the relation between the PDF and the CDF is that", "start": 607.19, "duration": 4.63}, {"text": "the density [NOISE] is equal to the first derivative, right?", "start": 611.82, "duration": 5.46}, {"text": "Uh, F prime. So if you take the derivative of the CDF,", "start": 617.28, "duration": 4.4}, {"text": "then you should recover the PDF, [NOISE] okay?", "start": 621.68, "duration": 3.255}, {"text": "But so I think, um, in order to specify, you know,", "start": 624.935, "duration": 3.405}, {"text": "some random variable, we could either specify the PDF, right?", "start": 628.34, "duration": 4.33}, {"text": "The probability density function,", "start": 632.67, "duration": 1.365}, {"text": "or you can specify the CDF which is just, you know,", "start": 634.035, "duration": 3.195}, {"text": "let's tell me what's the chance of the random variable taking on", "start": 637.23, "duration": 3.14}, {"text": "any value less than any particular value S. And by taking the derivative of this,", "start": 640.37, "duration": 3.96}, {"text": "you can always recover the PDF,", "start": 644.33, "duration": 1.48}, {"text": "and by integrating this you can always go to the CDF, okay.", "start": 645.81, "duration": 3.65}, {"text": "And so, um, what we're going to do in, um,", "start": 649.46, "duration": 4.43}, {"text": "ICA is instead of specifying a PDF for how speakers' voices sound,", "start": 653.89, "duration": 5.78}, {"text": "we're instead going to specify a CDF, and, uh,", "start": 659.67, "duration": 2.76}, {"text": "we'll have to choose a CDF that is not the Gaussian density CDF,", "start": 662.43, "duration": 4.8}, {"text": "because we have to assume that the data is non-Gaussian.", "start": 667.23, "duration": 2.895}, {"text": "Uh, um, uh, and the CDF, you know,", "start": 670.125, "duration": 3.66}, {"text": "is a function that always goes from, right, 0 to 1, okay?", "start": 673.785, "duration": 4.425}, {"text": "So, um, [NOISE].", "start": 678.21, "duration": 15.78}, {"text": "All right. So we'll specify [NOISE].", "start": 693.99, "duration": 9.99}, {"text": "So in a little bit, we'll specify some CDF", "start": 703.98, "duration": 3.765}, {"text": "for the density of the sources of what human voices sound like let's say.", "start": 707.745, "duration": 4.095}, {"text": "And if you differentiate this, uh,", "start": 711.84, "duration": 2.745}, {"text": "you will get the PDF of the density of s, right?", "start": 714.585, "duration": 6.075}, {"text": "Was equal to that. Now, we're um,", "start": 720.66, "duration": 4.44}, {"text": "going to derive a maximum likelihood estimate mission algorithm in a minute.", "start": 725.1, "duration": 3.87}, {"text": "But our model is [NOISE] that, X is equal to A_s, um,", "start": 728.97, "duration": 4.77}, {"text": "which is equal to, I guess w inverse of s,", "start": 733.74, "duration": 3.315}, {"text": "and s is equal to w_x, right?", "start": 737.055, "duration": 2.625}, {"text": "So that- that's, that's the model.", "start": 739.68, "duration": 2.055}, {"text": "And in order to derive a maximum likelihood estimate for the parameters, um,", "start": 741.735, "duration": 5.505}, {"text": "when you have- [NOISE] so this is going to be the density of x.", "start": 747.24, "duration": 13.68}, {"text": "Okay? So this is a relationship between, um, ah,", "start": 760.92, "duration": 5.59}, {"text": "this is the relationship between x and s. X is equal to A_s,", "start": 766.55, "duration": 5.02}, {"text": "equals W inverse s and s equals W_x, right?", "start": 771.57, "duration": 2.88}, {"text": "So this is the model. And what I'd like to do is,", "start": 774.45, "duration": 3.72}, {"text": "let's say you know what's the density of s. Um,", "start": 778.17, "duration": 5.205}, {"text": "what is the density of x if x is computed as the matrix A times s?", "start": 783.375, "duration": 8.85}, {"text": "Right? So one step that's tempting to take is to just say,", "start": 792.225, "duration": 5.595}, {"text": "well, s is equal to W times X.", "start": 797.82, "duration": 3.27}, {"text": "So the probability of x is just equal to", "start": 801.09, "duration": 2.91}, {"text": "the probability of s taking on the certain value, right?", "start": 804.0, "duration": 3.03}, {"text": "So, so I mean this is s, right?", "start": 807.03, "duration": 3.57}, {"text": "And so the probability of seeing a certain value of x is equal", "start": 810.6, "duration": 3.66}, {"text": "to the probability of s taking on that corresponding value Because assuming,", "start": 814.26, "duration": 4.335}, {"text": "W is an invertible matrix, is a bijection.", "start": 818.595, "duration": 2.175}, {"text": "There's one-to-one mapping between x and s. So to find the probability of X,", "start": 820.77, "duration": 4.32}, {"text": "just find the probably of s and compute the corresponding probability.", "start": 825.09, "duration": 3.96}, {"text": "Um, it turns out this is- this is incorrect,", "start": 829.05, "duration": 3.3}, {"text": "and this works with probability mass functions, for discrete probability distributions,", "start": 832.35, "duration": 4.08}, {"text": "um, that take on discrete values.", "start": 836.43, "duration": 1.71}, {"text": "But this is actually incorrect for continuous probability densities.", "start": 838.14, "duration": 4.08}, {"text": "So let me- let me, um, uh, show an illustration,", "start": 842.22, "duration": 2.939}, {"text": "and we'll go back to derive what is a correct way of", "start": 845.159, "duration": 3.181}, {"text": "computing the density of x. Oh, and we'll want,", "start": 848.34, "duration": 3.87}, {"text": "uh, density of x because um,", "start": 852.21, "duration": 2.52}, {"text": "when you get the training set,", "start": 854.73, "duration": 1.74}, {"text": "you only get to observe x, and so for, uh,", "start": 856.47, "duration": 3.72}, {"text": "finding the maximum likelihood estimate parameters,", "start": 860.19, "duration": 1.92}, {"text": "you need to know, um,", "start": 862.11, "duration": 1.665}, {"text": "what's the density of x you can map, you know,", "start": 863.775, "duration": 1.74}, {"text": "choose the parameters, choose the parameters W that maximizes the likelihood.", "start": 865.515, "duration": 3.225}, {"text": "Okay? [NOISE] So that's what we want to compute the density of x.", "start": 868.74, "duration": 2.79}, {"text": "But, um, let's, let's use a simple example.", "start": 871.53, "duration": 2.61}, {"text": "[NOISE] Let's say the density of s is a indicator,", "start": 874.14, "duration": 4.665}, {"text": "s is between 0 and 1.", "start": 878.805, "duration": 4.065}, {"text": "Okay? So this is, um,", "start": 882.87, "duration": 1.68}, {"text": "s is distributed uniform from [NOISE] 0 to 1.", "start": 884.55, "duration": 7.05}, {"text": "Um, and let's say [NOISE] x is equal to 2 times s. Okay?", "start": 891.6, "duration": 4.905}, {"text": "So now notation, A is equal to 2,", "start": 896.505, "duration": 2.535}, {"text": "[NOISE] W is equal to one-half.", "start": 899.04, "duration": 1.89}, {"text": "Ah, this is, uh, n equals 1, 1-dimensional example.", "start": 900.93, "duration": 3.705}, {"text": "So, um, this is the density of s, right?", "start": 904.635, "duration": 8.175}, {"text": "Uniform distribution from 0 to 1.", "start": 912.81, "duration": 3.6}, {"text": "And if x is equal to 2 times s,", "start": 916.41, "duration": 2.91}, {"text": "then this seems like X should be equal- X is", "start": 919.32, "duration": 3.9}, {"text": "distributed uniformly from 0 to 2, right?", "start": 923.22, "duration": 4.95}, {"text": "Because if s is uniform from 0 to 1,", "start": 928.17, "duration": 2.16}, {"text": "you multiply it by 2,", "start": 930.33, "duration": 1.29}, {"text": "X is distributed uniformly from 0 to 2.", "start": 931.62, "duration": 2.925}, {"text": "And so the density for X is equal to", "start": 934.545, "duration": 3.075}, {"text": "this, 1, 2 [NOISE].", "start": 937.62, "duration": 8.73}, {"text": "Right? And it's now half as tall because, uh,", "start": 946.35, "duration": 2.49}, {"text": "probability density function z to integrate to 1, right?", "start": 948.84, "duration": 2.88}, {"text": "So this is a uniform from 0 to 2 probability density function.", "start": 951.72, "duration": 5.175}, {"text": "And so the correct formula, um, is P of x,", "start": 956.895, "duration": 7.095}, {"text": "x equals one-half, times", "start": 963.99, "duration": 5.28}, {"text": "indicator 0 less equal to x less than equal to 2.", "start": 969.27, "duration": 6.51}, {"text": "Right?", "start": 975.78, "duration": 5.775}, {"text": "Um, [NOISE].", "start": 981.555, "duration": 4.125}, {"text": "Okay? And, uh, more generally,", "start": 985.68, "duration": 2.309}, {"text": "the correct formula for this is actually this times, um,", "start": 987.989, "duration": 9.046}, {"text": "this is the determinant [NOISE] of the matrix W.", "start": 997.035, "duration": 9.075}, {"text": "Uh, and in the case of a real number,", "start": 1006.11, "duration": 1.74}, {"text": "the determinant of a row number is just this absolute value which is why,", "start": 1007.85, "duration": 4.635}, {"text": "um, we have the density of x equals one-half.", "start": 1012.485, "duration": 4.59}, {"text": "You know, that's the absolute value of the determinant of W, um,", "start": 1017.075, "duration": 6.265}, {"text": "times, times your- times indicator whether 2 times s is within 0, 0 to 1.", "start": 1023.34, "duration": 8.465}, {"text": "Okay? Um, yeah, right.", "start": 1031.805, "duration": 5.775}, {"text": "So I guess this, uh,", "start": 1037.58, "duration": 1.44}, {"text": "this is indicator 0 less equal to one-half x less or equals to 1,", "start": 1039.02, "duration": 5.4}, {"text": "right, since that's s. Okay?", "start": 1044.42, "duration": 3.39}, {"text": "So this is illustration showing why this is the right way with", "start": 1047.81, "duration": 4.47}, {"text": "the determinant of W multiplied here as the-", "start": 1052.28, "duration": 3.0}, {"text": "as a way to compute the density of X.", "start": 1055.28, "duration": 2.28}, {"text": "Um, and, er, for the- for those of you familiar with, um,", "start": 1057.56, "duration": 3.585}, {"text": "determinants and- oh, and determinants is", "start": 1061.145, "duration": 2.475}, {"text": "a function you can call you know, in NumPy to compute, um, ah,", "start": 1063.62, "duration": 3.165}, {"text": "but also, uh, the intuition of a determinant is it measures how", "start": 1066.785, "duration": 3.735}, {"text": "much it stretches out a, um, local walking,", "start": 1070.52, "duration": 3.54}, {"text": "and so you need to, uh, uh, er,", "start": 1074.06, "duration": 2.61}, {"text": "sort of divide by the determinant of A or multiply by the determinant of W,", "start": 1076.67, "duration": 4.89}, {"text": "um, in order to make sure your distribution still normalizes to 1.", "start": 1081.56, "duration": 4.065}, {"text": "Right? So that's where that comes from.", "start": 1085.625, "duration": 1.515}, {"text": "[NOISE] So, um, we're nearly done.", "start": 1087.14, "duration": 6.48}, {"text": "Just one more decision,", "start": 1093.62, "duration": 1.455}, {"text": "and then we can derive the maximum likelihood estimation,", "start": 1095.075, "duration": 3.915}, {"text": "uh, to derive a maximum likelihood estimate of this, of the parameters.", "start": 1098.99, "duration": 3.81}, {"text": "The last thing we need to do is, um,", "start": 1102.8, "duration": 2.4}, {"text": "[NOISE] choose the density of what your speakers' voices sound like.", "start": 1105.2, "duration": 6.93}, {"text": "[NOISE] And as I said just now, um,", "start": 1112.13, "duration": 2.88}, {"text": "what we're going to do,", "start": 1115.01, "duration": 2.205}, {"text": "is, uh, choose a non-Gaussian distribution.", "start": 1117.215, "duration": 3.39}, {"text": "Right? And so [NOISE] while F of s is equal to the chance of this person's voice.", "start": 1120.605, "duration": 8.01}, {"text": "Right? Random variable s being less than a certain value.", "start": 1128.615, "duration": 2.625}, {"text": "And we need a smooth function that goes between, you know,", "start": 1131.24, "duration": 3.72}, {"text": "0 and 1, um,", "start": 1134.96, "duration": 1.815}, {"text": "where, we need a smooth function that has vaguely that shape.", "start": 1136.775, "duration": 3.375}, {"text": "And so well, what functions we know that are vaguely that shape?", "start": 1140.15, "duration": 3.645}, {"text": "Right? Let's pick the sigmoid function.", "start": 1143.795, "duration": 4.665}, {"text": "Um, and it turns out this will- this will work.", "start": 1148.46, "duration": 3.24}, {"text": "Okay. There are many choices that actually work fine.", "start": 1151.7, "duration": 2.505}, {"text": "Um, it turns out that if you choose a sigmoid function to be the CDF,", "start": 1154.205, "duration": 5.1}, {"text": "then if you look at the PDF this induces,", "start": 1159.305, "duration": 3.66}, {"text": "if you take the derivatives of this.", "start": 1162.965, "duration": 1.215}, {"text": "Right? So take P of x [NOISE] equals the derivative of the CDF.", "start": 1164.18, "duration": 4.935}, {"text": "Um, it turns out that if this is the Gaussian,", "start": 1169.115, "duration": 6.055}, {"text": "then the PDF that this choice induces,", "start": 1175.75, "duration": 4.72}, {"text": "is, uh, something with fatter tails, right?", "start": 1180.47, "duration": 3.645}, {"text": "Um, by which I mean that it goes to 0, you know.", "start": 1184.115, "duration": 4.515}, {"text": "[NOISE] So Gaussian density", "start": 1188.63, "duration": 9.45}, {"text": "goes to 0 very quickly,", "start": 1198.08, "duration": 1.755}, {"text": "right, it's like e to the negative x squared, right?", "start": 1199.835, "duration": 2.755}, {"text": "That's the Gaussian is a square in the exponent of the density.", "start": 1202.59, "duration": 2.855}, {"text": "And it turns out that this particular density, uh,", "start": 1205.445, "duration": 2.745}, {"text": "taken by compute derivative of a sigmoid it goes to 0 more slowly and this captures", "start": 1208.19, "duration": 5.939}, {"text": "human voice and many natural phenomena better than", "start": 1214.129, "duration": 2.761}, {"text": "a Gaussian density because there are a larger number of extreme outliers,", "start": 1216.89, "duration": 3.6}, {"text": "that are more than one or two standard deviations away,", "start": 1220.49, "duration": 2.805}, {"text": "um, but there are actually multiple distributions that work.", "start": 1223.295, "duration": 2.58}, {"text": "You could- if you use a double, double exponential distribution.", "start": 1225.875, "duration": 3.27}, {"text": "So this is an exponential distribution- exponential density.", "start": 1229.145, "duration": 2.955}, {"text": "If you take a symmetric with two sided exponential density for P of s,", "start": 1232.1, "duration": 3.84}, {"text": "it will also work quite well for ICA.", "start": 1235.94, "duration": 1.875}, {"text": "But I think, um, early history of ICA,", "start": 1237.815, "duration": 2.909}, {"text": "you know, researchers, I think it was,", "start": 1240.724, "duration": 2.296}, {"text": "um, might been Terry Sejnowski, uh,", "start": 1243.02, "duration": 2.58}, {"text": "down at the Salk Institute,", "start": 1245.6, "duration": 1.485}, {"text": "just needed a function with these properties.", "start": 1247.085, "duration": 2.595}, {"text": "He picked the sigmoid and plugged it in and it works just fine.", "start": 1249.68, "duration": 2.52}, {"text": "It's been a good enough default that,", "start": 1252.2, "duration": 2.085}, {"text": "um, it's still- it's still widely used, right?", "start": 1254.285, "duration": 2.64}, {"text": "But, but, but, but, but I've used this um,", "start": 1256.925, "duration": 2.655}, {"text": "double-sided exponential or sometimes also called the Laplacian distribution.", "start": 1259.58, "duration": 3.66}, {"text": "This, this works fine as well as a choice of a P of s. Okay?", "start": 1263.24, "duration": 4.32}, {"text": "[NOISE]", "start": 1267.56, "duration": 10.23}, {"text": "So the final step.", "start": 1277.79, "duration": 1.5}, {"text": "Um, the density of s is equal to [NOISE].", "start": 1279.29, "duration": 20.01}, {"text": "Right? The product of the, uh, uh, um, um, let's see.", "start": 1299.3, "duration": 6.0}, {"text": "Ah, it's a product from i equals 1 through", "start": 1305.3, "duration": 3.12}, {"text": "your n sources of the probability of each of the speakers emitting that sound, right?", "start": 1308.42, "duration": 5.79}, {"text": "Ah, because the n speakers", "start": 1314.21, "duration": 2.88}, {"text": "[NOISE] are speaking independently, right? Yeah.", "start": 1317.09, "duration": 9.77}, {"text": "[inaudible].", "start": 1326.86, "duration": 6.34}, {"text": "Say, say that again?", "start": 1333.2, "duration": 0.84}, {"text": "[inaudible] .", "start": 1334.04, "duration": 10.68}, {"text": "Oh, yes. You're right. Sorry about that.", "start": 1344.72, "duration": 2.085}, {"text": "Yes, this should have been [NOISE].", "start": 1346.805, "duration": 6.585}, {"text": "Sorry, yes this should have been a P_s.", "start": 1353.39, "duration": 2.655}, {"text": "Yeah. Thank you. [NOISE] Right,", "start": 1356.045, "duration": 4.845}, {"text": "go from a CDF to PDF by taking derivatives.", "start": 1360.89, "duration": 3.43}, {"text": "All right. Cool. So, um, er,", "start": 1365.83, "duration": 4.09}, {"text": "S is the vector of all, you know,", "start": 1369.92, "duration": 3.3}, {"text": "two speakers' or all five speakers' voices at one moment in time.", "start": 1373.22, "duration": 3.615}, {"text": "So the density of S, right?", "start": 1376.835, "duration": 2.415}, {"text": "S as an RN is the product of the individual speakers' probabilities,", "start": 1379.25, "duration": 4.89}, {"text": "and, um, this is the key assumption of ICA that,", "start": 1384.14, "duration": 3.705}, {"text": "you know, your two speakers or your five speakers are having independent conversations,", "start": 1387.845, "duration": 4.125}, {"text": "and so at every moment in time,", "start": 1391.97, "duration": 1.62}, {"text": "they choose independently of each other what sound to emit.", "start": 1393.59, "duration": 3.675}, {"text": "All right.", "start": 1397.265, "duration": 0.945}, {"text": "Um, and so using the formulas we worked out just now.", "start": 1398.21, "duration": 4.08}, {"text": "The density of x is equal to, um, well, as we did,", "start": 1402.29, "duration": 5.94}, {"text": "the density of, uh, W_x times the determinant of W. [NOISE] Right?", "start": 1408.23, "duration": 14.17}, {"text": "Uh, so- and this is equal to [NOISE]", "start": 1422.5, "duration": 15.36}, {"text": "Okay. Um, and this notation,", "start": 1437.86, "duration": 4.02}, {"text": "uh, W_I transpose x, this is, um [NOISE] right?", "start": 1441.88, "duration": 4.42}, {"text": "Because W_I is the I th row of the matrix W and so,", "start": 1446.3, "duration": 5.235}, {"text": "um, you know, I guess S- S_j is equal to,", "start": 1451.535, "duration": 4.8}, {"text": "um, W_j transpose X, right?", "start": 1456.335, "duration": 3.105}, {"text": "So t- take the corresponding row and multiply it by x to get a corresponding source.", "start": 1459.44, "duration": 5.16}, {"text": "Actually, sorry. I think this right, yeah,", "start": 1464.6, "duration": 1.995}, {"text": "let me use j there to make this clearer.", "start": 1466.595, "duration": 2.055}, {"text": "[NOISE] Okay.", "start": 1468.65, "duration": 3.12}, {"text": "[NOISE] And so, um, this writes out- so this shows", "start": 1471.77, "duration": 11.55}, {"text": "what is the density of x, um, expressed as a function of,", "start": 1483.32, "duration": 6.11}, {"text": "um, P_s, which we've assumed- which effects as a CDF of the Sigmoid as", "start": 1489.43, "duration": 5.25}, {"text": "a, as the derivative of the Sigmoid and as a function of the parameter W. Right?", "start": 1494.68, "duration": 6.605}, {"text": "So this is a model that,", "start": 1501.285, "duration": 2.63}, {"text": "given a setting of the parameter W which is a square matrix,", "start": 1503.915, "duration": 3.555}, {"text": "um, allows us to write down what's the density of X.", "start": 1507.47, "duration": 3.96}, {"text": "[NOISE]", "start": 1511.43, "duration": 9.539}, {"text": "So the final step is,", "start": 1520.969, "duration": 1.696}, {"text": "um, we could use [NOISE] maximum likelihood estimation to estimate the parameters w.", "start": 1522.665, "duration": 7.095}, {"text": "Um, so the log-likelihood of W is equal to sum over", "start": 1529.76, "duration": 5.67}, {"text": "the training examples of log- of, you know, [NOISE]", "start": 1535.43, "duration": 14.94}, {"text": "times by W. Right.", "start": 1550.37, "duration": 3.315}, {"text": "And, um, you can use stochastic gradient ascent.", "start": 1553.685, "duration": 2.655}, {"text": "[NOISE] All right.", "start": 1556.34, "duration": 6.54}, {"text": "Take the derivative of w with respect to the log-likelihood.", "start": 1562.88, "duration": 3.54}, {"text": "Um, and it turns out- this is derived in the lecture notes.", "start": 1566.42, "duration": 4.575}, {"text": "I'll just write it out here. [NOISE]", "start": 1570.995, "duration": 12.285}, {"text": "Times x i. [NOISE]", "start": 1583.28, "duration": 1.32}, {"text": "I hope I got that right. Yeah. Okay. Right. [NOISE] Um, yeah.", "start": 1584.6, "duration": 17.52}, {"text": "And it turns out that, um,", "start": 1602.12, "duration": 2.52}, {"text": "if you use this formula don- don't worry about", "start": 1604.64, "duration": 3.42}, {"text": "the formula for the derivatives, there are full derivations given in the lecture notes.", "start": 1608.06, "duration": 2.94}, {"text": "But it turns out that, um,", "start": 1611.0, "duration": 1.725}, {"text": "if you use the derivative of the log-likelihood with respect to parameter", "start": 1612.725, "duration": 3.825}, {"text": "matrix W and use stochastic gradient ascent to maximize the log likelihood,", "start": 1616.55, "duration": 5.595}, {"text": "uh, run this for a while, then you can get, um,", "start": 1622.145, "duration": 3.945}, {"text": "ICA to find a pretty good matrix W,", "start": 1626.09, "duration": 4.035}, {"text": "um, for unmixing the sources, okay?", "start": 1630.125, "duration": 2.715}, {"text": "So just to recap the whole algorithm, right?", "start": 1632.84, "duration": 5.415}, {"text": "You would have a training set of X_1 [NOISE] up through X_m,", "start": 1638.255, "duration": 8.475}, {"text": "where each of your training examples is the, um,", "start": 1646.73, "duration": 3.33}, {"text": "er, microphone recordings at one moment in time,", "start": 1650.06, "duration": 3.39}, {"text": "[NOISE] and so the time goes from 1 through M.", "start": 1653.45, "duration": 2.505}, {"text": "What you do is initialize the matrix W, say,", "start": 1655.955, "duration": 3.615}, {"text": "randomly and use gradient ascent with", "start": 1659.57, "duration": 4.4}, {"text": "this formula for the derivative in order to maximize the log-likelihood of the data,", "start": 1663.97, "duration": 4.65}, {"text": "and after gradient ascent converges,", "start": 1668.62, "duration": 2.7}, {"text": "you then have a matrix W and you can then recover the sources as S equals W_x.", "start": 1671.32, "duration": 6.88}, {"text": "And then now, we have the sources, you can take, um, say,", "start": 1678.2, "duration": 5.47}, {"text": "S_1_1 through S_1_m and play that through your,", "start": 1683.89, "duration": 6.61}, {"text": "um, your laptop speaker in order to see what source one sounds like.", "start": 1690.5, "duration": 5.76}, {"text": "Right? And so that's how you would take, you know,", "start": 1696.26, "duration": 3.075}, {"text": "overlapping voices and [NOISE] try to unmix them.", "start": 1699.335, "duration": 3.225}, {"text": "Okay. Oh, yeah.", "start": 1702.56, "duration": 1.59}, {"text": "[inaudible]", "start": 1704.15, "duration": 6.99}, {"text": "Oh why is choices A point not a rotation matrix?", "start": 1711.14, "duration": 2.34}, {"text": "Uh, er, boy how to visualize that.", "start": 1713.48, "duration": 4.215}, {"text": "Try plotting it in, um, NumPy, matplotlib I guess.", "start": 1717.695, "duration": 5.1}, {"text": "If you plot the contours of the- so it turns out that if this is S_1 and S_2,", "start": 1722.795, "duration": 5.88}, {"text": "what you do not want is the den- density whose contours look like that.", "start": 1728.675, "duration": 4.815}, {"text": "Um, I haven't done this for a while.", "start": 1733.49, "duration": 2.94}, {"text": "I believe if you take this distribution,", "start": 1736.43, "duration": 2.34}, {"text": "the contours will look like that.", "start": 1738.77, "duration": 1.98}, {"text": "[NOISE] It's been a while since I looked at this,", "start": 1740.75, "duration": 3.21}, {"text": "but I think it'll look like that.", "start": 1743.96, "duration": 1.2}, {"text": "So this is not rotational symmetry.", "start": 1745.16, "duration": 1.92}, {"text": "You're on it. Well, it's Laplace.", "start": 1747.08, "duration": 2.955}, {"text": "Yeah. Okay. Yeah. Oh, yes. Laplace definitely looks like that.", "start": 1750.035, "duration": 2.04}, {"text": "I think Sigmoid looks a bit like that too. Yeah, little like that.", "start": 1752.075, "duration": 3.105}, {"text": "Plot it and see if I'm right, or post on Piazza, if one of you plots it.", "start": 1755.18, "duration": 3.81}, {"text": "So you can see it, I haven't done that for a long time.", "start": 1758.99, "duration": 2.265}, {"text": "Yeah, at the back.", "start": 1761.255, "duration": 1.305}, {"text": "[inaudible]", "start": 1762.56, "duration": 16.965}, {"text": "Oh, um, um, why don't you interact with the derivative of the log? The- th- actually,", "start": 1779.525, "duration": 5.31}, {"text": "yes, the log should be like this, I think. Yes.", "start": 1784.835, "duration": 3.285}, {"text": "[BACKGROUND]", "start": 1788.12, "duration": 5.49}, {"text": "Oh, sorry, uh, g is the sigmoid function.", "start": 1793.61, "duration": 2.31}, {"text": "Yes, so g of z. Yeah, thank you. Right, more questions?", "start": 1795.92, "duration": 7.08}, {"text": "[inaudible]", "start": 1803.0, "duration": 13.32}, {"text": "Sure. What's the, you know,", "start": 1816.32, "duration": 2.055}, {"text": "um, what's the closest non-linear extension of this?", "start": 1818.375, "duration": 4.02}, {"text": "Um, I don't- we don't a have a great answer to that right now frankly, um, uh,", "start": 1822.395, "duration": 7.645}, {"text": "so a bunch of people including,", "start": 1830.05, "duration": 4.135}, {"text": "you know, my former students and me,", "start": 1834.185, "duration": 2.745}, {"text": "have done research to try to extend this to", "start": 1836.93, "duration": 2.04}, {"text": "nonlinear versions and there's some stuff that kind of works,", "start": 1838.97, "duration": 2.82}, {"text": "but I don't think there's like, uh,", "start": 1841.79, "duration": 1.5}, {"text": "tried and true algorithm that I'm ready to say this is a right way to do it.", "start": 1843.29, "duration": 4.125}, {"text": "Um, uh, yeah,", "start": 1847.415, "duration": 3.705}, {"text": "actually maybe I should [NOISE] think I could", "start": 1851.12, "duration": 1.44}, {"text": "say a little bit more about that if you're interested.", "start": 1852.56, "duration": 1.875}, {"text": "Well, yeah, actually, uh, let me- let me try to- [NOISE]", "start": 1854.435, "duration": 32.145}, {"text": "All right. Let's see.", "start": 1886.58, "duration": 2.31}, {"text": "So, so for several-", "start": 1888.89, "duration": 11.805}, {"text": "several years ago and- and still kind of ongoing,", "start": 1900.695, "duration": 2.655}, {"text": "there's been research, um,", "start": 1903.35, "duration": 2.235}, {"text": "some done by my collaborators and me,", "start": 1905.585, "duration": 1.875}, {"text": "some done by others on trying to build nonlinear versions of ICA,", "start": 1907.46, "duration": 3.84}, {"text": "and so some of you might have seen this slightly infamous,", "start": 1911.3, "duration": 4.07}, {"text": "um, Google cat result, right?", "start": 1915.37, "duration": 2.55}, {"text": "Uh, so this one was in the Google Brain project, one of the first projects we did.", "start": 1917.92, "duration": 3.36}, {"text": "This is a few years ago now where, um,", "start": 1921.28, "duration": 2.205}, {"text": "we trained a neural network,", "start": 1923.485, "duration": 2.02}, {"text": "uh, uh, on, um, was it many,", "start": 1925.505, "duration": 3.425}, {"text": "many hours of YouTube videos, uh, and,", "start": 1928.93, "duration": 2.46}, {"text": "and eventually it learnt to", "start": 1931.39, "duration": 2.79}, {"text": "detect cats because apparently there are a lot of cats in YouTube videos.", "start": 1934.18, "duration": 3.57}, {"text": "Um, uh, and so it turns out that the algorithm we used was a,", "start": 1937.75, "duration": 5.35}, {"text": "um, was sparse coding which is actually very closely related to ICA.", "start": 1943.1, "duration": 5.595}, {"text": "Um, and so this rough algorithm was attempting to build a nonlinear version of ICA,", "start": 1948.695, "duration": 4.95}, {"text": "where you train one version one- train- train train one layer of sparse coding let's say,", "start": 1953.645, "duration": 4.485}, {"text": "to extract low level features and then recursively apply this on top,", "start": 1958.13, "duration": 3.57}, {"text": "to learn not just edge detectors,", "start": 1961.7, "duration": 1.89}, {"text": "but object part detectors,", "start": 1963.59, "duration": 1.17}, {"text": "and then eventually, you know,", "start": 1964.76, "duration": 1.53}, {"text": "the somewhat infamous, um,", "start": 1966.29, "duration": 1.41}, {"text": "uh, this somewhat infamous Google cat.", "start": 1967.7, "duration": 2.535}, {"text": "Um, but I think that this is actually still ongoing research.", "start": 1970.235, "duration": 3.24}, {"text": "Um, I think the most interesting research, uh,", "start": 1973.475, "duration": 3.015}, {"text": "some of the most interesting research has been on hierarchical versions of sparse coding,", "start": 1976.49, "duration": 3.66}, {"text": "sparse coding is a different algorithm that turns out to be very closely related to ICA,", "start": 1980.15, "duration": 4.14}, {"text": "and then you can show that they're optimizing very similar things.", "start": 1984.29, "duration": 3.03}, {"text": "So, so I say sparse coding is very similar to ICA,", "start": 1987.32, "duration": 2.28}, {"text": "uh, but they're hierarchical versions of this,", "start": 1989.6, "duration": 2.1}, {"text": "they tried to turn this as a multilayered neural network and it kinda worked,", "start": 1991.7, "duration": 3.48}, {"text": "wherever that shows it can learn interesting features.", "start": 1995.18, "duration": 2.295}, {"text": "But what happened was, uh,", "start": 1997.475, "duration": 1.47}, {"text": "supervised learning then really took off and the whole world shifted a lot of", "start": 1998.945, "duration": 3.465}, {"text": "this attention to supervised learning and", "start": 2002.41, "duration": 2.07}, {"text": "building deeper supervised learning neural networks.", "start": 2004.48, "duration": 2.1}, {"text": "And so, the hierarchical sparse coding running", "start": 2006.58, "duration": 2.64}, {"text": "ICA over and over to learn nonlinear versions.", "start": 2009.22, "duration": 3.045}, {"text": "There- there's very less, uh,", "start": 2012.265, "duration": 1.965}, {"text": "attention from research on the- on that topic than it- than it really deserves.", "start": 2014.23, "duration": 4.17}, {"text": "So may- maybe you or someone in a class could go back and do more research on that.", "start": 2018.4, "duration": 4.245}, {"text": "I, I still think is a promising area. All right.", "start": 2022.645, "duration": 4.305}, {"text": "Um, so let me wrap up with, uh, some ICA examples, um,", "start": 2026.95, "duration": 10.455}, {"text": "so this is actually a former TA from the class, um, Catie Chang.", "start": 2037.405, "duration": 5.775}, {"text": "Um, and so it turns out that, uh,", "start": 2043.18, "duration": 3.33}, {"text": "ICAs are routinely used to clean up EEG data today,", "start": 2046.51, "duration": 4.74}, {"text": "so what's an EEG, right?", "start": 2051.25, "duration": 1.65}, {"text": "Um, place many electrodes on your scalp, uh,", "start": 2052.9, "duration": 3.06}, {"text": "to measure low electrical recordings,", "start": 2055.96, "duration": 2.73}, {"text": "uh, on the surface of your scalp.", "start": 2058.69, "duration": 1.755}, {"text": "So, you know, wha- what does the human brain do, right?", "start": 2060.445, "duration": 2.67}, {"text": "Human brain, your neurons in your brain right now,", "start": 2063.115, "duration": 2.76}, {"text": "uh, fire, generate little pulses of electricity,", "start": 2065.875, "duration": 2.625}, {"text": "and if you put- place electrodes on your scalp,", "start": 2068.5, "duration": 2.385}, {"text": "you can get very weak measurements of the,", "start": 2070.885, "duration": 2.265}, {"text": "um, of the voltage of the electrical activity,", "start": 2073.15, "duration": 2.415}, {"text": "in a, you know, at a certain point in your scalp.", "start": 2075.565, "duration": 2.58}, {"text": "So the analogy to- um, oh, excuse me.", "start": 2078.145, "duration": 3.615}, {"text": "Uh, oh, what's wrong. All right.", "start": 2081.76, "duration": 2.835}, {"text": "So the analogy to the cocktail party problem, the, um,", "start": 2084.595, "duration": 3.18}, {"text": "overlapping speakers' voices is that, you know,", "start": 2087.775, "duration": 3.195}, {"text": "your- your brain [NOISE] does a lot of things at the same time, right?", "start": 2090.97, "duration": 4.755}, {"text": "Your brain helps regulate your heartbeat, um,", "start": 2095.725, "duration": 3.06}, {"text": "part of your brain does that,", "start": 2098.785, "duration": 1.23}, {"text": "another part of your brain, you know,", "start": 2100.015, "duration": 1.755}, {"text": "makes your eyes blink every now and then,", "start": 2101.77, "duration": 1.89}, {"text": "another part of your brain- part of your brain is also", "start": 2103.66, "duration": 1.89}, {"text": "responsible for making sure that you breathe,", "start": 2105.55, "duration": 2.445}, {"text": "another part of your brain is responsible to", "start": 2107.995, "duration": 1.785}, {"text": "thinking about machine learning and stuff like that, right?", "start": 2109.78, "duration": 2.13}, {"text": "[LAUGHTER] So, so your brain actually handles many,", "start": 2111.91, "duration": 2.04}, {"text": "many tasks at the same time.", "start": 2113.95, "duration": 1.755}, {"text": "And as your brain, um,", "start": 2115.705, "duration": 1.86}, {"text": "sorry, not sure what's wrong with this.", "start": 2117.565, "duration": 2.145}, {"text": "Okay. And as your brain, um, uh,", "start": 2119.71, "duration": 2.73}, {"text": "carries out these different tasks in parallel,", "start": 2122.44, "duration": 3.12}, {"text": "uh, different parts of your brain generate different electrical impulses.", "start": 2125.56, "duration": 3.63}, {"text": "So think of there as, um,", "start": 2129.19, "duration": 1.905}, {"text": "imagine that you have a, you know,", "start": 2131.095, "duration": 1.74}, {"text": "cocktail party in your head, right?", "start": 2132.835, "duration": 1.905}, {"text": "So many overlapping voices,", "start": 2134.74, "duration": 2.16}, {"text": "so this is now voices in your head, uh, just going back,", "start": 2136.9, "duration": 3.45}, {"text": "but one- one- one part of your brain is saying,", "start": 2140.35, "duration": 2.31}, {"text": "all right heart, go and beat, heart go and beat,", "start": 2142.66, "duration": 1.89}, {"text": "heart go and beat, and another part of the brain is saying, hey,", "start": 2144.55, "duration": 1.785}, {"text": "breathe in and breathe out, breathe in and breathe out,", "start": 2146.335, "duration": 1.635}, {"text": "another part of the brain is ooh, you know.", "start": 2147.97, "duration": 2.04}, {"text": "What's wrong with this PowerPoint?", "start": 2150.01, "duration": 1.47}, {"text": "[LAUGHTER] That's what my brain is saying, right?", "start": 2151.48, "duration": 2.085}, {"text": "Um, and uh, what each electrode on the surface of your scalp does is it", "start": 2153.565, "duration": 5.835}, {"text": "measures an overlapping combination of all of these voices", "start": 2159.4, "duration": 2.91}, {"text": "because different parts of your brain are sending these electrical impulses,", "start": 2162.31, "duration": 3.18}, {"text": "they add up and so any one point on the surface of your brain,", "start": 2165.49, "duration": 3.225}, {"text": "reflects a sum or a mixture,", "start": 2168.715, "duration": 2.28}, {"text": "re- really a sum of these different voices,", "start": 2170.995, "duration": 2.625}, {"text": "of these different things your brain is doing.", "start": 2173.62, "duration": 2.265}, {"text": "Um, and so, uh, if you- just- just zooming in to the EEG plot, um,", "start": 2175.885, "duration": 6.255}, {"text": "each line is a voltage measured at a single electrode, right?", "start": 2182.14, "duration": 4.185}, {"text": "On say your scalp and, um, these, uh, signals are quite correlated,", "start": 2186.325, "duration": 5.76}, {"text": "you see that when there's a massive voice in your brain shouting,", "start": 2192.085, "duration": 3.87}, {"text": "you know, like, uh, uh, uh,", "start": 2195.955, "duration": 1.71}, {"text": "uh, right, beat your heart or blink your eyes,", "start": 2197.665, "duration": 2.985}, {"text": "that signal can go through all of the different electrodes,", "start": 2200.65, "duration": 3.87}, {"text": "which is why you can see these artifacts reflected in all of", "start": 2204.52, "duration": 2.61}, {"text": "these electrodes, um, uh, sorry.", "start": 2207.13, "duration": 3.375}, {"text": "All right. Turns out a pretty good way to clean up", "start": 2210.505, "duration": 3.075}, {"text": "this data is to take all of these time series", "start": 2213.58, "duration": 3.645}, {"text": "pre- pretty much exactly as we learned about it with", "start": 2217.225, "duration": 3.225}, {"text": "the ICA algorithm [NOISE] and separate it out into the independent components,", "start": 2220.45, "duration": 4.2}, {"text": "and so, um, it turns out in this example,", "start": 2224.65, "duration": 2.595}, {"text": "there are two components corresponding to driving the heartbeat,", "start": 2227.245, "duration": 3.93}, {"text": "um, that's actually the eye blink component,", "start": 2231.175, "duration": 2.865}, {"text": "and so one way to clean up this data- sorry,", "start": 2234.04, "duration": 2.82}, {"text": "I should really wonder what's wrong with this.", "start": 2236.86, "duration": 1.785}, {"text": "All right. Let me try something, [NOISE] um, maybe if I,", "start": 2238.645, "duration": 4.155}, {"text": "[NOISE] uh, oh, that's interesting. All right.", "start": 2242.8, "duration": 10.75}, {"text": "Okay, well, all right.", "start": 2259.96, "duration": 3.31}, {"text": "Um, if you, uh, uh, right,", "start": 2263.27, "duration": 4.38}, {"text": "it says heartbeat, there's eye blink,", "start": 2267.65, "duration": 1.845}, {"text": "and, uh, you don't get, all right.", "start": 2269.495, "duration": 4.05}, {"text": "And, um, if you run the ICA and then remove outs,", "start": 2273.545, "duration": 3.255}, {"text": "I have a person say,", "start": 2276.8, "duration": 1.515}, {"text": "\"Oh this heartbeat, this eye blink, can remove,", "start": 2278.315, "duration": 1.815}, {"text": "subtract all those components,", "start": 2280.13, "duration": 1.59}, {"text": "then you can end up with a, um,", "start": 2281.72, "duration": 2.22}, {"text": "much more cleaned up EEG signal,", "start": 2283.94, "duration": 2.49}, {"text": "which you can then use for downstream processing.", "start": 2286.43, "duration": 2.25}, {"text": "So actually we possibly- is, there's been a lot of research on.", "start": 2288.68, "duration": 2.295}, {"text": "You've taken an EEG reading to try to guess at a high-level what you're thinking, right?", "start": 2290.975, "duration": 4.635}, {"text": "It turns out that, uh, uh, if your train a, train a, train a,", "start": 2295.61, "duration": 3.63}, {"text": "you know, supervised learning algorithm, uh,", "start": 2299.24, "duration": 2.13}, {"text": "to try to decide, are you thinking of a noun or a verb, are you thinking of,", "start": 2301.37, "duration": 3.33}, {"text": "uh, something edible, or are you thinking of,", "start": 2304.7, "duration": 2.1}, {"text": "uh, uh, something inedible.", "start": 2306.8, "duration": 1.335}, {"text": "There's been very interesting research, uh,", "start": 2308.135, "duration": 2.1}, {"text": "trying to use an EEG to figure out just at a very coarse level, um,", "start": 2310.235, "duration": 4.17}, {"text": "not- not- not- not quite mindreading every thought you're thinking, but,", "start": 2314.405, "duration": 4.035}, {"text": "but, uh, uh, uh, but, uh,", "start": 2318.44, "duration": 1.755}, {"text": "can we categorize very coarse level thoughts?", "start": 2320.195, "duration": 2.565}, {"text": "Like, are you thinking of a person,", "start": 2322.76, "duration": 2.13}, {"text": "are you thinking of an object?", "start": 2324.89, "duration": 1.095}, {"text": "And you can actually do that to some extent using EEG readings.", "start": 2325.985, "duration": 2.805}, {"text": "But cleaning up the data to get rid of the eye blink, and", "start": 2328.79, "duration": 2.52}, {"text": "the heartbeat artifacts is a very useful, um,", "start": 2331.31, "duration": 2.985}, {"text": "pre-processing step to get cleaner data,", "start": 2334.295, "duration": 2.505}, {"text": "to feed into the learning algorithm,", "start": 2336.8, "duration": 2.01}, {"text": "to try to figure out, try to categorize,", "start": 2338.81, "duration": 1.695}, {"text": "you know, some coarse category of what you're thinking.", "start": 2340.505, "duration": 2.025}, {"text": "Okay. Um, and then more research here,", "start": 2342.53, "duration": 3.33}, {"text": "it turns out that- uh, we're kind of- I,", "start": 2345.86, "duration": 1.605}, {"text": "I mentioned the Google cat thing just now.", "start": 2347.465, "duration": 2.025}, {"text": "It turns out that, um, if you, um, uh, train ICA, uh,", "start": 2349.49, "duration": 5.88}, {"text": "oh, the font is messed up.", "start": 2355.37, "duration": 1.185}, {"text": "Um, if you train ICA on, uh, natural images, um,", "start": 2356.555, "duration": 4.35}, {"text": "ICA will say that the natu- the independent components of natural images are these edges.", "start": 2360.905, "duration": 6.06}, {"text": "Uh, and as in that, you know,", "start": 2366.965, "duration": 1.935}, {"text": "when you see a little image patch in the world,", "start": 2368.9, "duration": 1.92}, {"text": "when you've seen, you know, look, look,", "start": 2370.82, "duration": 1.215}, {"text": "look somewhere in the world, look at just a tiny little piece of the image, right?", "start": 2372.035, "duration": 3.045}, {"text": "Like 10 pixels by 10 pixels.", "start": 2375.08, "duration": 2.01}, {"text": "Um, and if you take that data and model in this ICA,", "start": 2377.09, "duration": 2.73}, {"text": "ICA will say that, uh,", "start": 2379.82, "duration": 1.635}, {"text": "the world is made up of edges or made up of patches like these and that, uh,", "start": 2381.455, "duration": 5.01}, {"text": "the way you end up with images in the world is by each of these patches,", "start": 2386.465, "duration": 3.87}, {"text": "you know, independently saying is there a vertical edge,", "start": 2390.335, "duration": 1.785}, {"text": "is there a horizontal edge, was there,", "start": 2392.12, "duration": 1.515}, {"text": "is there this type of, um,", "start": 2393.635, "duration": 1.395}, {"text": "uh, light on the left, dark on the right?", "start": 2395.03, "duration": 2.97}, {"text": "Is there this type of, uh, lighter on top,", "start": 2398.0, "duration": 1.83}, {"text": "darker on the bottom and so on.", "start": 2399.83, "duration": 1.86}, {"text": "And just by adding all of these voices that you get a typical image fashion of the world.", "start": 2401.69, "duration": 3.81}, {"text": "So they're, they're interesting theories in neuroscience about whether this is how,", "start": 2405.5, "duration": 4.065}, {"text": "you know, the human brain learns to see as well.", "start": 2409.565, "duration": 2.255}, {"text": "So, so very, very same work on, um,", "start": 2411.82, "duration": 2.175}, {"text": "ICA and sparse coding to try to use these mechanisms to explain how, you know,", "start": 2413.995, "duration": 4.695}, {"text": "the human brain tries to explain,", "start": 2418.69, "duration": 2.58}, {"text": "um, uh, tries, tries to learn to perceive images, for example.", "start": 2421.27, "duration": 3.305}, {"text": "Okay? Um, so all right.", "start": 2424.575, "duration": 5.31}, {"text": "So [NOISE] that's it for, um,", "start": 2429.885, "duration": 3.555}, {"text": "uh, the algorithms of ICA,", "start": 2433.44, "duration": 6.154}, {"text": "um, just the final comments.", "start": 2439.594, "duration": 1.996}, {"text": "Um, I think on Monday someone asked,", "start": 2441.59, "duration": 3.63}, {"text": "\"Do the number of speakers and number of microphones need to be equal?\"", "start": 2445.22, "duration": 3.63}, {"text": "So it turns out that, um, if the number of, uh, um,", "start": 2448.85, "duration": 3.975}, {"text": "microphones is larger than the number of speakers,", "start": 2452.825, "duration": 2.805}, {"text": "that's actually fine, right?", "start": 2455.63, "duration": 1.755}, {"text": "If you- if the number of microphones is larger than the number of speakers,", "start": 2457.385, "duration": 2.385}, {"text": "then if you run ICA or, or a slightly modified version of it,", "start": 2459.77, "duration": 3.9}, {"text": "you'll find that some of the speakers are just silent speakers.", "start": 2463.67, "duration": 2.955}, {"text": "Um, uh, and so, you know,", "start": 2466.625, "duration": 1.92}, {"text": "if you have, uh, 10 microphones and five speakers,", "start": 2468.545, "duration": 2.415}, {"text": "if you run this algorithm on 10 microphones, you can find that, well,", "start": 2470.96, "duration": 3.54}, {"text": "maybe five of the sources are just silent or there are", "start": 2474.5, "duration": 2.82}, {"text": "ways to just not model those five sources as well, right?", "start": 2477.32, "duration": 2.865}, {"text": "If, if you think that, uh, they're just some sources of silence.", "start": 2480.185, "duration": 3.375}, {"text": "So, so, this, so,", "start": 2483.56, "duration": 1.65}, {"text": "so a slightly modified version of this works quite well if,", "start": 2485.21, "duration": 3.525}, {"text": "um, uh, the number of speakers is larger than the number of microphones.", "start": 2488.735, "duration": 3.885}, {"text": "Um, if the- excuse me,", "start": 2492.62, "duration": 1.86}, {"text": "if the number of microphones is larger than the number of speakers,", "start": 2494.48, "duration": 1.89}, {"text": "this, this, this works quite well.", "start": 2496.37, "duration": 1.545}, {"text": "If the number of microphones is smaller than the number of speakers,", "start": 2497.915, "duration": 3.615}, {"text": "then that's still, um, uh,", "start": 2501.53, "duration": 1.71}, {"text": "very much a cutting edge research problem.", "start": 2503.24, "duration": 2.355}, {"text": "Uh, so, so for example, uh,", "start": 2505.595, "duration": 1.5}, {"text": "if you have two speakers and one microphone, um, uh,", "start": 2507.095, "duration": 4.005}, {"text": "it turns out that if you have one male and one female speaker,", "start": 2511.1, "duration": 3.975}, {"text": "so one relatively higher pitch and one much lower pitch,", "start": 2515.075, "duration": 2.55}, {"text": "then you can sometimes have some algorithms", "start": 2517.625, "duration": 2.925}, {"text": "that separate out two voices with one microphone.", "start": 2520.55, "duration": 3.675}, {"text": "Um, but it doesn't work that reliably,", "start": 2524.225, "duration": 2.175}, {"text": "it's a little bit finicky but there have been", "start": 2526.4, "duration": 1.47}, {"text": "research papers published showing that, you know,", "start": 2527.87, "duration": 2.22}, {"text": "you could make a reasonable attempt at separating out, um,", "start": 2530.09, "duration": 3.18}, {"text": "two voices with mi- one microphone if", "start": 2533.27, "duration": 1.98}, {"text": "the pitches are quite different such as this one male one female voice.", "start": 2535.25, "duration": 3.39}, {"text": "Um, uh, uh, but separating out two male voices or two female voices is still very hard,", "start": 2538.64, "duration": 5.835}, {"text": "um, uh, and, and then there's ongoing research in, in those settings.", "start": 2544.475, "duration": 4.86}, {"text": "Right? So that's ICA,", "start": 2549.335, "duration": 4.395}, {"text": "um, and I guess you get to play more of it in your,", "start": 2553.73, "duration": 2.82}, {"text": "um, homework problem as well.", "start": 2556.55, "duration": 1.8}, {"text": "Okay? Any last questions about ICA?", "start": 2558.35, "duration": 2.58}, {"text": "[inaudible]", "start": 2560.93, "duration": 0.45}, {"text": "Oh sorry, say it again?", "start": 2561.38, "duration": 8.94}, {"text": "[inaudible] [NOISE]", "start": 2570.32, "duration": 21.43}, {"text": "Wait, sorry, was because a-", "start": 2591.75, "duration": 2.52}, {"text": "So I'm just wondering why is it that hard [inaudible]", "start": 2594.7, "duration": 12.94}, {"text": "Oh, [NOISE] yeah so, um,", "start": 2607.64, "duration": 2.58}, {"text": "uh, I think- actually if you go through a lot of the math it,", "start": 2610.22, "duration": 3.54}, {"text": "it, it, it just breaks down, I think.", "start": 2613.76, "duration": 2.01}, {"text": "Um, because there- you can have two independent sources", "start": 2615.77, "duration": 3.3}, {"text": "but W is now no longer a square matrix, right?", "start": 2619.07, "duration": 3.585}, {"text": "Of your, what is it?", "start": 2622.655, "duration": 2.22}, {"text": "Um, uh, uh, so- uh, uh, right.", "start": 2624.875, "duration": 3.93}, {"text": "Is that x is equal to AS, right?", "start": 2628.805, "duration": 3.255}, {"text": "And so if, um,", "start": 2632.06, "duration": 1.305}, {"text": "x is a real number and S was two-dimensional,", "start": 2633.365, "duration": 4.225}, {"text": "so I guess this would be, um,", "start": 2639.76, "duration": 3.26}, {"text": "uh, uh, A would be 2 by 1,", "start": 2643.57, "duration": 4.045}, {"text": "S would be- uh, S- uh,", "start": 2647.615, "duration": 1.245}, {"text": "A would be 2 by 1, S would be 2- excuse me,", "start": 2648.86, "duration": 2.925}, {"text": "A would be 1 by 2 and S would be a 2 by 1,", "start": 2651.785, "duration": 4.515}, {"text": "and this is 1 by 1, then,", "start": 2656.3, "duration": 2.325}, {"text": "you know, A inverse kind of doesn't exist, right?", "start": 2658.625, "duration": 2.265}, {"text": "So you need to come up with a way to form the maximum likelihood model.", "start": 2660.89, "duration": 3.915}, {"text": "And, and when you have one microphone,", "start": 2664.805, "duration": 1.455}, {"text": "it's just how do you separate out two overlapping voices,right?", "start": 2666.26, "duration": 3.48}, {"text": "Does that make sense?", "start": 2669.74, "duration": 1.44}, {"text": "So it takes much higher level knowledge,", "start": 2671.18, "duration": 1.935}, {"text": "um, uh, yeah, to separate out two voices.", "start": 2673.115, "duration": 4.275}, {"text": "Does this make sense? Um, so go ahead.", "start": 2677.39, "duration": 2.22}, {"text": "[inaudible]", "start": 2679.61, "duration": 14.79}, {"text": "Oh I see, right, uh, let's see.", "start": 2694.4, "duration": 4.005}, {"text": "So right, so if you don't know how many speakers there are,", "start": 2698.405, "duration": 2.04}, {"text": "you have all these microphones where you have", "start": 2700.445, "duration": 1.455}, {"text": "all- the number of electrodes you have is fixed,", "start": 2701.9, "duration": 1.71}, {"text": "so that's just your data set.", "start": 2703.61, "duration": 1.29}, {"text": "And it turns out that, uh, um,", "start": 2704.9, "duration": 3.06}, {"text": "if you run ICA with a large number of speakers,", "start": 2707.96, "duration": 2.7}, {"text": "you find there are many speakers are silent.", "start": 2710.66, "duration": 1.71}, {"text": "There are also some versions of ICA that you- so if you think that there are,", "start": 2712.37, "duration": 5.565}, {"text": "um, uh, let's see, boy- those transfer some of this.", "start": 2717.935, "duration": 6.27}, {"text": "But it turns out that, um,", "start": 2724.205, "duration": 2.025}, {"text": "if you think that there is a relatively small number of speakers,", "start": 2726.23, "duration": 3.615}, {"text": "then you don't need to explicitly model all the speakers.", "start": 2729.845, "duration": 3.78}, {"text": "Instead, what you would model wou- so again,", "start": 2733.625, "duration": 3.45}, {"text": "um, uh, suppose it's a maximum likelihood estimation problem.", "start": 2737.075, "duration": 4.08}, {"text": "Um, let's say that, uh, x is an R10, right?", "start": 2741.155, "duration": 4.275}, {"text": "So you have 10 recordings.", "start": 2745.43, "duration": 1.26}, {"text": "But you suspect that you only have five speakers.", "start": 2746.69, "duration": 2.49}, {"text": "Then in this case,", "start": 2749.18, "duration": 1.29}, {"text": "I guess the ma- matrix A would be um, what is it?", "start": 2750.47, "duration": 4.425}, {"text": "Uh, was it?", "start": 2754.895, "duration": 1.965}, {"text": "It would be 10 by 5, is it?", "start": 2756.86, "duration": 2.29}, {"text": "Right? To mix the 5 sources into 10 speakers.", "start": 2760.21, "duration": 4.795}, {"text": "And you could, um,", "start": 2765.005, "duration": 1.845}, {"text": "form the maximum likelihood estimation problem assuming the existence of", "start": 2766.85, "duration": 3.87}, {"text": "only five speakers without modeling", "start": 2770.72, "duration": 2.065}, {"text": "a lot of speakers and then finding later that they're all silent.", "start": 2772.785, "duration": 3.295}, {"text": "Does that make sense? So if you form the- so if- if you", "start": 2776.08, "duration": 2.76}, {"text": "parameterize the model like this using A instead of W, um, uh,", "start": 2778.84, "duration": 3.825}, {"text": "then you could form the maximum likelihood estimation problem", "start": 2782.665, "duration": 3.12}, {"text": "where you just assume that there are", "start": 2785.785, "duration": 1.975}, {"text": "five speakers and S is generated by", "start": 2787.76, "duration": 3.03}, {"text": "five speakers mixing through a linear thing plus noise.", "start": 2790.79, "duration": 3.42}, {"text": "But I just think that if you don't know how many speakers you", "start": 2794.21, "duration": 2.72}, {"text": "have or even what you are- what speakers you are working on,", "start": 2796.93, "duration": 2.92}, {"text": "how would you know if you probably had enough microphones?", "start": 2799.85, "duration": 3.03}, {"text": "Oh I see, sure, right.", "start": 2802.88, "duration": 1.515}, {"text": "How do you know if you have- how do you know how many speakers you have?", "start": 2804.395, "duration": 2.205}, {"text": "So I, I think it's one of those things that's a little bit like k-means,", "start": 2806.6, "duration": 3.06}, {"text": "I guess, where you try it and see what works.", "start": 2809.66, "duration": 2.19}, {"text": "And if you find that, uh,", "start": 2811.85, "duration": 1.425}, {"text": "the first few, you know,", "start": 2813.275, "duration": 1.425}, {"text": "speakers will capture most of the variance,", "start": 2814.7, "duration": 2.25}, {"text": "you find that digital speakers are quite silent and they're quite small,", "start": 2816.95, "duration": 3.045}, {"text": "you could just cut it off at that time.", "start": 2819.995, "duration": 1.575}, {"text": "I don't wanna go too much into the different numbers of speakers and,", "start": 2821.57, "duration": 3.375}, {"text": "and, and, uh, microphones, ICA algorithms.", "start": 2824.945, "duration": 3.87}, {"text": "Uh, uh, but let me just take a couple of last questions", "start": 2828.815, "duration": 2.685}, {"text": "and move on. You have a question? Yeah.", "start": 2831.5, "duration": 2.47}, {"text": "Do you ever see a problem with W?", "start": 2834.01, "duration": 2.755}, {"text": "Say it again?", "start": 2836.765, "duration": 1.485}, {"text": "Do you ever see a problem with W?", "start": 2838.25, "duration": 1.755}, {"text": "Oh, do you ever see a problem with W?", "start": 2840.005, "duration": 2.13}, {"text": "Um, I'm sure you can.", "start": 2842.135, "duration": 3.705}, {"text": "It's not usually done in this version of the algorithm,", "start": 2845.84, "duration": 3.975}, {"text": "but I would not be surprised if there are some other versions where you do.", "start": 2849.815, "duration": 2.88}, {"text": "I've, I've not seen that a lot myself actually.", "start": 2852.695, "duration": 2.115}, {"text": "All right, cool.", "start": 2854.81, "duration": 4.02}, {"text": "All right, cool.", "start": 2858.83, "duration": 3.96}, {"text": "Um, let's see.", "start": 2862.79, "duration": 2.1}, {"text": "All right, good, we're far enough along.", "start": 2864.89, "duration": 2.58}, {"text": "Okay, good. Um, so-", "start": 2867.47, "duration": 16.62}, {"text": "[NOISE] Circumstantial- All right.", "start": 2884.09, "duration": 5.67}, {"text": "All right, yeah, let's do these- [NOISE]", "start": 2889.76, "duration": 12.39}, {"text": "All right. Um- [NOISE]", "start": 2902.15, "duration": 17.25}, {"text": "All right. So that wraps up,", "start": 2919.4, "duration": 4.185}, {"text": "um, our chapter on unsupervised learning.", "start": 2923.585, "duration": 5.295}, {"text": "So, um, you learned about I,", "start": 2928.88, "duration": 2.64}, {"text": "guess, k-means clustering, um,", "start": 2931.52, "duration": 2.955}, {"text": "the EM algorithm for mixture of Gaussians, uh,", "start": 2934.475, "duration": 3.615}, {"text": "or really mixture of Gaussians model, um,", "start": 2938.09, "duration": 2.505}, {"text": "factor analysis model, and also PCA.", "start": 2940.595, "duration": 3.255}, {"text": "And then, you know,", "start": 2943.85, "duration": 1.305}, {"text": "today the ICA or independent components analysis algorithm.", "start": 2945.155, "duration": 3.81}, {"text": "And all of these are algorithms that could take as input an unlabeled training set,", "start": 2948.965, "duration": 4.365}, {"text": "just the xi's and no labels.", "start": 2953.33, "duration": 1.83}, {"text": "And we'll find various interesting structures in the data such as", "start": 2955.16, "duration": 2.94}, {"text": "clusters or subspaces or in the case of ICA,", "start": 2958.1, "duration": 2.88}, {"text": "the voices of the independent speakers.", "start": 2960.98, "duration": 2.055}, {"text": "And, and you implement ICA and play with it yourself in the homework problem,", "start": 2963.035, "duration": 4.665}, {"text": "where you get to separate out many five overlapping, um, voices.", "start": 2967.7, "duration": 4.815}, {"text": "The last of the four major topics, I want to cover in this class.", "start": 2972.515, "duration": 4.485}, {"text": "We've talked about supervised learning,", "start": 2977.0, "duration": 1.29}, {"text": "kind of device machine learning, unsupervised learning,", "start": 2978.29, "duration": 3.255}, {"text": "and the fourth and the final major topic we'll cover in this course will be", "start": 2981.545, "duration": 4.335}, {"text": "on reinforcement learning [NOISE].", "start": 2985.88, "duration": 6.21}, {"text": "Okay. So, um, so to motivate reinforcement learning.", "start": 2992.09, "duration": 6.54}, {"text": "Um, let's say you want to have a computer,", "start": 2998.63, "duration": 5.43}, {"text": "uh, learn to fly a helicopter, right?", "start": 3004.06, "duration": 2.925}, {"text": "I think I showed you some of the videos that are in the first lecture,", "start": 3006.985, "duration": 2.745}, {"text": "and so I just skipped that here.", "start": 3009.73, "duration": 1.395}, {"text": "But it turns out that, um,", "start": 3011.125, "duration": 2.295}, {"text": "if you are at every point in time given the position of a helicopter,", "start": 3013.42, "duration": 4.065}, {"text": "called the state of a helicopter,", "start": 3017.485, "duration": 1.574}, {"text": "and you're asked to take an action on how to move the control sticks,", "start": 3019.059, "duration": 3.406}, {"text": "you know, to make the helicopter fly in a certain trajectory.", "start": 3022.465, "duration": 3.18}, {"text": "It turns out that it's very difficult to know what's", "start": 3025.645, "duration": 2.655}, {"text": "the one right answer for how to move the control sticks of a helicopter.", "start": 3028.3, "duration": 4.035}, {"text": "Right. So if you don't have a mapping from X to Y because", "start": 3032.335, "duration": 3.165}, {"text": "you can't quite specify the one true way to fly a helicopter,", "start": 3035.5, "duration": 3.239}, {"text": "um, it's hard to use supervised learning for that, right.", "start": 3038.739, "duration": 4.291}, {"text": "And what reinforcement learning does is, is, is an,", "start": 3043.03, "duration": 3.51}, {"text": "an algorithm that doesn't ask you to tell it the right answer at every step,", "start": 3046.54, "duration": 4.11}, {"text": "it doesn't ask you to tell it exactly what's", "start": 3050.65, "duration": 1.74}, {"text": "the one true way to move the controls of a helicopter at any moment in time.", "start": 3052.39, "duration": 3.885}, {"text": "Instead, your responsibility as a designer", "start": 3056.275, "duration": 2.745}, {"text": "or machine learning engineer or AI engineer is to", "start": 3059.02, "duration": 2.55}, {"text": "specify a reward function that just tells", "start": 3061.57, "duration": 2.64}, {"text": "the helicopter when it's flying well and when it's flying poorly.", "start": 3064.21, "duration": 3.27}, {"text": "So your job as a designer is to write the cost function or", "start": 3067.48, "duration": 3.66}, {"text": "a reward function that gives a helicopter a high reward whenever it's doing well.", "start": 3071.14, "duration": 4.92}, {"text": "Flying accurately, flying the trajectory you want it to,", "start": 3076.06, "duration": 2.145}, {"text": "and it gives the helicopter a larger negative reward,", "start": 3078.205, "duration": 2.76}, {"text": "um, whenever it crashes or does something bad, right?", "start": 3080.965, "duration": 2.805}, {"text": "And I think I, I, I remember, I think, you know, think of it as like training a dog, right?", "start": 3083.77, "duration": 4.08}, {"text": "When do you say good dog, when do you say bad dog?", "start": 3087.85, "duration": 2.1}, {"text": "And the dog figures out when to do more of the good dog things.", "start": 3089.95, "duration": 3.0}, {"text": "And your job is not to tell the dog,", "start": 3092.95, "duration": 2.37}, {"text": "when you can't actually talk to the dog,", "start": 3095.32, "duration": 1.665}, {"text": "and tell it what to do. I guess that doesn't work.", "start": 3096.985, "duration": 1.95}, {"text": "But you can tell it good dog and bad dog,", "start": 3098.935, "duration": 2.235}, {"text": "and hopefully it learns from those positive and negative rewards", "start": 3101.17, "duration": 2.355}, {"text": "how to do more of the good things.", "start": 3103.525, "duration": 1.89}, {"text": "Okay. Um, another example.", "start": 3105.415, "duration": 3.225}, {"text": "Um, let's say you want to write a program to play chess or I guess most, you know,", "start": 3108.64, "duration": 4.995}, {"text": "somewhat famously and, uh, uh,", "start": 3113.635, "duration": 2.235}, {"text": "arguably somewhat slightly overhyped Go, AlphaGo, right.", "start": 3115.87, "duration": 4.44}, {"text": "Um, so it's very difficult to know in", "start": 3120.31, "duration": 3.54}, {"text": "given a certain chess board position or checkers or Go board position,", "start": 3123.85, "duration": 4.17}, {"text": "what is the one true move,", "start": 3128.02, "duration": 1.29}, {"text": "what's the one best move.", "start": 3129.31, "duration": 1.17}, {"text": "So it's very difficult to formulate, um, you know,", "start": 3130.48, "duration": 2.55}, {"text": "playing chess, uh, uh,", "start": 3133.03, "duration": 1.665}, {"text": "as a supervised learning problem.", "start": 3134.695, "duration": 2.115}, {"text": "And instead, um, the mechanisms used to play", "start": 3136.81, "duration": 2.97}, {"text": "chess are much more like reinforcement learning,", "start": 3139.78, "duration": 3.09}, {"text": "where you can, um,", "start": 3142.87, "duration": 1.905}, {"text": "let your program play chess or Go or whatever.", "start": 3144.775, "duration": 2.91}, {"text": "And whenever it wins you go, \"Oh good computer.\"", "start": 3147.685, "duration": 3.104}, {"text": "And when it loses you go, \"Oh bad computer.\"", "start": 3150.789, "duration": 2.581}, {"text": "So that's a reward function.", "start": 3153.37, "duration": 1.605}, {"text": "And the learning algorithm's job is to figure out by", "start": 3154.975, "duration": 2.565}, {"text": "itself how to get more of the positive rewards, right?", "start": 3157.54, "duration": 2.925}, {"text": "And actually common rewards for, uh, learning to play,", "start": 3160.465, "duration": 3.015}, {"text": "uh, chess or checkers or Othello or Go is, uh,", "start": 3163.48, "duration": 3.63}, {"text": "plus a reward of plus 1 for a win,", "start": 3167.11, "duration": 3.48}, {"text": "minus 1 for a lose, and a 0 for a tie, right?", "start": 3170.59, "duration": 5.94}, {"text": "So as you write your chess-playing programs, there has to be a common choice for a reward.", "start": 3176.53, "duration": 3.165}, {"text": "Um, where R is the reward function and S is the state.", "start": 3179.695, "duration": 4.41}, {"text": "Okay. And I will go into the notation, um, in a little bit.", "start": 3184.105, "duration": 4.71}, {"text": "And so as you can imagine, um,", "start": 3188.815, "duration": 3.885}, {"text": "given only this type of information so say a chess-playing program,", "start": 3192.7, "duration": 3.795}, {"text": "it places much more burden on the program to figure out what to do.", "start": 3196.495, "duration": 3.7}, {"text": "Right. In fact, one of the challenges of reinforcement learning is,", "start": 3200.195, "duration": 4.57}, {"text": "uh- so this is called a reward,", "start": 3204.765, "duration": 2.505}, {"text": "and that's called the state.", "start": 3207.27, "duration": 2.49}, {"text": "And the state means, um,", "start": 3209.76, "duration": 1.785}, {"text": "the status of the chessboard.", "start": 3211.545, "duration": 1.47}, {"text": "Where are the P's in the chessboard?", "start": 3213.015, "duration": 1.335}, {"text": "Or the status of the helicopter.", "start": 3214.35, "duration": 2.04}, {"text": "Where exactly is the helicopter?", "start": 3216.39, "duration": 1.62}, {"text": "And you're either right-side up or you're upside down,", "start": 3218.01, "duration": 1.71}, {"text": "and where are you, right?", "start": 3219.72, "duration": 1.65}, {"text": "Um, and it turns out one of the challenges,", "start": 3221.37, "duration": 3.73}, {"text": "one of the things that makes, um,", "start": 3228.03, "duration": 2.935}, {"text": "reinforcement learning hard is,", "start": 3230.965, "duration": 2.28}, {"text": "uh, the credit assignment problem.", "start": 3233.245, "duration": 1.68}, {"text": "And that means that if, uh,", "start": 3234.925, "duration": 1.995}, {"text": "your program is playing a game of chess,", "start": 3236.92, "duration": 2.46}, {"text": "and let's say it loses on move 50.", "start": 3239.38, "duration": 3.57}, {"text": "You know, so it plays a game,", "start": 3242.95, "duration": 1.425}, {"text": "and then on move 50, right, is checkmated and loses to its opponent.", "start": 3244.375, "duration": 3.675}, {"text": "So it gets a reward of negative 1.", "start": 3248.05, "duration": 2.4}, {"text": "But how can the program actually figure out", "start": 3250.45, "duration": 2.58}, {"text": "what it did well and what it did poorly, right?", "start": 3253.03, "duration": 2.4}, {"text": "If you lose a game on move 50,", "start": 3255.43, "duration": 2.1}, {"text": "it might be that the program made a really bad move,", "start": 3257.53, "duration": 3.09}, {"text": "made a blunder at move 20.", "start": 3260.62, "duration": 2.205}, {"text": "And then, you know,", "start": 3262.825, "duration": 1.305}, {"text": "but it just took another 30 moves before its fate was sealed, right.", "start": 3264.13, "duration": 3.51}, {"text": "So in a game of chess, you made a bad mistake early on,", "start": 3267.64, "duration": 2.475}, {"text": "you can still take many, many games- many,", "start": 3270.115, "duration": 2.295}, {"text": "many moves in the game of chess before,", "start": 3272.41, "duration": 2.205}, {"text": "before the final outcome of,", "start": 3274.615, "duration": 1.545}, {"text": "of losing or winning or losing is reached.", "start": 3276.16, "duration": 3.225}, {"text": "Or, um, in a, uh, initiate another- it turns out that, uh,", "start": 3279.385, "duration": 5.1}, {"text": "if you are trying to build a self-driving car,", "start": 3284.485, "duration": 2.94}, {"text": "um, if ever car crashes, right,", "start": 3287.425, "duration": 3.375}, {"text": "chances are the thing the car was doing right before it crashes was brake,", "start": 3290.8, "duration": 4.395}, {"text": "but it's not braking that caused the crash.", "start": 3295.195, "duration": 1.995}, {"text": "It's probably something else that caused it many,", "start": 3297.19, "duration": 1.5}, {"text": "many seconds ago that led to the bad outcome.", "start": 3298.69, "duration": 2.31}, {"text": "So there's a bad outcome.", "start": 3301.0, "duration": 1.365}, {"text": "How does the algorithm know of all the things that it did before,", "start": 3302.365, "duration": 3.27}, {"text": "how does it know what it did well?", "start": 3305.635, "duration": 1.935}, {"text": "What it should do more of and what they should- did poorly,", "start": 3307.57, "duration": 2.31}, {"text": "what it should do less of.", "start": 3309.88, "duration": 1.725}, {"text": "And, and conversely, if there's a good outcome,", "start": 3311.605, "duration": 2.445}, {"text": "you know, like it wins a game of chess.", "start": 3314.05, "duration": 1.605}, {"text": "Well, how do you know what you did well, right?", "start": 3315.655, "duration": 2.25}, {"text": "So that's called the credit assignment problem,", "start": 3317.905, "duration": 2.145}, {"text": "which is when your algorithm gets some reward,", "start": 3320.05, "duration": 2.385}, {"text": "how, how do you actually figure out what you did well and what you did poorly?", "start": 3322.435, "duration": 2.805}, {"text": "So you know what to do you more of and what to do less of, right?", "start": 3325.24, "duration": 3.885}, {"text": "So, um, as we develop reinforcing learning algorithms,", "start": 3329.125, "duration": 3.72}, {"text": "we'll see that the algorithms we use have to at least indirectly,", "start": 3332.845, "duration": 4.86}, {"text": "um, try to solve the credit assignment problem.", "start": 3337.705, "duration": 2.67}, {"text": "Okay. So, um,", "start": 3340.375, "duration": 4.725}, {"text": "reinforcement learning problems like playing chess or flying helicopters or, um, uh,", "start": 3345.1, "duration": 5.64}, {"text": "you know, building these various robots is modeled using the,", "start": 3350.74, "duration": 5.38}, {"text": "um, MDP or the Markov decision process formalism. [NOISE]", "start": 3357.51, "duration": 9.219}, {"text": "Um, and this is a way-", "start": 3366.729, "duration": 14.551}, {"text": "this is a notation and the formalism for modeling how the world works,", "start": 3381.28, "duration": 3.27}, {"text": "and then reinforcement learning algorithms will solve problems using this formalism.", "start": 3384.55, "duration": 4.71}, {"text": "So what's an MDP?", "start": 3389.26, "duration": 1.98}, {"text": "So an MDP is a five tuple.", "start": 3391.24, "duration": 4.24}, {"text": "And let me explain what each of these are.", "start": 3398.37, "duration": 3.115}, {"text": "Um, so S is the set of states.", "start": 3401.485, "duration": 5.185}, {"text": "So for example, uh,", "start": 3409.05, "duration": 2.5}, {"text": "in chess this would be the set of all possible chess positions or in,", "start": 3411.55, "duration": 4.08}, {"text": "uh, flying a helicopter.", "start": 3415.63, "duration": 1.245}, {"text": "This would be the set of all the possible positions,", "start": 3416.875, "duration": 2.385}, {"text": "and orientations, and velocities of your helicopter.", "start": 3419.26, "duration": 3.49}, {"text": "A is the set of actions, um, where, uh,", "start": 3423.84, "duration": 6.655}, {"text": "in the helicopter this would be all the positions you could move", "start": 3430.495, "duration": 3.345}, {"text": "your controls sticks or in chess this would be all the moves you can make, you know, in a,", "start": 3433.84, "duration": 4.175}, {"text": "in a game of chess. [NOISE].", "start": 3438.015, "duration": 21.48}, {"text": "Uh, P subscript sa is a-a state transition probabilities and so, um,", "start": 3459.495, "duration": 6.87}, {"text": "we'll see later these-these state transition probabilities tell you,", "start": 3466.365, "duration": 4.14}, {"text": "if you take a certain, uh, action a and a certain state s,", "start": 3470.505, "duration": 4.23}, {"text": "what is the chance of you ending up at a particular different state s prime?", "start": 3474.735, "duration": 6.505}, {"text": "Great.", "start": 3481.24, "duration": 2.21}, {"text": "Um, gamma is", "start": 3494.09, "duration": 2.215}, {"text": "a discount factor, that's a number between 0 and 1.", "start": 3496.305, "duration": 2.91}, {"text": "Uh, don't worry about this for now,", "start": 3499.215, "duration": 1.425}, {"text": "we'll come back to this in a minute,", "start": 3500.64, "duration": 1.71}, {"text": "and R is that all important reward function.", "start": 3502.35, "duration": 6.96}, {"text": "Okay, so, um,", "start": 3509.31, "duration": 3.25}, {"text": "in order to develop a reinforcement learning algorithm, um,", "start": 3521.78, "duration": 5.995}, {"text": "I'm going to use, as a running example,", "start": 3527.775, "duration": 2.775}, {"text": "a simplified MDP that we can draw on the whiteboard.", "start": 3530.55, "duration": 3.15}, {"text": "Right, so helicopters and chess and go and so on are really complicated MDPs.", "start": 3533.7, "duration": 4.29}, {"text": "So just to illustrate the algorithms,", "start": 3537.99, "duration": 2.31}, {"text": "I want to use a simpler MDP, uh, and this is, um,", "start": 3540.3, "duration": 4.05}, {"text": "an example we've drawn from the textbook Russell and Norvig.", "start": 3544.35, "duration": 3.48}, {"text": "Um, I'm going to use", "start": 3547.83, "duration": 3.9}, {"text": "simplified MDP in which you have a robot navigating this simple maze,", "start": 3551.73, "duration": 5.52}, {"text": "ah, and there's an obstacle.", "start": 3557.25, "duration": 1.59}, {"text": "So this is a grid work, right.", "start": 3558.84, "duration": 1.32}, {"text": "So a robot, you know- well the R2D2 like robot.", "start": 3560.16, "duration": 4.23}, {"text": "Yes, right, um, and it's navigating this very simple maze,", "start": 3564.39, "duration": 4.095}, {"text": "uh, and this is a pillar or this is a wall,", "start": 3568.485, "duration": 2.565}, {"text": "so you can't walk into that wall,", "start": 3571.05, "duration": 2.22}, {"text": "[NOISE] and let me just use,", "start": 3573.27, "duration": 2.19}, {"text": "um, indexing on the states as follows.", "start": 3575.46, "duration": 5.34}, {"text": "Um, so this MDP- let's- let's go through the five top points and talk about what,", "start": 3580.8, "duration": 6.06}, {"text": "uh, the- the- each of the five things are.", "start": 3586.86, "duration": 3.09}, {"text": "So this MDP has 11 states", "start": 3589.95, "duration": 3.24}, {"text": "corresponding to the 11 possible positions that the robot could be in,", "start": 3593.19, "duration": 5.04}, {"text": "right, each of these bank squares.", "start": 3598.23, "duration": 1.32}, {"text": "So there are 11 possible states,", "start": 3599.55, "duration": 1.755}, {"text": "and the actions, um,", "start": 3601.305, "duration": 7.92}, {"text": "are North, South, East and West, right?", "start": 3609.225, "duration": 2.325}, {"text": "You can command your robot to move in any of these directions.", "start": 3611.55, "duration": 3.975}, {"text": "Um, and I don't know if- if you worked with robots before, you know that, um,", "start": 3615.525, "duration": 3.975}, {"text": "when you command a robot, uh, you know,", "start": 3619.5, "duration": 1.83}, {"text": "to head straight, um,", "start": 3621.33, "duration": 1.83}, {"text": "it doesn't always go exactly straight.", "start": 3623.16, "duration": 2.19}, {"text": "Sometimes the wheel slips and veers off at a slight angle,", "start": 3625.35, "duration": 2.745}, {"text": "and so just simplifying the example,", "start": 3628.095, "duration": 2.07}, {"text": "we're going to model it as that, um,", "start": 3630.165, "duration": 2.7}, {"text": "if you command the robot to go North from a certain state,", "start": 3632.865, "duration": 5.115}, {"text": "that there is a 0.8% chance of successfully go the way you told it", "start": 3637.98, "duration": 5.28}, {"text": "to and a 0.1 chance that it will", "start": 3643.26, "duration": 3.06}, {"text": "accidentally veer off to the left or accidentally veer off to the right, okay?", "start": 3646.32, "duration": 3.405}, {"text": "Um, if you're working on real robots,", "start": 3649.725, "duration": 3.255}, {"text": "right, What's a real robot?", "start": 3652.98, "duration": 1.335}, {"text": "Uh, it is actually important to model the noisy dynamics of a robot wheel slipping slightly.", "start": 3654.315, "duration": 5.055}, {"text": "Or the orientation being slightly off.", "start": 3659.37, "duration": 2.22}, {"text": "Now, um, in a real robot,", "start": 3661.59, "duration": 1.905}, {"text": "you have a much bigger state space than the 11 states,", "start": 3663.495, "duration": 2.46}, {"text": "right, so- so this is simplified.", "start": 3665.955, "duration": 1.905}, {"text": "So this is not a realistic model for how", "start": 3667.86, "duration": 2.4}, {"text": "robots actually slip but because of using such a small state space,", "start": 3670.26, "duration": 2.97}, {"text": "I think just for illustration purposes,", "start": 3673.23, "duration": 2.07}, {"text": "we'll- we'll- we'll use this.", "start": 3675.3, "duration": 2.115}, {"text": "Um, and so for example,", "start": 3677.415, "duration": 3.085}, {"text": "the state transition probability would specify these.", "start": 3683.33, "duration": 2.98}, {"text": "You'd say that if you're in the state 3, 1.", "start": 3686.31, "duration": 2.205}, {"text": "So this state 3, 1,", "start": 3688.515, "duration": 1.905}, {"text": "and you command it to go North,", "start": 3690.42, "duration": 2.22}, {"text": "that the chance of getting to the state 3, 2 is,", "start": 3692.64, "duration": 6.015}, {"text": "uh, 0.8, and the chance", "start": 3698.655, "duration": 6.885}, {"text": "of getting to the state 4, 1 is 0.1,", "start": 3705.54, "duration": 4.96}, {"text": "chance again to 2, 1 is 0.1,", "start": 3711.83, "duration": 4.63}, {"text": "um, and the chance of getting to other states is like 3, 3 and other states is equal to 0, okay?", "start": 3716.46, "duration": 8.985}, {"text": "So the state transition probabilities will capture that,", "start": 3725.445, "duration": 3.15}, {"text": "if you're here and decide to go North,", "start": 3728.595, "duration": 1.605}, {"text": "there is a 0.8 chance you are going here,", "start": 3730.2, "duration": 1.89}, {"text": "0.1 chance you are going here,", "start": 3732.09, "duration": 1.575}, {"text": "0.1 chance you are going here, and you know,", "start": 3733.665, "duration": 1.995}, {"text": "you've got 0.0 chance of, right, hopping two steps.", "start": 3735.66, "duration": 3.45}, {"text": "Okay. Um, and- and again just simplifying the MDP example.", "start": 3739.11, "duration": 9.81}, {"text": "We'll just assume that the-the robot, you know,", "start": 3748.92, "duration": 2.355}, {"text": "hits a wall, it just bounces off the wall and stays where it is.", "start": 3751.275, "duration": 2.7}, {"text": "So if you told it to go East,", "start": 3753.975, "duration": 1.595}, {"text": "it slips off and just bounced off the wall and stays exactly where it is.", "start": 3755.57, "duration": 4.33}, {"text": "Now, let's specify the reward function,", "start": 3761.08, "duration": 4.78}, {"text": "uh, we'll come back to discount factor later.", "start": 3765.86, "duration": 1.925}, {"text": "But let's say you want the robot to navigate", "start": 3767.785, "duration": 4.325}, {"text": "to this cell in the upper right-hand corner, um,", "start": 3772.11, "duration": 4.785}, {"text": "and so to incentivize the reward- incentivize the robot to get to this square,", "start": 3776.895, "duration": 4.845}, {"text": "you know, that's the prize or that's the goal anyways,", "start": 3781.74, "duration": 2.235}, {"text": "let's put a plus 1 reward there and, um,", "start": 3783.975, "duration": 3.315}, {"text": "let's say you really don't want the robot to go to this cell,", "start": 3787.29, "duration": 3.66}, {"text": "you could put a negative 1 reward there.", "start": 3790.95, "duration": 2.805}, {"text": "Alright. So, um, the way you specify", "start": 3793.755, "duration": 3.72}, {"text": "the tasks for a robot to do is in designing the reward function.", "start": 3797.475, "duration": 5.455}, {"text": "So in our example,", "start": 3819.68, "duration": 4.19}, {"text": "um, well let me just copy that again, plus 1, minus 1.", "start": 3824.48, "duration": 7.69}, {"text": "Um, we have that the reward at the cell 4, 3 is plus 1,", "start": 3832.17, "duration": 10.26}, {"text": "and the reward at the cell 4, 2 is negative 1.", "start": 3842.43, "duration": 5.19}, {"text": "Um, and then, you know,", "start": 3847.62, "duration": 1.635}, {"text": "if you want the robot to get to the +1 reward cell as quickly as possible,", "start": 3849.255, "duration": 6.059}, {"text": "then, um, again there- there are many ways of designing reward functions.", "start": 3855.314, "duration": 4.231}, {"text": "Well, one common choice would be to,", "start": 3859.545, "duration": 2.565}, {"text": "um, put the negative penalty,", "start": 3862.11, "duration": 2.835}, {"text": "a very small negative penalty,", "start": 3864.945, "duration": 2.725}, {"text": "right, such as a set the reward to negative 0.02 for all other states.", "start": 3873.35, "duration": 5.485}, {"text": "And the effect of a small negative reward like this is to charge it,", "start": 3878.835, "duration": 4.71}, {"text": "right, every- every step it's just loitering around.", "start": 3883.545, "duration": 2.985}, {"text": "So charge it a little bit for using up electricity and wandering around, uh,", "start": 3886.53, "duration": 3.93}, {"text": "because this incentivizes the robot to hurry up and get to the plus 1 reward.", "start": 3890.46, "duration": 4.92}, {"text": "So you give a small penalty, you know,", "start": 3895.38, "duration": 2.56}, {"text": "for- for loitering and wasting electricity.", "start": 3897.94, "duration": 2.405}, {"text": "So this is how an MDP works.", "start": 3900.345, "duration": 14.82}, {"text": "Um, your robot wakes up at some state as 0,", "start": 3915.165, "duration": 4.59}, {"text": "um, at time 0, you know,", "start": 3919.755, "duration": 1.425}, {"text": "as you turn on the robot and the robot says,", "start": 3921.18, "duration": 1.665}, {"text": "\"Oh, I'm at that state.\"", "start": 3922.845, "duration": 1.425}, {"text": "And based on what state it is in, um,", "start": 3924.27, "duration": 3.195}, {"text": "it will get to choose some action,", "start": 3927.465, "duration": 3.625}, {"text": "a0, so decide to only go North,", "start": 3931.16, "duration": 2.98}, {"text": "South, East or West and choose some action.", "start": 3934.14, "duration": 2.7}, {"text": "Based on the action,", "start": 3936.84, "duration": 2.325}, {"text": "the consequence of the choice is it will get to some state S1.", "start": 3939.165, "duration": 4.605}, {"text": "Uh, the state at the next time step,", "start": 3943.77, "duration": 2.61}, {"text": "which is distributed according to", "start": 3946.38, "duration": 2.865}, {"text": "the state transition probabilities governed by", "start": 3949.245, "duration": 2.775}, {"text": "the previous state and the action it chose, right.", "start": 3952.02, "duration": 2.34}, {"text": "So depending on what action it chooses,", "start": 3954.36, "duration": 1.65}, {"text": "there is different chances of moving North, South, East or West.", "start": 3956.01, "duration": 3.705}, {"text": "Now, that there's an S1,", "start": 3959.715, "duration": 2.43}, {"text": "it then has to choose a new action a1,", "start": 3962.145, "duration": 5.52}, {"text": "and as a consequence of the action a1,", "start": 3967.665, "duration": 3.525}, {"text": "it will get to some new state S2,", "start": 3971.19, "duration": 2.79}, {"text": "which is governed by,", "start": 3973.98, "duration": 2.01}, {"text": "um, the state transition probabilities, you know, s1,", "start": 3975.99, "duration": 3.705}, {"text": "a1, and so on, okay?", "start": 3979.695, "duration": 4.395}, {"text": "And- and the robot just keeps on running.", "start": 3984.09, "duration": 2.8}, {"text": "And so the robots will go through a sequence of states S_0,", "start": 3993.78, "duration": 6.265}, {"text": "S_1, S_2 and so on,", "start": 4000.045, "duration": 3.27}, {"text": "depending on the choices it receives,", "start": 4003.315, "duration": 2.175}, {"text": "depending on actions it chooses.", "start": 4005.49, "duration": 1.965}, {"text": "And the total payoff", "start": 4007.455, "duration": 3.445}, {"text": "is written as follows with one more detail,", "start": 4013.55, "duration": 5.87}, {"text": "is that term Gamma.", "start": 4024.68, "duration": 4.615}, {"text": "So think of Gamma as a number like 0.99.", "start": 4029.295, "duration": 4.335}, {"text": "So Gamma is usually chosen to be just slightly less than one,", "start": 4033.63, "duration": 3.45}, {"text": "and what the- so the total payoff", "start": 4037.08, "duration": 3.825}, {"text": "is the sum of rewards or more technically is a sum of discounted rewards,", "start": 4040.905, "duration": 4.05}, {"text": "and what this does is it adds up all the rewards that the robot receives over time,", "start": 4044.955, "duration": 5.34}, {"text": "but the further reward is into the future, um, you know,", "start": 4050.295, "duration": 4.755}, {"text": "the- the- the smaller the Gamma is the power of time that that reward is multiplied by.", "start": 4055.05, "duration": 5.355}, {"text": "Okay. So any reward you get at time 1,", "start": 4060.405, "duration": 2.685}, {"text": "you get all of that.", "start": 4063.09, "duration": 1.365}, {"text": "Every reward you get at time 2 is multiplied by 0.99.", "start": 4064.455, "duration": 3.045}, {"text": "And the reward you get at the next step is multiplied by 0.99 squared, 0.99 cubed and so on.", "start": 4067.5, "duration": 4.08}, {"text": "And so what the, um, discount factor, ah, does,", "start": 4071.58, "duration": 4.395}, {"text": "is it has the effect of giving a smaller weight to rewards in the distant future, um,", "start": 4075.975, "duration": 5.715}, {"text": "and this means that this encourages the robot to also get deposited rewards faster,", "start": 4081.69, "duration": 5.88}, {"text": "um, or postpone the negative rewards, right?", "start": 4087.57, "duration": 2.4}, {"text": "And so in, uh, financial applications, um,", "start": 4089.97, "duration": 3.45}, {"text": "the discount factor has a natural interpretation,", "start": 4093.42, "duration": 3.84}, {"text": "as the time value of money,", "start": 4097.26, "duration": 1.695}, {"text": "because if you have a dollar today,", "start": 4098.955, "duration": 1.71}, {"text": "you know, you're better off having", "start": 4100.665, "duration": 2.055}, {"text": "a dollar today that have being a year- dollar a year from now.", "start": 4102.72, "duration": 2.565}, {"text": "Right? Because when you put the dollar in the bank and earn interests, uh, uh,", "start": 4105.285, "duration": 3.615}, {"text": "for a year on your dollar and so a dollar", "start": 4108.9, "duration": 1.92}, {"text": "today is strictly better than the dollar the future.", "start": 4110.82, "duration": 2.07}, {"text": "Um, and conversely, having to pay $100,", "start": 4112.89, "duration": 3.75}, {"text": "or having to pay one dollar a year from now is also", "start": 4116.64, "duration": 2.49}, {"text": "better than having to pay a dollar today, right?", "start": 4119.13, "duration": 2.58}, {"text": "Because if you could, you know,", "start": 4121.71, "duration": 1.26}, {"text": "save your money and earn interest and then issue", "start": 4122.97, "duration": 2.34}, {"text": "a payment to someone else a year from now rather than now,", "start": 4125.31, "duration": 2.82}, {"text": "then you're actually slightly wealthier, um, and so, uh, uh,", "start": 4128.13, "duration": 3.87}, {"text": "and so Gamma in financial applications has the interpretation as the time value of money,", "start": 4132.0, "duration": 5.37}, {"text": "um, uh, or as the interest rate, I guess.", "start": 4137.37, "duration": 3.48}, {"text": "Um, uh, and but, but, but,", "start": 4140.85, "duration": 2.61}, {"text": "more generally even for non-financial applications, ah, mostly wrote,", "start": 4143.46, "duration": 3.66}, {"text": "most- most the- there are some financial application reinforcement learning programs,", "start": 4147.12, "duration": 2.91}, {"text": "there are lots of non-financial applications as well.", "start": 4150.03, "duration": 1.8}, {"text": "Um, this mechanism of using a discount factor has the effect of", "start": 4151.83, "duration": 4.05}, {"text": "encouraging the system to get to the positive rewards as quickly as possible,", "start": 4155.88, "duration": 4.965}, {"text": "uh, but then also conversely to try to push", "start": 4160.845, "duration": 2.085}, {"text": "the negative rewards as far into the future is possible, right?", "start": 4162.93, "duration": 4.15}, {"text": "And I think, uh, to be pragmatic,", "start": 4168.71, "duration": 2.769}, {"text": "there are two reasons why people use Gamma.", "start": 4171.479, "duration": 2.416}, {"text": "The story I just told, time value of money,", "start": 4173.895, "duration": 2.895}, {"text": "your frontal deposit of rewards, postponed rewards.", "start": 4176.79, "duration": 2.625}, {"text": "That's, uh, that's the story you tend to people- you tend to,", "start": 4179.415, "duration": 4.155}, {"text": "uh, uh, hear people say in terms of why we have a discount factor.", "start": 4183.57, "duration": 3.255}, {"text": "Uh, the other reason we have the discount factor is", "start": 4186.825, "duration": 2.565}, {"text": "actually  much more pragmatic one which is that", "start": 4189.39, "duration": 1.86}, {"text": "all the reinforcement learning algorithms you see,", "start": 4191.25, "duration": 1.995}, {"text": "they converge much faster or they weren't", "start": 4193.245, "duration": 1.905}, {"text": "much better if you're willing to have a discount factor.", "start": 4195.15, "duration": 2.355}, {"text": "Whereas it turns out that if Gamma is,", "start": 4197.505, "duration": 2.22}, {"text": "is, equal to 1, if,", "start": 4199.725, "duration": 1.47}, {"text": "if Gamma is not strictly less than 1,", "start": 4201.195, "duration": 2.07}, {"text": "um, uh, it's much harder or,", "start": 4203.265, "duration": 1.935}, {"text": "or there, there are many reinforcement learning algorithms that, uh, may not converge.", "start": 4205.2, "duration": 4.08}, {"text": "It's much harder to prove convergence or they may not converge.", "start": 4209.28, "duration": 2.46}, {"text": "So just as a pragmatic thing.", "start": 4211.74, "duration": 1.35}, {"text": "Um, this makes the job much easier for your algebra.", "start": 4213.09, "duration": 2.865}, {"text": "So I see some of you shaking your heads in dis- in disapproval. [LAUGHTER].", "start": 4215.955, "duration": 3.405}, {"text": "[inaudible].", "start": 4219.36, "duration": 0.1}, {"text": "Yeah.", "start": 4219.46, "duration": 1.14}, {"text": "Yeah.", "start": 4221.93, "duration": 11.2}, {"text": "Yes yeah that's a good point.", "start": 4233.13, "duration": 1.08}, {"text": "Yes. So one of the things if there's no Gamma is that, uh, the rewards,", "start": 4234.21, "duration": 3.72}, {"text": "some of the rewards, you know,", "start": 4237.93, "duration": 1.185}, {"text": "could be- can increase or decrease without bounds.", "start": 4239.115, "duration": 2.625}, {"text": "So by having Gamma it does guarantees that", "start": 4241.74, "duration": 2.25}, {"text": "the total payoff is a finite value or is a bounded value.", "start": 4243.99, "duration": 3.6}, {"text": "So that, that's, that's one of the parts that go into some of", "start": 4247.59, "duration": 3.3}, {"text": "the proofs or some of the reasoning behind", "start": 4250.89, "duration": 1.68}, {"text": "why reinforcement learning algorithms converge.", "start": 4252.57, "duration": 2.325}, {"text": "So, cool, that's good insight.", "start": 4254.895, "duration": 3.46}, {"text": "Um, okay. So the goal of reinforcement learning is to", "start": 4258.355, "duration": 6.965}, {"text": "choose actions over time,", "start": 4265.32, "duration": 7.51}, {"text": "to maximize the expected total payoff.", "start": 4273.56, "duration": 4.7}, {"text": "Okay. And in particular, um,", "start": 4288.56, "duration": 10.78}, {"text": "what most reinforcement learning algorithms will come up with,", "start": 4299.34, "duration": 3.87}, {"text": "is a policy, um, that maps from states to actions.", "start": 4303.21, "duration": 13.02}, {"text": "Right? So the output of most reinforcement learning algorithms will be a policy, um, or,", "start": 4316.23, "duration": 4.905}, {"text": "or controller, in the R world we", "start": 4321.135, "duration": 3.105}, {"text": "tend to use the term policy but policy just means controller,", "start": 4324.24, "duration": 3.359}, {"text": "the maps of states to actions.", "start": 4327.599, "duration": 1.741}, {"text": "So it turns out that, um,", "start": 4329.34, "duration": 1.785}, {"text": "for the MDP that we have, um, right?", "start": 4331.125, "duration": 13.32}, {"text": "It turns out that this is the optimal policy.", "start": 4344.445, "duration": 5.155}, {"text": "So for example, ah,", "start": 4355.49, "duration": 2.14}, {"text": "if you take this example,", "start": 4357.63, "duration": 1.725}, {"text": "this, this cell here,", "start": 4359.355, "duration": 2.485}, {"text": "this cell over here,", "start": 4363.92, "duration": 2.2}, {"text": "this policy is saying pi applied to the state 3, 1,", "start": 4366.12, "duration": 5.115}, {"text": "is equal to West.", "start": 4371.235, "duration": 3.3}, {"text": "Hey, and that- so [NOISE] excuse me.", "start": 4374.535, "duration": 5.235}, {"text": "So it separately worked out,", "start": 4379.77, "duration": 1.68}, {"text": "what is the optimal policy,", "start": 4381.45, "duration": 1.26}, {"text": "and this turns out to be optimal policy in the sense that,", "start": 4382.71, "duration": 3.03}, {"text": "if you, um, we say execute this policy,", "start": 4385.74, "duration": 3.585}, {"text": "so to execute the policy means that whenever you're in the state S,", "start": 4389.325, "duration": 5.155}, {"text": "take the action given by Pi of S,", "start": 4398.78, "duration": 7.01}, {"text": "so that's what it means to execute a certain policy.", "start": 4411.05, "duration": 4.075}, {"text": "And it turns out that, um,", "start": 4415.125, "duration": 2.04}, {"text": "this policy will- I, I,", "start": 4417.165, "duration": 1.77}, {"text": "I worked all separately, right, offline.", "start": 4418.935, "duration": 1.98}, {"text": "Yeah. And, in, um,", "start": 4420.915, "duration": 1.155}, {"text": "uh, on my laptop, uh, uh,", "start": 4422.07, "duration": 1.995}, {"text": "that this is the optimal policy for this MDP,", "start": 4424.065, "duration": 2.73}, {"text": "and it turns out that if you execute this policy,", "start": 4426.795, "duration": 2.97}, {"text": "meaning whenever you're in a certain state, you know,", "start": 4429.765, "duration": 2.025}, {"text": "you'll take the action indicated by the arrow,", "start": 4431.79, "duration": 2.295}, {"text": "that this is the policy that will maximize the expected total payoff.", "start": 4434.085, "duration": 5.28}, {"text": "Okay. Um, and the problem in reinforcement learning is,", "start": 4439.365, "duration": 5.79}, {"text": "given a definition for an MDP,", "start": 4445.155, "duration": 2.88}, {"text": "or given a problem to pose,", "start": 4448.035, "duration": 2.82}, {"text": "the problem as an MDP,", "start": 4450.855, "duration": 2.175}, {"text": "figure out what's the set of states,", "start": 4453.03, "duration": 1.71}, {"text": "what's the set of actions,", "start": 4454.74, "duration": 1.26}, {"text": "um, what are the state transition probabilities,", "start": 4456.0, "duration": 2.115}, {"text": "specified discount factor and specified reward function.", "start": 4458.115, "duration": 3.69}, {"text": "And then to have a reinforcement learning algorithm,", "start": 4461.805, "duration": 2.565}, {"text": "find the policy Pi that maximizes the expected payoff.", "start": 4464.37, "duration": 4.605}, {"text": "And then when you want your robot to act or when you want your chess playing program to act,", "start": 4468.975, "duration": 4.695}, {"text": "um, whenever you're in some state S,", "start": 4473.67, "duration": 2.175}, {"text": "take the action given by Pi of S,", "start": 4475.845, "duration": 2.31}, {"text": "and hopefully this will result in a robot that,", "start": 4478.155, "duration": 2.715}, {"text": "you know, efficiently navigates to the plus 1 state.", "start": 4480.87, "duration": 3.45}, {"text": "Okay? So it turns out that MDPs are quite good at making fine distinctions.", "start": 4484.32, "duration": 6.54}, {"text": "So one example, um,", "start": 4490.86, "duration": 1.92}, {"text": "it's actually not totally obvious whether here", "start": 4492.78, "duration": 2.025}, {"text": "you're better off going North or going West.", "start": 4494.805, "duration": 2.25}, {"text": "Right? And it turns out that there is a trade off.", "start": 4497.055, "duration": 3.15}, {"text": "If you go west here, then, you know,", "start": 4500.205, "duration": 2.31}, {"text": "you're gonna take a longer route to get to the plus one.", "start": 4502.515, "duration": 2.46}, {"text": "So you take longer,", "start": 4504.975, "duration": 1.35}, {"text": "uh, the plus minus is discounted more heavily,", "start": 4506.325, "duration": 2.325}, {"text": "you're taking these penalties along the way, [NOISE] excuse me.", "start": 4508.65, "duration": 4.875}, {"text": "But on the flip side,", "start": 4513.525, "duration": 1.995}, {"text": "if you were to try to go North,", "start": 4515.52, "duration": 2.07}, {"text": "you could try to get there faster.", "start": 4517.59, "duration": 2.295}, {"text": "But on this step,", "start": 4519.885, "duration": 1.725}, {"text": "there's a 0.1% chance that you accidentally slip off to the minus 1 state.", "start": 4521.61, "duration": 4.875}, {"text": "So, so what is the optimal action?", "start": 4526.485, "duration": 2.205}, {"text": "Right? It's actually quite hard to just look at it with your eyes and make a decision.", "start": 4528.69, "duration": 3.975}, {"text": "But it turns out that if you solve for the optimal set of actions in", "start": 4532.665, "duration": 3.435}, {"text": "this MDP in this example is I just take a longer and safer route. Question?", "start": 4536.1, "duration": 4.41}, {"text": "[inaudible]", "start": 4540.51, "duration": 6.57}, {"text": "Does this advice take us in policies?", "start": 4547.08, "duration": 1.62}, {"text": "So, uh, uh, if the optimal set of actions is to cycle around,", "start": 4548.7, "duration": 5.16}, {"text": "then it should find out, uh, uh,", "start": 4553.86, "duration": 2.58}, {"text": "I mean for example, if there are only penalties", "start": 4556.44, "duration": 1.77}, {"text": "everywhere and you just go and run in a circle,", "start": 4558.21, "duration": 2.475}, {"text": "you know, then, then, the- the algorithm will actually chose to do that.", "start": 4560.685, "duration": 3.45}, {"text": "Uh, but in this case,", "start": 4564.135, "duration": 1.275}, {"text": "you want to get to the plus one as quickly as possible.", "start": 4565.41, "duration": 2.34}, {"text": "Right? And so what we'll see, um,", "start": 4567.75, "duration": 2.94}, {"text": "there's one more question. Go ahead.", "start": 4570.69, "duration": 2.79}, {"text": "[inaudible]", "start": 4573.48, "duration": 17.25}, {"text": "So, so, so chess and checkers and go and so on- they're, one,", "start": 4590.73, "duration": 3.62}, {"text": "one complication that is, you take a move.", "start": 4594.35, "duration": 2.115}, {"text": "So actually, all right.", "start": 4596.465, "duration": 1.125}, {"text": "To refine the description to chess.", "start": 4597.59, "duration": 1.695}, {"text": "Um, what happens in playing chess is a state- status,", "start": 4599.285, "duration": 3.95}, {"text": "um, your board, right? So there's your move.", "start": 4603.235, "duration": 2.405}, {"text": "So you see a board that's a state,", "start": 4605.64, "duration": 1.665}, {"text": "and so you make a move and then the opponent makes a move and then that's the new state.", "start": 4607.305, "duration": 3.825}, {"text": "So the state is when you and your opponent", "start": 4611.13, "duration": 2.49}, {"text": "both make- take turns then this comes back to you.", "start": 4613.62, "duration": 2.565}, {"text": "Right? Um, and because you don't know exactly what your opponent will do,", "start": 4616.185, "duration": 3.915}, {"text": "there is a probability distribution over if", "start": 4620.1, "duration": 1.71}, {"text": "I make a move or what's the other person gonna do?", "start": 4621.81, "duration": 2.76}, {"text": "Uh, I guess one last question.", "start": 4624.57, "duration": 2.475}, {"text": "Yeah. Go ahead.", "start": 4627.045, "duration": 7.545}, {"text": "[inaudible] [NOISE]", "start": 4634.59, "duration": 0.29}, {"text": "Yeah. Right. The probabilities are assigned per node,", "start": 4634.88, "duration": 1.68}, {"text": "the 0.8, 0.1, 0.1 where does that come from?", "start": 4636.56, "duration": 2.34}, {"text": "Um, so we'll talk about that later,", "start": 4638.9, "duration": 2.16}, {"text": "uh, in some applications does this learn?", "start": 4641.06, "duration": 2.13}, {"text": "So if you build a robot,", "start": 4643.19, "duration": 1.785}, {"text": "you might not know is it 0.8, 0.1, 0.1 or, you know, 0.7, 0.115, 0.115.", "start": 4644.975, "duration": 5.085}, {"text": "So it's quite common to use data to learn those state transition probabilities as well.", "start": 4650.06, "duration": 4.785}, {"text": "We'll we'll, we'll, see a specific example of that nature.", "start": 4654.845, "duration": 2.24}, {"text": "Okay. So all right.", "start": 4657.085, "duration": 1.595}, {"text": "So where we are just to summarize,", "start": 4658.68, "duration": 2.175}, {"text": "this is how you formulate the problem as an MDP,", "start": 4660.855, "duration": 3.795}, {"text": "um, and then the, the, the,", "start": 4664.65, "duration": 2.34}, {"text": "the job reinforcing learning algorithm is ready to go from the MDP.", "start": 4666.99, "duration": 4.5}, {"text": "to telling you what is a good policy.", "start": 4671.49, "duration": 2.7}, {"text": "Okay. So let's break,", "start": 4674.19, "duration": 2.025}, {"text": "um, and have a good Thanksgiving everyone,", "start": 4676.215, "duration": 2.265}, {"text": "[NOISE] I won't see you for like a week and a half, uh, uh,", "start": 4678.48, "duration": 2.46}, {"text": "enjoy yourselves and we'll, we'll reconvene after Thanksgiving with, uh, with this.", "start": 4680.94, "duration": 5.26}]