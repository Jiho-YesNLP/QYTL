[{"text": "Hi everyone. [NOISE] Welcome,", "start": 3.5, "duration": 4.075}, {"text": "welcome to the second lecture on deep learning for CS229.", "start": 7.575, "duration": 4.675}, {"text": "So a quick announcement before we start.", "start": 12.25, "duration": 2.575}, {"text": "There is a Piazza post Number 695 which is the mid-quarter survey for CS229,", "start": 14.825, "duration": 6.775}, {"text": "so fill it in when you have time.", "start": 21.6, "duration": 2.37}, {"text": "Okay. So let's get back to deep learning.", "start": 23.97, "duration": 4.245}, {"text": "So last week together we've seen, uh,", "start": 28.215, "duration": 4.185}, {"text": "what a neural network is and we started by", "start": 32.4, "duration": 2.89}, {"text": "defining the logistic regression from a neural network perspective.", "start": 35.29, "duration": 3.36}, {"text": "We said that logistic regression can be viewed as", "start": 38.65, "duration": 3.645}, {"text": "a one-neuron neural network where there is", "start": 42.295, "duration": 2.755}, {"text": "a linear part and an activation part which was sigmoid in that case.", "start": 45.05, "duration": 4.67}, {"text": "We se- we've seen that sigmoid is a common activation function to be used for", "start": 49.72, "duration": 4.48}, {"text": "classification tasks because it casts", "start": 54.2, "duration": 3.15}, {"text": "a number between minus infinity and plus infinity in 0,", "start": 57.35, "duration": 3.34}, {"text": "into 0, 1 interval which can be interpreted as a probability.", "start": 60.69, "duration": 4.1}, {"text": "And then we introduced the neural network,", "start": 64.79, "duration": 1.95}, {"text": "so we started to stack some neurons inside a layer and then stack layers", "start": 66.74, "duration": 4.7}, {"text": "on top of each other and we said that the", "start": 71.44, "duration": 2.26}, {"text": "more we stack layers the more parameters we have,", "start": 73.7, "duration": 2.48}, {"text": "and the more parameters we have, the more our network is", "start": 76.18, "duration": 3.49}, {"text": "able to copy the complexity of our data because it becomes more flexible.", "start": 79.67, "duration": 5.075}, {"text": "So, uh, we stopped at a point where we did a forward propagation,", "start": 84.745, "duration": 5.605}, {"text": "we had an example during training,", "start": 90.35, "duration": 1.83}, {"text": "we forward propagated through the network, we get the output,", "start": 92.18, "duration": 3.035}, {"text": "then we compute the cost function which compares this output to the ground truth,", "start": 95.215, "duration": 4.615}, {"text": "and we were in the process of backpropagating the error to tell", "start": 99.83, "duration": 3.45}, {"text": "our parameters how they should move in order to detect cats more properly.", "start": 103.28, "duration": 5.07}, {"text": "Does that make sense for this part?", "start": 108.35, "duration": 2.62}, {"text": "So today, we're going to continue that.", "start": 110.97, "duration": 2.72}, {"text": "So we're in the second part, neural networks,", "start": 113.69, "duration": 2.205}, {"text": "we're going to derive the backpropagation with the chain rule and after that,", "start": 115.895, "duration": 3.795}, {"text": "ah, we're going to talk about how to improve our neural networks.", "start": 119.69, "duration": 3.705}, {"text": "Because in practice, it's not because you", "start": 123.395, "duration": 1.995}, {"text": "designed a neural network that it's going to work,", "start": 125.39, "duration": 1.94}, {"text": "there's a lot of hacks and tricks that you", "start": 127.33, "duration": 2.74}, {"text": "need to know in order to make a neural network work.", "start": 130.07, "duration": 2.955}, {"text": "Okay, let's go.", "start": 133.025, "duration": 2.235}, {"text": "So first thing that we talked about is", "start": 135.26, "duration": 5.44}, {"text": "in order to define our optimization problem and find the right parameters,", "start": 140.7, "duration": 5.03}, {"text": "we need to define a cost function,", "start": 145.73, "duration": 2.24}, {"text": "and usually we said we would use the letter j to denote the cost function.", "start": 147.97, "duration": 5.53}, {"text": "So here, when I talk about cost function,", "start": 153.5, "duration": 2.16}, {"text": "I'm talking about the batch of examples.", "start": 155.66, "duration": 2.265}, {"text": "It means I'm forward propagating m examples at a time.", "start": 157.925, "duration": 3.945}, {"text": "You remember why we do that?", "start": 161.87, "duration": 2.05}, {"text": "What's the reason we use a batch instead of a single example?", "start": 163.92, "duration": 5.555}, {"text": "Vectorization. We want to use what our", "start": 169.475, "duration": 3.395}, {"text": "GPU can do and parallelize the computation. So that's what we do.", "start": 172.87, "duration": 4.2}, {"text": "So we have m examples that go- forward propagate in the network.", "start": 177.07, "duration": 6.69}, {"text": "And each of them has a loss function associated with them,", "start": 184.22, "duration": 3.8}, {"text": "the average of the loss functions over the batch give us the cost function.", "start": 188.02, "duration": 3.3}, {"text": "And we had defined these loss function together.", "start": 191.32, "duration": 4.005}, {"text": "L of i. Assuming we're still,", "start": 195.325, "duration": 3.27}, {"text": "and just as a reminder,", "start": 198.595, "duration": 1.62}, {"text": "we're still in this network where,", "start": 200.215, "duration": 2.085}, {"text": "where we had a cat, remember?", "start": 202.3, "duration": 3.885}, {"text": "This one. Remember this guy.", "start": 206.185, "duration": 2.805}, {"text": "x_1 to x_n.", "start": 209.42, "duration": 2.995}, {"text": "The cat was flattened into a vector,", "start": 212.415, "duration": 2.19}, {"text": "RGB matrix into one vector and then there was a neural network with three neurons,", "start": 214.605, "duration": 5.275}, {"text": "then two neurons, then one neuron.", "start": 219.88, "duration": 1.95}, {"text": "Remember? Fully-connected here.", "start": 221.83, "duration": 2.55}, {"text": "Everything. Up, up,", "start": 224.38, "duration": 7.55}, {"text": "and then we add y hat.", "start": 231.93, "duration": 1.605}, {"text": "You remember this one? I think that was this one here. Yeah, okay.", "start": 233.535, "duration": 4.515}, {"text": "So now, we're here, we take m images of cats or non-cats,", "start": 238.05, "duration": 3.99}, {"text": "forward propagate everything in the network,", "start": 242.04, "duration": 1.63}, {"text": "compute our loss function for each of them,", "start": 243.67, "duration": 2.11}, {"text": "average it, and get the cost function.", "start": 245.78, "duration": 2.36}, {"text": "So our last function was the binary cross-entropy or also called", "start": 248.14, "duration": 4.14}, {"text": "the loss function- the logistic loss function and it was the following.", "start": 252.28, "duration": 4.265}, {"text": "y_i log of y hat i plus 1", "start": 256.545, "duration": 4.775}, {"text": "minus y_i log of 1 minus y hat i.", "start": 261.32, "duration": 7.54}, {"text": "So let me circle this one,", "start": 270.46, "duration": 5.69}, {"text": "it's an important one.", "start": 276.15, "duration": 1.825}, {"text": "And what we said is that this network has many parameters.", "start": 277.975, "duration": 3.955}, {"text": "And we said, the first layer has w_1,", "start": 281.93, "duration": 3.09}, {"text": "b_1, the second layer has w_2,", "start": 285.02, "duration": 3.73}, {"text": "b_2, and the third layer has w_3,", "start": 288.75, "duration": 4.695}, {"text": "b_3 where the square brackets dis- denotes the layer.", "start": 293.445, "duration": 5.91}, {"text": "And we have to train all these parameters.", "start": 299.355, "duration": 2.66}, {"text": "One thing we notice is that because we want to make a good use of the chain rule,", "start": 302.015, "duration": 3.825}, {"text": "we're going to start by,", "start": 305.84, "duration": 1.485}, {"text": "by computing the derivative of these guys,", "start": 307.325, "duration": 2.805}, {"text": "w_3 and b_3 and then come back and do w_2 and b_2 and then back again w_1 and b_1.", "start": 310.13, "duration": 6.875}, {"text": "In order to use our formulas of", "start": 317.005, "duration": 4.405}, {"text": "the update of the gradient descent where w would be equal to w minus Alpha", "start": 321.41, "duration": 5.25}, {"text": "derivative of the cost with respect to w and this for", "start": 326.66, "duration": 3.81}, {"text": "any layer l between 1 and 3, same for b.", "start": 330.47, "duration": 6.82}, {"text": "Okay, so let's try to do it.", "start": 341.6, "duration": 3.62}, {"text": "This is the first number we want to compute.", "start": 345.96, "duration": 3.875}, {"text": "And remember, the reason we want to compute derivative of", "start": 349.835, "duration": 3.125}, {"text": "the cost with respect to w_3 is because the relationship", "start": 352.96, "duration": 3.33}, {"text": "between w_3 and the cost is easier than the relationship between w_1 and the cost", "start": 356.29, "duration": 5.65}, {"text": "because w_1 had much more connection going through", "start": 361.94, "duration": 2.63}, {"text": "the network before ending up in the cost computation.", "start": 364.57, "duration": 4.6}, {"text": "So one thing we should notice", "start": 371.46, "duration": 3.91}, {"text": "before starting this calculation is that the derivative is linear.", "start": 375.37, "duration": 3.585}, {"text": "So this, if I take the derivative of j,", "start": 378.955, "duration": 3.515}, {"text": "I can just take the derivative of l, and it's the same thing,", "start": 382.47, "duration": 3.575}, {"text": "I just need to add the summation prior to that because derivative is a linear operation.", "start": 386.045, "duration": 5.235}, {"text": "That makes sense to everyone? So instead of computing this,", "start": 391.28, "duration": 5.04}, {"text": "I'm going to compute that and then I will add the summation,", "start": 396.32, "duration": 5.62}, {"text": "it will just make our notation easier.", "start": 401.94, "duration": 2.555}, {"text": "So I'm taking the derivative of a loss of", "start": 404.495, "duration": 2.835}, {"text": "one example propagated to the network with respect to w_3.", "start": 407.33, "duration": 3.435}, {"text": "So let's do the calculation together.", "start": 410.765, "duration": 1.905}, {"text": "I have a 1, I have", "start": 412.67, "duration": 2.92}, {"text": "a minus y_i derivative with respect to w_3, of what?", "start": 415.59, "duration": 7.34}, {"text": "We remember that y hat was equal to sigmoid of w_3 x", "start": 422.93, "duration": 5.01}, {"text": "plus b or w_3 a_2 plus b because a_2 is the input to the second layer, remember.", "start": 427.94, "duration": 5.47}, {"text": "So I would write it down here,", "start": 433.41, "duration": 1.74}, {"text": "sigmoid of w_3 a_2 plus b_3.", "start": 435.15, "duration": 8.82}, {"text": "Okay?", "start": 443.97, "duration": 0.48}, {"text": "Yeah.", "start": 444.45, "duration": 14.38}, {"text": "It's good like that? It's too small?", "start": 460.41, "duration": 4.67}, {"text": "w_3 a_2 plus b_3.", "start": 469.59, "duration": 4.735}, {"text": "It's good like that, yeah?", "start": 474.325, "duration": 3.255}, {"text": "Okay. So we have this term and then we have the second term which is plus", "start": 477.58, "duration": 4.98}, {"text": "1 minus y_i times derivative of w_3.", "start": 482.56, "duration": 5.94}, {"text": "Derivative with respect to w_3 of 1.", "start": 488.5, "duration": 3.405}, {"text": "Oh sorry, I forgot the logarithm here.", "start": 491.905, "duration": 3.445}, {"text": "Of log of 1 minus sigmoid of", "start": 497.31, "duration": 6.49}, {"text": "w_3 a_2 plus b_3.", "start": 503.8, "duration": 6.945}, {"text": "And so just a reminder,", "start": 510.745, "duration": 1.395}, {"text": "the reason we have this is because we've", "start": 512.14, "duration": 2.13}, {"text": "written the forward propagation in the previous class.", "start": 514.27, "duration": 2.205}, {"text": "You guys remember the pro- forward propagation?", "start": 516.475, "duration": 2.25}, {"text": "We had z_3, which took a_2 as inputs and computed the linear part,", "start": 518.725, "duration": 5.415}, {"text": "as sigmoid is- is the activation function used in the last neuron over here.", "start": 524.14, "duration": 5.07}, {"text": "Okay. So let's try to- to compute this derivative.", "start": 529.21, "duration": 4.45}, {"text": "y_i, so the derivative of log,", "start": 535.41, "duration": 2.86}, {"text": "[NOISE] log prime equals 1 over log.", "start": 538.27, "duration": 4.77}, {"text": "Remember this- this- this formula,", "start": 543.04, "duration": 3.525}, {"text": "so I will just take 1 over,", "start": 546.565, "duration": 3.72}, {"text": "sorry, 1 over x minus- 1  over x if you put an x here.", "start": 550.285, "duration": 4.05}, {"text": "So log prime of x.", "start": 554.335, "duration": 3.135}, {"text": "So I will take one over sigmoid of w_3 a_2 plus b_3.", "start": 557.47, "duration": 4.95}, {"text": "I know that thing can be written a_3, right?", "start": 562.42, "duration": 2.985}, {"text": "So I will just write a_3 instead of writing the single a again.", "start": 565.405, "duration": 3.66}, {"text": "So we have 1 over a_3 times the derivative of a_3 with respect to w_3.", "start": 569.065, "duration": 7.635}, {"text": "We remember that, I'm going to write it down here.", "start": 576.7, "duration": 2.625}, {"text": "If we take the derivative of sigmoid of blah, blah, blah.", "start": 579.325, "duration": 3.855}, {"text": "Let's say, derivative of log of sigmoid over w. What we have is", "start": 583.18, "duration": 7.74}, {"text": "1 over the sigmoid times the derivative with respect to w_3 of the sigmoid.", "start": 590.92, "duration": 8.175}, {"text": "Does that makes sense? That's what we're using here.", "start": 599.095, "duration": 3.78}, {"text": "So the derivative of sigmoid,", "start": 602.875, "duration": 2.34}, {"text": "sigmoid-prime of x is actually pretty easy to compute.", "start": 605.215, "duration": 4.53}, {"text": "It's sigmoid of x times 1 minus sigmoid of x.", "start": 609.745, "duration": 4.345}, {"text": "Okay. So I'm just going to take the derivative.", "start": 615.8, "duration": 3.37}, {"text": "It's going to give me a- a_3 times 1 minus a_3.", "start": 619.17, "duration": 6.88}, {"text": "There's still one step because there is a composition of three functions here.", "start": 627.69, "duration": 4.54}, {"text": "There is a logarithm, there's a sigmoid,", "start": 632.23, "duration": 2.175}, {"text": "and there is also a linear function,", "start": 634.405, "duration": 1.68}, {"text": "w_x plus b or w a_2 plus b.", "start": 636.085, "duration": 3.36}, {"text": "So I also need to take the derivative of the linear part with respect to w_3.", "start": 639.445, "duration": 4.11}, {"text": "Because I know that sigmoid of w_3, a_2 plus b_3.", "start": 643.555, "duration": 7.38}, {"text": "If I wanna take the derivative of that with respect to w_3,", "start": 650.935, "duration": 4.95}, {"text": "I need to go inside and take the derivative of what's inside, okay?", "start": 655.885, "duration": 6.09}, {"text": "So this will give me the sigmoid or whatever a_3 times 1 minus", "start": 661.975, "duration": 6.915}, {"text": "a_3 times the derivative with respect to w_3 of the linear part.", "start": 668.89, "duration": 6.93}, {"text": "[NOISE] Does this make sense?", "start": 675.82, "duration": 8.17}, {"text": "So I am going to write it here bigger.", "start": 686.1, "duration": 3.16}, {"text": "Here, I need to take the derivative of the linear part with respect to w_3,", "start": 689.26, "duration": 5.58}, {"text": "which is equal to a_2 transpose.", "start": 694.84, "duration": 3.88}, {"text": "So one thing you- you may wanna check,", "start": 699.63, "duration": 4.165}, {"text": "is when we compute- when I'm trying to compute this derivative.", "start": 703.795, "duration": 8.085}, {"text": "[NOISE]", "start": 711.88, "duration": 11.79}, {"text": "I'm trying to compute this derivative.", "start": 723.67, "duration": 1.41}, {"text": "Why is there a transpose that comes out?", "start": 725.08, "duration": 2.325}, {"text": "How do you come up with that?", "start": 727.405, "duration": 1.95}, {"text": "You look at the shape here.", "start": 729.355, "duration": 2.115}, {"text": "What's the shape of w_3? Someone remembers?", "start": 731.47, "duration": 10.24}, {"text": "1 by 2.", "start": 743.552, "duration": 1.568}, {"text": "1 by 2. Yeah, why 1 by 2?", "start": 745.12, "duration": 2.49}, {"text": "[BACKGROUND]", "start": 747.61, "duration": 7.26}, {"text": "Yeah, it's connecting two neurons to one neuron.", "start": 754.87, "duration": 2.88}, {"text": "So it has to be 1 by 2. Usually flip it.", "start": 757.75, "duration": 2.91}, {"text": "And in order to come back to that,", "start": 760.66, "duration": 2.76}, {"text": "you can write your forward propagation,", "start": 763.42, "duration": 1.56}, {"text": "make the shape analysis,", "start": 764.98, "duration": 1.334}, {"text": "and find out that it's a 1 by 2 matrix.", "start": 766.314, "duration": 1.981}, {"text": "How about this thing?", "start": 768.295, "duration": 2.155}, {"text": "What's the shape of that?", "start": 771.9, "duration": 2.02}, {"text": "[NOISE].", "start": 773.92, "duration": 3.09}, {"text": "The scalar.", "start": 777.01, "duration": 1.24}, {"text": "It's a scalar, yeah. So scalar.", "start": 778.43, "duration": 3.22}, {"text": "So it's 1 by 1. How do you know?", "start": 781.65, "duration": 1.695}, {"text": "It's because this thing is basically z_3.", "start": 783.345, "duration": 2.66}, {"text": "It's the linear part of the last neuron and a_3,", "start": 786.005, "duration": 3.035}, {"text": "we know that it's y-hat.", "start": 789.04, "duration": 1.125}, {"text": "So it's a scalar between 0 and 1.", "start": 790.165, "duration": 2.085}, {"text": "So this has to be a scalar as well.", "start": 792.25, "duration": 1.845}, {"text": "Because taking the sigmoid should not change the shape.", "start": 794.095, "duration": 3.27}, {"text": "So now, the question is what's the shape of this entire thing?", "start": 797.365, "duration": 5.305}, {"text": "The shape of this entire thing should be the shape", "start": 804.36, "duration": 3.76}, {"text": "of w_3 because you're taking the derivative of", "start": 808.12, "duration": 3.3}, {"text": "a scalar with respect to a higher-dimensional matrix or vector here called a row vector.", "start": 811.42, "duration": 7.02}, {"text": "Then it means, that the shape of this has to be the same shape of w_3. So 1 by 2.", "start": 818.44, "duration": 5.91}, {"text": "And you know that when you take this simple derivative in- in real life,", "start": 824.35, "duration": 4.89}, {"text": "like in- in, uh, with scalars,", "start": 829.24, "duration": 2.01}, {"text": "not with high-dimensional, you know that this is an easy derivative.", "start": 831.25, "duration": 3.12}, {"text": "It just should- it should give you a_2, right?", "start": 834.37, "duration": 3.195}, {"text": "But in higher dimension,", "start": 837.565, "duration": 1.635}, {"text": "sometimes you have transpose that come up.", "start": 839.2, "duration": 1.95}, {"text": "And how do you know that the answer is a_2 transpose?", "start": 841.15, "duration": 3.03}, {"text": "It's because you know that a_2 is a 2 by 1 matrix.", "start": 844.18, "duration": 3.39}, {"text": "[NOISE] So this is not possible.", "start": 847.57, "duration": 3.525}, {"text": "It's not possible to get a_2,", "start": 851.095, "duration": 2.205}, {"text": "because otherwise it wouldn't match the derivative that you are calculating.", "start": 853.3, "duration": 2.79}, {"text": "So it has to be a_2 transpose.", "start": 856.09, "duration": 1.83}, {"text": "So either you- you learn the formula by heart or you- you learn how to analyze shapes,", "start": 857.92, "duration": 5.325}, {"text": "okay? Any questions on that?", "start": 863.245, "duration": 3.955}, {"text": "Okay. So that's why it's a_2 transpose.", "start": 868.23, "duration": 5.18}, {"text": "Now, l minus y_i.", "start": 873.93, "duration": 7.9}, {"text": "So I'm- I'm on this one now.", "start": 881.83, "duration": 1.995}, {"text": "The second term of the- of the derivative.", "start": 883.825, "duration": 2.4}, {"text": "And I take the derivative of this.", "start": 886.225, "duration": 2.115}, {"text": "So I get 1 over 1 minus a_3.", "start": 888.34, "duration": 3.87}, {"text": "a_3 denotes the sigmoid.", "start": 892.21, "duration": 1.56}, {"text": "So I'm just copying this back using", "start": 893.77, "duration": 2.22}, {"text": "the fact that the derivative of the logarithm is 1 over x,", "start": 895.99, "duration": 2.805}, {"text": "and then I will multiply this by the derivative of 1 minus a_3 with respect to w_3.", "start": 898.795, "duration": 5.88}, {"text": "I know that there is a minus that needs to come up.", "start": 904.675, "duration": 2.115}, {"text": "So I will write it down here,", "start": 906.79, "duration": 1.755}, {"text": "minus 1 and I also have", "start": 908.545, "duration": 1.755}, {"text": "the derivative of the sigmoid with respect to what's inside the sigmoid.", "start": 910.3, "duration": 4.605}, {"text": "So a_3 [NOISE] times 1 minus a_3.", "start": 914.905, "duration": 5.445}, {"text": "And what's the last term?", "start": 920.35, "duration": 2.025}, {"text": "The last term is simply the one we just talked about.", "start": 922.375, "duration": 2.995}, {"text": "It's the derivative of what's inside the sigmoid with respect to w_3.", "start": 925.37, "duration": 5.09}, {"text": "So it's a_2 transpose again.", "start": 930.46, "duration": 2.92}, {"text": "Okay. So now, I will just simplify.", "start": 937.08, "duration": 9.655}, {"text": "I know this scalar simplifies with this one.", "start": 946.735, "duration": 3.805}, {"text": "This one simplifies with that one.", "start": 950.54, "duration": 2.985}, {"text": "We're going to copy back all the results minus [NOISE] y_i times 1 minus a_3", "start": 953.525, "duration": 10.144}, {"text": "a_2 transpose plus 1 minus", "start": 963.669, "duration": 5.911}, {"text": "y_i times the minus- I'm going to put the minus here.", "start": 969.58, "duration": 5.89}, {"text": "So I'm taking the minus putting it on- on the front times a_3 times a_2 transpose.", "start": 975.47, "duration": 8.73}, {"text": "And then, quickly looking at that I see that some of the terms will cancel out, right?", "start": 984.2, "duration": 8.13}, {"text": "Okay. So I have one term here,", "start": 995.18, "duration": 3.04}, {"text": "y-hat- y_i times minus", "start": 998.22, "duration": 3.925}, {"text": "a_3 a_2 transpose would cancel out with plus y_i a_3 a_2 transpose.", "start": 1002.145, "duration": 6.135}, {"text": "This makes sense? So like,", "start": 1008.28, "duration": 2.22}, {"text": "the term that we multiply this number,", "start": 1010.5, "duration": 2.67}, {"text": "we cancel out with the term, we multiply this number.", "start": 1013.17, "duration": 3.37}, {"text": "We need to continue.", "start": 1018.05, "duration": 2.11}, {"text": "[NOISE] It gives me y_i times a_2 transpose, this part,", "start": 1020.16, "duration": 7.77}, {"text": "minus a_3 times a_2 transpose.", "start": 1027.93, "duration": 5.715}, {"text": "I, I can factor this because I have the same term a_2 transpose.", "start": 1033.645, "duration": 4.45}, {"text": "And it gives me finally,", "start": 1038.095, "duration": 1.949}, {"text": "y_i minus a_3 times a_2 transpose.", "start": 1040.044, "duration": 6.086}, {"text": "Okay, so it doesn't look that bad actually.", "start": 1047.57, "duration": 4.61}, {"text": "I don't know, when- when we take a derivative of something kin- kinda ugly we- we", "start": 1052.64, "duration": 4.405}, {"text": "expect something ugly to come out but this doesn't seem too bad.", "start": 1057.045, "duration": 4.455}, {"text": "Any questions on that?", "start": 1064.34, "duration": 2.065}, {"text": "I let you write it quickly,", "start": 1066.405, "duration": 1.635}, {"text": "and then we're going to move through to the rest.", "start": 1068.04, "duration": 2.25}, {"text": "So once I get these results,", "start": 1070.29, "duration": 1.455}, {"text": "I can just write down the costs of the derivative with respect to w_3.", "start": 1071.745, "duration": 5.165}, {"text": "I know it's just one minus.", "start": 1076.91, "duration": 2.66}, {"text": "I just need to- to take the summation of this thing.", "start": 1079.57, "duration": 6.185}, {"text": "So y_i minus a_3 times [NOISE] y_2 transpose- a_2 transpose.", "start": 1085.755, "duration": 9.695}, {"text": "And I have a minus sign coming upfront.", "start": 1095.45, "duration": 2.735}, {"text": "So that's my derivative.", "start": 1098.185, "duration": 1.425}, {"text": "[NOISE]", "start": 1099.61, "duration": 6.605}, {"text": "Okay. So we're done with that.", "start": 1106.215, "duration": 2.055}, {"text": "And we can, we can just take this formula,", "start": 1108.27, "duration": 3.255}, {"text": "plugging it back in our gradient descent update rule, and update w_3.", "start": 1111.525, "duration": 5.145}, {"text": "Yeah. Now, the question is,", "start": 1116.67, "duration": 6.915}, {"text": "you can do the same thing as,", "start": 1123.585, "duration": 1.455}, {"text": "as we just did but with b_3.", "start": 1125.04, "duration": 2.535}, {"text": "It's going to be the similar difficulty.", "start": 1127.575, "duration": 2.52}, {"text": "We're going to do it with w_2 now,", "start": 1130.095, "duration": 2.49}, {"text": "and think how does that backpropagate to w_2.", "start": 1132.585, "duration": 3.99}, {"text": "So now it's w_2 star.", "start": 1136.575, "duration": 2.46}, {"text": "We want to compute the derivative of l,", "start": 1139.035, "duration": 2.385}, {"text": "the loss, with respect to w of the second layer.", "start": 1141.42, "duration": 3.85}, {"text": "The question is how I'm gonna get this one without having too much work.", "start": 1145.31, "duration": 5.695}, {"text": "I'm not gonna start over here as we said last time,", "start": 1151.005, "duration": 2.295}, {"text": "I'm going to use the chain rule of calculus.", "start": 1153.3, "duration": 2.49}, {"text": "So I'm going to try to decompose this derivative into several derivatives.", "start": 1155.79, "duration": 5.22}, {"text": "So I know that y hat is the first thing that is connected to the loss function, right.", "start": 1161.01, "duration": 6.48}, {"text": "The output neuron is directly connected to the loss function.", "start": 1167.49, "duration": 3.255}, {"text": "So I'm going to take the derivative of the loss function with respect to", "start": 1170.745, "duration": 3.015}, {"text": "y hat, also called a_3.", "start": 1173.76, "duration": 3.135}, {"text": "Right? This is the easiest one I can calculate.", "start": 1176.895, "duration": 2.76}, {"text": "I also know that a_3,", "start": 1179.655, "duration": 2.595}, {"text": "which is the output activation of the last neuron,", "start": 1182.25, "duration": 2.58}, {"text": "is connected with the linear part of the last neuron, which is z_3.", "start": 1184.83, "duration": 3.72}, {"text": "So I can take the derivative of a_3 with respect to z_3.", "start": 1188.55, "duration": 5.655}, {"text": "Do you remember what this is going to be?", "start": 1194.205, "duration": 2.865}, {"text": "Derivative of a_3 with respect to z_3?", "start": 1197.07, "duration": 3.52}, {"text": "Derivative of Sigmoid.", "start": 1202.79, "duration": 2.245}, {"text": "I know that a_3 equals Sigmoid of z_3.", "start": 1205.035, "duration": 2.115}, {"text": "So this derivative is very simple. It's just that.", "start": 1207.15, "duration": 3.645}, {"text": "It's just a_3 times 1 minus a_3.", "start": 1210.795, "duration": 2.91}, {"text": "All right. So I'm going to continue.", "start": 1213.705, "duration": 3.15}, {"text": "I know that z_3, z_3 is equal to what?", "start": 1216.855, "duration": 4.26}, {"text": "It's equal to w_3, a_2 plus b.", "start": 1221.115, "duration": 3.405}, {"text": "Which path did I need- do I need to take in order to backpropagate?", "start": 1224.52, "duration": 3.465}, {"text": "I don't wanna take the derivative with respect to w_3 because I will only get stuck.", "start": 1227.985, "duration": 4.02}, {"text": "I don't wanna take the derivative with respect to b_3 because I will get stuck.", "start": 1232.005, "duration": 3.21}, {"text": "I will take the derivative with respect to a_2.", "start": 1235.215, "duration": 2.73}, {"text": "Because a_2 will be connected to z_2,", "start": 1237.945, "duration": 2.79}, {"text": "z_2 will be connected to a_1,", "start": 1240.735, "duration": 1.515}, {"text": "and I can backpropagate from this path.", "start": 1242.25, "duration": 3.015}, {"text": "So I'm going to take derivative of z_3 with respect to", "start": 1245.265, "duration": 3.825}, {"text": "a_2 to have my error backpropagate, and so on.", "start": 1249.09, "duration": 4.745}, {"text": "I know that a_2 is equal to Sigmoid of z_2.", "start": 1253.835, "duration": 5.34}, {"text": "So I'm just going to do that.", "start": 1259.175, "duration": 1.245}, {"text": "And I know that this derivative is going to be easy as well.", "start": 1260.42, "duration": 3.87}, {"text": "And finally, I also know that z_2 is connected to w_2.", "start": 1264.29, "duration": 5.55}, {"text": "So I'm going to take derivative of z_2 with respect to w_2.", "start": 1269.84, "duration": 4.69}, {"text": "So just what I want you to get is the thought process of this chain rule.", "start": 1275.15, "duration": 4.825}, {"text": "Why don't we take a derivative with respect to w_3 or b_3?", "start": 1279.975, "duration": 3.195}, {"text": "It's because we will get stuck.", "start": 1283.17, "duration": 1.305}, {"text": "We want the error to back propagate.", "start": 1284.475, "duration": 1.755}, {"text": "And in order for the error to backpropagate,", "start": 1286.23, "duration": 1.74}, {"text": "we have to go through variables that are connected to each other. Does this makes sense?", "start": 1287.97, "duration": 7.0}, {"text": "So now the question is how can we use this?", "start": 1297.89, "duration": 3.49}, {"text": "How can we use the derivative we already have in order to,", "start": 1301.38, "duration": 4.83}, {"text": "to, to, to compute the derivative with respect to w_2?", "start": 1306.21, "duration": 4.98}, {"text": "Can someone tell me how we can use the results from this calculation,", "start": 1311.19, "duration": 4.305}, {"text": "in order not to do it again?", "start": 1315.495, "duration": 1.725}, {"text": "Cache it.", "start": 1317.22, "duration": 1.33}, {"text": "You cache it? Um, so there's another discussion on caching,", "start": 1324.8, "duration": 6.429}, {"text": "which is, which is correct that in order to", "start": 1331.229, "duration": 2.221}, {"text": "get this result very quickly we will use cache.", "start": 1333.45, "duration": 2.625}, {"text": "But, uh, what I want here is to- you to tell me if", "start": 1336.075, "duration": 2.745}, {"text": "these results appear somewhere here. Yeah?", "start": 1338.82, "duration": 3.81}, {"text": "[inaudible] the first three terms.", "start": 1342.63, "duration": 1.95}, {"text": "The first three terms. So this one, this one, and this one?", "start": 1344.58, "duration": 4.6}, {"text": "I'm not sure.", "start": 1349.52, "duration": 2.1}, {"text": "Yeah. Is it the first two terms or the first three terms?", "start": 1351.62, "duration": 2.29}, {"text": "Two.", "start": 1353.91, "duration": 0.81}, {"text": "The first two terms. Yeah. But good intuition.", "start": 1354.72, "duration": 1.62}, {"text": "Yeah. So this result is actually the first two terms here.", "start": 1356.34, "duration": 3.96}, {"text": "We just calculated it.", "start": 1360.3, "duration": 2.22}, {"text": "Okay. What- how do we know that? It's not easy to see.", "start": 1362.52, "duration": 3.69}, {"text": "One thing we know based on what we've written very big on", "start": 1366.21, "duration": 3.12}, {"text": "this board is that the derivative of z_3,", "start": 1369.33, "duration": 4.2}, {"text": "because this is z_3, right?", "start": 1373.53, "duration": 2.475}, {"text": "Derivative of z_3 with respect to w_3 is a_2 transpose.", "start": 1376.005, "duration": 4.68}, {"text": "Right. So I could write here that this thing is derivative", "start": 1380.685, "duration": 5.43}, {"text": "of z_3 with respect to w_3.", "start": 1386.115, "duration": 5.745}, {"text": "Is it correct? So I know", "start": 1391.86, "duration": 3.135}, {"text": "that because I wanted to compute the derivative of the loss to w_3,", "start": 1394.995, "duration": 3.825}, {"text": "I know that I could have written derivative of loss with respect to", "start": 1398.82, "duration": 3.36}, {"text": "w_3 as derivative of loss with respect to z_3,", "start": 1402.18, "duration": 6.0}, {"text": "times derivative of z_3 with respect to w_3.", "start": 1408.18, "duration": 6.915}, {"text": "Correct. And I know that this is a_2 transpose.", "start": 1415.095, "duration": 4.275}, {"text": "So it means that this thing is the derivative of the loss with respect to z_3.", "start": 1419.37, "duration": 5.44}, {"text": "Does that make sense? So I got,", "start": 1424.81, "duration": 3.455}, {"text": "I got my decomposition of the derivative we had.", "start": 1428.265, "duration": 2.445}, {"text": "If we wanted to use the chain rule from here on,", "start": 1430.71, "duration": 2.205}, {"text": "we could have just separated it into two terms, and took the derivative here.", "start": 1432.915, "duration": 4.005}, {"text": "Okay. So I know the result of this thing.", "start": 1436.92, "duration": 4.365}, {"text": "I know that this thing is basically a_3 minus y, times a_2 transpose.", "start": 1441.285, "duration": 10.425}, {"text": "I just flipped it because of the minus sign.", "start": 1451.71, "duration": 2.685}, {"text": "Okay. Is it mine?", "start": 1454.395, "duration": 2.025}, {"text": "[NOISE].", "start": 1456.42, "duration": 3.66}, {"text": "Okay. [NOISE]. Now, tell me what's this term.", "start": 1460.08, "duration": 7.99}, {"text": "What is this term? Let's go there. Yeah.", "start": 1469.88, "duration": 5.83}, {"text": "Sigmoid.", "start": 1475.71, "duration": 0.585}, {"text": "So Sigmoid. I'm just going to write it a_2 times 1 minus a_2.", "start": 1476.295, "duration": 4.965}, {"text": "Does that make sense? Sigmoid times 1 minus Sigmoid.", "start": 1481.26, "duration": 4.18}, {"text": "What is this term?", "start": 1486.2, "duration": 1.9}, {"text": "Uh, oh sorry my bad.", "start": 1488.1, "duration": 10.815}, {"text": "That's not the right one. This one, this one is that.", "start": 1498.915, "duration": 3.385}, {"text": "This one is Sigmoid.", "start": 1505.07, "duration": 2.5}, {"text": "a_2 is Sigmoid of z_2.", "start": 1507.57, "duration": 1.59}, {"text": "So this result comes from this term.", "start": 1509.16, "duration": 1.635}, {"text": "Was- what about this term?", "start": 1510.795, "duration": 1.965}, {"text": "w_3.", "start": 1512.76, "duration": 4.38}, {"text": "Sorry.", "start": 1517.14, "duration": 1.23}, {"text": "w_3.", "start": 1518.37, "duration": 0.99}, {"text": "w_3. Is it w_3 or no? I heard transpose.", "start": 1519.36, "duration": 4.77}, {"text": "How do we know if it's w_3 or w_3 transpose?", "start": 1524.13, "duration": 3.33}, {"text": "So let's look at the shape of this. What's z_3?", "start": 1527.46, "duration": 2.88}, {"text": "One by one.", "start": 1530.34, "duration": 1.65}, {"text": "It's one by one. It's a scalar.", "start": 1531.99, "duration": 2.025}, {"text": "It's the linear part of the last neuron.", "start": 1534.015, "duration": 1.8}, {"text": "What's the shape of that? This is 2, 1.", "start": 1535.815, "duration": 3.615}, {"text": "We have two neurons in the layer.", "start": 1539.43, "duration": 1.95}, {"text": "w_3. We said that it was a 1 by 2 matrix,", "start": 1541.38, "duration": 4.44}, {"text": "so we have to transpose it.", "start": 1545.82, "duration": 1.515}, {"text": "So the result of that is w_3 transpose.", "start": 1547.335, "duration": 5.355}, {"text": "And how about the last term?", "start": 1552.69, "duration": 2.62}, {"text": "Same as here. One layer before.", "start": 1561.17, "duration": 4.37}, {"text": "Yeah, someone said they won't transpose.", "start": 1567.05, "duration": 5.0}, {"text": "Okay. Yeah?", "start": 1577.94, "duration": 2.89}, {"text": "The numbers are [inaudible] that one.", "start": 1580.83, "duration": 1.45}, {"text": "This one?", "start": 1586.01, "duration": 2.065}, {"text": "Yeah.", "start": 1588.075, "duration": 0.645}, {"text": "There is a transpose here.", "start": 1588.72, "duration": 1.2}, {"text": "[inaudible] w_5.", "start": 1589.92, "duration": 9.585}, {"text": "Oh yeah, yeah. You're correct.", "start": 1599.505, "duration": 1.395}, {"text": "You're correct. Thank you.", "start": 1600.9, "duration": 1.47}, {"text": "That's what you mean? Yeah. Yeah. This one was from the z_3, to w_2.", "start": 1602.37, "duration": 4.71}, {"text": "We didn't end up using that because we will get stuck,", "start": 1607.08, "duration": 2.685}, {"text": "so there's no a_2 transpose here.", "start": 1609.765, "duration": 1.545}, {"text": "Thanks. Any other questions or remarks?", "start": 1611.31, "duration": 4.81}, {"text": "So that's cool. Let's, let's, let's write- let's write", "start": 1617.63, "duration": 3.64}, {"text": "down our derivative cleanly on the board.", "start": 1621.27, "duration": 5.35}, {"text": "So we have derivative of our loss function with respect to w_2,", "start": 1628.43, "duration": 8.545}, {"text": "which seems to be equal to a_3 minus y,", "start": 1636.975, "duration": 7.665}, {"text": "from the first term.", "start": 1644.64, "duration": 1.995}, {"text": "The second term seems to be equal to, uh, w_3 transpose.", "start": 1646.635, "duration": 7.455}, {"text": "Then we have a term which is a_2 times 1 minus a_2.", "start": 1656.06, "duration": 6.65}, {"text": "Okay. And finally, finally we have another term that is a_1 transpose.", "start": 1662.71, "duration": 8.21}, {"text": "So are we done or not?", "start": 1672.71, "duration": 3.14}, {"text": "So actually there is that- the thing is there's two ways to compute derivatives.", "start": 1681.08, "duration": 6.31}, {"text": "Either you go very rigorously and do what we did here for w_2,", "start": 1687.39, "duration": 6.525}, {"text": "or you try to do a chain rule analysis,", "start": 1693.915, "duration": 3.405}, {"text": "and you try to fit the terms.", "start": 1697.32, "duration": 1.845}, {"text": "The problem is this result is not completely correct.", "start": 1699.165, "duration": 3.765}, {"text": "There is a shape problem.", "start": 1702.93, "duration": 1.44}, {"text": "It means when we took our derivatives,", "start": 1704.37, "duration": 1.92}, {"text": "we should have flipped some of the terms. We didn't.", "start": 1706.29, "duration": 3.375}, {"text": "There is actually- we,", "start": 1709.665, "duration": 1.455}, {"text": "we won't have time to go into details in", "start": 1711.12, "duration": 2.01}, {"text": "this lecture because we have other things to see, but there is,", "start": 1713.13, "duration": 3.315}, {"text": "uh, a section note I think on the website,", "start": 1716.445, "duration": 2.475}, {"text": "which details the other method which is more rigorous,", "start": 1718.92, "duration": 2.64}, {"text": "which is like that for all the derivatives.", "start": 1721.56, "duration": 1.92}, {"text": "What we are going to see is how you can use chain rule plus", "start": 1723.48, "duration": 2.49}, {"text": "shape analysis to come up with the results very quickly.", "start": 1725.97, "duration": 2.82}, {"text": "Okay. So let's, let's analyze the shape of all that.", "start": 1728.79, "duration": 2.775}, {"text": "We know that the first term is a scalar.", "start": 1731.565, "duration": 1.995}, {"text": "It is a 1 by 1. We know that the second term is the transpose of 1 by 2. So it's 2 by 1.", "start": 1733.56, "duration": 7.68}, {"text": "And we know that this thing here a_2 times 1 minus a_2 is,", "start": 1741.24, "duration": 6.21}, {"text": "uh, 2 by 1.", "start": 1747.45, "duration": 2.52}, {"text": "It's an element-wise product.", "start": 1749.97, "duration": 2.235}, {"text": "And this one is a_1 transpose,", "start": 1752.205, "duration": 2.415}, {"text": "so it's 3 by 1 transpose.", "start": 1754.62, "duration": 1.605}, {"text": "So it is 1 by 3. So there seems to be a problem here.", "start": 1756.225, "duration": 3.915}, {"text": "There is no match between these two operations for example.", "start": 1760.14, "duration": 3.73}, {"text": "Right? So the question is, how- how can,", "start": 1764.27, "duration": 3.55}, {"text": "we how can we put everything together?", "start": 1767.82, "duration": 2.94}, {"text": "If we do it very rigorously,", "start": 1770.76, "duration": 1.62}, {"text": "we know how to put it together.", "start": 1772.38, "duration": 1.47}, {"text": "If you're used to doing the chain rule,", "start": 1773.85, "duration": 2.19}, {"text": "you can quickly sh- quickly do it around.", "start": 1776.04, "duration": 2.37}, {"text": "So after experience, you will be able to,", "start": 1778.41, "duration": 2.175}, {"text": "to fit all these together.", "start": 1780.585, "duration": 2.07}, {"text": "The important thing to know is that here there is an element-wise product, which is here.", "start": 1782.655, "duration": 7.26}, {"text": "So every time you will take the derivative of the Sigmoid", "start": 1789.915, "duration": 3.285}, {"text": "it's going to end up being an element-wise product.", "start": 1793.2, "duration": 3.0}, {"text": "And it's the case whatever the activation that you're using is.", "start": 1796.2, "duration": 4.14}, {"text": "So the right result is this one.", "start": 1800.34, "duration": 2.89}, {"text": "So here I have my element-wise product of a 2 by 1 [NOISE] by a 2 by 1.", "start": 1809.08, "duration": 8.395}, {"text": "So it gives me a 2 by", "start": 1817.475, "duration": 1.335}, {"text": "1 column vector and then I need something that is 1 by 1 and 1 by 3.", "start": 1818.81, "duration": 9.495}, {"text": "How do I know, wha- what I need to have,", "start": 1828.305, "duration": 2.25}, {"text": "I know that the shape of this thing.", "start": 1830.555, "duration": 1.995}, {"text": "W3 needs to be 2 by 3.", "start": 1832.55, "duration": 5.835}, {"text": "It's connecting three neurons to two neurons.", "start": 1838.385, "duration": 2.79}, {"text": "So W2 has to be 2 by 3.", "start": 1841.175, "duration": 2.04}, {"text": "In order to end up with this,", "start": 1843.215, "duration": 1.53}, {"text": "I know that this has to come here A3 minus y and A1 transpose comes at the end.", "start": 1844.745, "duration": 6.21}, {"text": "And here I get my correct answer.", "start": 1850.955, "duration": 2.965}, {"text": "Don't worry if it's the first time th- the chain rule is going quickly, don't worry.", "start": 1870.13, "duration": 4.915}, {"text": "Read the lecture notes with the rigorous parts.", "start": 1875.045, "duration": 3.51}, {"text": "Taking the derivative, it will make more sense.", "start": 1878.555, "duration": 2.79}, {"text": "But I feel it's, uh,", "start": 1881.345, "duration": 2.43}, {"text": "usually in practice, we don't compute these chain rules anymore, uh,", "start": 1883.775, "duration": 4.275}, {"text": "because- because programming frameworks do it for us", "start": 1888.05, "duration": 3.3}, {"text": "but it's important to know at least how the chain rule decomposes,", "start": 1891.35, "duration": 3.93}, {"text": "uh, and also how to make these, compute these derivatives.", "start": 1895.28, "duration": 4.05}, {"text": "If you read research papers specifically.", "start": 1899.33, "duration": 2.28}, {"text": "Any questions on that?", "start": 1901.61, "duration": 1.92}, {"text": "I think I wanna go back to what you mentioned with the cache.", "start": 1903.53, "duration": 2.955}, {"text": "So why is cache very important? That was your question as well?", "start": 1906.485, "duration": 3.825}, {"text": "[BACKGROUND]", "start": 1910.31, "duration": 3.72}, {"text": "Yeah, yeah it has to be.", "start": 1914.03, "duration": 2.055}, {"text": "Right. So it means when you take the derivative of Sigmoid,", "start": 1916.085, "duration": 4.185}, {"text": "you take derivative with respect to", "start": 1920.27, "duration": 1.5}, {"text": "every entry of the matrix which gives you an element-wise product.", "start": 1921.77, "duration": 2.52}, {"text": "Um, going back to the cache.", "start": 1924.29, "duration": 4.215}, {"text": "So one thing is,", "start": 1928.505, "duration": 2.565}, {"text": "it seems that during backpropagation,", "start": 1931.07, "duration": 2.85}, {"text": "there is a lot of terms that appear that were computed during forward propagation.", "start": 1933.92, "duration": 4.32}, {"text": "Right. All these terms; a1 transpose,", "start": 1938.24, "duration": 2.729}, {"text": "a2, a3, all these,", "start": 1940.969, "duration": 2.626}, {"text": "we have it from the forward propagation.", "start": 1943.595, "duration": 1.89}, {"text": "So if we don't cache anything,", "start": 1945.485, "duration": 1.785}, {"text": "we have to recompute them.", "start": 1947.27, "duration": 1.35}, {"text": "It means I'm going backwards but then I feel,", "start": 1948.62, "duration": 3.585}, {"text": "oh, I need a2 actually.", "start": 1952.205, "duration": 1.635}, {"text": "So I have to re- go forward again to get a2.", "start": 1953.84, "duration": 2.925}, {"text": "I go backwards, I need a1.", "start": 1956.765, "duration": 1.785}, {"text": "I need to forward propagate my x again to get a1. I don't wanna do that.", "start": 1958.55, "duration": 3.645}, {"text": "So in order to avoid that,", "start": 1962.195, "duration": 1.47}, {"text": "when I do my forward propagation,", "start": 1963.665, "duration": 1.56}, {"text": "I would keep in memory almost all the values that I'm getting", "start": 1965.225, "duration": 4.215}, {"text": "including the Ws because as you see to compute", "start": 1969.44, "duration": 2.94}, {"text": "the derivative of loss with respect to W2 we need W3,", "start": 1972.38, "duration": 3.555}, {"text": "but also, the activation or linear variables.", "start": 1975.935, "duration": 4.935}, {"text": "So I'm going to save them in my,", "start": 1980.87, "duration": 2.91}, {"text": "in my network during the forward propagation in order to", "start": 1983.78, "duration": 3.51}, {"text": "use it during the backward propagation. So it makes sense.", "start": 1987.29, "duration": 4.42}, {"text": "And again, it's all for computational ef- efficiency.", "start": 1991.99, "duration": 3.895}, {"text": "It has some memory costs.", "start": 1995.885, "duration": 2.455}, {"text": "Okay. So that was backpropagation.", "start": 2002.28, "duration": 2.965}, {"text": "And now I can use my formula of", "start": 2005.245, "duration": 4.095}, {"text": "the costs with respect to the loss function.", "start": 2009.34, "duration": 7.335}, {"text": "And I know that this is going to be my update.", "start": 2016.675, "duration": 2.565}, {"text": "[NOISE] This is going to be used in order to update W2 and I will do the same for W1.", "start": 2019.24, "duration": 9.93}, {"text": "Then you guys can do it at home.", "start": 2029.17, "duration": 1.71}, {"text": "If you wanna meet, wanna make sure you understood,", "start": 2030.88, "duration": 1.635}, {"text": "take the derivative with respect to W1.", "start": 2032.515, "duration": 2.815}, {"text": "Okay. So let's move on to the next part,", "start": 2041.04, "duration": 2.98}, {"text": "[NOISE] which is improving your neural network.", "start": 2044.02, "duration": 7.93}, {"text": "So in practice, when you,", "start": 2054.99, "duration": 3.01}, {"text": "when you do this process of training forward propagation,", "start": 2058.0, "duration": 2.85}, {"text": "backward propagation updates, you don't end up", "start": 2060.85, "duration": 2.82}, {"text": "having a good network mo- most of the time.", "start": 2063.67, "duration": 3.5}, {"text": "In order to get a good network, you need to improve it.", "start": 2067.17, "duration": 2.385}, {"text": "You need to use a bunch of techniques that will make your network work in practice.", "start": 2069.555, "duration": 4.53}, {"text": "The first, the first trick is to use different activation functions.", "start": 2074.085, "duration": 7.465}, {"text": "So together, we've seen one activation function which was Sigmoid.", "start": 2084.56, "duration": 5.33}, {"text": "And we remember the graph of Sigmoid is getting a number", "start": 2092.63, "duration": 4.48}, {"text": "between minus infinity and plus infinity and casting it between 0 and 1.", "start": 2097.11, "duration": 4.745}, {"text": "And we know that the formula is Sigmoid of z equals", "start": 2101.855, "duration": 3.485}, {"text": "1 over 1 plus exponent so minus z.", "start": 2105.34, "duration": 4.02}, {"text": "We also know that the derivative of Sigmoid is Sigmoid of z times 1 minus Sigmoid of z.", "start": 2109.36, "duration": 7.63}, {"text": "Okay. Another very common,", "start": 2117.08, "duration": 4.015}, {"text": "uh, activation function is ReLU.", "start": 2121.095, "duration": 2.91}, {"text": "We talked quickly about it last time.", "start": 2124.005, "duration": 3.27}, {"text": "ReLU of z which is equal to 0 if z is less than 0 and z if z is positive.", "start": 2127.275, "duration": 8.995}, {"text": "So the graph of ReLU looks like something like this.", "start": 2136.27, "duration": 4.87}, {"text": "And finally, another one we were using commonly as well is", "start": 2150.96, "duration": 4.72}, {"text": "tan h. So hyperbolic tangents and", "start": 2155.68, "duration": 4.515}, {"text": "tan h of z exponential z minus", "start": 2160.195, "duration": 4.215}, {"text": "exponential minus z over exponential z plus exponential minus z.", "start": 2164.41, "duration": 5.55}, {"text": "The derivative of tan h is", "start": 2169.96, "duration": 5.79}, {"text": "1 minus tan h squared of z.", "start": 2175.75, "duration": 9.66}, {"text": "And the graph looks kind of like Sigmoid,", "start": 2185.41, "duration": 4.425}, {"text": "but, but it goes between minus 1 and plus 1.", "start": 2189.835, "duration": 5.695}, {"text": "So one question.", "start": 2199.65, "duration": 2.65}, {"text": "Now that I've given you three activation functions,", "start": 2202.3, "duration": 5.13}, {"text": "can you guess why we would use one instead of the other and,", "start": 2207.43, "duration": 4.47}, {"text": "and which one has more benefits?", "start": 2211.9, "duration": 3.28}, {"text": "So when I talk about activation functions,", "start": 2216.0, "duration": 2.635}, {"text": "I talk about the functions that you will put in these neurons after the linear part.", "start": 2218.635, "duration": 6.415}, {"text": "What do you think is the main advantage of Sigmoid? Yeah.", "start": 2228.48, "duration": 5.2}, {"text": "We use it for classification.", "start": 2233.68, "duration": 6.105}, {"text": "Yep. You use it for classification,", "start": 2239.785, "duration": 2.805}, {"text": "between it gives you a probability.", "start": 2242.59, "duration": 1.395}, {"text": "What's the main disadvantage of Sigmoid?", "start": 2243.985, "duration": 2.775}, {"text": "It's easy.", "start": 2246.76, "duration": 5.43}, {"text": "It's easy. That should be an advantage,", "start": 2252.19, "duration": 2.58}, {"text": "should be a benefit. Yeah?", "start": 2254.77, "duration": 1.95}, {"text": "[BACKGROUND]", "start": 2256.72, "duration": 6.9}, {"text": "Correct. If you're at high activation,", "start": 2263.62, "duration": 2.28}, {"text": "if you are at high z's or low z's,", "start": 2265.9, "duration": 2.085}, {"text": "your gradient is very close to 0.", "start": 2267.985, "duration": 1.68}, {"text": "So look here. Based on this graph we know that if z is very big.", "start": 2269.665, "duration": 4.56}, {"text": "If z is very big our gradient is going to be very small,", "start": 2274.225, "duration": 3.945}, {"text": "the slope of this,", "start": 2278.17, "duration": 1.29}, {"text": "of this graph is very, very small. It's almost flat.", "start": 2279.46, "duration": 3.06}, {"text": "Same for z's that are very low in the negative.", "start": 2282.52, "duration": 3.045}, {"text": "Right. What's the problem with having low gradients is when I'm back propagating.", "start": 2285.565, "duration": 4.95}, {"text": "If the z I cached was big,", "start": 2290.515, "duration": 2.895}, {"text": "the gradient is going to be very small and it will be super hard to update", "start": 2293.41, "duration": 3.75}, {"text": "my parameters that are early in the network because the gradient is just going to vanish.", "start": 2297.16, "duration": 4.41}, {"text": "Does that makes sense?", "start": 2301.57, "duration": 1.995}, {"text": "So Sigmoid is one of these activations which,", "start": 2303.565, "duration": 3.0}, {"text": "which works very well in the linear regime,", "start": 2306.565, "duration": 3.63}, {"text": "but has trouble working in saturating", "start": 2310.195, "duration": 3.195}, {"text": "regimes because the network doesn't update the parameters properly.", "start": 2313.39, "duration": 3.165}, {"text": "It goes very, very slowly.", "start": 2316.555, "duration": 1.98}, {"text": "We're going to talk about that a little more.", "start": 2318.535, "duration": 2.235}, {"text": "How about tan h? Very similar, right?", "start": 2320.77, "duration": 6.045}, {"text": "Similar like high z's and low z's lead to saturation of a tan h activation.", "start": 2326.815, "duration": 6.03}, {"text": "ReLU on the other hand doesn't have this problem.", "start": 2332.845, "duration": 3.51}, {"text": "If z is very big in the positives, there is no saturation.", "start": 2336.355, "duration": 4.95}, {"text": "The gradient just passes and the gradient is 1, when we were here.", "start": 2341.305, "duration": 4.425}, {"text": "The slope is equal to 1.", "start": 2345.73, "duration": 1.89}, {"text": "So it's actually just directing the gradient to some entry.", "start": 2347.62, "duration": 2.775}, {"text": "Is not multiplying it by anything when you backpropagate.", "start": 2350.395, "duration": 3.495}, {"text": "So you know this term here,", "start": 2353.89, "duration": 1.92}, {"text": "this term that I have here.", "start": 2355.81, "duration": 1.44}, {"text": "All the a3 minus a3 times 1 minus a3 or 1 minus a2.", "start": 2357.25, "duration": 5.55}, {"text": "If we use ReLU activations,", "start": 2362.8, "duration": 1.89}, {"text": "we would change this with what's-", "start": 2364.69, "duration": 3.995}, {"text": "with- with the derivative of ReLU and the derivative of", "start": 2368.685, "duration": 4.275}, {"text": "ReLU can be written indicator function", "start": 2372.96, "duration": 6.24}, {"text": "of z being positive.", "start": 2379.2, "duration": 2.21}, {"text": "You've seen indicator functions.", "start": 2381.41, "duration": 2.825}, {"text": "So this is equal to 1 if z is positive, 0 otherwise.", "start": 2384.235, "duration": 4.495}, {"text": "Okay. So we will see why we use ReLU mostly. Yeah?", "start": 2390.42, "duration": 3.52}, {"text": "[BACKGROUND]", "start": 2393.94, "duration": 6.31}, {"text": "Yeah. You remember the house prediction example?", "start": 2400.25, "duration": 3.585}, {"text": "In that case, if you want to,", "start": 2403.835, "duration": 1.725}, {"text": "if you want to predict the price of a house based on some features, you would use ReLU.", "start": 2405.56, "duration": 3.675}, {"text": "Because you know that the output should be", "start": 2409.235, "duration": 1.545}, {"text": "a positive number between 0 and plus infinity,", "start": 2410.78, "duration": 2.655}, {"text": "it doesn't make sense to use 1 of tan h or similar. Yep.", "start": 2413.435, "duration": 4.425}, {"text": "[BACKGROUND]", "start": 2417.86, "duration": 5.94}, {"text": "Doesn't really matter. I think if,", "start": 2423.8, "duration": 1.725}, {"text": "if I want my output to be between 0 and 1 I would use Sigmoid,", "start": 2425.525, "duration": 3.315}, {"text": "if I want my output to be between minus 1 and", "start": 2428.84, "duration": 2.37}, {"text": "1 I would use tan h. So you know, there is,", "start": 2431.21, "duration": 2.85}, {"text": "there are some tasks where the output is kind of", "start": 2434.06, "duration": 3.33}, {"text": "a reward or a minus reward that you want to get.", "start": 2437.39, "duration": 4.095}, {"text": "Like in reinforcement learning,", "start": 2441.485, "duration": 1.755}, {"text": "you would use tan h as an output activation which is", "start": 2443.24, "duration": 2.325}, {"text": "because minus 1 looks like a negative reward,", "start": 2445.565, "duration": 2.625}, {"text": "plus 1 looks like a positive reward,", "start": 2448.19, "duration": 1.725}, {"text": "and you want to decide what should be the reward.", "start": 2449.915, "duration": 2.725}, {"text": "Why do we consider these functions?", "start": 2456.49, "duration": 1.69}, {"text": "Good question. Why do we consider these functions?", "start": 2458.18, "duration": 2.625}, {"text": "We can actually consider any functions apart", "start": 2460.805, "duration": 2.385}, {"text": "from the identity function. So let's see why.", "start": 2463.19, "duration": 3.51}, {"text": "Thanks for the transition. [LAUGHTER] Like why do we need activation functions?", "start": 2466.7, "duration": 11.56}, {"text": "So let's assume that we have a network which is the same as before.", "start": 2482.77, "duration": 4.54}, {"text": "So our network is three neurons casting into two neurons casting into one neuron, ah,", "start": 2487.31, "duration": 5.31}, {"text": "and we're trying to use activations are equal to identity functions.", "start": 2492.62, "duration": 9.84}, {"text": "So it means z is given to z.", "start": 2502.46, "duration": 4.395}, {"text": "Let's try to derive the forward propagation, y_hat equals a_3,", "start": 2506.855, "duration": 6.27}, {"text": "equals z_3, equals w_3, a_2 plus b_3.", "start": 2513.125, "duration": 9.595}, {"text": "I know that a_2,", "start": 2522.97, "duration": 3.289}, {"text": "a_2 is equal to z_2 because there is no activation and z_2 is equal to w_2 a_1 plus b_2.", "start": 2526.75, "duration": 7.825}, {"text": "So I can cast here w_2,", "start": 2534.575, "duration": 3.525}, {"text": "w_2 a_1 plus b_2 plus b_3.", "start": 2538.1, "duration": 10.465}, {"text": "I can continue.", "start": 2548.565, "duration": 1.87}, {"text": "I know that a_1 is equal to z_1,", "start": 2550.435, "duration": 2.73}, {"text": "and I know that z_1 is w_1 x plus b,", "start": 2553.165, "duration": 3.745}, {"text": "and b equals w_3 times w_2 times", "start": 2605.55, "duration": 13.84}, {"text": "b_1 plus w_3 times", "start": 2619.39, "duration": 5.43}, {"text": "b_2 plus b_3.", "start": 2624.82, "duration": 5.87}, {"text": "So what's the insight here?", "start": 2632.22, "duration": 3.32}, {"text": "Is that we need activation functions.", "start": 2636.79, "duration": 3.025}, {"text": "The reason is, if you don't choose activation functions,", "start": 2639.815, "duration": 3.27}, {"text": "no matter how deep is your network,", "start": 2643.085, "duration": 1.725}, {"text": "it's going to be equivalent to a linear regression.", "start": 2644.81, "duration": 2.715}, {"text": "So the complexity of the network comes from the activation function.", "start": 2647.525, "duration": 4.095}, {"text": "And the reason we can understand- if we're trying to detect cats,", "start": 2651.62, "duration": 4.89}, {"text": "what we're trying to do is to train a network that", "start": 2656.51, "duration": 2.655}, {"text": "will mimic the formula of detecting cats.", "start": 2659.165, "duration": 2.91}, {"text": "We don't know this formula,", "start": 2662.075, "duration": 1.185}, {"text": "so we want to mimic it using a lot of parameters.", "start": 2663.26, "duration": 3.315}, {"text": "If we just have a linear regression,", "start": 2666.575, "duration": 2.22}, {"text": "we cannot mimic this because we are going to look", "start": 2668.795, "duration": 3.105}, {"text": "at pixel by pixel and assign every weight to a certain pixel.", "start": 2671.9, "duration": 4.335}, {"text": "If I give you an example, it's not going to work anymore. Yeah, yeah.", "start": 2676.235, "duration": 9.645}, {"text": "So I think that's,", "start": 2685.88, "duration": 1.26}, {"text": "that, that goes back to your question as well.", "start": 2687.14, "duration": 1.83}, {"text": "So this is why we need activation functions.", "start": 2688.97, "duration": 2.475}, {"text": "And then the question was, can we use different activation functions and how do we,", "start": 2691.445, "duration": 3.555}, {"text": "how do we put them inside a layer or inside neurons?", "start": 2695.0, "duration": 2.685}, {"text": "Usually, we would use,", "start": 2697.685, "duration": 1.965}, {"text": "there are more activation functions.", "start": 2699.65, "duration": 1.92}, {"text": "I think in CS230 we'll go over a few more but not, not, not today.", "start": 2701.57, "duration": 5.13}, {"text": "These have been designed with experience,", "start": 2706.7, "duration": 2.294}, {"text": "so these are the ones that's,", "start": 2708.994, "duration": 1.711}, {"text": "that, that's work better and lets our networks train.", "start": 2710.705, "duration": 4.23}, {"text": "There are plenty of other activation functions that have been tested.", "start": 2714.935, "duration": 4.005}, {"text": "Usually, you would, you would, uh,", "start": 2718.94, "duration": 2.19}, {"text": "use the same activation functions inside every layer.", "start": 2721.13, "duration": 3.27}, {"text": "So when you, it's,", "start": 2724.4, "duration": 2.31}, {"text": "it's a, it's, it's for, for training.", "start": 2726.71, "duration": 1.89}, {"text": "It doesn't have any special reason I think but when you have a network like that,", "start": 2728.6, "duration": 5.175}, {"text": "you would call this layer a ReLU layer", "start": 2733.775, "duration": 2.37}, {"text": "meaning it's a fully connected layer with ReLU activation.", "start": 2736.145, "duration": 2.655}, {"text": "This one a Sigmoid layer,", "start": 2738.8, "duration": 1.695}, {"text": "it means it's a fully connected layer with the Sigmoid activation.", "start": 2740.495, "duration": 2.73}, {"text": "And the last one is Sigmoid.", "start": 2743.225, "duration": 1.935}, {"text": "I, I think people have been trying a lot of putting,", "start": 2745.16, "duration": 3.75}, {"text": "activat- different activations in different neurons in a layer,", "start": 2748.91, "duration": 2.88}, {"text": "in different layers and the consensus was using one activation in", "start": 2751.79, "duration": 5.22}, {"text": "the layer and also using one of these three activations.", "start": 2757.01, "duration": 6.405}, {"text": "Yeah. So if someone comes up with a better activation that is", "start": 2763.415, "duration": 4.155}, {"text": "obviously helping training our models on different datasets,", "start": 2767.57, "duration": 4.695}, {"text": "people would adopt it but right now these are the ones that work better.", "start": 2772.265, "duration": 4.165}, {"text": "And you know, last time we talked about hyper-parameters a little bit.", "start": 2784.75, "duration": 4.12}, {"text": "These are all hyper-parameters.", "start": 2788.87, "duration": 1.515}, {"text": "So in practice, you're not going to choose these randomly,", "start": 2790.385, "duration": 2.49}, {"text": "you're going to try a bunch of them and choose some", "start": 2792.875, "duration": 2.685}, {"text": "of them that seem to help your model train.", "start": 2795.56, "duration": 2.88}, {"text": "There's a lot of experimental results in deep learning and we don't really", "start": 2798.44, "duration": 3.84}, {"text": "understand fully why certain activations work better than others.", "start": 2802.28, "duration": 4.78}, {"text": "Okay, let's move on.", "start": 2807.1, "duration": 2.17}, {"text": "[NOISE]", "start": 2809.27, "duration": 24.765}, {"text": "Okay, let's go over initialization techniques.", "start": 2834.035, "duration": 2.835}, {"text": "[NOISE]", "start": 2836.87, "duration": 19.26}, {"text": "Uh, actually, let me use this board.", "start": 2856.13, "duration": 3.675}, {"text": "So another trick that you can use", "start": 2859.805, "duration": 11.655}, {"text": "in order to help your network train", "start": 2871.46, "duration": 2.295}, {"text": "are initialization methods and normalization methods.", "start": 2873.755, "duration": 4.375}, {"text": "So, um, earlier we talked about the fact that if z is too big,", "start": 2883.12, "duration": 7.765}, {"text": "or z is too low in the negative numbers,", "start": 2890.885, "duration": 3.0}, {"text": "it will lead to saturation of the network.", "start": 2893.885, "duration": 2.28}, {"text": "So in order to avoid that you can use normalization of the input.", "start": 2896.165, "duration": 3.985}, {"text": "So assume that you have a network where the data is two-dimensional,", "start": 2906.25, "duration": 5.515}, {"text": "x_1, x_2 is our two-dimensional input.", "start": 2911.765, "duration": 4.165}, {"text": "You can assume that x_1, x_2", "start": 2919.0, "duration": 2.74}, {"text": "is distributed like this, let's say.", "start": 2921.74, "duration": 3.825}, {"text": "So this is if I plot x_1 against x_2 for a lot of data,", "start": 2925.565, "duration": 4.35}, {"text": "I will get that type of graph.", "start": 2929.915, "duration": 2.115}, {"text": "Uh, the problem is that if I do my wx plus b,", "start": 2932.03, "duration": 4.005}, {"text": "to compute my z_1,", "start": 2936.035, "duration": 1.41}, {"text": "if xs are very big,", "start": 2937.445, "duration": 1.515}, {"text": "it will lead to very big zs which will lead to saturated activations.", "start": 2938.96, "duration": 4.395}, {"text": "In order to avoid that, one method is to compute the mean of", "start": 2943.355, "duration": 6.615}, {"text": "this data using Mu equals 1", "start": 2949.97, "duration": 3.18}, {"text": "over the size of the batch of data that you have in the training sets.", "start": 2953.15, "duration": 4.33}, {"text": "Sum of xis.", "start": 2957.52, "duration": 2.59}, {"text": "So it's just giving you the mean for x_1,", "start": 2960.11, "duration": 3.96}, {"text": "and the mean for x_2.", "start": 2964.07, "duration": 2.62}, {"text": "You would compute the operation x equals x minus Mu,", "start": 2967.51, "duration": 4.645}, {"text": "and you will get that type of plot.", "start": 2972.155, "duration": 3.03}, {"text": "If you replot the transform data,", "start": 2975.185, "duration": 3.805}, {"text": "let's say x_1 tilde, x_2 tilde.", "start": 2980.49, "duration": 3.94}, {"text": "So here is a little better,", "start": 2984.43, "duration": 2.355}, {"text": "but it's still not good.", "start": 2986.785, "duration": 1.79}, {"text": "In order to solve the problem fully,", "start": 2988.575, "duration": 2.57}, {"text": "we are going to compute Sigma squared,", "start": 2991.145, "duration": 4.365}, {"text": "which is basically the standard deviation squared, so the variance of the data,", "start": 2995.51, "duration": 5.235}, {"text": "and then you will divide by, uh, Sigma squared.", "start": 3000.745, "duration": 5.005}, {"text": "So you would do that and you would make the transformation of", "start": 3017.64, "duration": 3.715}, {"text": "x being equal to x divided by Sigma,", "start": 3021.355, "duration": 4.05}, {"text": "and it will give you a graph that is", "start": 3025.405, "duration": 2.385}, {"text": "centered up here.", "start": 3027.79, "duration": 10.02}, {"text": "So you, you usually prefer to,", "start": 3037.81, "duration": 1.815}, {"text": "to work with a centered data. Yeah?", "start": 3039.625, "duration": 2.715}, {"text": "[inaudible] tilde?", "start": 3042.34, "duration": 1.12}, {"text": "Sorry, oh yeah, yeah,", "start": 3045.57, "duration": 2.53}, {"text": "sorry, sorry, yeah, correct.", "start": 3048.1, "duration": 2.26}, {"text": "So if we subtract the mean of x_1 and x_2,", "start": 3050.73, "duration": 3.4}, {"text": "it will be", "start": 3054.13, "duration": 0.6}, {"text": "[inaudible].", "start": 3054.73, "duration": 11.91}, {"text": "Sorry, it should look like this, but it would be centered.", "start": 3066.64, "duration": 2.92}, {"text": "Okay, and then if you stan- if you standardize it,", "start": 3073.02, "duration": 3.64}, {"text": "it looks like something like that.", "start": 3076.66, "duration": 1.605}, {"text": "So why is it better?", "start": 3078.265, "duration": 1.86}, {"text": "Because if you look at you- your loss function now,", "start": 3080.125, "duration": 3.435}, {"text": "before the loss function would look like something like this.", "start": 3083.56, "duration": 4.11}, {"text": "[NOISE] And after normalizing the inputs,", "start": 3087.67, "duration": 8.85}, {"text": "it may look like something, something like this.", "start": 3096.52, "duration": 5.115}, {"text": "So what's the difference between these two loss functions?", "start": 3101.635, "duration": 2.685}, {"text": "Why is this one easier to train?", "start": 3104.32, "duration": 1.74}, {"text": "It's because if you have the starting point that is here let's say,", "start": 3106.06, "duration": 4.02}, {"text": "their gradient descent algorithm is going to go to", "start": 3110.08, "duration": 2.865}, {"text": "towards approximately the steepest slope.", "start": 3112.945, "duration": 3.42}, {"text": "So we're going to go like there,", "start": 3116.365, "duration": 1.785}, {"text": "and then this one is going to go there,", "start": 3118.15, "duration": 1.935}, {"text": "and then you're going to go there,", "start": 3120.085, "duration": 1.74}, {"text": "and then you're going to go there like that and so on,", "start": 3121.825, "duration": 3.315}, {"text": "until you end up at the right points.", "start": 3125.14, "duration": 3.12}, {"text": "But the steeper slope in this loss contour is always pointing towards the middle.", "start": 3128.26, "duration": 5.67}, {"text": "So if you start somewhere,", "start": 3133.93, "duration": 1.59}, {"text": "it will directly go towards the minimum of your loss function.", "start": 3135.52, "duration": 5.1}, {"text": "So that's why it's helpful usually to normalize.", "start": 3140.62, "duration": 2.98}, {"text": "So this is one method, uh,", "start": 3144.54, "duration": 3.535}, {"text": "and in practice, the way you initialize your weights is very important. Yeah?", "start": 3148.075, "duration": 4.785}, {"text": "[BACKGROUND]", "start": 3152.86, "duration": 0.1}, {"text": "Uh, yes. So.", "start": 3152.96, "duration": 7.73}, {"text": "[BACKGROUND]", "start": 3160.69, "duration": 6.72}, {"text": "Exactly. So here I used a very simple case but you would divide elementwise by,", "start": 3167.41, "duration": 6.555}, {"text": "by the Sigma here, okay?", "start": 3173.965, "duration": 2.685}, {"text": "So like every entry of your matrix you would divide it by the Sigma.", "start": 3176.65, "duration": 4.2}, {"text": "One, one other thing that is important to notice.", "start": 3180.85, "duration": 2.64}, {"text": "This Sigma and Mu are computed over the training set.", "start": 3183.49, "duration": 3.495}, {"text": "You have a training set, you compute the mean of the training, set the standard deviation,", "start": 3186.985, "duration": 3.165}, {"text": "of the training set, and these Sigma and Mu have to be used on the test set as well.", "start": 3190.15, "duration": 4.635}, {"text": "It means now that you want to test your algorithm on the test set,", "start": 3194.785, "duration": 2.865}, {"text": "you should not compute the mean of the test set,", "start": 3197.65, "duration": 2.82}, {"text": "and the standard deviation of the test set and normalize", "start": 3200.47, "duration": 2.61}, {"text": "your test inputs through the network.", "start": 3203.08, "duration": 3.195}, {"text": "Instead, you should use the Mu and the Sigma that were computed on the train set", "start": 3206.275, "duration": 4.515}, {"text": "because your network is used to seeing this type of transformation as an input.", "start": 3210.79, "duration": 5.16}, {"text": "So you want the distribution of the inputs at the first neuron to be always the same,", "start": 3215.95, "duration": 5.175}, {"text": "no matter if it's a train or the test set.", "start": 3221.125, "duration": 2.115}, {"text": "What you do is that [inaudible]", "start": 3223.24, "duration": 6.15}, {"text": "Here? Likely, yeah.", "start": 3229.39, "duration": 1.8}, {"text": "This leads to fewer iterations.", "start": 3231.19, "duration": 2.86}, {"text": "Okay, we have a lot to see so I will,", "start": 3234.93, "duration": 3.355}, {"text": "I will skip a few questions.", "start": 3238.285, "duration": 2.665}, {"text": "So let's, let's delve a little more into vanishing and exploding gradients.", "start": 3242.04, "duration": 5.54}, {"text": "So in order to get an intuition of why we", "start": 3259.08, "duration": 2.92}, {"text": "have these vanishing or exploding gradient problem,", "start": 3262.0, "duration": 2.46}, {"text": "we can consider a network which is very, very", "start": 3264.46, "duration": 4.17}, {"text": "deep and has a two-dimensional input, okay?", "start": 3268.63, "duration": 8.95}, {"text": "And so on. So let's say we have,", "start": 3278.28, "duration": 3.355}, {"text": "let's say we have ten layers in total.", "start": 3281.635, "duration": 2.755}, {"text": "Ten layers plus an output layer.", "start": 3285.99, "duration": 3.86}, {"text": "So assume, assume all the activations are identity functions,", "start": 3290.28, "duration": 8.26}, {"text": "and assume that these biases are equal to 0.", "start": 3298.54, "duration": 4.26}, {"text": "If you compute y hats,", "start": 3302.8, "duration": 1.875}, {"text": "the output of the network with respect to the input.", "start": 3304.675, "duration": 5.175}, {"text": "You know that y hat will be equal to w of layer L,", "start": 3309.85, "duration": 4.545}, {"text": "capital L denotes the last layer,", "start": 3314.395, "duration": 2.595}, {"text": "times a l minus 1 plus bL,", "start": 3316.99, "duration": 5.73}, {"text": "but bL is 0 so we can remove it.", "start": 3322.72, "duration": 2.4}, {"text": "w_l times a_L minus 1.", "start": 3325.12, "duration": 1.875}, {"text": "You know that a_L minus 1 is w_l minus 1", "start": 3326.995, "duration": 6.375}, {"text": "times a_L minus 2 because the activation is an identity function and so on.", "start": 3333.37, "duration": 7.485}, {"text": "You can back propagate, you can go back and you will get that y hat equals", "start": 3340.855, "duration": 6.195}, {"text": "w_L times w_l minus 1 times blah, blah, blah, times w_1 times x.", "start": 3347.05, "duration": 9.825}, {"text": "You get something like that, right?", "start": 3356.875, "duration": 4.68}, {"text": "So now, let's consider two cases.", "start": 3361.555, "duration": 3.465}, {"text": "Let us consider the case where", "start": 3365.02, "duration": 2.295}, {"text": "the w_l matrices are a little bigger than the identity function,", "start": 3367.315, "duration": 6.84}, {"text": "a little larger than the identity function in terms of values.", "start": 3374.155, "duration": 3.255}, {"text": "Let's say w_l, including all these.", "start": 3377.41, "duration": 3.86}, {"text": "So all these matrices which are 2 by 2 matrices,", "start": 3381.27, "duration": 4.2}, {"text": "right, are these ones.", "start": 3385.47, "duration": 5.1}, {"text": "What's the consequence?", "start": 3390.57, "duration": 2.54}, {"text": "The consequences that this whole thing here is going to be equal to 1.5 to the power L,", "start": 3393.11, "duration": 9.71}, {"text": "1.5 to the power L, 0, 0.", "start": 3402.82, "duration": 4.06}, {"text": "It will make y hat explode.", "start": 3409.01, "duration": 3.52}, {"text": "It will make the value of y hat explode,", "start": 3412.53, "duration": 2.01}, {"text": "just because this number is a tiny little bit more than 1.", "start": 3414.54, "duration": 4.01}, {"text": "Same phenomenon, if we had 0.5 instead of 1.5 here, the value,", "start": 3418.55, "duration": 6.64}, {"text": "the multiplicative value of all these matrices will be 0.5 to the power L here,", "start": 3425.19, "duration": 5.445}, {"text": "0.5 to the power L here,", "start": 3430.635, "duration": 2.055}, {"text": "and y hat will always be very close to 0.", "start": 3432.69, "duration": 3.72}, {"text": "So you see, the issue with vanishing exploding gradients is", "start": 3436.41, "duration": 3.99}, {"text": "that all the errors add up like multiply each other.", "start": 3440.4, "duration": 3.78}, {"text": "And if you end up with numbers that are smaller than one,", "start": 3444.18, "duration": 3.495}, {"text": "you will get a totally vanished gradient.", "start": 3447.675, "duration": 2.96}, {"text": "When you go back, if you have", "start": 3450.635, "duration": 2.36}, {"text": "values that are a little bigger than 1 you will get exploding gradients.", "start": 3452.995, "duration": 3.48}, {"text": "So we did it as a forward propagation equation,", "start": 3456.475, "duration": 2.775}, {"text": "we could have done it exactly the same analysis.", "start": 3459.25, "duration": 2.58}, {"text": "We did derivatives, assuming the derivatives", "start": 3461.83, "duration": 4.44}, {"text": "of the weight matrices are a little lower than the identity,", "start": 3466.27, "duration": 4.56}, {"text": "or a little higher than the identity.", "start": 3470.83, "duration": 1.875}, {"text": "So we want to avoid that.", "start": 3472.705, "duration": 1.605}, {"text": "One way that is not perfect to,", "start": 3474.31, "duration": 2.115}, {"text": "to avoid this is to initialize your weights properly,", "start": 3476.425, "duration": 3.33}, {"text": "initialize them into the right range of values.", "start": 3479.755, "duration": 2.91}, {"text": "So you agree that we would prefer the weights to be around 1,", "start": 3482.665, "duration": 3.54}, {"text": "as close as possible to 1.", "start": 3486.205, "duration": 1.845}, {"text": "If they're very close to 1,", "start": 3488.05, "duration": 1.545}, {"text": "we probably can avoid the vanishing and exploding gradient problem.", "start": 3489.595, "duration": 3.805}, {"text": "So let's look at the initialization problem.", "start": 3495.06, "duration": 4.52}, {"text": "The first thing to look at is example of the one neuron.", "start": 3504.21, "duration": 4.15}, {"text": "[NOISE]", "start": 3508.36, "duration": 6.42}, {"text": "If you consider this neuron here,", "start": 3514.78, "duration": 3.04}, {"text": "which has a bunch of inputs and outputs and activation a.", "start": 3519.12, "duration": 8.41}, {"text": "[NOISE] You know that the equation inside the neuron is", "start": 3527.53, "duration": 5.64}, {"text": "a equals whatever function, let's say sigmoid of Z and you know", "start": 3533.17, "duration": 5.4}, {"text": "that z is equal to W_1 X_1 plus W_2 X_2 plus blah,", "start": 3538.57, "duration": 5.19}, {"text": "blah, blah plus W_n X_n.", "start": 3543.76, "duration": 2.835}, {"text": "So it is a dot product between the W's and the X's.", "start": 3546.595, "duration": 4.65}, {"text": "So the interesting thing to notice is that we have n terms here.", "start": 3551.245, "duration": 6.42}, {"text": "So in order for Z to not explode,", "start": 3557.665, "duration": 2.4}, {"text": "we would like all of these terms to be small.", "start": 3560.065, "duration": 3.825}, {"text": "If W's are too big,", "start": 3563.89, "duration": 2.535}, {"text": "then this term will explode with the size of the inputs of the layer.", "start": 3566.425, "duration": 4.845}, {"text": "So instead if we have a large n, it means the input is very large,", "start": 3571.27, "duration": 6.21}, {"text": "what we want is very small W_i's.", "start": 3577.48, "duration": 3.48}, {"text": "So the larger n, the smaller it has to be W_i.", "start": 3580.96, "duration": 4.095}, {"text": "So based on this intuition,", "start": 3585.055, "duration": 2.805}, {"text": "it seems that it would be a good idea to initialize", "start": 3587.86, "duration": 5.22}, {"text": "W_i's with something that is close to 1 over n. We have n terms,", "start": 3593.08, "duration": 7.065}, {"text": "the more terms we have, the more likely Z is going to be big.", "start": 3600.145, "duration": 3.45}, {"text": "But if our initialization says", "start": 3603.595, "duration": 2.415}, {"text": "the more terms you have, the smaller the value of the weights,", "start": 3606.01, "duration": 2.49}, {"text": "we should be able to keep Z in a certain range", "start": 3608.5, "duration": 2.445}, {"text": "that is appropriate to avoid vanishing and exploding gradients.", "start": 3610.945, "duration": 3.93}, {"text": "So this seems to be a possible initialization scheme.", "start": 3614.875, "duration": 5.035}, {"text": "So in practice, I'm going to write a few initialization schemes that we're not gonna prove.", "start": 3622.77, "duration": 5.455}, {"text": "If you're interested in seeing more proofs of that,", "start": 3628.225, "duration": 2.715}, {"text": "you can take CS230,", "start": 3630.94, "duration": 1.305}, {"text": "where we prove this initialization scheme.", "start": 3632.245, "duration": 2.995}, {"text": "May I take down the board?", "start": 3641.52, "duration": 3.05}, {"text": "So there are a few initializations that are commonly used and again, this is,", "start": 3648.63, "duration": 5.17}, {"text": "this is very practical and people have been testing a lot of initializations,", "start": 3653.8, "duration": 3.885}, {"text": "but they ended up using those.", "start": 3657.685, "duration": 1.665}, {"text": "[NOISE] So one is to initialize the weights.", "start": 3659.35, "duration": 4.41}, {"text": "I'm writing the code for those of you who know numPy.", "start": 3663.76, "duration": 3.66}, {"text": "I'm not gonna compile it here.", "start": 3667.42, "duration": 3.605}, {"text": "With whatever shape you are using,", "start": 3671.025, "duration": 3.12}, {"text": "elementwise times the square root", "start": 3674.145, "duration": 3.895}, {"text": "of 1 over n of L minus 1.", "start": 3681.06, "duration": 5.15}, {"text": "So what does that mean? It means that I will look at the number of inputs.", "start": 3687.21, "duration": 4.24}, {"text": "I'm writing an L minus 1 here, n to the L minus 1.", "start": 3691.45, "duration": 4.815}, {"text": "I'm looking at how many inputs are coming to my layer", "start": 3696.265, "duration": 3.405}, {"text": "assuming we're at layer L. How many inputs are coming.", "start": 3699.67, "duration": 4.005}, {"text": "I'm going to initialize the weights of", "start": 3703.675, "duration": 3.165}, {"text": "this layer proportionally to the number of inputs that are coming in.", "start": 3706.84, "duration": 4.65}, {"text": "So the intuition is very similar to what we described there.", "start": 3711.49, "duration": 2.985}, {"text": "So this initialization has been shown to work very well for sigmoid activations.", "start": 3714.475, "duration": 5.115}, {"text": "So if you use sigmoid.", "start": 3719.59, "duration": 2.68}, {"text": "What's interesting is if you use ReLU, it's been,", "start": 3727.02, "duration": 4.375}, {"text": "it's been observed that putting a 2 here", "start": 3731.395, "duration": 2.745}, {"text": "instead of a 1 would make the network train better.", "start": 3734.14, "duration": 3.66}, {"text": "And again, it's very practical.", "start": 3737.8, "duration": 3.24}, {"text": "It's one of the fields that,", "start": 3741.04, "duration": 1.98}, {"text": "that we need more theory on it,", "start": 3743.02, "duration": 2.835}, {"text": "but a lot of observations had been made so far.", "start": 3745.855, "duration": 3.865}, {"text": "Do you guys want to just do that as a project to see", "start": 3750.75, "duration": 3.46}, {"text": "why is this happening? It would be interesting.", "start": 3754.21, "duration": 3.76}, {"text": "Okay. [NOISE] And finally,", "start": 3758.4, "duration": 3.58}, {"text": "there is a more common one that is used which is called the Xavier initialization,", "start": 3761.98, "duration": 7.21}, {"text": "which proposes to update the weights [NOISE] using,", "start": 3774.69, "duration": 6.759}, {"text": "uh, square root of 1 over n_ l minus 1 for tan h. This is another one.", "start": 3781.449, "duration": 9.526}, {"text": "And another one that is I believe called Glorot initialization", "start": 3790.975, "duration": 4.795}, {"text": "recommends to initialize the weights of a layer using the following formula.", "start": 3798.87, "duration": 10.7}, {"text": "So quickly, the, the quick int- intuition behind the last one.", "start": 3811.77, "duration": 4.48}, {"text": "The last one is, is very often used.", "start": 3816.25, "duration": 2.55}, {"text": "The quick intuition is that we're doing", "start": 3818.8, "duration": 2.19}, {"text": "the same thing but also for the backpropagated gradients.", "start": 3820.99, "duration": 3.27}, {"text": "So we're saying the weights are going to multiply the backpropagated gradients.", "start": 3824.26, "duration": 4.02}, {"text": "So we also need to look at,", "start": 3828.28, "duration": 1.395}, {"text": "at how many inputs do we have during the backpropagation.", "start": 3829.675, "duration": 3.555}, {"text": "And L is the number of inputs you have during backpropagation", "start": 3833.23, "duration": 3.045}, {"text": "and L minus 1 is the number of inputs you have during forward propagation.", "start": 3836.275, "duration": 3.27}, {"text": "So taking an average,", "start": 3839.545, "duration": 1.38}, {"text": "a geometric average of those.", "start": 3840.925, "duration": 1.365}, {"text": "[NOISE]", "start": 3842.29, "duration": 15.48}, {"text": "And the reason we have a random function here is because", "start": 3857.77, "duration": 2.835}, {"text": "if you don't initialize your weights randomly,", "start": 3860.605, "duration": 3.21}, {"text": "you will end up with some problem called the symmetry", "start": 3863.815, "duration": 3.015}, {"text": "problem where every neuron is going to learn kind of the same thing.", "start": 3866.83, "duration": 3.66}, {"text": "To avoid that, you will make the neuron starts at different places and let", "start": 3870.49, "duration": 3.87}, {"text": "them evolve independently from each other as much as possible.", "start": 3874.36, "duration": 4.5}, {"text": "So now we have two choices.", "start": 3878.86, "duration": 1.8}, {"text": "Either we go over regularization or optimization.", "start": 3880.66, "duration": 3.345}, {"text": "How much have you talked about regularization so far L1,", "start": 3884.005, "duration": 3.225}, {"text": "L2, early stopping, all that?", "start": 3887.23, "duration": 3.3}, {"text": "Early stopping, everybody remembers what it is?", "start": 3890.53, "duration": 2.28}, {"text": "No? Little bit?", "start": 3892.81, "duration": 1.755}, {"text": "So let's go over optimization, I guess,", "start": 3894.565, "duration": 2.01}, {"text": "and then we will do some regularization depending on the time we have.", "start": 3896.575, "duration": 3.285}, {"text": "[NOISE]", "start": 3899.86, "duration": 10.02}, {"text": "So I believe", "start": 3909.88, "duration": 0.54}, {"text": "so far you've seen", "start": 3910.42, "duration": 1.215}, {"text": "gradient descent and stochastic gradient descent as two possible optimization algorithms.", "start": 3911.635, "duration": 5.13}, {"text": "In practice, there is a trade-off", "start": 3916.765, "duration": 2.145}, {"text": "between these two which is called mini-batch gradient descent.", "start": 3918.91, "duration": 2.865}, {"text": "What is the trade-off?", "start": 3921.775, "duration": 1.65}, {"text": "The trade-off is that batch gradient descent is cool because you can use vectorization,", "start": 3923.425, "duration": 6.21}, {"text": "you can give a batch inputs, forward", "start": 3929.635, "duration": 2.025}, {"text": "propagate it all at once doing vec- using a vectorized code.", "start": 3931.66, "duration": 4.2}, {"text": "Stochastic gradient descent's advantage is that the updates are very quick.", "start": 3935.86, "duration": 4.35}, {"text": "And imagine that you have a dataset with one million images.", "start": 3940.21, "duration": 3.69}, {"text": "One million images in the dataset and you wanna do batch gradient descent.", "start": 3943.9, "duration": 4.08}, {"text": "Do you know how long it's going to take to do one update? Very long.", "start": 3947.98, "duration": 4.29}, {"text": "So we don't want that because maybe we don't need to go", "start": 3952.27, "duration": 2.34}, {"text": "over the full dataset in order to have a good update.", "start": 3954.61, "duration": 2.715}, {"text": "Maybe the updates based on 1,000 examples", "start": 3957.325, "duration": 3.195}, {"text": "might already give us the right direction for the gradient [NOISE] of where to go.", "start": 3960.52, "duration": 3.36}, {"text": "It's not gonna be as good as on", "start": 3963.88, "duration": 1.26}, {"text": "the median example where it's going to be a very good approximation.", "start": 3965.14, "duration": 3.045}, {"text": "So that's why most people would use mini-batch gradient descent,", "start": 3968.185, "duration": 3.75}, {"text": "where you have a trade-off between stochasticity and also vectorization.", "start": 3971.935, "duration": 5.755}, {"text": "So in terms of notation,", "start": 3978.87, "duration": 2.26}, {"text": "[NOISE] I'm going to call X the matrix x_1,", "start": 3981.13, "duration": 7.71}, {"text": "x_2, x_m, and capital Y the same matrix with y_m.", "start": 3988.84, "duration": 9.285}, {"text": "So we have m training examples.", "start": 3998.125, "duration": 2.94}, {"text": "And I'm going to split these into batches.", "start": 4001.065, "duration": 3.72}, {"text": "So I'm going to call the first batch x_1 like", "start": 4004.785, "duration": 5.445}, {"text": "this until x maybe T like that.", "start": 4010.23, "duration": 5.955}, {"text": "And x_1 can contain probably x_1 until x_1,000.", "start": 4016.185, "duration": 6.12}, {"text": "Assuming it's a batch of 1,000 examples.", "start": 4022.305, "duration": 2.88}, {"text": "X_2 then will contain x_1,001 until x_2,000 and so on.", "start": 4025.185, "duration": 6.06}, {"text": "So this is the notation for the batch when I use curly brackets.", "start": 4031.245, "duration": 4.56}, {"text": "Same for Y. [NOISE]", "start": 4035.805, "duration": 17.66}, {"text": "So in terms of algorithm,", "start": 4053.465, "duration": 2.845}, {"text": "how does the Mini-batch gradient descent algorithm work?", "start": 4057.01, "duration": 5.33}, {"text": "We're going to iterate. So for iteration t from 1 to blah, blah, blah,", "start": 4064.06, "duration": 6.775}, {"text": "to how many iteration you wanna do.", "start": 4070.835, "duration": 2.01}, {"text": "We're going to select a batch,", "start": 4072.845, "duration": 2.725}, {"text": "select a batch of x_t- x_t, y_t.", "start": 4084.61, "duration": 4.345}, {"text": "You will forward propagate the batch,", "start": 4088.955, "duration": 3.115}, {"text": "and you will backpropagate the batch.", "start": 4093.97, "duration": 3.56}, {"text": "So by forward propagation, I mean,", "start": 4101.89, "duration": 2.83}, {"text": "you send all the batch to", "start": 4104.72, "duration": 1.23}, {"text": "the network and you compute the loss functions for every example of the batch,", "start": 4105.95, "duration": 4.89}, {"text": "you sum them together and you compute the cost function over the entire batch,", "start": 4110.84, "duration": 3.63}, {"text": "which is the average of the loss functions.", "start": 4114.47, "duration": 2.65}, {"text": "And so assuming- assuming the batch is of size 1,000,", "start": 4119.61, "duration": 7.015}, {"text": "this would be the- the formula to compute the batch over 1,000 examples.", "start": 4126.625, "duration": 10.475}, {"text": "And after the backpropagation, of course,", "start": 4141.1, "duration": 3.22}, {"text": "updates, W_l and D_l for all the l's, for all the layers.", "start": 4144.32, "duration": 8.61}, {"text": "This is the- the equation.", "start": 4152.93, "duration": 3.01}, {"text": "So in terms of graph,", "start": 4170.14, "duration": 3.23}, {"text": "what you're likely to see is that for batch gradient descent,", "start": 4176.26, "duration": 5.905}, {"text": "your cost function j would have looked like that,", "start": 4182.165, "duration": 4.095}, {"text": "if you plot it against the number of iterations.", "start": 4186.26, "duration": 3.76}, {"text": "On the other hand, if you use a Mini-batch gradient descent,", "start": 4191.01, "duration": 4.525}, {"text": "you're most likely to see something like this.", "start": 4195.535, "duration": 2.84}, {"text": "So it is also decreasing as a trend,", "start": 4198.375, "duration": 2.72}, {"text": "but because the gradient is approximated and doesn't necessarily", "start": 4201.095, "duration": 4.035}, {"text": "go straight to the- to the middle of", "start": 4205.13, "duration": 2.1}, {"text": "your loss fun- to the lower point of the loss function,", "start": 4207.23, "duration": 2.37}, {"text": "you will see a kind of graph like that.", "start": 4209.6, "duration": 2.535}, {"text": "The smaller the batch, the more stochasticity.", "start": 4212.135, "duration": 3.03}, {"text": "So the more noise you will have on your cost function graph.", "start": 4215.165, "duration": 4.615}, {"text": "And of course, if you- if we plot", "start": 4227.53, "duration": 2.83}, {"text": "again- if we plot the loss function and this was gradient descent,", "start": 4230.36, "duration": 7.155}, {"text": "so this is the top view of the loss function,", "start": 4237.515, "duration": 2.145}, {"text": "assuming we're in two dimensions.", "start": 4239.66, "duration": 2.145}, {"text": "Your stochastic gradient descent or batch gradient descent would do something like that.", "start": 4241.805, "duration": 6.025}, {"text": "So the difference is- there seem to be less iteration with the red algorithm,", "start": 4249.82, "duration": 6.205}, {"text": "but the iterations are much heavier to compute.", "start": 4256.025, "duration": 3.555}, {"text": "So each of the green iterations are going to be very- very- very quick,", "start": 4259.58, "duration": 4.41}, {"text": "while the red ones are going to be slow to compute. This is a trade off.", "start": 4263.99, "duration": 6.25}, {"text": "Now there is another algorithm that I wanna go over which is called", "start": 4271.0, "duration": 5.02}, {"text": "the momentum- momentum algorithm.", "start": 4276.02, "duration": 7.06}, {"text": "Sometimes called gradient descent plus momentum algorithm.", "start": 4284.98, "duration": 5.51}, {"text": "So what's the intuition behind momentum?", "start": 4291.31, "duration": 3.77}, {"text": "The intuition is, let's look at this loss contour plot.", "start": 4298.0, "duration": 8.0}, {"text": "And I'm doing an extreme case just to illustrate the intuition.", "start": 4309.1, "duration": 5.03}, {"text": "Assume you have the loss that is very extended in one direction.", "start": 4316.65, "duration": 5.875}, {"text": "So this direction is very extended and the other one is smaller.", "start": 4322.525, "duration": 6.025}, {"text": "You're starting at a point like this one.", "start": 4328.55, "duration": 3.66}, {"text": "Your gradient descent algorithm itself is going to follow the falling bar,", "start": 4332.21, "duration": 4.245}, {"text": "it's going to be orthogonal to the current contour,", "start": 4336.455, "duration": 4.65}, {"text": "uh, iso- iso term.", "start": 4341.105, "duration": 1.395}, {"text": "Contour loss is going to go there,", "start": 4342.5, "duration": 2.04}, {"text": "and then there, and then there,", "start": 4344.54, "duration": 2.025}, {"text": "and then there, and so on.", "start": 4346.565, "duration": 2.335}, {"text": "So what you would like is to move it faster", "start": 4350.34, "duration": 4.39}, {"text": "on the horizontal line and slower to the vertical- on the vertical side.", "start": 4354.73, "duration": 4.51}, {"text": "So on this axis you would like to move with smaller updates.", "start": 4359.83, "duration": 6.43}, {"text": "And on this axis,", "start": 4366.26, "duration": 1.44}, {"text": "you wanna move with larger updates, correct?", "start": 4367.7, "duration": 3.48}, {"text": "If this happened, we would probably end up", "start": 4371.18, "duration": 2.67}, {"text": "in the minimum much quicker than we currently are.", "start": 4373.85, "duration": 3.31}, {"text": "So in order to do that, we're going to use a technique called momentum,", "start": 4377.16, "duration": 3.415}, {"text": "which is going to look at the past gradients.", "start": 4380.575, "duration": 2.715}, {"text": "So look at the past updates. Assume we're here.", "start": 4383.29, "duration": 4.03}, {"text": "Assume we are somewhere here.", "start": 4387.67, "duration": 2.77}, {"text": "Gradient descent doesn't look at its past at all.", "start": 4390.44, "duration": 3.615}, {"text": "You just will compute the forward propagation,", "start": 4394.055, "duration": 2.085}, {"text": "compute the backdrop, look at the direction and go to that direction.", "start": 4396.14, "duration": 3.24}, {"text": "What momentum is going to say is look at the past updates that you did", "start": 4399.38, "duration": 3.585}, {"text": "and try to consider these past updates in order to find the right way to go.", "start": 4402.965, "duration": 4.635}, {"text": "So if you look at the past update and you take an average of the past update.", "start": 4407.6, "duration": 4.08}, {"text": "You would take an average of these update going up and the update after it going down.", "start": 4411.68, "duration": 5.295}, {"text": "The average on the vertical side is going to be small,", "start": 4416.975, "duration": 2.715}, {"text": "because one went up, one went down.", "start": 4419.69, "duration": 2.205}, {"text": "But on the horizontal axis,", "start": 4421.895, "duration": 2.49}, {"text": "both went to the same direction.", "start": 4424.385, "duration": 2.46}, {"text": "So the update will not change too much on the vert- on- on this axis.", "start": 4426.845, "duration": 4.755}, {"text": "So you're most likely to do something like that if you use momentum.", "start": 4431.6, "duration": 10.35}, {"text": "Does it make sense the intuition behind it?", "start": 4441.95, "duration": 3.19}, {"text": "So that's the intuition why we want to use momentum.", "start": 4445.75, "duration": 3.28}, {"text": "And for those of you who do physics,", "start": 4449.03, "duration": 2.28}, {"text": "sometimes you can think of momentum as friction.", "start": 4451.31, "duration": 2.73}, {"text": "You know like- like if you- if you launch a rocket and you wanna move it quickly around.", "start": 4454.04, "duration": 6.705}, {"text": "It's not gonna move, because the rocket has a certain weight and has a certain momentum.", "start": 4460.745, "duration": 3.57}, {"text": "You cannot change its direction very, very noisily.", "start": 4464.315, "duration": 3.615}, {"text": "[NOISE]", "start": 4467.93, "duration": 10.77}, {"text": "So let's see", "start": 4478.7, "duration": 0.54}, {"text": "the implementation of- of- of momentum gradient descent.", "start": 4479.24, "duration": 5.17}, {"text": "Oh, and I believe we- we're almost done, right?", "start": 4484.69, "duration": 3.175}, {"text": "Yeah. Okay. [NOISE] So let's look at the- the implementation quickly.", "start": 4487.865, "duration": 4.89}, {"text": "So gradient descent was w equals w minus Alpha,", "start": 4492.755, "duration": 4.68}, {"text": "derivative of the loss with respect to w. What", "start": 4497.435, "duration": 3.185}, {"text": "we are going to do is we're going to use another variable called velocity,", "start": 4500.62, "duration": 3.94}, {"text": "which is going to be the average of the previous velocity and the current weight updates.", "start": 4504.56, "duration": 9.19}, {"text": "So we're going to use that,", "start": 4518.77, "duration": 2.245}, {"text": "and instead of the updates being the derivative directly,", "start": 4521.015, "duration": 3.225}, {"text": "we're going to update the velocity.", "start": 4524.24, "duration": 1.935}, {"text": "So the velocity is going to be a variable that tracks the direction that we should", "start": 4526.175, "duration": 6.345}, {"text": "take regarding the current update and also", "start": 4532.52, "duration": 4.05}, {"text": "the past updates with a factor Beta that is be- going to be the weights.", "start": 4536.57, "duration": 5.41}, {"text": "The interesting point is that in terms of implementation it's one more line of code,", "start": 4542.38, "duration": 5.17}, {"text": "in terms of memory,", "start": 4547.55, "duration": 1.23}, {"text": "it's just one additional variable,", "start": 4548.78, "duration": 1.98}, {"text": "and it actually has a big impact on the optimization.", "start": 4550.76, "duration": 2.805}, {"text": "There are much more optimization algorithms that we're not going to see together today.", "start": 4553.565, "duration": 5.115}, {"text": "In CS230, we teach something called RMSProp and Atom.", "start": 4558.68, "duration": 4.065}, {"text": "That are most likely the- the- the ones that are used the most in deep learning.", "start": 4562.745, "duration": 6.18}, {"text": "Uh, and the reason is, uh,", "start": 4568.925, "duration": 2.325}, {"text": "if you come up with an optimization algorithm,", "start": 4571.25, "duration": 2.115}, {"text": "you still have to prove that it works very well on the wide variety of", "start": 4573.365, "duration": 3.345}, {"text": "application between- before researchers adopt it for their research.", "start": 4576.71, "duration": 4.77}, {"text": "So Atom brings momentum to the deep learning optimization algorithms.", "start": 4581.48, "duration": 6.0}, {"text": "Okay. Thanks guys.", "start": 4587.48, "duration": 1.92}, {"text": "Uh, and that's all for deep learning in CS229 so far.", "start": 4589.4, "duration": 4.27}]