[{"text": "The following content is\nprovided under a Creative", "start": 0.06, "duration": 2.44}, {"text": "Commons license.", "start": 2.5, "duration": 1.519}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 4.019, "duration": 2.341}, {"text": "continue to offer high quality\neducational resources for free.", "start": 6.36, "duration": 4.37}, {"text": "To make a donation or\nview additional materials", "start": 10.73, "duration": 2.6}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 13.33, "duration": 3.887}, {"text": "at ocw.mit.edu.", "start": 17.217, "duration": 0.625}, {"text": "PROFESSOR: All right.", "start": 21.546, "duration": 0.984}, {"text": "I want to complete\nthe discussion", "start": 22.53, "duration": 2.119}, {"text": "on volatility modeling in\nthe first part of the lecture", "start": 24.649, "duration": 2.291}, {"text": "today.", "start": 26.94, "duration": 0.93}, {"text": "And last time we\naddressed the definition", "start": 27.87, "duration": 5.08}, {"text": "of ARCH models, which allow\nfor time-varying volatility", "start": 32.95, "duration": 6.47}, {"text": "in modeling the returns of\na financial time series.", "start": 39.42, "duration": 3.52}, {"text": "And we were looking\nlast time at modeling", "start": 42.94, "duration": 2.1}, {"text": "the euro-dollar\nexchange rate returns.", "start": 45.04, "duration": 3.73}, {"text": "And we went through fitting\nARCH models to those returns,", "start": 48.77, "duration": 6.2}, {"text": "and also looked at fitting the\nGARCH model to those returns.", "start": 54.97, "duration": 4.91}, {"text": "And to recap, the GARCH model\nextends upon the ARCH model", "start": 59.88, "duration": 8.1}, {"text": "by adding some extra terms.", "start": 67.98, "duration": 2.13}, {"text": "So if you look at this\nexpression for the GARCH model,", "start": 70.11, "duration": 2.78}, {"text": "the first two terms for the\ntime-varying volatility sigma", "start": 72.89, "duration": 3.06}, {"text": "squared t is a linear\ncombination of the past sort", "start": 75.95, "duration": 4.865}, {"text": "of residual returns squared.", "start": 80.815, "duration": 2.935}, {"text": "That's the ARCH\nmodel, p of those.", "start": 83.75, "duration": 1.94}, {"text": "So the current\nvolatility depends", "start": 85.69, "duration": 2.06}, {"text": "upon what's happened in\nexcess returns over the last p", "start": 87.75, "duration": 2.77}, {"text": "periods.", "start": 90.52, "duration": 0.94}, {"text": "But then we add an\nextra term, which", "start": 91.46, "duration": 2.5}, {"text": "is corresponds to q levels\nof the previous volatility.", "start": 93.96, "duration": 5.42}, {"text": "And so what we're\ndoing with GARCH", "start": 99.38, "duration": 3.29}, {"text": "models is adding extra\nparameters to the ARCH,", "start": 102.67, "duration": 3.48}, {"text": "but an advantage of considering\nthese extra parameters which", "start": 106.15, "duration": 3.69}, {"text": "relate basically the current\nvolatility sigma squared t", "start": 109.84, "duration": 3.6}, {"text": "with the previous or lagged\nvalue sigma squared t", "start": 113.44, "duration": 2.84}, {"text": "minus j for lags\nj is that we may", "start": 116.28, "duration": 2.66}, {"text": "be able to have a model\nwith many fewer parameters.", "start": 118.94, "duration": 3.97}, {"text": "So indeed, if we fit these\nmodels to the exchange rate", "start": 122.91, "duration": 7.02}, {"text": "returns, what we found last\ntime-- let me go through", "start": 129.93, "duration": 5.74}, {"text": "and show that--\nwas-- basically here", "start": 135.67, "duration": 4.3}, {"text": "are various fits of the\nthree cases of ARCH models.", "start": 139.97, "duration": 5.92}, {"text": "ARCH orders 1, 2, and\n10, thinking we maybe", "start": 145.89, "duration": 3.32}, {"text": "need many lags to\nfit volatility.", "start": 149.21, "duration": 2.55}, {"text": "And then the GARCH model 1,1,\nwhere we only have one ARCH", "start": 151.76, "duration": 3.89}, {"text": "term and one GARCH term.", "start": 155.65, "duration": 3.24}, {"text": "And so basically the green line,\nor rather the blue line in this", "start": 158.89, "duration": 6.22}, {"text": "graph, shows the plot of\nthe fitted GARCH(1,1) model", "start": 165.11, "duration": 3.79}, {"text": "as compared with\nthe ARCH models.", "start": 168.9, "duration": 2.64}, {"text": "Now, in looking\nat this graph, one", "start": 171.54, "duration": 4.42}, {"text": "can actually see\nsome features of how", "start": 175.96, "duration": 3.02}, {"text": "these models are fitting\nvolatility, which", "start": 178.98, "duration": 2.26}, {"text": "is important to understand.", "start": 181.24, "duration": 2.85}, {"text": "One is that the ARCH\nmodels have a hard lower", "start": 184.09, "duration": 3.73}, {"text": "bound on the volatility.", "start": 187.82, "duration": 3.13}, {"text": "Basically there's\na constant term", "start": 190.95, "duration": 3.08}, {"text": "in the volatility equation.", "start": 194.03, "duration": 2.04}, {"text": "And because the additional terms\nare squared excess returns,", "start": 196.07, "duration": 6.34}, {"text": "it-- basically, the volatility\ndoes have the lower bound", "start": 202.41, "duration": 2.56}, {"text": "of that intercept.", "start": 204.97, "duration": 0.83}, {"text": "So depending on what range\nyou fit the data over,", "start": 205.8, "duration": 3.43}, {"text": "that lower bound is\ngoing to be defined by--", "start": 209.23, "duration": 3.96}, {"text": "or it will be determined by\nthe data you're fitting to.", "start": 213.19, "duration": 3.89}, {"text": "As you increase the ARCH\norder, you basically", "start": 217.08, "duration": 4.62}, {"text": "allow for a greater range of--\nor a lower lower bound of that.", "start": 221.7, "duration": 5.24}, {"text": "And with the GARCH\nmodel you can see", "start": 226.94, "duration": 1.65}, {"text": "that this blue line\nis actually predicting", "start": 228.59, "duration": 3.56}, {"text": "very different\nlevels of volatility", "start": 232.15, "duration": 2.88}, {"text": "over the entire\nrange of the series.", "start": 235.03, "duration": 2.27}, {"text": "So it really is\nmuch more flexible.", "start": 237.3, "duration": 2.01}, {"text": "Now-- and in these fits, we are\nassuming Gaussian distributions", "start": 242.5, "duration": 5.08}, {"text": "for the innovations\nin the return series.", "start": 247.58, "duration": 3.56}, {"text": "We'll soon pursue looking\nat alternatives to that,", "start": 251.14, "duration": 3.86}, {"text": "but let me talk just\na little bit more", "start": 255.0, "duration": 3.1}, {"text": "about the GARCH model going\nback to lecture notes here.", "start": 258.1, "duration": 5.88}, {"text": "So let me expand this.", "start": 263.98, "duration": 2.315}, {"text": "OK.", "start": 271.15, "duration": 0.5}, {"text": "So there's the specification.", "start": 271.65, "duration": 1.208}, {"text": "The GARCH(1,1) model.", "start": 272.858, "duration": 2.142}, {"text": "One thing to note is that this\nGARCH(1,1) model does relate", "start": 275.0, "duration": 8.0}, {"text": "to an ARMA, an autoregressive\nmoving average process", "start": 283.0, "duration": 4.06}, {"text": "in the squared residuals.", "start": 287.06, "duration": 2.35}, {"text": "So if we look at the top\nline, which is the equation", "start": 289.41, "duration": 4.6}, {"text": "for the GARCH(1,1) model,\nconsider eliminating sigma", "start": 294.01, "duration": 6.19}, {"text": "squared t by using a new\ninnovation term, little u_t,", "start": 300.2, "duration": 8.89}, {"text": "which is the difference\nbetween the squared residual", "start": 309.09, "duration": 2.6}, {"text": "and the true volatility\ngiven by the model.", "start": 311.69, "duration": 3.37}, {"text": "So if you plug in the difference\nbetween our squared excess", "start": 315.06, "duration": 4.85}, {"text": "return and the\ncurrent volatility,", "start": 319.91, "duration": 2.77}, {"text": "that should have mean\n0 because sigma squared", "start": 322.68, "duration": 5.03}, {"text": "t, the t-th volatility squared,\nis equal to the square--", "start": 327.71, "duration": 5.65}, {"text": "or is equal to the expectation\nof the squared excess residual", "start": 333.36, "duration": 4.71}, {"text": "return, epsilon_t squared.", "start": 338.07, "duration": 2.37}, {"text": "So if we plug that\nin, we basically", "start": 340.44, "duration": 2.21}, {"text": "get an ARMA model for\nthe squared residuals.", "start": 342.65, "duration": 4.68}, {"text": "And so epsilon_t squared\nis alpha_0 plus alpha_1", "start": 347.33, "duration": 3.42}, {"text": "plus beta_1 the squared t minus\n1 lag plus u_t minus beta_1", "start": 350.75, "duration": 5.65}, {"text": "u_t.", "start": 356.4, "duration": 0.9}, {"text": "And so what this implies is an\nARMA(1,1 model with white noise", "start": 357.3, "duration": 7.5}, {"text": "that has mean 0 and variance\n2 sigma to the fourth.", "start": 364.8, "duration": 3.94}, {"text": "Just plugging things in.", "start": 368.74, "duration": 2.41}, {"text": "And through our\nknowledge, understanding,", "start": 371.15, "duration": 4.23}, {"text": "of univariate time series\nmodels, ARMA models,", "start": 375.38, "duration": 3.74}, {"text": "we can express this ARMA model\nfor the squared residuals", "start": 379.12, "duration": 4.25}, {"text": "as basically a polynomial\nlag of the squared residuals", "start": 383.37, "duration": 4.29}, {"text": "is equal to a polynomial\nlag of the innovations.", "start": 387.66, "duration": 5.03}, {"text": "And so we have this expression\nfor what the innovations are.", "start": 392.69, "duration": 5.64}, {"text": "And it's required that the\nroots of this a of L operator,", "start": 398.33, "duration": 8.69}, {"text": "when it thought of\non the complex plane,", "start": 407.02, "duration": 2.8}, {"text": "have roots outside\nthe unit circle, which", "start": 409.82, "duration": 3.28}, {"text": "corresponds to alpha_1 plus\nbeta_1 being less than 1", "start": 413.1, "duration": 3.27}, {"text": "in magnitude.", "start": 416.37, "duration": 1.32}, {"text": "So in order for these\nvolatility models", "start": 417.69, "duration": 3.33}, {"text": "not to blow up and be\nstationary, covariance", "start": 421.02, "duration": 3.76}, {"text": "stationary, we have these\nbounds on the parameters.", "start": 424.78, "duration": 3.7}, {"text": "OK, let's look at the\nunconditional volatility", "start": 434.79, "duration": 4.54}, {"text": "or long-run variance\nof the GARCH model.", "start": 439.33, "duration": 3.93}, {"text": "If you take expectations on\nboth sides of the GARCH model", "start": 443.26, "duration": 5.93}, {"text": "equation, you basically have\nthe expectation of sigma", "start": 449.19, "duration": 5.08}, {"text": "squared sub t--\nin the long run is", "start": 454.27, "duration": 2.56}, {"text": "sigma star squared-- is alpha_0\nplus alpha_1 plus beta_1 sigma", "start": 456.83, "duration": 3.92}, {"text": "star squared.", "start": 460.75, "duration": 2.88}, {"text": "So that sigma star\nsquared there is", "start": 463.63, "duration": 1.92}, {"text": "the expectation of the t\nminus 1 volatility squared", "start": 465.55, "duration": 12.19}, {"text": "in the limit.", "start": 477.74, "duration": 0.781}, {"text": "And then you can\njust solve for this", "start": 478.521, "duration": 1.499}, {"text": "and see that sigma star\nsquared is equal alpha_0 over 1", "start": 480.02, "duration": 2.47}, {"text": "minus alpha_1 minus beta_1.", "start": 482.49, "duration": 2.56}, {"text": "And in terms of the stationarity\nconditions for the process,", "start": 485.05, "duration": 5.13}, {"text": "if the long-run variance, in\norder for that to be finite,", "start": 490.18, "duration": 4.87}, {"text": "you need alpha_1 plus beta_1\nto be less than 1 in magnitude.", "start": 495.05, "duration": 5.45}, {"text": "And if you consider the\ngeneral GARCH(p,1) model,", "start": 500.5, "duration": 6.21}, {"text": "then the same argument leads to\na long-run variance being equal", "start": 506.71, "duration": 4.75}, {"text": "to alpha_0, the sort of\nintercept term in the GARCH", "start": 511.46, "duration": 4.03}, {"text": "model, divided by 1 minus the\nsum of all the parameters.", "start": 515.49, "duration": 4.54}, {"text": "So these GARCH models\nlead to constraints", "start": 520.03, "duration": 3.71}, {"text": "on the parameters that are\nimportant to incorporate", "start": 523.74, "duration": 6.32}, {"text": "when we're doing any estimation\nof these underlying parameters.", "start": 530.06, "duration": 4.81}, {"text": "And it does complicate\nthings, actually.", "start": 534.87, "duration": 1.78}, {"text": "So with maximum\nlikelihood estimation,", "start": 539.46, "duration": 5.36}, {"text": "the routine for maximum\nlikelihood estimation", "start": 544.82, "duration": 2.45}, {"text": "is the same for all models.", "start": 547.27, "duration": 1.57}, {"text": "We basically want to\ndetermine the likelihood", "start": 548.84, "duration": 2.25}, {"text": "function of our data given\nthe unknown parameters.", "start": 551.09, "duration": 3.34}, {"text": "And the likelihood function\nis the probability density", "start": 554.43, "duration": 2.66}, {"text": "function of the data\nconditional on the parameters.", "start": 557.09, "duration": 4.29}, {"text": "So our likelihood\nfunction as a function", "start": 561.38, "duration": 1.9}, {"text": "of the unknown parameters\nc, alpha, and beta", "start": 563.28, "duration": 3.19}, {"text": "is the value of the probability\ndensity, the joint density", "start": 566.47, "duration": 3.92}, {"text": "of all the data conditional\non those parameters.", "start": 570.39, "duration": 4.14}, {"text": "And that joint\ndensity function can", "start": 574.53, "duration": 3.29}, {"text": "be expressed as the product\nof successive conditional", "start": 577.82, "duration": 3.21}, {"text": "expectations of the time series.", "start": 581.03, "duration": 1.57}, {"text": "And those conditional densities\nare normal random variables.", "start": 585.42, "duration": 6.18}, {"text": "So we can just plug\nin what we know", "start": 591.6, "duration": 2.05}, {"text": "to be the probability\ndensities of normals", "start": 593.65, "duration": 2.64}, {"text": "for the t-th\ninnovation epsilon_t.", "start": 596.29, "duration": 4.98}, {"text": "And we just optimize\nthat function.", "start": 601.27, "duration": 3.07}, {"text": "Now, the challenge with\nestimating these GARCH models", "start": 604.34, "duration": 4.71}, {"text": "in part is the constraints\non the underlying parameters.", "start": 609.05, "duration": 4.94}, {"text": "Those need to be enforced.", "start": 613.99, "duration": 2.01}, {"text": "So we have to have that the\nalpha_i are greater than 0.", "start": 616.0, "duration": 3.25}, {"text": "Also, the beta_j\nare greater than 0.", "start": 619.25, "duration": 2.61}, {"text": "And the sum of all of\nthem is between 0 and 1.", "start": 621.86, "duration": 3.94}, {"text": "Who in this class has had\ncourses in numerical analysis", "start": 629.406, "duration": 4.454}, {"text": "and done some\noptimization of functions?", "start": 633.86, "duration": 5.82}, {"text": "Non-linear functions?", "start": 639.68, "duration": 1.16}, {"text": "Anybody?", "start": 640.84, "duration": 2.38}, {"text": "OK.", "start": 643.22, "duration": 0.77}, {"text": "Well, in addressing this kind\nof problem, which will come up", "start": 643.99, "duration": 4.5}, {"text": "with any complex model\nthat you need to estimate,", "start": 648.49, "duration": 2.81}, {"text": "say via maximum likelihood,\nthe optimization methods", "start": 651.3, "duration": 5.34}, {"text": "do really well if you're\noptimizing a convex function,", "start": 656.64, "duration": 5.97}, {"text": "finding the minimum\nof a convex function.", "start": 662.61, "duration": 2.41}, {"text": "And it's always nice\nto do minimization", "start": 665.02, "duration": 4.5}, {"text": "over sort of an unconstrained\nrange of underlying parameters.", "start": 669.52, "duration": 4.94}, {"text": "And one of the tricks in\nsolving these problems", "start": 674.46, "duration": 4.17}, {"text": "is to transform the\nparameters to a scale", "start": 678.63, "duration": 4.44}, {"text": "where they're unlimited\nin range, basically.", "start": 683.07, "duration": 4.41}, {"text": "So if you have a\npositive random variable,", "start": 687.48, "duration": 1.94}, {"text": "you might use to log of\nthat variable as the thing", "start": 689.42, "duration": 2.78}, {"text": "to be optimizing over.", "start": 692.2, "duration": 1.66}, {"text": "If the variable's\nbetween 0 and 1,", "start": 693.86, "duration": 2.32}, {"text": "then you might use\nthat variable divided", "start": 696.18, "duration": 2.87}, {"text": "by 1 minus that variable and\nthen take the log of that.", "start": 699.05, "duration": 4.649}, {"text": "And that's unconstrained.", "start": 703.699, "duration": 1.041}, {"text": "So there are tricks for how\nyou do this optimization, which", "start": 704.74, "duration": 3.56}, {"text": "come into play.", "start": 708.3, "duration": 1.28}, {"text": "Anyway, that's the likelihood\nwith the normal distribution.", "start": 709.58, "duration": 3.71}, {"text": "And we have computer\nprograms that", "start": 713.29, "duration": 4.367}, {"text": "will solve that directly\nso we don't have to worry", "start": 717.657, "duration": 2.083}, {"text": "about this particular case.", "start": 719.74, "duration": 1.53}, {"text": "Once we fit this model,\nwe want to evaluate", "start": 725.43, "duration": 2.89}, {"text": "how good it is and\nthe evaluation is", "start": 728.32, "duration": 4.94}, {"text": "based upon looking at the\nresiduals from the model.", "start": 733.26, "duration": 3.6}, {"text": "So what we have are\nthese innovations,", "start": 736.86, "duration": 2.59}, {"text": "epsilon hat t, which should\nbe distributed with variance", "start": 739.45, "duration": 5.8}, {"text": "or volatility sigma hat t.", "start": 745.25, "duration": 3.63}, {"text": "Those should be uncorrelated\nwith themselves or at least", "start": 748.88, "duration": 5.72}, {"text": "to the extent that they can be.", "start": 754.6, "duration": 3.02}, {"text": "And the squared\nstandardized residuals", "start": 757.62, "duration": 1.9}, {"text": "should also be uncorrelated.", "start": 759.52, "duration": 1.212}, {"text": "What we're trying to\ndo with these models", "start": 760.732, "duration": 1.708}, {"text": "is to capture the\ndependence, actually,", "start": 762.44, "duration": 3.55}, {"text": "in the squared residuals, which\nis measuring the magnitude", "start": 765.99, "duration": 5.57}, {"text": "of the excess returns.", "start": 771.56, "duration": 1.067}, {"text": "So those should be uncorrelated.", "start": 772.627, "duration": 1.333}, {"text": "There are various\ntest for normality.", "start": 776.69, "duration": 1.65}, {"text": "I've listed some of those that\nare the most popular here.", "start": 778.34, "duration": 3.29}, {"text": "And then there's issues of model\nselection for deciding sort", "start": 781.63, "duration": 5.18}, {"text": "of which GARCH model to apply.", "start": 786.81, "duration": 3.66}, {"text": "I wanted to go\nthrough an example", "start": 790.47, "duration": 4.93}, {"text": "of this analysis with the\neuro-dollar exchange rate.", "start": 795.4, "duration": 5.23}, {"text": "So let me go to this\ncase study note.", "start": 800.63, "duration": 6.85}, {"text": "So let's see.", "start": 807.48, "duration": 4.22}, {"text": "There's a package in R called\nrugarch for univariate GARCH", "start": 811.7, "duration": 5.78}, {"text": "models, which fits various\nGARCH models with different--", "start": 817.48, "duration": 6.94}, {"text": "and fits them by\nmaximum likelihood.", "start": 824.42, "duration": 3.36}, {"text": "So with this packet-- with\nthis particular library in R,", "start": 827.78, "duration": 4.5}, {"text": "I fit the GARCH\nmodel after actually", "start": 832.28, "duration": 16.04}, {"text": "fitting the mean process for\nthe exchange rate returns.", "start": 848.32, "duration": 4.15}, {"text": "Now, when we looked\nat things last time,", "start": 852.47, "duration": 2.0}, {"text": "we basically looked at\nmodeling the squared returns.", "start": 854.47, "duration": 2.48}, {"text": "In fact, there may be an\nunderlying mean process that", "start": 856.95, "duration": 2.61}, {"text": "needs to be specified as well.", "start": 859.56, "duration": 1.76}, {"text": "So in this section\nof the case note,", "start": 861.32, "duration": 2.69}, {"text": "I initially fit an\nautoregressive process", "start": 864.01, "duration": 5.96}, {"text": "using the Akaike\ninformation criterion", "start": 869.97, "duration": 2.05}, {"text": "to choose the order of\nthe autoregressive process", "start": 872.02, "duration": 3.57}, {"text": "and then fit a GARCH model\nwith normal GARCH terms.", "start": 875.59, "duration": 6.61}, {"text": "And this is a plot of\nthe normal q-q plot", "start": 882.2, "duration": 5.53}, {"text": "of the autoregressive residuals.", "start": 887.73, "duration": 4.07}, {"text": "And what you can see is\nthat the points lie along", "start": 891.8, "duration": 4.42}, {"text": "a straight line sort of in\nthe middle of the range.", "start": 896.22, "duration": 3.13}, {"text": "But on the extremes, they\ndepart from that straight line.", "start": 899.35, "duration": 3.91}, {"text": "This basically is a measure\nof standardized quantiles.", "start": 903.26, "duration": 6.25}, {"text": "So in terms of\nstandard units away", "start": 909.51, "duration": 2.61}, {"text": "from the mean for\nthe residuals, we", "start": 912.12, "duration": 4.0}, {"text": "tend to get many more high\nvalues and many more low values", "start": 916.12, "duration": 3.379}, {"text": "with the Gaussian distribution.", "start": 919.499, "duration": 1.291}, {"text": "So that really isn't\nfitting very well.", "start": 920.79, "duration": 3.83}, {"text": "If we proceed and fit--\nOK, actually that plot", "start": 924.62, "duration": 8.68}, {"text": "was just the simple ARCH\nmodel with no GARCH terms.", "start": 933.3, "duration": 4.29}, {"text": "And then this is the graph of\nthe q-q plot with the Gaussian", "start": 937.59, "duration": 8.65}, {"text": "assumption.", "start": 946.24, "duration": 1.11}, {"text": "So here we can see that the\nresiduals from this model", "start": 947.35, "duration": 3.49}, {"text": "are suggesting that it may\ndo a pretty good job when", "start": 950.84, "duration": 3.47}, {"text": "things are only a few standard\ndeviations away from the mean.", "start": 954.31, "duration": 3.29}, {"text": "Less than 2, 2.5.", "start": 957.6, "duration": 1.71}, {"text": "But when we get to\nmore extreme values,", "start": 959.31, "duration": 3.17}, {"text": "this isn't modeling things well.", "start": 962.48, "duration": 2.21}, {"text": "So one alternative\nis to consider", "start": 964.69, "duration": 4.29}, {"text": "a heavier-tailed distribution\nthan the normal, namely", "start": 968.98, "duration": 3.76}, {"text": "the t distribution.", "start": 972.74, "duration": 2.18}, {"text": "And consider identifying\nwhat t distribution best", "start": 974.92, "duration": 4.31}, {"text": "fits the data.", "start": 979.23, "duration": 0.77}, {"text": "So let's just look at what ends\nup being the maximum likelihood", "start": 984.49, "duration": 7.13}, {"text": "estimate for the degrees\nof freedom parameter, which", "start": 991.62, "duration": 2.72}, {"text": "is 10 degrees of freedom.", "start": 994.34, "duration": 2.81}, {"text": "This shows the q-q\nplot when you have", "start": 997.15, "duration": 2.96}, {"text": "a non-Gaussian\ndistribution that's", "start": 1000.11, "duration": 1.77}, {"text": "t with 10 degrees of freedom.", "start": 1001.88, "duration": 3.5}, {"text": "It basically is explaining\nthese residuals quite well,", "start": 1005.38, "duration": 2.5}, {"text": "so that's accommodating the\nheavier-tailed distribution", "start": 1007.88, "duration": 5.81}, {"text": "of these values.", "start": 1013.69, "duration": 1.555}, {"text": "With this GARCH\nmodel, let's see--", "start": 1019.63, "duration": 6.92}, {"text": "if you compare sort of estimates\nof volatility under the GARCH", "start": 1026.55, "duration": 4.85}, {"text": "and ARCH models--\nthe GARCH models", "start": 1031.4, "duration": 6.07}, {"text": "with the t distribution-- sorry\nt distribution versus Gaussian.", "start": 1037.47, "duration": 6.57}, {"text": "Here's just a graph\nshowing time series plots", "start": 1044.04, "duration": 2.829}, {"text": "of the estimated\nvolatility over time, which", "start": 1046.869, "duration": 1.908}, {"text": "actually look quite close.", "start": 1048.777, "duration": 1.083}, {"text": "But when you look\nat the differences,", "start": 1049.86, "duration": 1.541}, {"text": "there really are differences.", "start": 1051.401, "duration": 2.239}, {"text": "And so it turns out that\nthe volatility function", "start": 1053.64, "duration": 9.58}, {"text": "or the volatility estimate\nGARCH models with Gaussian", "start": 1063.22, "duration": 3.16}, {"text": "versus GARCH with\nt distributions", "start": 1066.38, "duration": 2.08}, {"text": "are really very, very similar.", "start": 1068.46, "duration": 3.1}, {"text": "And the heavier\ntailed distribution", "start": 1071.56, "duration": 1.69}, {"text": "of the t distribution\nmeans that the distribution", "start": 1073.25, "duration": 5.49}, {"text": "of actual volatility is greater.", "start": 1078.74, "duration": 4.46}, {"text": "But in terms of\nestimating the volatility,", "start": 1083.2, "duration": 2.04}, {"text": "you have quite similar estimates\nof the volatility coming out.", "start": 1085.24, "duration": 4.14}, {"text": "And this display--\nwhich you'll be", "start": 1089.38, "duration": 3.73}, {"text": "able to see more clearly in the\ncase notes that I'll post up--", "start": 1093.11, "duration": 3.49}, {"text": "show that these are really\nquite similar in magnitude.", "start": 1096.6, "duration": 2.275}, {"text": "And the value at risk concept\nthat was just-- by Ken couple", "start": 1102.26, "duration": 9.09}, {"text": "weeks ago in his lecture\nfrom Morgan Stanley--", "start": 1111.35, "duration": 4.6}, {"text": "concerns the issue\nof estimating what", "start": 1115.95, "duration": 2.55}, {"text": "is the likelihood of returns\nexceeding some threshold.", "start": 1118.5, "duration": 5.98}, {"text": "And if we use the t distribution\nfor measuring variability", "start": 1124.48, "duration": 7.83}, {"text": "of the excess returns, then\nthe computations in the notes", "start": 1132.31, "duration": 5.11}, {"text": "indicate how you would compute\nthese value at risk limits.", "start": 1137.42, "duration": 5.58}, {"text": "If you compare\nthe t distribution", "start": 1143.0, "duration": 2.36}, {"text": "with a Gaussian distribution at\nthese nominal levels for value", "start": 1145.36, "duration": 3.56}, {"text": "at risk of like 2.5%\nor 5%, surprisingly you", "start": 1148.92, "duration": 4.22}, {"text": "won't get too much difference.", "start": 1153.14, "duration": 2.93}, {"text": "It's really in looking at\nsort of the extreme tails", "start": 1156.07, "duration": 3.62}, {"text": "of the distribution that\nthings come into play.", "start": 1159.69, "duration": 4.71}, {"text": "And so I wanted to show you how\nthat plays out by showing you", "start": 1164.4, "duration": 9.13}, {"text": "another graph here.", "start": 1173.53, "duration": 2.5}, {"text": "Those of you who have had\na statistics course before", "start": 1176.03, "duration": 4.05}, {"text": "have heard that sort\nof a t distribution", "start": 1180.08, "duration": 3.27}, {"text": "can be a good\napproximation to a normal--", "start": 1183.35, "duration": 3.451}, {"text": "or it can be approximated\nwell by a normal", "start": 1186.801, "duration": 1.749}, {"text": "if the degrees of freedom\nfor the t are at some level.", "start": 1188.55, "duration": 4.51}, {"text": "And who wants to suggest\na degrees of freedom", "start": 1193.06, "duration": 3.61}, {"text": "that you might\nhave before you're", "start": 1196.67, "duration": 3.47}, {"text": "comfortable approximating\na t with a normal?", "start": 1200.14, "duration": 2.19}, {"text": "Danny?", "start": 1205.48, "duration": 0.5}, {"text": "AUDIENCE: 30 or 40.", "start": 1205.98, "duration": 0.791}, {"text": "PROFESSOR: 30 or 40.", "start": 1206.771, "duration": 1.829}, {"text": "Sometimes people say even 25.", "start": 1208.6, "duration": 1.63}, {"text": "Above 25, you can almost\nexpect the t distribution", "start": 1210.23, "duration": 2.98}, {"text": "to be a good approximation\nto the normal.", "start": 1213.21, "duration": 2.05}, {"text": "Well, this is a graph the\nPDF for a standard normal", "start": 1215.26, "duration": 4.08}, {"text": "versus a standard t with\n30 degrees of freedom.", "start": 1219.34, "duration": 3.01}, {"text": "And you can see that the density\nfunctions are very, very close.", "start": 1222.35, "duration": 3.73}, {"text": "The standard-- the CDFs,\nthe cumulative distribution", "start": 1226.08, "duration": 2.97}, {"text": "functions, which is\nthe likelihood of being", "start": 1229.05, "duration": 2.14}, {"text": "less than or equal to the\nhorizontal value, ranges", "start": 1231.19, "duration": 4.2}, {"text": "between 0 and 1, is\nalmost indistinguishable.", "start": 1235.39, "duration": 2.389}, {"text": "But if you look at the\ntails of the distribution,", "start": 1237.779, "duration": 2.041}, {"text": "here I've computed the\nlog of the CDF function.", "start": 1239.82, "duration": 3.65}, {"text": "You basically have\nto move much more", "start": 1243.47, "duration": 2.82}, {"text": "than two standard deviations\naway from the mean", "start": 1246.29, "duration": 2.0}, {"text": "before there's really\na difference in the t", "start": 1248.29, "duration": 2.09}, {"text": "distribution with 30\ndegrees of freedom.", "start": 1250.38, "duration": 3.362}, {"text": "Now I'm going to\npage up by reducing", "start": 1253.742, "duration": 2.268}, {"text": "the degrees of freedom.", "start": 1256.01, "duration": 2.38}, {"text": "Let's see.", "start": 1258.39, "duration": 2.238}, {"text": "If we could do a page down here.", "start": 1260.628, "duration": 3.852}, {"text": "Page down.", "start": 1264.48, "duration": 2.94}, {"text": "Oh, page up.", "start": 1267.42, "duration": 1.43}, {"text": "OK.", "start": 1268.85, "duration": 1.28}, {"text": "So here is 20\ndegrees of freedom.", "start": 1270.13, "duration": 5.39}, {"text": "Here's 10 degrees of\nfreedom, in our case,", "start": 1275.52, "duration": 4.95}, {"text": "which turns out to be sort\nof the best fit of the t", "start": 1280.47, "duration": 2.54}, {"text": "distribution.", "start": 1283.01, "duration": 1.17}, {"text": "And what you can see is that,\nin terms of standard deviation", "start": 1284.18, "duration": 5.06}, {"text": "units, up to about two standard\ndeviations below the mean,", "start": 1289.24, "duration": 2.63}, {"text": "we're basically getting\nvirtually the same probability", "start": 1291.87, "duration": 2.25}, {"text": "mass at the extreme below.", "start": 1294.12, "duration": 2.92}, {"text": "But as we go to four or\nsix standard deviations,", "start": 1297.04, "duration": 3.82}, {"text": "then we get heavier mass\nwith the t distribution.", "start": 1300.86, "duration": 4.35}, {"text": "In discussion of\nresults in finance", "start": 1305.21, "duration": 4.13}, {"text": "when you sort of fit models,\npeople talk about, oh, there", "start": 1309.34, "duration": 2.39}, {"text": "was six standard deviation\nmove or-- which is just", "start": 1311.73, "duration": 3.94}, {"text": "virtually impossible to occur.", "start": 1315.67, "duration": 1.54}, {"text": "Well, with t distributions a\nsix standard deviation move", "start": 1317.21, "duration": 4.89}, {"text": "occurs about 1 in 10,000\ntimes according to this fit.", "start": 1322.1, "duration": 5.6}, {"text": "And so it actually is a\ncommon [? idiomatic. ?]", "start": 1327.7, "duration": 3.54}, {"text": "And so it's important to know\nthat these t distributions are", "start": 1331.24, "duration": 5.22}, {"text": "benefiting us by giving us\na much better gauge of what", "start": 1336.46, "duration": 3.46}, {"text": "the tail distribution is like.", "start": 1339.92, "duration": 2.33}, {"text": "And we call these\ndistributions leptokurtic,", "start": 1342.25, "duration": 4.97}, {"text": "meaning they're heavier tailed\nthan a normal distribution.", "start": 1347.22, "duration": 3.04}, {"text": "Actually, lepto means\nslender, I believe,", "start": 1350.26, "duration": 3.69}, {"text": "if you're Greek or have the\nGreek origin of the word.", "start": 1353.95, "duration": 6.05}, {"text": "And you can see that\nthe blue curve, which", "start": 1360.0, "duration": 4.1}, {"text": "is the t distribution, is\nsort of a bit more slender", "start": 1364.1, "duration": 2.97}, {"text": "in the center of the\ndistribution, which allows", "start": 1367.07, "duration": 2.309}, {"text": "it to have heavier tails.", "start": 1369.379, "duration": 1.041}, {"text": "All right.", "start": 1376.27, "duration": 0.5}, {"text": "So t distributions\nare very useful.", "start": 1376.77, "duration": 2.94}, {"text": "Let's go back to\nthis case note here", "start": 1379.71, "duration": 10.52}, {"text": "which discusses-- this\ncase note goes through,", "start": 1390.23, "duration": 4.71}, {"text": "actually, fitting\nthe t distribution--", "start": 1394.94, "duration": 3.01}, {"text": "identifying the degrees of\nfreedom for this t model.", "start": 1397.95, "duration": 3.21}, {"text": "And so with the\nrugarch package, we", "start": 1401.16, "duration": 3.9}, {"text": "can get the log-likelihood\nof the data fit", "start": 1405.06, "duration": 5.29}, {"text": "under the t\ndistribution assumption.", "start": 1410.35, "duration": 2.8}, {"text": "And here's a graph of the\nnegative log-likelihood", "start": 1413.15, "duration": 2.88}, {"text": "versus the degrees of\nfreedom in the t model.", "start": 1416.03, "duration": 4.87}, {"text": "So with maximum\nlikelihood we identify", "start": 1420.9, "duration": 3.34}, {"text": "the value which minimizes\nthe negative log likelihood.", "start": 1424.24, "duration": 3.8}, {"text": "And that comes out\nas that 10 value.", "start": 1428.04, "duration": 2.972}, {"text": "All right.", "start": 1434.26, "duration": 2.02}, {"text": "Let's go back to these\nnotes and see what", "start": 1436.28, "duration": 1.84}, {"text": "else we want to talk about.", "start": 1438.12, "duration": 1.173}, {"text": "All right.", "start": 1445.25, "duration": 0.5}, {"text": "OK, with these GARCH\nmodels we actually", "start": 1453.38, "duration": 3.23}, {"text": "are able to model\nvolatility clustering.", "start": 1456.61, "duration": 3.25}, {"text": "And volatility clustering\nis where, over time, you", "start": 1459.86, "duration": 6.84}, {"text": "expect volatility to be\nhigh during some periods", "start": 1466.7, "duration": 3.93}, {"text": "and to be low during\nother periods.", "start": 1470.63, "duration": 2.43}, {"text": "And the GARCH model\ncan accommodate that.", "start": 1473.06, "duration": 2.84}, {"text": "So large volatilities\ntend to be followed", "start": 1475.9, "duration": 2.18}, {"text": "by large, small\nvolatilities tend", "start": 1478.08, "duration": 1.75}, {"text": "to be followed by small ones.", "start": 1479.83, "duration": 2.881}, {"text": "OK.", "start": 1482.711, "duration": 0.499}, {"text": "The returns have heavier tails\nthan Gaussian distributions.", "start": 1483.21, "duration": 6.03}, {"text": "Actually, even if we have\nGaussian errors in the GARCH", "start": 1489.24, "duration": 3.78}, {"text": "model, it's still heavier\ntailed than a Gaussian.", "start": 1493.02, "duration": 3.85}, {"text": "The homework goes into\nthat a little bit.", "start": 1496.87, "duration": 3.12}, {"text": "And the-- well, actually\none of the original papers", "start": 1499.99, "duration": 6.6}, {"text": "by Engle with Bollerslev, who\nintroduced the GARCH model,", "start": 1506.59, "duration": 4.1}, {"text": "discusses these\nfeatures and how useful", "start": 1510.69, "duration": 2.8}, {"text": "they are for modeling\nfinancial time series.", "start": 1513.49, "duration": 3.51}, {"text": "Now, a property of these models\nthat may be obvious, perhaps,", "start": 1517.0, "duration": 8.86}, {"text": "but it is-- OK,\nthese are models that", "start": 1525.86, "duration": 3.23}, {"text": "are appropriate for modeling\ncovariance stationary time", "start": 1529.09, "duration": 4.22}, {"text": "series.", "start": 1533.31, "duration": 0.92}, {"text": "So the volatility\nmeasure, which is", "start": 1534.23, "duration": 3.74}, {"text": "a measure of the\nsquared excess return,", "start": 1537.97, "duration": 4.57}, {"text": "is basically a covariance\nstationary process.", "start": 1542.54, "duration": 4.41}, {"text": "So what does that mean?", "start": 1546.95, "duration": 1.37}, {"text": "That means that's going\nto have a long-term mean.", "start": 1548.32, "duration": 3.55}, {"text": "So with these GARCH models\nthat are covariance stationary,", "start": 1551.87, "duration": 3.19}, {"text": "there's going to be a long-term\nmean of the GARCH process.", "start": 1555.06, "duration": 3.26}, {"text": "And this discussion here\ndetails how this GARCH process", "start": 1558.32, "duration": 8.5}, {"text": "is essentially a mean\nreversion of the volatility", "start": 1566.82, "duration": 7.57}, {"text": "to that value.", "start": 1574.39, "duration": 2.03}, {"text": "So basically, the sort\nof excess volatility", "start": 1576.42, "duration": 4.69}, {"text": "of the squared\nresiduals relative", "start": 1581.11, "duration": 2.82}, {"text": "to their long-term\naverage is some multiple", "start": 1583.93, "duration": 3.86}, {"text": "of the previous period's\nexcess volatility.", "start": 1587.79, "duration": 4.51}, {"text": "So if we build forecasting\nmodels of volatility", "start": 1592.3, "duration": 4.47}, {"text": "with GARCH models,\nwhat's going to happen?", "start": 1596.77, "duration": 5.62}, {"text": "Basically, in the\nlong run we predict", "start": 1602.39, "duration": 3.45}, {"text": "that any volatility\nvalue is going to revert", "start": 1605.84, "duration": 2.77}, {"text": "to this long-run average.", "start": 1608.61, "duration": 2.86}, {"text": "And in the short run, it's\ngoing to move incrementally", "start": 1611.47, "duration": 3.13}, {"text": "to that value.", "start": 1614.6, "duration": 2.14}, {"text": "So these GARCH models are very\ngood for describing volatility", "start": 1616.74, "duration": 5.34}, {"text": "relative to the\nlong-term average.", "start": 1622.08, "duration": 1.57}, {"text": "In terms of their\nusefulness for prediction,", "start": 1623.65, "duration": 2.97}, {"text": "well, they really\npredict that volatility", "start": 1626.62, "duration": 2.96}, {"text": "is going to revert back\nto the mean at some rate.", "start": 1629.58, "duration": 3.67}, {"text": "And the rate at which the\nvolatility reverts back", "start": 1633.25, "duration": 5.59}, {"text": "is given by alpha_1 plus beta_1.", "start": 1638.84, "duration": 3.31}, {"text": "So that number,\nwhich is less than 1", "start": 1642.15, "duration": 3.72}, {"text": "for covariance\nstationarity, is sort", "start": 1645.87, "duration": 2.15}, {"text": "of measuring, basically, how\nquickly you are reverting back", "start": 1648.02, "duration": 5.13}, {"text": "to the mean.", "start": 1653.15, "duration": 0.75}, {"text": "And that sum is actually\ncalled a persistence parameter", "start": 1653.9, "duration": 2.96}, {"text": "in GARCH models as well.", "start": 1656.86, "duration": 1.96}, {"text": "So is volatility\npersisting or not?", "start": 1658.82, "duration": 2.15}, {"text": "Well, the larger\nalpha_1 plus beta_1", "start": 1660.97, "duration": 1.69}, {"text": "is, the more persistent\nvolatility is, meaning it's", "start": 1662.66, "duration": 4.63}, {"text": "reverting back to that long-run\naverage very, very slowly.", "start": 1667.29, "duration": 4.04}, {"text": "In the implementation\nof volatility estimates", "start": 1671.33, "duration": 2.85}, {"text": "with the risk\nmetrics methodology,", "start": 1674.18, "duration": 4.23}, {"text": "they actually don't assume that\nthere is a long-run volatility.", "start": 1678.41, "duration": 5.68}, {"text": "And so that basically you'll\nhave alpha_1 be equal to 0", "start": 1684.09, "duration": 4.45}, {"text": "and beta_1 equal to, say, 0.95.", "start": 1688.54, "duration": 5.59}, {"text": "So or rather the alpha_0 is\n0 and the alpha_1 and beta_1", "start": 1694.13, "duration": 8.14}, {"text": "will actually sum to 1.", "start": 1702.27, "duration": 1.84}, {"text": "And so you actually are tracking\na potentially non-stationary", "start": 1704.11, "duration": 4.11}, {"text": "volatility, which allows you\nto be estimating the volatility", "start": 1708.22, "duration": 7.33}, {"text": "without presuming a\nlong-run average is", "start": 1715.55, "duration": 3.689}, {"text": "consistent with the past.", "start": 1719.239, "duration": 1.041}, {"text": "There are many extensions\nof the GARCH models.", "start": 1725.29, "duration": 3.12}, {"text": "And there's wide\nliterature on that.", "start": 1728.41, "duration": 4.752}, {"text": "For this course, I think\nit's important to understand", "start": 1733.162, "duration": 2.208}, {"text": "the fundamentals of\nthese models in terms", "start": 1735.37, "duration": 2.83}, {"text": "of how they're specified under\nGaussian and t assumptions.", "start": 1738.2, "duration": 4.69}, {"text": "Extending them can\nbe very interesting.", "start": 1742.89, "duration": 4.16}, {"text": "And there are many papers\nto look at for that.", "start": 1747.05, "duration": 5.6}, {"text": "OK.", "start": 1752.65, "duration": 0.5}, {"text": "let's pause for a minute\nand get the next topic.", "start": 1756.26, "duration": 4.95}, {"text": "All right.", "start": 1774.921, "duration": 0.499}, {"text": "The next topic is time series,\nmultivariate time series.", "start": 1775.42, "duration": 7.21}, {"text": "In two lectures ago\nof mine, we talked", "start": 1782.63, "duration": 2.66}, {"text": "about univariate time series\nand basic methodologies there.", "start": 1785.29, "duration": 3.98}, {"text": "We're now going to be\nextending that to multivariate", "start": 1789.27, "duration": 2.97}, {"text": "time series.", "start": 1792.24, "duration": 2.8}, {"text": "Turns out there's a multivariate\nWold representation theorem,", "start": 1795.04, "duration": 5.07}, {"text": "extension of the univariate one.", "start": 1800.11, "duration": 2.75}, {"text": "There are\nautoregressive processes", "start": 1802.86, "duration": 1.89}, {"text": "for multivariate\ncases, which are vector", "start": 1804.75, "duration": 2.3}, {"text": "autoregressive processes.", "start": 1807.05, "duration": 2.1}, {"text": "Least squares estimation\ncomes into play.", "start": 1809.15, "duration": 2.89}, {"text": "And then we'll see where\nour regression analysis", "start": 1812.04, "duration": 5.42}, {"text": "understanding\nallows us to specify", "start": 1817.46, "duration": 2.63}, {"text": "these vector autoregressive\nprocesses nicely.", "start": 1820.09, "duration": 5.58}, {"text": "There's an optimality properties\nof ordinary least squares", "start": 1825.67, "duration": 4.45}, {"text": "estimates component wise, which\nwe'll highlight in about a half", "start": 1830.12, "duration": 4.9}, {"text": "an hour.", "start": 1835.02, "duration": 0.51}, {"text": "And go through the maximum\nlikelihood estimation model", "start": 1835.53, "duration": 4.34}, {"text": "selection methods,\nwhich are just", "start": 1839.87, "duration": 4.81}, {"text": "very straightforward\nextensions of the same concepts", "start": 1844.68, "duration": 2.53}, {"text": "for univariate time series\nand univariate regressions.", "start": 1847.21, "duration": 7.06}, {"text": "So let's talk-- let's\nintroduce the notation", "start": 1854.27, "duration": 2.67}, {"text": "for multivariate time series.", "start": 1856.94, "duration": 2.07}, {"text": "We have a stochastic process,\nwhich now is multivariate.", "start": 1859.01, "duration": 5.98}, {"text": "So we have bold X of t is\nsome m-dimensional valued", "start": 1864.99, "duration": 6.33}, {"text": "random variable.", "start": 1871.32, "duration": 2.07}, {"text": "And it's a stochastic process\nthat varies over time t.", "start": 1873.39, "duration": 7.78}, {"text": "And we can think of this\nas m different time series", "start": 1881.17, "duration": 7.74}, {"text": "corresponding to the m\ncomponents of the given", "start": 1888.91, "duration": 3.03}, {"text": "process.", "start": 1891.94, "duration": 0.5}, {"text": "So, say, with exchange\nrates we could", "start": 1892.44, "duration": 2.46}, {"text": "be modeling m different exchange\nrate values and want to model", "start": 1894.9, "duration": 5.23}, {"text": "those jointly as a time series.", "start": 1900.13, "duration": 1.96}, {"text": "Or we could have collections\nof stocks that we're modeling.", "start": 1902.09, "duration": 5.36}, {"text": "So each of the\ncomponents individually", "start": 1907.45, "duration": 1.71}, {"text": "can be treated as univariate\nseries with univariate methods.", "start": 1909.16, "duration": 4.52}, {"text": "With the multivariate\ncase, we extend", "start": 1918.07, "duration": 2.67}, {"text": "the definition of\ncovariance stationarity", "start": 1920.74, "duration": 3.26}, {"text": "to correspond to finite, bounded\nfirst and second order moments.", "start": 1924.0, "duration": 7.39}, {"text": "So we need to talk\nabout the first order", "start": 1931.39, "duration": 2.67}, {"text": "moment of the\nmultivariate time series.", "start": 1934.06, "duration": 3.76}, {"text": "Mu now is an m vector, which is\nthe vector of expected values", "start": 1937.82, "duration": 5.15}, {"text": "of the individual\ncomponents, which we can", "start": 1942.97, "duration": 2.05}, {"text": "denote by mu_1 through mu_m.", "start": 1945.02, "duration": 2.45}, {"text": "So we basically have m\nvectors for our mean.", "start": 1947.47, "duration": 4.065}, {"text": "Then for the\nvariance/covariance matrix,", "start": 1955.83, "duration": 4.54}, {"text": "let's define gamma_0 to be\nthe variance/covariance matrix", "start": 1960.37, "duration": 6.12}, {"text": "of the t-th observation of\nour multivariate process.", "start": 1966.49, "duration": 4.95}, {"text": "So that's equal to the\nexpected value of X_t minus mu", "start": 1971.44, "duration": 6.3}, {"text": "X_t minus mu prime.", "start": 1977.74, "duration": 2.03}, {"text": "So when we write that\ndown, we have X_t minus mu.", "start": 1979.77, "duration": 11.13}, {"text": "This is basically\nan m by 1 vector", "start": 1990.9, "duration": 5.26}, {"text": "and then X_t minus mu\nprime is a 1 by m vector.", "start": 1996.16, "duration": 7.37}, {"text": "And so the product of that\nis an m by m quantity.", "start": 2003.53, "duration": 4.48}, {"text": "So the 1, 1 element of that\nproduct is the variance", "start": 2008.01, "duration": 6.74}, {"text": "of X_(1,t).", "start": 2014.75, "duration": 1.516}, {"text": "And the diagonal entries\nare the variances", "start": 2016.266, "duration": 1.749}, {"text": "of the components series.", "start": 2018.015, "duration": 2.785}, {"text": "And the off-diagonal\nvalues are the covariance", "start": 2020.8, "duration": 2.99}, {"text": "between the i-th row series\nand the j-th column series,", "start": 2023.79, "duration": 6.43}, {"text": "as given by the i-th row of\nX and the j-th column of X", "start": 2030.22, "duration": 5.34}, {"text": "transpose.", "start": 2035.56, "duration": 2.54}, {"text": "So we're just\ncollecting together", "start": 2038.1, "duration": 1.58}, {"text": "all the variances/covariances\ntogether.", "start": 2039.68, "duration": 3.09}, {"text": "And the notation is very\nstraightforward and simple", "start": 2042.77, "duration": 4.86}, {"text": "with the matrix\nnotation given here.", "start": 2047.63, "duration": 3.17}, {"text": "Now, the correlation matrix,\nr_0, is obtained by pre-", "start": 2050.8, "duration": 10.23}, {"text": "and post-multiplying this\ncovariance matrix gamma_0", "start": 2061.03, "duration": 4.47}, {"text": "by a diagonal matrix\nwith the square roots", "start": 2065.5, "duration": 5.61}, {"text": "of the diagonal of this matrix.", "start": 2071.11, "duration": 1.859}, {"text": "Now what's a correlation?", "start": 2072.969, "duration": 1.041}, {"text": "Correlation is the correlation\nbetween two random variables", "start": 2074.01, "duration": 4.79}, {"text": "where we've standardized\nthe variables", "start": 2078.8, "duration": 3.589}, {"text": "to have mean 0 and variance 1.", "start": 2082.389, "duration": 3.191}, {"text": "So what we want to do is\nbasically divide through all", "start": 2089.02, "duration": 4.42}, {"text": "of these variables by\ntheir standard deviation", "start": 2093.44, "duration": 4.27}, {"text": "and compute the covariance\nmatrix on that new scaling.", "start": 2097.71, "duration": 4.73}, {"text": "That's equivalent to just\npre- and post-multiplying", "start": 2102.44, "duration": 2.54}, {"text": "by that diagonal of the inverse\nof the standard deviations.", "start": 2104.98, "duration": 3.41}, {"text": "So with matrix\nalgebra, that formula", "start": 2108.39, "duration": 3.24}, {"text": "is-- I think it's very clear.", "start": 2111.63, "duration": 6.88}, {"text": "And this is-- now with-- the\nprevious discussion was just", "start": 2118.51, "duration": 7.81}, {"text": "looking at the sort of\ncontemporaneous covariance", "start": 2126.32, "duration": 2.91}, {"text": "matrix of the time series\nvalues at the given time", "start": 2129.23, "duration": 3.34}, {"text": "t with itself.", "start": 2132.57, "duration": 2.25}, {"text": "We want to look at, also, the\ncross-covariance matrices.", "start": 2134.82, "duration": 4.15}, {"text": "So how are the current values\nof the multivariate time series,", "start": 2138.97, "duration": 5.92}, {"text": "X_t-- how do they covary with\nthe k-th lag of those values?", "start": 2144.89, "duration": 7.11}, {"text": "So gamma_k is looking at how\nthe current period vector", "start": 2152.0, "duration": 6.34}, {"text": "values is covaried with the\nk-th lag of those values.", "start": 2158.34, "duration": 6.41}, {"text": "So this covariance matrix\nhas covariance elements", "start": 2164.75, "duration": 4.92}, {"text": "given in this display.", "start": 2169.67, "duration": 4.42}, {"text": "And we can define the\ncross-correlation matrix", "start": 2174.09, "duration": 4.8}, {"text": "by similarly pre-\nand post-multiplying", "start": 2178.89, "duration": 2.56}, {"text": "by the inverse of the\nstandard deviations.", "start": 2181.45, "duration": 3.51}, {"text": "The diagonal of gamma_0\nis the covariance--", "start": 2184.96, "duration": 3.605}, {"text": "or is the matrix of diagonal\nentries of variances.", "start": 2188.565, "duration": 5.115}, {"text": "Now, properties of\nthese matrices is-- OK,", "start": 2193.68, "duration": 2.3}, {"text": "gamma_0 is a symmetric\nmatrix that we had before.", "start": 2195.98, "duration": 4.17}, {"text": "But gamma k where k is\ngreater than 1 or less than--", "start": 2200.15, "duration": 3.992}, {"text": "or greater or equal to 1 or\nless than-- basically different", "start": 2204.142, "duration": 2.458}, {"text": "from 0.", "start": 2206.6, "duration": 1.1}, {"text": "This is not symmetric.", "start": 2207.7, "duration": 3.32}, {"text": "Basically, you may have\nlags of some variables that", "start": 2211.02, "duration": 5.66}, {"text": "are positively correlated with\nothers and not vice versa.", "start": 2216.68, "duration": 4.33}, {"text": "So the off-diagonal entries\nhere aren't necessarily even", "start": 2221.01, "duration": 7.5}, {"text": "of the same sign, let\nalone equal and symmetric.", "start": 2228.51, "duration": 3.42}, {"text": "So with these\ncovariance matrices,", "start": 2231.93, "duration": 5.87}, {"text": "one can look at\nhow things covary", "start": 2237.8, "duration": 3.02}, {"text": "and whether they are--\nwhether there is, basically,", "start": 2240.82, "duration": 4.47}, {"text": "a dependence between them.", "start": 2245.29, "duration": 3.01}, {"text": "And you can define--\nit's basically the j star", "start": 2248.3, "duration": 2.56}, {"text": "series-- the j star component\nof the multivariate time series", "start": 2250.86, "duration": 6.03}, {"text": "may lead the j-th one if the\ncovariance of the k-th lag of j", "start": 2256.89, "duration": 6.9}, {"text": "star is different from 0--\nor the covariance of j star k", "start": 2263.79, "duration": 6.86}, {"text": "lags ago is non-zero,\ncovaries with the j-th lag.", "start": 2270.65, "duration": 6.86}, {"text": "Sorry.", "start": 2277.51, "duration": 1.26}, {"text": "The current lag.", "start": 2278.77, "duration": 1.01}, {"text": "So X_(t, j star)\nwill lead X_(t, j).", "start": 2279.78, "duration": 2.48}, {"text": "Basically, there's information\nin the lagged values", "start": 2282.26, "duration": 5.03}, {"text": "of j star for the component j.", "start": 2287.29, "duration": 5.82}, {"text": "So if we're trying to build\nmodels-- linear regression", "start": 2293.11, "duration": 5.99}, {"text": "models, even, where we're\ntrying to look at how-- trying", "start": 2299.1, "duration": 3.71}, {"text": "to predict values, then if\nthere's a non-zero covariance,", "start": 2302.81, "duration": 3.71}, {"text": "then we can use those\nvariables' information", "start": 2306.52, "duration": 2.59}, {"text": "to actually project what the\none variable is given the other.", "start": 2309.11, "duration": 5.65}, {"text": "Now, it can be the case that\nyou have non-zero covariance", "start": 2314.76, "duration": 7.14}, {"text": "in both directions.", "start": 2321.9, "duration": 2.1}, {"text": "And so that suggests\nthat there can", "start": 2324.0, "duration": 2.28}, {"text": "be sort of feedback\nbetween these variables.", "start": 2326.28, "duration": 2.12}, {"text": "It's not just that one\nvariable causes another,", "start": 2328.4, "duration": 3.02}, {"text": "but there can\nactually be feedback.", "start": 2331.42, "duration": 2.75}, {"text": "In economics and\nfinance, there's", "start": 2334.17, "duration": 1.8}, {"text": "a notion of Granger causality.", "start": 2335.97, "duration": 4.82}, {"text": "And basically that--\nwell, Granger and Engle", "start": 2340.79, "duration": 3.41}, {"text": "got the Nobel Prize number of\nyears ago based on their work.", "start": 2344.2, "duration": 4.69}, {"text": "And that work deals\nwith identifying,", "start": 2348.89, "duration": 4.3}, {"text": "in part, judgments of\ncausality between--", "start": 2353.19, "duration": 5.25}, {"text": "or Granger causality\nbetween variables", "start": 2358.44, "duration": 2.107}, {"text": "in economic time series.", "start": 2360.547, "duration": 1.923}, {"text": "And so Granger\ncausality basically", "start": 2362.47, "duration": 2.31}, {"text": "is sort of positive or non-zero\ncorrelation between variables", "start": 2364.78, "duration": 7.12}, {"text": "where lags of one variable will\ncause another or cause changes", "start": 2371.9, "duration": 3.82}, {"text": "in another.", "start": 2375.72, "duration": 0.93}, {"text": "All right.", "start": 2381.31, "duration": 0.73}, {"text": "I want to just alert you to\nthe existence of this Wold", "start": 2382.04, "duration": 3.52}, {"text": "decomposition theorem.", "start": 2385.56, "duration": 2.19}, {"text": "This is an advanced theorem,\nbut it's a useful theorem", "start": 2387.75, "duration": 5.31}, {"text": "to know exists.", "start": 2393.06, "duration": 2.54}, {"text": "And this extends the univariate\nWold decomposition theorem,", "start": 2395.6, "duration": 5.4}, {"text": "which concerns the--\nwhenever we have", "start": 2401.0, "duration": 2.93}, {"text": "a covariant stationary\nprocess, there", "start": 2403.93, "duration": 2.82}, {"text": "exists a representation\nof that process, which", "start": 2406.75, "duration": 4.99}, {"text": "is the sum of a deterministic\nprocess and a moving", "start": 2411.74, "duration": 7.31}, {"text": "average process\nof a white noise.", "start": 2419.05, "duration": 4.11}, {"text": "So if you're modeling\na time series", "start": 2423.16, "duration": 4.7}, {"text": "and you're going\nto be specifying", "start": 2427.86, "duration": 2.3}, {"text": "a covariance stationary\nprocess for that,", "start": 2430.16, "duration": 3.73}, {"text": "there does exist a Wold\ndecomposition representation", "start": 2433.89, "duration": 4.23}, {"text": "of that.", "start": 2438.12, "duration": 1.07}, {"text": "You can basically\ndetermine-- identify", "start": 2439.19, "duration": 5.47}, {"text": "the deterministic process\nthat the process might follow.", "start": 2444.66, "duration": 3.08}, {"text": "It might be a linear trend over\ntime or an exponential trend.", "start": 2447.74, "duration": 4.91}, {"text": "And if you remove that sort\nof deterministic process V_t,", "start": 2452.65, "duration": 5.84}, {"text": "then what remains\nis a process that", "start": 2458.49, "duration": 4.03}, {"text": "can be modeled with a moving\naverage of white noise, these.", "start": 2462.52, "duration": 6.87}, {"text": "Now here, everything is\nchanged from univariate case", "start": 2469.39, "duration": 3.22}, {"text": "to multivariate case, so we have\nmatrices in place of constants", "start": 2472.61, "duration": 4.06}, {"text": "from before.", "start": 2476.67, "duration": 1.61}, {"text": "So these-- new\nconcepts here are we", "start": 2478.28, "duration": 6.05}, {"text": "have a multivariate\nwhite noise process.", "start": 2484.33, "duration": 2.225}, {"text": "That's going to be a process\neta_t which is m-dimensional", "start": 2489.18, "duration": 4.64}, {"text": "which has mean 0.", "start": 2493.82, "duration": 2.84}, {"text": "And the variance\nmatrix of this m-vector", "start": 2496.66, "duration": 5.36}, {"text": "is going to be sigma, which is\nnow m by m variance/covariance", "start": 2502.02, "duration": 4.45}, {"text": "matrix of the components.", "start": 2506.47, "duration": 3.34}, {"text": "And that must be a\npositive semi-definite.", "start": 2509.81, "duration": 3.12}, {"text": "And for white noise, we have\ncovariances between, say,", "start": 2512.93, "duration": 5.95}, {"text": "the current t innovation and\na lag of its value are 0.", "start": 2518.88, "duration": 4.44}, {"text": "So these are uncorrelated\nmultivariate white noise", "start": 2523.32, "duration": 4.12}, {"text": "processes.", "start": 2527.44, "duration": 1.81}, {"text": "And so they're uncorrelated\nwith each other at various lags.", "start": 2529.25, "duration": 4.88}, {"text": "And the innovation eta_t\nhas a covariance of 0", "start": 2534.13, "duration": 5.34}, {"text": "with the deterministic process.", "start": 2539.47, "duration": 2.6}, {"text": "Actually, that's\npretty much a given", "start": 2542.07, "duration": 3.65}, {"text": "if we have a\ndeterministic process.", "start": 2545.72, "duration": 2.75}, {"text": "Now, the term\npsi_k-- basically we", "start": 2548.47, "duration": 3.53}, {"text": "have this m-vector X_t is equal\nto some m-vectored process", "start": 2552.0, "duration": 4.02}, {"text": "V_t plus this weighted\naverage of innovations.", "start": 2556.02, "duration": 2.545}, {"text": "What's required is that the sum\nof this-- basically each term", "start": 2561.97, "duration": 6.33}, {"text": "psi_k and its\ntranspose converges.", "start": 2568.3, "duration": 3.51}, {"text": "Now, if you were to\ntake that X_t process", "start": 2571.81, "duration": 3.18}, {"text": "and say let me compute the\nvariance/covariance matrix", "start": 2574.99, "duration": 2.93}, {"text": "of that representation,\nthen you would basically", "start": 2577.92, "duration": 4.39}, {"text": "get terms in the\ncovariance matrix which", "start": 2582.31, "duration": 2.99}, {"text": "includes this sum of terms.", "start": 2585.3, "duration": 3.41}, {"text": "So that sum has to be\nfinite in order for this", "start": 2588.71, "duration": 3.55}, {"text": "to be covariance stationary.", "start": 2592.26, "duration": 3.249}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2595.509, "duration": 0.916}, {"text": "PROFESSOR: Yes?", "start": 2596.425, "duration": 0.805}, {"text": "AUDIENCE: Could you define\nwhat you mean by innovation?", "start": 2597.23, "duration": 3.04}, {"text": "PROFESSOR: Oh, OK.", "start": 2600.27, "duration": 2.69}, {"text": "Well, the innovation\nis-- let's see.", "start": 2602.96, "duration": 4.09}, {"text": "With-- let me go back up here.", "start": 2607.05, "duration": 5.18}, {"text": "OK.", "start": 2612.23, "duration": 0.5}, {"text": "The innovation process--\ninnovation process.", "start": 2619.24, "duration": 4.46}, {"text": "OK, if we have, as\nin this case, we", "start": 2626.33, "duration": 3.89}, {"text": "have sort of our X_t\nstochastic process.", "start": 2630.22, "duration": 8.54}, {"text": "And we have sort of, say,\nf sub t minus 1 equal", "start": 2638.76, "duration": 5.95}, {"text": "to the information on\nX_(t-1), X_(t-2)...", "start": 2644.71, "duration": 10.54}, {"text": "Basically consisting of the\ninformation set available", "start": 2655.25, "duration": 4.48}, {"text": "before time t.", "start": 2659.73, "duration": 1.79}, {"text": "Then we can model X_t to be\nthe expected value of X_t given", "start": 2661.52, "duration": 7.29}, {"text": "F_(t-1) plus an innovation.", "start": 2668.81, "duration": 9.72}, {"text": "And so our objective\nin these models", "start": 2678.53, "duration": 2.15}, {"text": "is to be thinking of how is that\nprocess evolving where we can", "start": 2680.68, "duration": 9.32}, {"text": "model the process as well as\npossible using information up", "start": 2690.0, "duration": 3.19}, {"text": "to time before t.", "start": 2693.19, "duration": 1.01}, {"text": "And then there's some\ndisturbance about that model.", "start": 2694.2, "duration": 5.4}, {"text": "There's something new that's\nhappened at time t that", "start": 2699.6, "duration": 2.68}, {"text": "wasn't available before.", "start": 2702.28, "duration": 2.18}, {"text": "And that's this\ninnovation process.", "start": 2704.46, "duration": 3.2}, {"text": "So this representation\nwith the Wold decomposition", "start": 2707.66, "duration": 3.46}, {"text": "is converting the-- or\nrepresenting, basically,", "start": 2711.12, "duration": 5.48}, {"text": "the bits of information that\nare affecting the process that", "start": 2716.6, "duration": 3.81}, {"text": "are occurring at time t and\nwasn't available prior to that.", "start": 2720.41, "duration": 3.884}, {"text": "All right.", "start": 2727.59, "duration": 1.47}, {"text": "Well, let's move on to vector\nautoregressive processes.", "start": 2729.06, "duration": 4.63}, {"text": "OK, this representation for a\nvector autoregressive process", "start": 2739.84, "duration": 3.76}, {"text": "is an extension of the\nunivariate autoregressive", "start": 2743.6, "duration": 3.98}, {"text": "process to p dimensions.", "start": 2747.58, "duration": 2.203}, {"text": "Sorry, to m dimensions.", "start": 2749.783, "duration": 2.597}, {"text": "And so our X_t is an m-vector.", "start": 2752.38, "duration": 4.25}, {"text": "That's going to be equal to some\nconstant vector C plus a matrix", "start": 2756.63, "duration": 8.49}, {"text": "phi_1 times lag of X_t\nfirst order, X_(t-1).", "start": 2765.12, "duration": 5.73}, {"text": "Plus another matrix, phi_2 times\nthe second lag of X_t, X_(t-2).", "start": 2770.85, "duration": 8.94}, {"text": "Up to the p-th term, which is\na phi_p, m by m matrix times,", "start": 2779.79, "duration": 5.25}, {"text": "X_(t-p) plus this\ninnovation term.", "start": 2785.04, "duration": 3.76}, {"text": "So this is essentially--\nthis is basically", "start": 2788.8, "duration": 2.59}, {"text": "how a univariate\nautoregressive process extends", "start": 2791.39, "duration": 3.06}, {"text": "to an m-variate case.", "start": 2794.45, "duration": 2.87}, {"text": "And what this\nallows one to do is", "start": 2797.32, "duration": 6.23}, {"text": "model how a given component\nof the multivariate series--", "start": 2803.55, "duration": 3.95}, {"text": "like how one\nexchange rate varies", "start": 2807.5, "duration": 3.53}, {"text": "depending on how other\nexchange rates might vary.", "start": 2811.03, "duration": 3.47}, {"text": "Exchange rates tend to co-move\ntogether in that example.", "start": 2814.5, "duration": 5.99}, {"text": "So if we look at\nwhat this represents", "start": 2820.49, "duration": 3.58}, {"text": "in terms of basically\na component series,", "start": 2824.07, "duration": 4.06}, {"text": "we can consider\nfixing j, a component", "start": 2828.13, "duration": 2.64}, {"text": "of the multivariate process.", "start": 2830.77, "duration": 3.06}, {"text": "It could be the first,\nthe last, or the j-th,", "start": 2833.83, "duration": 2.25}, {"text": "somewhere in the middle.", "start": 2836.08, "duration": 1.22}, {"text": "And that component\ntime series-- like", "start": 2837.3, "duration": 3.29}, {"text": "a fixed exchange\nrate series or time", "start": 2840.59, "duration": 3.055}, {"text": "series, whatever we're\nfocused on in our modeling--", "start": 2843.645, "duration": 3.275}, {"text": "is a generalization of the\nautoregressive model where", "start": 2846.92, "duration": 4.81}, {"text": "we have the autoregressive\nterms of the j-th series on lags", "start": 2851.73, "duration": 5.29}, {"text": "of the j-th series\nup to order p.", "start": 2857.02, "duration": 2.87}, {"text": "So we have the univariate\nautoregressive model,", "start": 2859.89, "duration": 3.52}, {"text": "but we also add to that\nterms corresponding", "start": 2863.41, "duration": 3.73}, {"text": "to the relationship\nbetween X_j and X_(j star).", "start": 2867.14, "duration": 4.99}, {"text": "So how does X_j,\nthe j-th component,", "start": 2872.13, "duration": 2.21}, {"text": "depend on other variables,\nother components", "start": 2874.34, "duration": 3.02}, {"text": "of the multivariate series.", "start": 2877.36, "duration": 1.43}, {"text": "And those are given here.", "start": 2878.79, "duration": 3.05}, {"text": "So it's a convenient way to\nallow for interdependence", "start": 2881.84, "duration": 6.05}, {"text": "among the components\nand model that.", "start": 2887.89, "duration": 2.329}, {"text": "OK.", "start": 2895.21, "duration": 1.64}, {"text": "This slide deals with\nrepresenting a p-th order", "start": 2896.85, "duration": 8.64}, {"text": "process as a first order process\nwith vector autoregressions.", "start": 2905.49, "duration": 7.23}, {"text": "Now the concept here is really\na very powerful concept that's", "start": 2912.72, "duration": 5.25}, {"text": "applied in time\nseries methods, which", "start": 2917.97, "duration": 3.78}, {"text": "is when you are modeling\ndependence that goes back, say,", "start": 2921.75, "duration": 9.36}, {"text": "a number of lags like\np lags, the structure", "start": 2931.11, "duration": 5.95}, {"text": "can actually be re-expressed\nas simply a first order", "start": 2937.06, "duration": 5.39}, {"text": "dependence only.", "start": 2942.45, "duration": 2.21}, {"text": "And so it's much easier sort\nof to deal with just a lag one", "start": 2944.66, "duration": 4.41}, {"text": "dependence than to\nconsider p lag dependence", "start": 2949.07, "duration": 4.6}, {"text": "and the complications\ninvolved with that.", "start": 2953.67, "duration": 3.89}, {"text": "So-- and this\ntechnique is one where,", "start": 2957.56, "duration": 4.94}, {"text": "in the early days of fitting,\nlike autoregressive moving", "start": 2962.5, "duration": 4.2}, {"text": "average processes and\nvarious smoothing methods,", "start": 2966.7, "duration": 7.82}, {"text": "the model-- basically\naccommodating", "start": 2974.52, "duration": 6.0}, {"text": "p lags complicated the\nanalysis enormously.", "start": 2980.52, "duration": 4.44}, {"text": "But one can actually\nre-express it just", "start": 2984.96, "duration": 1.78}, {"text": "as a first order lag problem.", "start": 2986.74, "duration": 2.19}, {"text": "So in this case, what\none does is one considers", "start": 2988.93, "duration": 4.93}, {"text": "for a vector autoregressive\nprocess of order of p,", "start": 2993.86, "duration": 3.72}, {"text": "simply stacking the\nvalues of the process.", "start": 2997.58, "duration": 10.51}, {"text": "So let me just highlight\nwhat's going on there.", "start": 3008.09, "duration": 3.55}, {"text": "So if we have basically--\nOK, so if we have X_1,", "start": 3017.62, "duration": 11.88}, {"text": "X_2, X_n, which are\nall m by 1 values,", "start": 3029.5, "duration": 9.18}, {"text": "m-vectors of the\nstochastic process.", "start": 3038.68, "duration": 3.96}, {"text": "Then consider defining Z_t\nto be equal to X_t transpose,", "start": 3042.64, "duration": 14.13}, {"text": "X_(t-1) transpose up\nto X_(t-p-1) transpose.", "start": 3056.77, "duration": 5.119}, {"text": "Or this is t minus (p-1).", "start": 3067.378, "duration": 2.552}, {"text": "So there are p terms.", "start": 3069.93, "duration": 0.985}, {"text": "And then if we consider\nthe lagged value of that,", "start": 3073.63, "duration": 6.656}, {"text": "that's X_(t-1), X_(t-2),\nX_(t-p) transpose.", "start": 3080.286, "duration": 10.184}, {"text": "So what we've done is\nwe're considering Z_t.", "start": 3090.47, "duration": 4.91}, {"text": "This is going to be m times p.", "start": 3095.38, "duration": 5.0}, {"text": "It's actually 1 by m\ntimes p in this notation.", "start": 3100.38, "duration": 6.512}, {"text": "Well, actually I guess I\nshould put transpose here.", "start": 3106.892, "duration": 3.958}, {"text": "So m times p by 1.", "start": 3110.85, "duration": 3.89}, {"text": "OK, in the lecture\nnotes it actually", "start": 3114.74, "duration": 2.3}, {"text": "is primed there to\nindicate the transpose.", "start": 3117.04, "duration": 3.6}, {"text": "Well, if you define Z_t\nand Z_(t-1) this way,", "start": 3120.64, "duration": 3.02}, {"text": "then Z_t is equal to D\nplus A of Z_(t-1) plus F.", "start": 3123.66, "duration": 6.95}, {"text": "Where this is d, basically\nthe constant term has the C", "start": 3130.61, "duration": 3.81}, {"text": "entering and then\n0's everywhere else.", "start": 3134.42, "duration": 2.24}, {"text": "And the A matrix is\nphi_1, phi_2, up to phi_p.", "start": 3136.66, "duration": 7.16}, {"text": "And so basically the Z_t\nvector transforms the Z_t--", "start": 3143.82, "duration": 12.59}, {"text": "or is the transform--\nthis linear transformation", "start": 3156.41, "duration": 4.18}, {"text": "of the Z_(t-1).", "start": 3160.59, "duration": 2.7}, {"text": "And we have sort of\na very simple form", "start": 3163.29, "duration": 3.17}, {"text": "for the constant term and a very\nsimple form for the F vector.", "start": 3166.46, "duration": 6.18}, {"text": "And this is-- renders the model\ninto a sort of a first order", "start": 3172.64, "duration": 7.06}, {"text": "time series model with a\nlarger multivariate series,", "start": 3179.7, "duration": 6.57}, {"text": "basically mp by 1.", "start": 3186.27, "duration": 3.32}, {"text": "Now, with this representation\nwe basically have-- we", "start": 3189.59, "duration": 11.79}, {"text": "can demonstrate that the process\nis going to be stationary", "start": 3201.38, "duration": 9.66}, {"text": "if all eigenvalues of\nthe companion matrix A", "start": 3211.04, "duration": 3.71}, {"text": "have modulus less than 1.", "start": 3214.75, "duration": 3.31}, {"text": "And let's see-- if we go\nback to the expression.", "start": 3218.06, "duration": 5.26}, {"text": "OK, if the eigenvalues of\nthis matrix A are less than 1,", "start": 3223.32, "duration": 7.43}, {"text": "then we won't get sort\nof an explosive behavior", "start": 3230.75, "duration": 4.69}, {"text": "of the process when this\nbasically increments over time", "start": 3235.44, "duration": 4.88}, {"text": "with every previous\nvalue getting", "start": 3240.32, "duration": 2.68}, {"text": "multiplied by the A matrix and\nscaling the process over time", "start": 3243.0, "duration": 5.42}, {"text": "by the A-th power.", "start": 3248.42, "duration": 2.46}, {"text": "So that is required.", "start": 3250.88, "duration": 1.57}, {"text": "All eigenvalues of A\nhave to be less than 1.", "start": 3252.45, "duration": 2.17}, {"text": "And equivalently, all\nroots of this equation", "start": 3254.62, "duration": 2.47}, {"text": "need to be outside\nthe unit circle.", "start": 3257.09, "duration": 4.21}, {"text": "You remember there was a\nconstraint of-- or a condition", "start": 3261.3, "duration": 4.26}, {"text": "for univariate\nautoregressive models", "start": 3265.56, "duration": 4.47}, {"text": "to be stationary, that the roots\nof the characteristic equation", "start": 3270.03, "duration": 5.07}, {"text": "are all outside the unit circle.", "start": 3275.1, "duration": 3.0}, {"text": "And the class notes\ngo through and went", "start": 3278.1, "duration": 2.07}, {"text": "through the derivation of that.", "start": 3280.17, "duration": 1.59}, {"text": "This is the extension of that\nto the multivariate case.", "start": 3281.76, "duration": 5.06}, {"text": "And so basically\none needs to solve", "start": 3286.82, "duration": 3.47}, {"text": "for roots of a polynomial in\nz and determine whether those", "start": 3290.29, "duration": 3.81}, {"text": "are outside the unit circle.", "start": 3294.1, "duration": 5.02}, {"text": "Who can tell me what the\norder of the polynomial", "start": 3299.12, "duration": 2.34}, {"text": "is here for this sort\nof determinant equation?", "start": 3301.46, "duration": 6.42}, {"text": "AUDIENCE: [INAUDIBLE] mp.", "start": 3307.88, "duration": 1.84}, {"text": "PROFESSOR: mp.", "start": 3309.72, "duration": 1.38}, {"text": "Yes.", "start": 3311.1, "duration": 0.5}, {"text": "It's basically of power mp.", "start": 3311.6, "duration": 2.07}, {"text": "So in a determinant\nyou basically", "start": 3313.67, "duration": 2.2}, {"text": "are taking products\nof the m components", "start": 3315.87, "duration": 4.04}, {"text": "in the matrix, various\nlinear combinations of those.", "start": 3319.91, "duration": 4.14}, {"text": "So that's going to be an\nmp-dimensional polynomial.", "start": 3324.05, "duration": 4.56}, {"text": "All right.", "start": 3328.61, "duration": 0.5}, {"text": "Well, the mean of the\nstationary VAR process", "start": 3329.11, "duration": 3.57}, {"text": "can be computed rather\neasily by taking expectations", "start": 3332.68, "duration": 4.54}, {"text": "of this on both sides.", "start": 3337.22, "duration": 4.44}, {"text": "So if we take the\nexpectation of X_t", "start": 3341.66, "duration": 3.06}, {"text": "and take expectations\nacross both sides,", "start": 3344.72, "duration": 3.99}, {"text": "we get that mu is the C vector\nplus the product of the phi_k's", "start": 3348.71, "duration": 9.15}, {"text": "times mu plus 0.", "start": 3357.86, "duration": 1.81}, {"text": "So mu, the unconditional\nmean of the process,", "start": 3359.67, "duration": 5.95}, {"text": "actually has this\nformula, just solving", "start": 3365.62, "duration": 5.02}, {"text": "for mu in the top-- in the\nsecond line to the third line.", "start": 3370.64, "duration": 8.17}, {"text": "So here we can see that\nbasically this expression 1", "start": 3378.81, "duration": 8.57}, {"text": "minus phi_1 through phi_p,\nthat inverse has to exist.", "start": 3387.38, "duration": 5.66}, {"text": "And actually, if we then\nplug in the value of C", "start": 3393.04, "duration": 3.85}, {"text": "in terms of the\nunconditional mean,", "start": 3396.89, "duration": 2.16}, {"text": "we get this expression\nfor the original process.", "start": 3399.05, "duration": 4.81}, {"text": "So the unconditional mean C,\nif we demeaned the process,", "start": 3403.86, "duration": 6.06}, {"text": "there's basically no mean term.", "start": 3409.92, "duration": 2.37}, {"text": "There's 0.", "start": 3412.29, "duration": 1.44}, {"text": "And so basically the\nmean-adjusted process", "start": 3413.73, "duration": 3.77}, {"text": "X follows this multivariate\nvector autoregression", "start": 3417.5, "duration": 4.96}, {"text": "with no mean, which is actually\nused when this is specified.", "start": 3422.46, "duration": 5.97}, {"text": "Now, this vector\nautoregression model", "start": 3431.45, "duration": 7.4}, {"text": "can be expressed as a system\nof regression equations.", "start": 3438.85, "duration": 6.91}, {"text": "And so what we have with the\nmultivariate series, if we have", "start": 3445.76, "duration": 8.06}, {"text": "multivariate data, we'll have\nn sample observations x_t,", "start": 3453.82, "duration": 5.05}, {"text": "which is basically the m-vector\nof the multivariate process", "start": 3458.87, "duration": 3.31}, {"text": "observed for n time points.", "start": 3462.18, "duration": 3.53}, {"text": "And for the\ncomputations here, we're", "start": 3465.71, "duration": 2.29}, {"text": "going to assume that\nwe have p sort of-- we", "start": 3468.0, "duration": 4.05}, {"text": "have pre-sample observations\navailable to us.", "start": 3472.05, "duration": 4.05}, {"text": "So we're essentially going\nto be considering models", "start": 3476.1, "duration": 2.88}, {"text": "where we condition\non the first p time", "start": 3478.98, "duration": 2.63}, {"text": "points in order to facilitate\nthe estimation methodology.", "start": 3481.61, "duration": 6.05}, {"text": "Then we can set up m\nregression models corresponding", "start": 3487.66, "duration": 4.38}, {"text": "to each component of\nthe m-variate series.", "start": 3492.04, "duration": 4.04}, {"text": "And so what we have\nis our original--", "start": 3496.08, "duration": 16.11}, {"text": "we have our collection of data\nvalues, which is x_1 transpose,", "start": 3512.19, "duration": 6.91}, {"text": "x_2 transpose, down\nto x_n transpose,", "start": 3519.1, "duration": 6.65}, {"text": "which is an n by m matrix.", "start": 3525.75, "duration": 6.54}, {"text": "OK, this is our\nmultivariate time series", "start": 3532.29, "duration": 2.25}, {"text": "where we were\njust-- the first row", "start": 3534.54, "duration": 1.69}, {"text": "corresponds to the first time\nvalues, nth row to the nth time", "start": 3536.23, "duration": 2.785}, {"text": "values.", "start": 3539.015, "duration": 0.5}, {"text": "And we can set up\nm regression models", "start": 3542.18, "duration": 3.4}, {"text": "where we're going\nto consider modeling", "start": 3545.58, "duration": 5.83}, {"text": "the j-th column of this matrix.", "start": 3551.41, "duration": 4.51}, {"text": "So we're just picking out\nthe univariate time series", "start": 3555.92, "duration": 3.26}, {"text": "corresponding to\nthe j-th component.", "start": 3559.18, "duration": 2.14}, {"text": "That's y j.", "start": 3561.32, "duration": 2.67}, {"text": "And we're going to\nmodel that as Z beta j", "start": 3563.99, "duration": 6.27}, {"text": "plus epsilon j where Z is given\nby the vector of lagged values", "start": 3570.26, "duration": 11.12}, {"text": "of the multivariate\nprocess where there's,", "start": 3581.38, "duration": 4.94}, {"text": "for the t-th-- t\nminus first value", "start": 3586.32, "duration": 2.72}, {"text": "we have that current\nvalue-- or the t", "start": 3589.04, "duration": 3.47}, {"text": "minus first, t minus\nsecond, up to t minus p.", "start": 3592.51, "duration": 2.38}, {"text": "So we have basically\np m-vectors here.", "start": 3594.89, "duration": 6.32}, {"text": "And so this j-th time\nseries has elements", "start": 3601.21, "duration": 7.89}, {"text": "that follow a linear\nregression model", "start": 3609.1, "duration": 5.09}, {"text": "on the lags of the entire\nmultivariate series up to p", "start": 3614.19, "duration": 3.91}, {"text": "lags with their regression\nparameter given by beta j.", "start": 3618.1, "duration": 5.28}, {"text": "And basically the beta\nj regression parameters", "start": 3623.38, "duration": 4.87}, {"text": "corresponds to the various\nelements of the phi matrices.", "start": 3628.25, "duration": 7.799}, {"text": "So now there's a one-to-one one\ncorrespondence between those.", "start": 3636.049, "duration": 2.541}, {"text": "All right.", "start": 3650.8, "duration": 0.5}, {"text": "So I'm using now a notation\nwhere superscript j corresponds", "start": 3651.3, "duration": 7.98}, {"text": "to the j-th component\nof the series,", "start": 3659.28, "duration": 3.65}, {"text": "of the multivariate\nstochastic process.", "start": 3662.93, "duration": 4.83}, {"text": "So we have an mp plus 1 vector\nof regression parameters", "start": 3667.76, "duration": 4.79}, {"text": "for each series j, and\nwe have an epsilon j", "start": 3672.55, "duration": 3.61}, {"text": "for-- an n-vector of innovation\nerrors for each series.", "start": 3676.16, "duration": 5.94}, {"text": "And so basically if this,\nthe j-th column, is y j,", "start": 3682.1, "duration": 9.87}, {"text": "we're modeling that to be\nequal to the simple matrix", "start": 3691.97, "duration": 3.77}, {"text": "Z times beta j plus epsilon\nj, where this is n by 1.", "start": 3695.74, "duration": 8.8}, {"text": "This is n by np plus 1.", "start": 3704.54, "duration": 3.25}, {"text": "And this beta j is the mp\nplus 1 regression parameter.", "start": 3711.52, "duration": 4.4}, {"text": "OK.", "start": 3724.845, "duration": 0.5}, {"text": "One might think,\nOK, one can consider", "start": 3730.14, "duration": 1.89}, {"text": "each of these regressions for\neach of the component series,", "start": 3732.03, "duration": 5.29}, {"text": "you could consider\nthem separately.", "start": 3737.32, "duration": 2.31}, {"text": "But to consider\nthem all together,", "start": 3739.63, "duration": 4.31}, {"text": "we can define the\nmultivariate regression model,", "start": 3743.94, "duration": 5.33}, {"text": "which has the following form.", "start": 3749.27, "duration": 4.15}, {"text": "We basically have the n-vectors\nfor the first component,", "start": 3753.42, "duration": 6.857}, {"text": "and then the second component\nup to nth component.", "start": 3760.277, "duration": 2.083}, {"text": "So an n by p matrix of\ndependent variables,", "start": 3762.36, "duration": 4.37}, {"text": "where each column corresponds\nto a different component series,", "start": 3766.73, "duration": 6.81}, {"text": "follows a linear\nregression model", "start": 3773.54, "duration": 2.28}, {"text": "with the same Z matrix\nwith different regression", "start": 3775.82, "duration": 4.17}, {"text": "coefficient parameters, beta\n1 through beta m corresponding", "start": 3779.99, "duration": 3.47}, {"text": "to the different components\nof the multivariate series.", "start": 3783.46, "duration": 4.58}, {"text": "And we have epsilon 1,\nepsilon 2, up to epsilon m.", "start": 3788.04, "duration": 6.29}, {"text": "So we're thinking of taking--\nso basically the y 1, y 2,", "start": 3794.33, "duration": 6.34}, {"text": "up to y m is essentially\nthis original matrix", "start": 3800.67, "duration": 5.49}, {"text": "of our multivariate\ntime series because it's", "start": 3806.16, "duration": 3.56}, {"text": "the first component\nin the first column", "start": 3809.72, "duration": 6.1}, {"text": "and the nth component\nin the nth column.", "start": 3815.82, "duration": 1.85}, {"text": "And the-- this\nregression parameter", "start": 3817.67, "duration": 4.56}, {"text": "or this explanatory variables\nmatrix X, Z in this case", "start": 3822.23, "duration": 4.86}, {"text": "corresponds to lags of the\nwhole process up to p lags.", "start": 3827.09, "duration": 6.12}, {"text": "So we're having\nlags of all the--", "start": 3833.21, "duration": 2.87}, {"text": "the m-variate\nprocess up to p lags.", "start": 3836.08, "duration": 2.15}, {"text": "So that's mp and then\nplus 1 for our constant.", "start": 3838.23, "duration": 4.47}, {"text": "So this is the set up for a\nmultivariate regression model.", "start": 3842.7, "duration": 3.09}, {"text": "In terms of how\none specifies this,", "start": 3852.88, "duration": 2.03}, {"text": "well, actually,\nin economic theory", "start": 3854.91, "duration": 2.72}, {"text": "this is also related\nto seemingly unrelated", "start": 3857.63, "duration": 3.3}, {"text": "regressions, which you'll\nfind in econometrics.", "start": 3860.93, "duration": 2.82}, {"text": "If we want to specify this\nmultivariate model, well,", "start": 3866.73, "duration": 6.12}, {"text": "what we could do is\nwe could actually", "start": 3872.85, "duration": 2.67}, {"text": "specify each of the\ncomponent models", "start": 3875.52, "duration": 2.03}, {"text": "separately because we\nbasically have sort of-- can", "start": 3877.55, "duration": 4.424}, {"text": "think of the\nunivariate regression", "start": 3881.974, "duration": 1.416}, {"text": "model for each component series.", "start": 3883.39, "duration": 3.62}, {"text": "And this slide\nindicates basically what", "start": 3887.01, "duration": 5.57}, {"text": "the formulas are for that.", "start": 3892.58, "duration": 1.23}, {"text": "So if we don't know anything\nabout multivariate regression", "start": 3893.81, "duration": 4.379}, {"text": "we can say, well,\nlet's start by just", "start": 3898.189, "duration": 1.541}, {"text": "doing the univariate regression\nof each component series", "start": 3899.73, "duration": 3.89}, {"text": "on the lags.", "start": 3903.62, "duration": 1.2}, {"text": "And so we get our beta\nhat j's least squares", "start": 3904.82, "duration": 2.72}, {"text": "estimates given by the\nusual formula where", "start": 3907.54, "duration": 2.79}, {"text": "the independent variables matrix\nZ goes Z transpose Z inverse Z", "start": 3910.33, "duration": 4.62}, {"text": "transpose Y are the residuals.", "start": 3914.95, "duration": 2.33}, {"text": "So these are familiar formulas.", "start": 3917.28, "duration": 2.81}, {"text": "And if we did this for each\nof the component series j,", "start": 3920.09, "duration": 8.59}, {"text": "then we would actually\nget sample estimates", "start": 3928.68, "duration": 5.07}, {"text": "of the innovation process,\nthe eta_1, basically", "start": 3933.75, "duration": 4.0}, {"text": "the whole eta series.", "start": 3937.75, "duration": 3.22}, {"text": "And we could actually\ndefine from these estimates", "start": 3940.97, "duration": 5.01}, {"text": "of the innovations\nour covariance matrix", "start": 3945.98, "duration": 3.13}, {"text": "for the innovations as\nthe sample covariance", "start": 3949.11, "duration": 3.39}, {"text": "matrix of these etas.", "start": 3952.5, "duration": 2.14}, {"text": "So all of these formulas\nare-- you're basically", "start": 3954.64, "duration": 3.53}, {"text": "applying very\nstraightforward estimation", "start": 3958.17, "duration": 2.66}, {"text": "methods for the parameters\nof a linear regression", "start": 3960.83, "duration": 4.61}, {"text": "and then estimating\nvariances/covariances", "start": 3965.44, "duration": 3.415}, {"text": "of these innovation terms.", "start": 3968.855, "duration": 2.585}, {"text": "So from this, we\nactually have estimates", "start": 3971.44, "duration": 3.03}, {"text": "of this process in terms of\nthe sigma and the beta hats.", "start": 3974.47, "duration": 5.95}, {"text": "But it's made\nassuming that we can", "start": 3980.42, "duration": 3.8}, {"text": "treat each of these component\nregressions separately.", "start": 3984.22, "duration": 2.535}, {"text": "A rather remarkable\nresult is that", "start": 3993.41, "duration": 1.9}, {"text": "these component-wise\nregressions are actually", "start": 3995.31, "duration": 4.99}, {"text": "the optimal estimates for\nthe multivariate regression", "start": 4000.3, "duration": 4.17}, {"text": "as well.", "start": 4004.47, "duration": 1.56}, {"text": "And as mathematicians,\nthis kind of result", "start": 4006.03, "duration": 5.81}, {"text": "is, I think, rather\nneat and elegant.", "start": 4011.84, "duration": 2.77}, {"text": "And maybe some of you will\nthink this is very obvious,", "start": 4014.61, "duration": 4.29}, {"text": "but it actually-- it\nisn't quite obvious.", "start": 4018.9, "duration": 6.82}, {"text": "That said, this\ncomponent-wise estimation", "start": 4025.72, "duration": 2.29}, {"text": "should be optimal as well.", "start": 4028.01, "duration": 2.42}, {"text": "And the next section\nof the lecture notes", "start": 4030.43, "duration": 2.67}, {"text": "goes through this argument.", "start": 4033.1, "duration": 3.865}, {"text": "And I'm going to, in\nthe interest of time,", "start": 4040.14, "duration": 2.34}, {"text": "go through this-- just sort of\nhighlight what the results are.", "start": 4042.48, "duration": 4.11}, {"text": "The details are in these\nnotes that you can go through.", "start": 4046.59, "duration": 2.99}, {"text": "And I will be happy to go\ninto more detail about them", "start": 4049.58, "duration": 5.17}, {"text": "during office hours.", "start": 4054.75, "duration": 2.51}, {"text": "But if we're fitting a vector\nautoregression model where", "start": 4057.26, "duration": 4.26}, {"text": "there are no constraints\non the coefficient", "start": 4061.52, "duration": 2.46}, {"text": "matrices phi_1\nthrough phi_p, then", "start": 4063.98, "duration": 3.64}, {"text": "these component-wise\nestimates, accounting", "start": 4067.62, "duration": 5.02}, {"text": "for arbitrary covariance matrix\nsigma for the innovations,", "start": 4072.64, "duration": 8.067}, {"text": "those basically are equal\nto the generalized least", "start": 4080.707, "duration": 2.083}, {"text": "squares estimates of these\nunderlying parameters.", "start": 4082.79, "duration": 3.24}, {"text": "You'll recall we talked about\nthe Gauss-Markov theorem", "start": 4086.03, "duration": 3.25}, {"text": "where we were able to extend the\nassumption of equal variances", "start": 4089.28, "duration": 6.09}, {"text": "across observations to unequal\nvariances and covariances.", "start": 4095.37, "duration": 4.819}, {"text": "Well, it turns out to\nthese component-wise OLS", "start": 4100.189, "duration": 3.643}, {"text": "estimates are, in fact, the\ngeneralized least squared", "start": 4103.832, "duration": 2.208}, {"text": "estimates.", "start": 4106.04, "duration": 1.14}, {"text": "And under the assumption\nof Gaussian distributions", "start": 4107.18, "duration": 2.98}, {"text": "for the innovations,\nthey, in fact,", "start": 4110.16, "duration": 2.25}, {"text": "are maximum\nlikelihood estimates.", "start": 4112.41, "duration": 2.159}, {"text": "And this theory applies\nKronecker products.", "start": 4114.569, "duration": 6.641}, {"text": "We're not going to have\nany homework with Kronecker", "start": 4121.21, "duration": 2.399}, {"text": "products.", "start": 4123.609, "duration": 0.971}, {"text": "These notes really\nare for those who", "start": 4124.58, "duration": 2.58}, {"text": "have some more extensive\nbackground in linear algebra.", "start": 4127.16, "duration": 3.96}, {"text": "But it's a very nice use\nof these Kronecker product", "start": 4131.12, "duration": 4.01}, {"text": "operators.", "start": 4135.13, "duration": 1.05}, {"text": "Basically, this\nnotation-- I don't", "start": 4136.18, "duration": 3.95}, {"text": "know, x circle, I'll\ncall it Kronecker--", "start": 4140.13, "duration": 4.41}, {"text": "is one where you take a\nmatrix A and a matrix B", "start": 4144.54, "duration": 4.02}, {"text": "and you consider\nthe matrix which", "start": 4148.56, "duration": 2.61}, {"text": "takes each element of A times\nthe whole matrix B. So we start", "start": 4151.17, "duration": 5.08}, {"text": "with an m by n matrix A and\nend up with an mp by qn matrix", "start": 4156.25, "duration": 5.97}, {"text": "by taking each element of\nA times the whole matrix B.", "start": 4162.22, "duration": 3.33}, {"text": "So it's, they say, has\nthis block structure.", "start": 4165.55, "duration": 3.46}, {"text": "So this is very\nsimple definition.", "start": 4169.01, "duration": 3.5}, {"text": "If you look at properties of\ntransposition of matrices,", "start": 4172.51, "duration": 4.57}, {"text": "you can prove these results.", "start": 4177.08, "duration": 1.46}, {"text": "These are properties of\nthe Kronecker product.", "start": 4178.54, "duration": 4.31}, {"text": "And there's a vec operator\nwhich takes a matrix", "start": 4182.85, "duration": 11.47}, {"text": "and simply stacks\nthe columns together.", "start": 4194.32, "duration": 4.15}, {"text": "And in the talk last Tuesday of\nIvan's, talking about modeling", "start": 4198.47, "duration": 6.23}, {"text": "the volatility surface,\nhe basically, he", "start": 4204.7, "duration": 3.145}, {"text": "was modeling a two dimensional\nsurface-- or a surface", "start": 4207.845, "duration": 3.565}, {"text": "in three dimensions,\nbut there was", "start": 4211.41, "duration": 2.5}, {"text": "two dimensions explaining it.", "start": 4213.91, "duration": 2.92}, {"text": "You basically can stack\ncolumns of the matrix", "start": 4216.83, "duration": 5.31}, {"text": "and be modeling a vector\ninstead of a matrix of values.", "start": 4222.14, "duration": 4.89}, {"text": "So the vectorizing operator\nallows us to manipulate terms", "start": 4227.03, "duration": 5.1}, {"text": "into a more convenient form.", "start": 4232.13, "duration": 3.27}, {"text": "And this multivariate\nregression model", "start": 4235.4, "duration": 3.64}, {"text": "is one where it's set up as\nsort of a n by m matrix Y,", "start": 4239.04, "duration": 11.91}, {"text": "having that structure.", "start": 4250.95, "duration": 2.54}, {"text": "It can be expressed in terms\nof the linear regression form", "start": 4253.49, "duration": 3.62}, {"text": "as y star equaling the\nvector, the vec of y.", "start": 4257.11, "duration": 9.27}, {"text": "So we basically have y 1, y\n2, down to y m all lined up.", "start": 4266.38, "duration": 8.96}, {"text": "So this is pm by 1.", "start": 4275.34, "duration": 3.14}, {"text": "That's going to be equal to\nsome matrix plus the epsilon 1,", "start": 4281.6, "duration": 8.455}, {"text": "epsilon 2, down to epsilon n.", "start": 4290.055, "duration": 3.865}, {"text": "And then there's\ngoing to be a matrix", "start": 4293.92, "duration": 4.93}, {"text": "and a regression\ncoefficient matrix beta", "start": 4298.85, "duration": 4.47}, {"text": "1, beta 2, down to beta p.", "start": 4303.32, "duration": 4.04}, {"text": "So we consider vectorizing\nthe beta matrix,", "start": 4307.36, "duration": 3.96}, {"text": "vectorizing epsilon,\nand vectorizing y.", "start": 4311.32, "duration": 4.02}, {"text": "And then in order\nto define this sort", "start": 4315.34, "duration": 4.125}, {"text": "of simple linear regression\nmodel, univariate regression", "start": 4319.465, "duration": 3.905}, {"text": "model, well, we need to\nhave a Z in the first column", "start": 4323.37, "duration": 5.38}, {"text": "here corresponding to beta 1 for\ny 1, and 0's everywhere else.", "start": 4328.75, "duration": 6.05}, {"text": "In the second block\nwe want to have", "start": 4334.8, "duration": 5.36}, {"text": "a Z in the second off diagonal\nwith 0's everywhere else and so", "start": 4340.16, "duration": 5.76}, {"text": "forth.", "start": 4345.92, "duration": 1.18}, {"text": "So this is just re-expressing\neverything in this notation.", "start": 4347.1, "duration": 3.86}, {"text": "But the notation is very nice\nbecause, at the end of the day", "start": 4350.96, "duration": 3.33}, {"text": "we basically have a regression\nmodel like we had when we were", "start": 4354.29, "duration": 3.246}, {"text": "doing our regression analysis.", "start": 4357.536, "duration": 1.494}, {"text": "So all the theory we have\nfor specifying these models", "start": 4359.03, "duration": 3.9}, {"text": "plays through with\nunivariate regression.", "start": 4362.93, "duration": 3.34}, {"text": "And one can go through\nthis technical argument", "start": 4366.27, "duration": 4.171}, {"text": "to show that the\ngeneralized least squares", "start": 4370.441, "duration": 1.749}, {"text": "estimate is, in\nfact, the equivalent", "start": 4372.19, "duration": 4.71}, {"text": "to the component-wise values.", "start": 4376.9, "duration": 2.53}, {"text": "And that's very, very good.", "start": 4379.43, "duration": 4.55}, {"text": "Maximum likelihood\nestimation with these models.", "start": 4383.98, "duration": 3.02}, {"text": "Well, we actually use\nthis vectorized notation", "start": 4387.0, "duration": 5.13}, {"text": "to define the\nlikelihood function.", "start": 4392.13, "duration": 2.92}, {"text": "And if these\nassumptions are made", "start": 4395.05, "duration": 5.51}, {"text": "about the linear\nregression model,", "start": 4400.56, "duration": 4.05}, {"text": "we basically have\nan n times m vector", "start": 4404.61, "duration": 4.13}, {"text": "of dependent variable values,\nwhereas your multivariate", "start": 4408.74, "duration": 6.04}, {"text": "normal with mean given\nby x star beta star", "start": 4414.78, "duration": 3.92}, {"text": "and then a covariance\nmatrix epsilon.", "start": 4418.7, "duration": 3.17}, {"text": "The covariance matrix of\nepsilon star is sigma star.", "start": 4421.87, "duration": 5.51}, {"text": "Well, sigma star is I_n\nKronecker product sigma.", "start": 4427.38, "duration": 3.52}, {"text": "So if you go through\nthe math of this,", "start": 4430.9, "duration": 3.23}, {"text": "everything matches up in terms\nof what the assumptions are.", "start": 4434.13, "duration": 5.12}, {"text": "And the conditional probability\ndensity function of this data", "start": 4439.25, "duration": 6.09}, {"text": "is the usual functions\nof log-normal", "start": 4445.34, "duration": 6.92}, {"text": "or of a normal sample.", "start": 4452.26, "duration": 2.67}, {"text": "So we have unknown\nparameters beta star sigma,", "start": 4454.93, "duration": 5.92}, {"text": "which are equal to\nthe joint density", "start": 4460.85, "duration": 6.11}, {"text": "of this normal linear\nregression model.", "start": 4466.96, "duration": 2.78}, {"text": "So this corresponds to what we\nhad before in our regression", "start": 4469.74, "duration": 4.61}, {"text": "analysis.", "start": 4474.35, "duration": 0.63}, {"text": "We just had this more\ncomplicated definition", "start": 4474.98, "duration": 2.07}, {"text": "of the independent\nvariables matrix X star.", "start": 4477.05, "duration": 3.85}, {"text": "And a more\ncomplicated definition", "start": 4480.9, "duration": 1.57}, {"text": "of our variance/covariance\nmatrix sigma star.", "start": 4482.47, "duration": 4.8}, {"text": "But the log-likelihood\nfunction ends up", "start": 4487.27, "duration": 2.98}, {"text": "being equal to a\nterm proportional", "start": 4490.25, "duration": 4.14}, {"text": "to the log of the determinant\nof our sigma matrix", "start": 4494.39, "duration": 4.7}, {"text": "and minus one half Q of beta\nsigma, where Q of beta sigma", "start": 4499.09, "duration": 4.7}, {"text": "is the least squares criterion\nfor each of the component", "start": 4503.79, "duration": 5.07}, {"text": "models summed up.", "start": 4508.86, "duration": 4.1}, {"text": "So the component-wise\nmaximum likelihood estimation", "start": 4512.96, "duration": 3.7}, {"text": "is-- for the\nunderlying parameters,", "start": 4516.66, "duration": 2.455}, {"text": "is the same as the large one.", "start": 4519.115, "duration": 4.145}, {"text": "And in terms of estimating\nthe covariance matrix,", "start": 4523.26, "duration": 8.62}, {"text": "there's a notion called the\nconcentrated log-likelihood,", "start": 4531.88, "duration": 5.54}, {"text": "which comes into play in\nmodels with many parameters.", "start": 4537.42, "duration": 7.78}, {"text": "In this model, we have\nunknown parameters--", "start": 4545.2, "duration": 3.19}, {"text": "our regression parameters\nbeta and our covariance matrix", "start": 4548.39, "duration": 3.84}, {"text": "for the innovations sigma.", "start": 4552.23, "duration": 2.96}, {"text": "It turns out that our estimate\nof the regression parameter", "start": 4555.19, "duration": 4.66}, {"text": "beta is independent, doesn't\ndepend-- not statistically", "start": 4559.85, "duration": 5.34}, {"text": "independent-- but\ndoes not depend", "start": 4565.19, "duration": 1.38}, {"text": "on the value of the\ncovariance matrix sigma.", "start": 4566.57, "duration": 4.13}, {"text": "So whatever sigma is, we have\nthe same maximum likelihood", "start": 4570.7, "duration": 3.41}, {"text": "estimate for the betas.", "start": 4574.11, "duration": 1.51}, {"text": "So we can consider\nthe log-likelihood", "start": 4575.62, "duration": 4.14}, {"text": "setting the beta parameter\nequal to its maximum likelihood", "start": 4579.76, "duration": 4.78}, {"text": "estimate.", "start": 4584.54, "duration": 0.87}, {"text": "And then we have a\nfunction that just", "start": 4585.41, "duration": 1.86}, {"text": "depends on the data and the\nunknown parameter sigma.", "start": 4587.27, "duration": 4.08}, {"text": "So that's a concentrated\nlikelihood function", "start": 4591.35, "duration": 2.88}, {"text": "that needs to be maximized.", "start": 4594.23, "duration": 1.98}, {"text": "And the maximization of the log\nof a determinant of a matrix", "start": 4596.21, "duration": 4.36}, {"text": "minus n over 2 the trace of that\nmatrix times an estimate of it,", "start": 4600.57, "duration": 5.12}, {"text": "that has been solved.", "start": 4605.69, "duration": 1.52}, {"text": "It's a bit involved.", "start": 4607.21, "duration": 3.03}, {"text": "But if you're interested in\nthe mathematics for how that's", "start": 4610.24, "duration": 2.52}, {"text": "actually solved and how you\ntake derivatives of determinants", "start": 4612.76, "duration": 2.71}, {"text": "and so forth, there's a\npaper by Anderson and Olkin", "start": 4615.47, "duration": 2.682}, {"text": "that goes through all\nthe details of that", "start": 4618.152, "duration": 1.708}, {"text": "that you can Google on the web.", "start": 4619.86, "duration": 1.762}, {"text": "Finally, let's see.", "start": 4625.91, "duration": 1.64}, {"text": "There's-- well, not finally.", "start": 4627.55, "duration": 1.72}, {"text": "There's model selection\ncriteria that can be applied.", "start": 4629.27, "duration": 2.97}, {"text": "These have been applied\nbefore for regression models", "start": 4632.24, "duration": 2.48}, {"text": "for univariate time series\nmodel, the Akaike Information", "start": 4634.72, "duration": 4.06}, {"text": "Criterion, the Bayes Information\nCriterion, Hannan-Quinn", "start": 4638.78, "duration": 3.99}, {"text": "Criterion.", "start": 4642.77, "duration": 1.87}, {"text": "These definitions\nare all consistent", "start": 4644.64, "duration": 2.42}, {"text": "with the other definitions.", "start": 4647.06, "duration": 2.62}, {"text": "They basically take\nthe likelihood function", "start": 4649.68, "duration": 3.65}, {"text": "and you try to maximize that\nplus a penalty for the number", "start": 4653.33, "duration": 4.24}, {"text": "of unknown parameters.", "start": 4657.57, "duration": 1.76}, {"text": "And that's given here.", "start": 4659.33, "duration": 4.05}, {"text": "OK, then the last\nsection goes through", "start": 4665.92, "duration": 1.86}, {"text": "an asymptotic distribution\nof least squares estimates.", "start": 4667.78, "duration": 5.72}, {"text": "And I'll let you read\nthat on your own.", "start": 4673.5, "duration": 3.45}, {"text": "Let's see.", "start": 4676.95, "duration": 0.5}, {"text": "For this lecture I put\ntogether an example", "start": 4677.45, "duration": 5.02}, {"text": "of fitting vector\nautoregressions", "start": 4682.47, "duration": 2.71}, {"text": "with some macroeconomic\nvariables.", "start": 4685.18, "duration": 4.15}, {"text": "And I just wanted to\npoint that out to you.", "start": 4689.33, "duration": 6.03}, {"text": "So let me go to\nthis document here.", "start": 4695.36, "duration": 8.378}, {"text": "What have we got here?", "start": 4703.738, "duration": 1.952}, {"text": "All right.", "start": 4709.594, "duration": 0.986}, {"text": "Well, OK.", "start": 4710.58, "duration": 0.73}, {"text": "Modeling macroeconomic time\nseries is an important topic.", "start": 4711.31, "duration": 6.1}, {"text": "It's what sort of\ncentral bankers do.", "start": 4717.41, "duration": 2.53}, {"text": "They want to\nunderstand what factors", "start": 4719.94, "duration": 2.18}, {"text": "are affecting the economy in\nterms of growth, inflation,", "start": 4722.12, "duration": 2.55}, {"text": "unemployment.", "start": 4724.67, "duration": 1.21}, {"text": "And what's the impact of\ninterest rate policies.", "start": 4725.88, "duration": 4.72}, {"text": "There are some really\nimportant papers", "start": 4730.6, "duration": 2.34}, {"text": "by Robert Litterman and\nChristopher Sims dealing", "start": 4732.94, "duration": 3.81}, {"text": "with fitting vector\nautoregression models", "start": 4736.75, "duration": 2.4}, {"text": "to a macroeconomic time series.", "start": 4739.15, "duration": 2.44}, {"text": "And actually, the\nframework within which", "start": 4741.59, "duration": 1.83}, {"text": "they specified these models\nwas a Bayesian framework,", "start": 4743.42, "duration": 4.26}, {"text": "which is an extension of the\nmaximum likelihood method where", "start": 4747.68, "duration": 3.64}, {"text": "you'll incorporate reasonable\nsort of prior assumptions", "start": 4751.32, "duration": 4.36}, {"text": "about what the\nparameters ought to be.", "start": 4755.68, "duration": 2.51}, {"text": "But in this note,\nI sort of basically", "start": 4758.19, "duration": 7.94}, {"text": "go through collecting various\nmacroeconomic variables", "start": 4766.13, "duration": 3.74}, {"text": "directly off the web\nusing the package R.", "start": 4769.87, "duration": 3.37}, {"text": "All this stuff\nis-- these are data", "start": 4773.24, "duration": 3.31}, {"text": "that you can get your hands on.", "start": 4776.55, "duration": 2.49}, {"text": "Here's the unemployment\nrate from January 1946", "start": 4779.04, "duration": 4.86}, {"text": "up through this past month.", "start": 4783.9, "duration": 3.13}, {"text": "Anyone can see how that's\nvaried between much less than 4%", "start": 4787.03, "duration": 5.64}, {"text": "to over 10%, as it was recently.", "start": 4792.67, "duration": 3.8}, {"text": "And there's also\nthe Fed funds rate,", "start": 4796.47, "duration": 3.08}, {"text": "which is one of\nthe key variables", "start": 4799.55, "duration": 2.55}, {"text": "that the Federal Reserve Open\nMarket Committee controls,", "start": 4802.1, "duration": 4.51}, {"text": "or I should say\ncontrolled in the past,", "start": 4806.61, "duration": 2.27}, {"text": "to try and affect the economy.", "start": 4808.88, "duration": 1.96}, {"text": "Now that value of that\nrate is set almost at zero", "start": 4810.84, "duration": 3.88}, {"text": "and other means\nare applied to have", "start": 4814.72, "duration": 4.11}, {"text": "an impact on economic growth\nand the economic situation", "start": 4818.83, "duration": 6.11}, {"text": "of the market-- of\nthe economy, rather.", "start": 4824.94, "duration": 6.4}, {"text": "Let's see.", "start": 4831.34, "duration": 0.78}, {"text": "There's also-- anyway, a\nbunch of other variables.", "start": 4832.12, "duration": 2.21}, {"text": "CPI, which is a\nmeasure of inflation.", "start": 4834.33, "duration": 4.14}, {"text": "What this note goes through\nis the specification", "start": 4838.47, "duration": 7.032}, {"text": "of vector autoregression\nmodels for these series.", "start": 4845.502, "duration": 6.568}, {"text": "And I use just a\nsmall set of cases.", "start": 4852.07, "duration": 2.42}, {"text": "I look at unemployment\nrate, federal funds,", "start": 4854.49, "duration": 4.15}, {"text": "and the CPI, which is\na measure of inflation.", "start": 4858.64, "duration": 3.83}, {"text": "And there's-- if\none goes through,", "start": 4862.47, "duration": 4.11}, {"text": "there are multivariate\nversions of the autocorrelation", "start": 4866.58, "duration": 4.2}, {"text": "function, as given on\nthe top right panel here,", "start": 4870.78, "duration": 3.89}, {"text": "between these variables.", "start": 4874.67, "duration": 2.44}, {"text": "And one can also do the partial\nautocorrelation function.", "start": 4877.11, "duration": 3.24}, {"text": "You'll recall that\nautocorrelation functions", "start": 4880.35, "duration": 2.939}, {"text": "and partial\nautocorrelation functions", "start": 4883.289, "duration": 1.541}, {"text": "are related to what kind of--\nor help us understand what kind", "start": 4884.83, "duration": 4.21}, {"text": "of order ARMA processes\nmight be appropriate", "start": 4889.04, "duration": 2.35}, {"text": "for univariate series.", "start": 4891.39, "duration": 1.28}, {"text": "For multivariate series,\nthen there are basically", "start": 4892.67, "duration": 4.08}, {"text": "cross lags between variables\nthat are important,", "start": 4896.75, "duration": 3.01}, {"text": "and these can call be captured\nwith vector autoregression", "start": 4899.76, "duration": 2.99}, {"text": "models.", "start": 4902.75, "duration": 0.77}, {"text": "So this goes through and\nshows how these things", "start": 4903.52, "duration": 4.3}, {"text": "are correlated with themselves.", "start": 4907.82, "duration": 2.79}, {"text": "And let's see.", "start": 4910.61, "duration": 1.36}, {"text": "At the end of this note,\nthere are some impulse", "start": 4911.97, "duration": 7.58}, {"text": "response functions\ngraphed, which", "start": 4919.55, "duration": 3.11}, {"text": "are looking at what is the\nimpact of an innovation in one", "start": 4922.66, "duration": 4.71}, {"text": "of the components of the\nmultivariate time series.", "start": 4927.37, "duration": 3.72}, {"text": "So like if Fed funds were to be\nincreased by a certain value,", "start": 4931.09, "duration": 5.48}, {"text": "what would the likely impact\nbe on the unemployment rate?", "start": 4936.57, "duration": 3.57}, {"text": "Or on GNP?", "start": 4940.14, "duration": 2.1}, {"text": "Basically, the production\nlevel of the economy.", "start": 4942.24, "duration": 3.3}, {"text": "And this looks at-- let's see.", "start": 4945.54, "duration": 5.25}, {"text": "Well, actually\nhere we're looking", "start": 4950.79, "duration": 1.47}, {"text": "at the impulse function.", "start": 4952.26, "duration": 2.281}, {"text": "You can look at the\nimpulse function", "start": 4954.541, "duration": 1.499}, {"text": "of innovations on any of\nthe component variables", "start": 4956.04, "duration": 3.21}, {"text": "on all the others.", "start": 4959.25, "duration": 0.75}, {"text": "And in this case,\non the left panel", "start": 4960.0, "duration": 2.15}, {"text": "here is-- it shows what\nhappens when unemployment", "start": 4962.15, "duration": 5.64}, {"text": "has a spike up, or unit spike.", "start": 4967.79, "duration": 2.57}, {"text": "A unit impulse up.", "start": 4970.36, "duration": 1.4}, {"text": "Well, this second\npanel shows what's", "start": 4971.76, "duration": 3.7}, {"text": "likely to happen to\nthe Fed funds rate.", "start": 4975.46, "duration": 1.73}, {"text": "It turns out that's\nlikely to go down.", "start": 4977.19, "duration": 2.54}, {"text": "And that sort of is\nindicating-- it's sort", "start": 4979.73, "duration": 1.94}, {"text": "of reflecting\nwhat, historically,", "start": 4981.67, "duration": 1.7}, {"text": "was the policy of the Fed to\nbasically reduce interest rates", "start": 4983.37, "duration": 5.81}, {"text": "if unemployment was rising.", "start": 4989.18, "duration": 2.37}, {"text": "And then-- so anyway, these\nimpulse response functions", "start": 4991.55, "duration": 4.85}, {"text": "correspond to essentially those\ninnovation terms on the Wold", "start": 4996.4, "duration": 3.13}, {"text": "decomposition.", "start": 4999.53, "duration": 0.92}, {"text": "And why are these important?", "start": 5000.45, "duration": 1.91}, {"text": "Well, this indicates a\nconnection, basically,", "start": 5002.36, "duration": 4.36}, {"text": "between that sort of moving\naverage representation", "start": 5006.72, "duration": 3.54}, {"text": "and these time series models.", "start": 5010.26, "duration": 1.61}, {"text": "And the way these\ngraphs are generated", "start": 5011.87, "duration": 3.61}, {"text": "is by essentially finding\nthe Wold decomposition", "start": 5015.48, "duration": 3.61}, {"text": "and then incorporating\nthat into these values.", "start": 5019.09, "duration": 4.79}, {"text": "So-- OK, we'll finish\nthere for today.", "start": 5023.88, "duration": 3.66}]