[{"text": "All right. Hi everyone, welcome back.", "start": 3.5, "duration": 3.955}, {"text": "Um, so what we'll see today is, um,", "start": 7.455, "duration": 6.375}, {"text": "additional, uh, elaborations on the EM,", "start": 13.83, "duration": 4.545}, {"text": "um, on the expectation maximization algorithm.", "start": 18.375, "duration": 3.84}, {"text": "And so, um, what you see today is,", "start": 22.215, "duration": 3.06}, {"text": "um, go over, you know,", "start": 25.275, "duration": 1.755}, {"text": "quick recap of what we talked about EM on Monday,", "start": 27.03, "duration": 3.075}, {"text": "and then describe how you can monitor if EM is converging.", "start": 30.105, "duration": 4.86}, {"text": "Um, and, um, uh, on,", "start": 34.965, "duration": 3.66}, {"text": "on Monday we talked about the mixture of Gaussians model,", "start": 38.625, "duration": 3.15}, {"text": "and started deriving EM for that.", "start": 41.775, "duration": 2.21}, {"text": "I want to just take these two equations and map it back to", "start": 43.985, "duration": 3.225}, {"text": "specifically the E and M steps that you saw for the mixture of Gaussians models, uh,", "start": 47.21, "duration": 4.28}, {"text": "to see exactly how these map to, um, uh,", "start": 51.49, "duration": 3.42}, {"text": "you know, updating the weights of the i and so on,", "start": 54.91, "duration": 2.58}, {"text": "um, how you actually derive the M step.", "start": 57.49, "duration": 2.11}, {"text": "Um, and then mostly what I want to spend today talking", "start": 59.6, "duration": 3.45}, {"text": "about is the model called the factor analysis model.", "start": 63.05, "duration": 3.735}, {"text": "Um, and this model useful for, um, for,", "start": 66.785, "duration": 4.02}, {"text": "for data, um, that can be very", "start": 70.805, "duration": 2.305}, {"text": "high-dimensional even when you have very few training examples.", "start": 73.11, "duration": 3.08}, {"text": "So what I wanna do is talk a bit about properties of Gaussian distributions, and then um,", "start": 76.19, "duration": 5.4}, {"text": "describe the factor analysis model, uh,", "start": 81.59, "duration": 2.46}, {"text": "some more about Gaussian distributions and then we'll", "start": 84.05, "duration": 2.16}, {"text": "derive EM for the factor analysis model.", "start": 86.21, "duration": 2.945}, {"text": "And, uh, I want to talk about factor analysis for two reasons, because one is,", "start": 89.155, "duration": 4.015}, {"text": "it's actually a useful algorithm in and of its own right.", "start": 93.17, "duration": 2.7}, {"text": "And second the derivation for EM for", "start": 95.87, "duration": 2.61}, {"text": "factor analysis is actually one of the trickier ones, and, uh,", "start": 98.48, "duration": 3.36}, {"text": "there are key steps in how you actually derive the E and M steps that I think you", "start": 101.84, "duration": 4.86}, {"text": "learn better or you better- master better by going through the factor analysis example.", "start": 106.7, "duration": 5.775}, {"text": "Okay. Um, so just to recap,", "start": 112.475, "duration": 3.9}, {"text": "last Monday or on Monday we had talked about the EM algorithm,", "start": 116.375, "duration": 6.085}, {"text": "uh, and we wound up figuring out this E-step and this M-step, right?", "start": 122.46, "duration": 4.19}, {"text": "And remember that if this is the log likelihood that you're trying to maximize,", "start": 126.65, "duration": 4.8}, {"text": "what the E-step does is it constructs a lower bound uh,", "start": 131.45, "duration": 3.385}, {"text": "that- this is a function of Theta.", "start": 134.835, "duration": 1.795}, {"text": "So this thing on the right hand side,", "start": 136.63, "duration": 3.28}, {"text": "this is a function of the parameters Theta.", "start": 139.91, "duration": 3.345}, {"text": "And what we proved last time was that, um,", "start": 143.255, "duration": 3.255}, {"text": "uh, that function is a lower bound of the log likelihood, right?", "start": 146.51, "duration": 5.2}, {"text": "And depending on what you choose for Q,", "start": 151.71, "duration": 2.54}, {"text": "you get different lower bound.", "start": 154.25, "duration": 1.17}, {"text": "So one choice of Q you may get this lower bound,", "start": 155.42, "duration": 1.92}, {"text": "for a different choice of Q you may get that lower bound.", "start": 157.34, "duration": 2.19}, {"text": "For a different choice of Q you may get that lower bound,", "start": 159.53, "duration": 2.315}, {"text": "and what the E-step does is it chooses Q to get the lower bound this tight,", "start": 161.845, "duration": 5.705}, {"text": "that just touches the log likelihood here at the current value of Theta,", "start": 167.55, "duration": 3.66}, {"text": "and what the M-step does is it chooses", "start": 171.21, "duration": 1.79}, {"text": "the parameters Theta that maximizes that lower bound, right?", "start": 173.0, "duration": 2.805}, {"text": "So that was the EM algorithm that we saw.", "start": 175.805, "duration": 4.455}, {"text": "Now um, I wanna step through how you would take this, you know,", "start": 180.26, "duration": 4.695}, {"text": "slightly abstract mathematical definition of EM", "start": 184.955, "duration": 2.925}, {"text": "and derive a concrete algorithm that you would implement, right?", "start": 187.88, "duration": 3.48}, {"text": "In, in, in, in, you know, um, in Python.", "start": 191.36, "duration": 3.075}, {"text": "And so let's, let's just step through this for the mixture of Gaussians model.", "start": 194.435, "duration": 5.415}, {"text": "Um, so for the mixture of Gaussians model we had a model for P of x i,", "start": 199.85, "duration": 6.72}, {"text": "z i which is P of x i given z i times p of z i, right?", "start": 206.57, "duration": 9.28}, {"text": "Um, and our model was that z is multinomial with some set of parameters phi,", "start": 215.85, "duration": 8.06}, {"text": "and so, you know, the probability of z i to be equal to j is equal to phi j, right?", "start": 223.91, "duration": 7.03}, {"text": "So phi is just a vector of numbers that sum to 1", "start": 230.94, "duration": 2.54}, {"text": "specifying what is the chance of z being each of the,", "start": 233.48, "duration": 2.955}, {"text": "um, k possible discrete values.", "start": 236.435, "duration": 2.935}, {"text": "And then we have that x i given z i equals j,", "start": 239.37, "duration": 5.58}, {"text": "that, that is Gaussian with some mean and some covariance, right?", "start": 244.95, "duration": 5.28}, {"text": "And what we said last time was that, um,", "start": 250.23, "duration": 2.08}, {"text": "this is a lot like the Gaussian discriminant analysis model,", "start": 252.31, "duration": 3.96}, {"text": "uh and uh the, the,", "start": 256.27, "duration": 2.505}, {"text": "the trivial- one trivial difference is this is Sigma j instead of Sigma, right?", "start": 258.775, "duration": 3.7}, {"text": "GDA, Gaussian discriminant analysis,", "start": 262.475, "duration": 1.565}, {"text": "had the same Sigma every class but that's not the key difference.", "start": 264.04, "duration": 2.52}, {"text": "The key difference is that in, um,", "start": 266.56, "duration": 2.7}, {"text": "this density estimation problem,", "start": 269.26, "duration": 2.025}, {"text": "z is not observed or z is a latent random variable, right?", "start": 271.285, "duration": 4.235}, {"text": "Which is why we have all this machinery of, um, of EM.", "start": 275.52, "duration": 6.3}, {"text": "So now that you have this, um, uh,", "start": 281.82, "duration": 5.205}, {"text": "model, um, let's see.", "start": 287.025, "duration": 6.505}, {"text": "So now that you have this model, um,", "start": 296.33, "duration": 3.68}, {"text": "this is how you would derive the E and the M steps, right?", "start": 300.01, "duration": 6.08}, {"text": "So the E-step is, you know,", "start": 306.09, "duration": 2.01}, {"text": "you have Q i of z i, right?", "start": 308.1, "duration": 3.72}, {"text": "But let, let me just write this as Q i of z i equals j. Thi- this is sort of", "start": 311.82, "duration": 3.67}, {"text": "the probability of z i equals j. I", "start": 315.49, "duration": 2.67}, {"text": "know this notation's a little bit strange but under the Q i distribution,", "start": 318.16, "duration": 3.24}, {"text": "whether you want the chance of z being equal to j, right?", "start": 321.4, "duration": 3.915}, {"text": "And so, um, in the E-step you would say that the p of z i equals", "start": 325.315, "duration": 4.565}, {"text": "j given x i parameterized by all of the parameters.", "start": 329.88, "duration": 7.635}, {"text": "And we actually saw with Bayes' rule, right,", "start": 337.515, "duration": 3.915}, {"text": "how you would flesh this out, okay?", "start": 341.43, "duration": 1.995}, {"text": "And what we do in the E-step is,", "start": 343.425, "duration": 2.97}, {"text": "um, store this number, right?", "start": 346.395, "duration": 3.465}, {"text": "In, uh, what we wrote as w i j last time, okay?", "start": 349.86, "duration": 4.86}, {"text": "So you remember, um,", "start": 354.72, "duration": 1.445}, {"text": "if you have a mixture of two Gaussians, maybe that's the first Gaussian,", "start": 356.165, "duration": 2.705}, {"text": "that's the second Gaussian,", "start": 358.87, "duration": 1.245}, {"text": "you have an example x i here so it looks like it's more likely to come from the", "start": 360.115, "duration": 3.705}, {"text": "first than the second Gaussian and so this would be reflected in w i j.", "start": 363.82, "duration": 3.81}, {"text": "That, that example is assigned more to the first Gaussian than to the second Gaussian.", "start": 367.63, "duration": 4.76}, {"text": "So what you implement in code is, you know,", "start": 372.39, "duration": 2.72}, {"text": "you write code to compute this number and store it in wij.", "start": 375.11, "duration": 5.73}, {"text": "Um, and then for the M-step,", "start": 383.3, "duration": 11.839}, {"text": "you will want to maximize over the parameters of the model, right?", "start": 395.139, "duration": 4.356}, {"text": "Phi, mu, and Sigma,", "start": 399.495, "duration": 1.365}, {"text": "these are the param- uh pa-parameters of the mixture of", "start": 400.86, "duration": 3.865}, {"text": "Gaussians of sum over i, sum over z i,", "start": 404.725, "duration": 6.065}, {"text": "right? Um, and so the way you would actually", "start": 424.25, "duration": 4.28}, {"text": "derive this is you write this as sum of i. Um,", "start": 428.53, "duration": 5.07}, {"text": "so z i, you know,", "start": 433.6, "duration": 1.305}, {"text": "takes on a certain distribution of values.", "start": 434.905, "duration": 1.755}, {"text": "So z i we tu- turn,", "start": 436.66, "duration": 1.37}, {"text": "turn z i into j, right?", "start": 438.03, "duration": 1.65}, {"text": "So z I can be I guess one or two,", "start": 439.68, "duration": 2.71}, {"text": "if you have a mixture of two Gaussians.", "start": 442.39, "duration": 1.14}, {"text": "So you sum over all the indices of the different clusters of w i j times log of,", "start": 443.53, "duration": 7.62}, {"text": "uh, the numerator is,", "start": 451.15, "duration": 2.375}, {"text": "um, going to be", "start": 453.525, "duration": 3.535}, {"text": "into the negative one half", "start": 463.37, "duration": 3.5}, {"text": "times phi j,", "start": 473.3, "duration": 3.595}, {"text": "um, that's the numerator.", "start": 476.895, "duration": 1.635}, {"text": "And so, you know,", "start": 478.53, "duration": 4.14}, {"text": "this term is equal to,", "start": 482.67, "duration": 1.92}, {"text": "um, this first Gaussian term times that second term right,", "start": 484.59, "duration": 3.77}, {"text": "because this term is p of xi um, given z i, right?", "start": 488.36, "duration": 6.09}, {"text": "And the parameters, and this is just p of z i. Does that make sense?", "start": 494.45, "duration": 5.23}, {"text": "Okay. Um, and then we ta- take this and divide it by w i j, okay?", "start": 499.68, "duration": 9.96}, {"text": "So I'm, I'm going to step you through the, the,", "start": 509.64, "duration": 1.845}, {"text": "the steps you would go through if you're deriving EM using that,", "start": 511.485, "duration": 3.875}, {"text": "you know, E-step and M-step we wrote up above.", "start": 515.36, "duration": 2.31}, {"text": "But if you're deriving this for the mixture of Gaussians model then these are the,", "start": 517.67, "duration": 5.175}, {"text": "um, steps of algebra, right?", "start": 522.845, "duration": 3.37}, {"text": "You would, you would take, okay?", "start": 526.215, "duration": 3.03}, {"text": "Sorry I'm just realizing that.", "start": 529.245, "duration": 3.025}, {"text": "So in order to perform this maximization,", "start": 533.72, "duration": 5.18}, {"text": "what you will do is, um,", "start": 540.13, "duration": 2.8}, {"text": "you want to maximize this formula, right?", "start": 542.93, "duration": 2.4}, {"text": "This big double summation with respect to each of the parameters phi, mu, and Sigma.", "start": 545.33, "duration": 5.94}, {"text": "And so what you would do is, you know,", "start": 551.27, "duration": 2.08}, {"text": "take this big formula, right?", "start": 553.35, "duration": 3.195}, {"text": "And take the derivatives with respect to each of the parameters.", "start": 556.545, "duration": 2.795}, {"text": "So you take the derivative with respect to mu j, dot, dot,", "start": 559.34, "duration": 3.45}, {"text": "dot there's that big formula on the left,", "start": 562.79, "duration": 2.04}, {"text": "set it to 0, right?", "start": 564.83, "duration": 2.025}, {"text": "And take, uh, and then,", "start": 566.855, "duration": 1.05}, {"text": "and then it turns out if you do this,", "start": 567.905, "duration": 1.74}, {"text": "um, you will, uh,", "start": 569.645, "duration": 2.415}, {"text": "derive that mu j should be equal to sum over i to", "start": 572.06, "duration": 6.3}, {"text": "the i j x i over sum over i to the i j.", "start": 578.36, "duration": 6.37}, {"text": "And, um, this is what we said was how you update the mean's mu, right?", "start": 584.73, "duration": 5.96}, {"text": "The w i j's are the strength with which x i.", "start": 590.69, "duration": 3.875}, {"text": "So w i j is the,", "start": 594.565, "duration": 2.6}, {"text": "informally this is the strength with", "start": 597.165, "duration": 5.085}, {"text": "which x i is assigned, right?", "start": 602.25, "duration": 6.39}, {"text": "To Gaussian j, um,", "start": 608.64, "duration": 4.79}, {"text": "and more formally this is really p of, um,", "start": 613.43, "duration": 3.315}, {"text": "z i equals j given x i and the parameters, right?", "start": 616.745, "duration": 5.7}, {"text": "And so, um, so you end up with this formula.", "start": 622.445, "duration": 3.15}, {"text": "But the way you compute this formula is by the, the,", "start": 625.595, "duration": 3.395}, {"text": "the rigorous way to show this is the right formula for updating mu j,", "start": 628.99, "duration": 3.805}, {"text": "is looking at this objective, taking derivatives,", "start": 632.795, "duration": 2.385}, {"text": "saying they're zero, zero to maximize, um,", "start": 635.18, "duration": 2.965}, {"text": "and therefore deriving that equation for mu j, you know, by,", "start": 638.145, "duration": 3.9}, {"text": "by, by solving for the value of mu j that maximizes this expression, right?", "start": 642.045, "duration": 4.455}, {"text": "And, uh, similarly, you know,", "start": 646.5, "duration": 1.5}, {"text": "you take derivatives respect of,", "start": 648.0, "duration": 2.225}, {"text": "of, of this thing,", "start": 650.225, "duration": 1.445}, {"text": "with respect to that phi and set it to 0,", "start": 651.67, "duration": 2.825}, {"text": "take derivatives of this thing, um, right?", "start": 654.495, "duration": 8.74}, {"text": "And set that to 0 and that's how you would derive", "start": 663.235, "duration": 2.995}, {"text": "the update equations in the M-step for phi and for Sigma as well.", "start": 666.23, "duration": 5.11}, {"text": "Okay? Um, so and, and so,", "start": 671.34, "duration": 4.2}, {"text": "for example, when you do this,", "start": 675.54, "duration": 1.89}, {"text": "you find that the optimal value for phi is, um.", "start": 677.43, "duration": 4.2}, {"text": "Let's see. Yeah, yeah.", "start": 687.63, "duration": 2.74}, {"text": "We had this at the - near the start of Monday's lecture as well, okay?", "start": 690.37, "duration": 5.73}, {"text": "Um, so this is a process of how you would look at how the", "start": 696.1, "duration": 5.1}, {"text": "E-steps and M-steps are relative, and apply it to", "start": 701.2, "duration": 3.03}, {"text": "a specific model such as a mixtures of Gaussians model and that's how you,", "start": 704.23, "duration": 4.635}, {"text": "you know, solve for the maximization in the M-step, okay?", "start": 708.865, "duration": 4.23}, {"text": "And so what I'd like to do today is describe the application of EM.", "start": 713.095, "duration": 4.83}, {"text": "It's a more complex model called the factor of analysis model,", "start": 717.925, "duration": 3.66}, {"text": "and so it's important that - so I hope you understand the mechanics of how you do this,", "start": 721.585, "duration": 4.875}, {"text": "because we're going to do this today for a different model, okay?", "start": 726.46, "duration": 3.68}, {"text": "Any questions about this before I move on?", "start": 730.14, "duration": 3.76}, {"text": "Okay.", "start": 738.68, "duration": 1.285}, {"text": "Cool.", "start": 739.965, "duration": 1.465}, {"text": "Oh, so in order to,", "start": 741.65, "duration": 10.82}, {"text": "you know, foreshadow a little bit what we'll see when it comes", "start": 752.47, "duration": 3.21}, {"text": "down to the mixture of Gaussians model, excuse me.", "start": 755.68, "duration": 3.44}, {"text": "The Factor Analysis model which we talked about,", "start": 759.12, "duration": 2.265}, {"text": "you know, which is what we'll spend most of the day talking about.", "start": 761.385, "duration": 2.61}, {"text": "In the factor analysis model,", "start": 763.995, "duration": 2.175}, {"text": "instead of zi being discrete,", "start": 766.17, "duration": 2.34}, {"text": "zi will be continuous, right?", "start": 768.51, "duration": 3.085}, {"text": "And the particular zi will be distributed Gaussian.", "start": 771.595, "duration": 3.525}, {"text": "So the mixture of Gaussians model we had a joint distribution for x and z,", "start": 775.12, "duration": 3.72}, {"text": "where x was a discrete random variable.", "start": 778.84, "duration": 2.175}, {"text": "So in the factor analysis model we'll,", "start": 781.015, "duration": 2.58}, {"text": "we'll describe a different model.", "start": 783.595, "duration": 1.755}, {"text": "You know, for p of x and z,", "start": 785.35, "duration": 1.86}, {"text": "where z is continuous.", "start": 787.21, "duration": 1.665}, {"text": "And so instead of sum over zi, this would be an integral over zi of dzi, right?", "start": 788.875, "duration": 7.065}, {"text": "So - so sum becomes an integral.", "start": 795.94, "duration": 2.145}, {"text": "And - and it turns out that, yeah. Well, right.", "start": 798.085, "duration": 7.195}, {"text": "Yeah. And - and it turns out that if you go through", "start": 805.8, "duration": 2.59}, {"text": "the derivation of the EM algorithm that we worked out on Monday,", "start": 808.39, "duration": 4.08}, {"text": "all of the steps with Jensen's inequality,  all of those steps work exactly as before.", "start": 812.47, "duration": 4.545}, {"text": "Meaning you check every single step for whether", "start": 817.015, "duration": 2.325}, {"text": "zi was continuous it'll work the same as before", "start": 819.34, "duration": 2.61}, {"text": "if you have changed the sum to an integral, okay? All right.", "start": 821.95, "duration": 8.14}, {"text": "So let's see.", "start": 841.17, "duration": 4.67}, {"text": "So I want to mention one other view of EM", "start": 851.82, "duration": 5.125}, {"text": "that's equivalent to everything we've seen up until now which is,", "start": 856.945, "duration": 5.175}, {"text": "let me define j of theta, Q as", "start": 862.12, "duration": 18.06}, {"text": "this, okay? So is that formula you've seen a few times now.", "start": 883.44, "duration": 6.4}, {"text": "What we proved on Monday,", "start": 889.84, "duration": 2.235}, {"text": "was L of theta", "start": 892.075, "duration": 5.37}, {"text": "is greater than or equal to J of theta, Q right?", "start": 897.445, "duration": 5.43}, {"text": "And this is true for any theta and any choice of Q, okay?", "start": 902.875, "duration": 5.235}, {"text": "So using - using Jensen's inequality,", "start": 908.11, "duration": 2.999}, {"text": "you can show that, you know,", "start": 911.109, "duration": 1.771}, {"text": "J for any choice of theta and Q is a lower bound for the log likelihood of theta.", "start": 912.88, "duration": 6.735}, {"text": "So it turns out that an equivalent view of EM as everything we've seen before,", "start": 919.615, "duration": 5.865}, {"text": "is that at an E-step,", "start": 925.48, "duration": 2.685}, {"text": "what you're doing is maximize J with respect to Q and in", "start": 928.165, "duration": 7.845}, {"text": "the M-step maximize J", "start": 936.01, "duration": 7.41}, {"text": "with respect to theta, right?", "start": 943.42, "duration": 3.12}, {"text": "So in the E-step you're picking the choice of Q that maximizes this,", "start": 946.54, "duration": 5.49}, {"text": "and it turns out that the choice of Q we have we'll set J equal to L,", "start": 952.03, "duration": 5.655}, {"text": "and then in the M-step maximize this with respect to", "start": 957.685, "duration": 3.615}, {"text": "theta and pushes the value of L even higher.", "start": 961.3, "duration": 3.795}, {"text": "So this algorithm is sometimes called coordinate ascent.", "start": 965.095, "duration": 2.64}, {"text": "If you have a function of two variables and you optimize with respect to this,", "start": 967.735, "duration": 3.75}, {"text": "and also with respect to this,", "start": 971.485, "duration": 1.125}, {"text": "then go back and forth and optimize with respect to one at a time.", "start": 972.61, "duration": 2.85}, {"text": "That - that's the procedure that sometimes we call coordinate ascent,", "start": 975.46, "duration": 2.88}, {"text": "because you're maximizing with respect to one coordinate at a time.", "start": 978.34, "duration": 2.805}, {"text": "And so EM is a coordinate ascent algorithm relative to this course function J, right?", "start": 981.145, "duration": 6.665}, {"text": "And - and, you know,", "start": 987.81, "duration": 1.44}, {"text": "and every iteration J ends up being sent to L which is", "start": 989.25, "duration": 3.24}, {"text": "why you know that as the algorithm increases J,", "start": 992.49, "duration": 3.6}, {"text": "you know that the log-likelihood is increasing with every iteration and if you", "start": 996.09, "duration": 4.05}, {"text": "want to track whether the EM algorithm is converging or how it is converging,", "start": 1000.14, "duration": 4.39}, {"text": "you can plot, you know,", "start": 1004.53, "duration": 1.725}, {"text": "the value of J or the value of L on successive iterations and see if", "start": 1006.255, "duration": 3.645}, {"text": "its validated, scoring", "start": 1009.9, "duration": 1.89}, {"text": "monotonically and then when it plateaus and isn't improving anymore,", "start": 1011.79, "duration": 3.315}, {"text": "then you might have a sense that the algorithm is converging, okay?", "start": 1015.105, "duration": 5.095}, {"text": "All right. Okay. So that's it.", "start": 1024.08, "duration": 11.275}, {"text": "The, basically an algorithm and a mixture of Gaussians.", "start": 1035.355, "duration": 4.635}, {"text": "What I want to do now is - and is going", "start": 1039.99, "duration": 3.0}, {"text": "to talk about the factor analysis algorithm. All right.", "start": 1042.99, "duration": 3.85}, {"text": "So you know, that the factor analysis algorithm will work, actually sorry.", "start": 1059.48, "duration": 7.45}, {"text": "So I want to compare and contrast mixture of", "start": 1066.93, "duration": 3.81}, {"text": "Gaussians with factor analysis we're talking about a little bit,", "start": 1070.74, "duration": 3.705}, {"text": "which is - for the mixture of Gaussians,", "start": 1074.445, "duration": 2.535}, {"text": "let's say n equals 2 and m equals 100, right?", "start": 1076.98, "duration": 5.025}, {"text": "So you have a data set with two features x 1 and x 2.", "start": 1082.005, "duration": 3.27}, {"text": "So n is equal to 2 and maybe you have a data set that looks like this.", "start": 1085.275, "duration": 4.705}, {"text": "You know, there's a mixture of two Gaussians.", "start": 1091.22, "duration": 2.56}, {"text": "We have a pretty good model for this data set, right?", "start": 1093.78, "duration": 2.19}, {"text": "Can fit one Gaussian there,", "start": 1095.97, "duration": 1.83}, {"text": "fit the second Gaussian here.", "start": 1097.8, "duration": 1.11}, {"text": "You kind of capture a distribution like this with a mixture of two Gaussians.", "start": 1098.91, "duration": 4.05}, {"text": "And this is one illustration of when,", "start": 1102.96, "duration": 3.765}, {"text": "when you apply mixture of Gaussians in this picture,", "start": 1106.725, "duration": 3.93}, {"text": "m is much bigger than n, right?", "start": 1110.655, "duration": 3.195}, {"text": "You have a lot more examples than you have dimensions.", "start": 1113.85, "duration": 5.17}, {"text": "Where I would not use mixture of Gaussians", "start": 1120.77, "duration": 4.61}, {"text": "and where you see the minute factor analysis will apply.", "start": 1125.87, "duration": 6.895}, {"text": "Maybe if m is about similar to n,", "start": 1132.765, "duration": 4.215}, {"text": "I don't know, even n is - or even m is much less than n, okay?", "start": 1136.98, "duration": 10.02}, {"text": "And so just for purpose of illustration let say", "start": 1147.0, "duration": 5.385}, {"text": "m equals 30 and n equals 100, right?", "start": 1152.385, "duration": 6.315}, {"text": "So let's say you have 100 dimensional data but only 30 examples.", "start": 1158.7, "duration": 5.97}, {"text": "And so to - to make this more concrete, you know,", "start": 1164.67, "duration": 6.015}, {"text": "many years ago there was", "start": 1170.685, "duration": 1.245}, {"text": "a Stanford PhD student that", "start": 1171.93, "duration": 3.24}, {"text": "was placing temperature sensors all around different Stanford buildings.", "start": 1175.17, "duration": 4.05}, {"text": "And so what you do is model,", "start": 1179.22, "duration": 2.52}, {"text": "you measure the temperature at many different places, right? Around campus.", "start": 1181.74, "duration": 5.505}, {"text": "But if you have 100 sensors,", "start": 1187.245, "duration": 3.865}, {"text": "you know, taking 100 temperature readings around campus.", "start": 1192.95, "duration": 5.14}, {"text": "But only 30 days of data or maybe 30 examples,", "start": 1198.09, "duration": 3.915}, {"text": "then you would have 100 dimensional data because", "start": 1202.005, "duration": 3.195}, {"text": "each example is a vector of 100 temperature readings,", "start": 1205.2, "duration": 4.935}, {"text": "you know, at different points around this building say.", "start": 1210.135, "duration": 2.43}, {"text": "But you may have only 30 examples of - of - if you have say 30 - 30 such vectors.", "start": 1212.565, "duration": 6.495}, {"text": "And so the application that the Stanford PhD student at the time was working on,", "start": 1219.06, "duration": 4.29}, {"text": "was she wanted to model p of x, right?", "start": 1223.35, "duration": 2.88}, {"text": "So this is x as a vector of 100 sensors, 100 temperature readings.", "start": 1226.23, "duration": 4.83}, {"text": "Because if something goes wrong or for", "start": 1231.06, "duration": 2.79}, {"text": "example because a bad case would be if there's a fire in one of the rooms,", "start": 1233.85, "duration": 3.345}, {"text": "then there'll be a very anomalous temperature reading in one place.", "start": 1237.195, "duration": 4.695}, {"text": "And if you can model p of x and if you observe a value of p of x that is very small.", "start": 1241.89, "duration": 5.535}, {"text": "You would say oh it looks like this anomaly there, right?", "start": 1247.425, "duration": 2.595}, {"text": "And we're actually less worried about fires on Stanford.", "start": 1250.02, "duration": 3.54}, {"text": "The use case was actually a - a- was it energy conservation.", "start": 1253.56, "duration": 4.455}, {"text": "If someone unexpectedly leaves the window open in the building you are studying,", "start": 1258.015, "duration": 3.615}, {"text": "you know, and it was hot and was it,", "start": 1261.63, "duration": 2.19}, {"text": "and it's winter and it's warm inside the building and cool air blows in,", "start": 1263.82, "duration": 4.92}, {"text": "and the temperature of one room drops in an anomalous way, you want", "start": 1268.74, "duration": 2.34}, {"text": "to realize if something was going wrong with the windows,", "start": 1271.08, "duration": 2.34}, {"text": "or the - or the temperature in part of the building, okay?", "start": 1273.42, "duration": 3.09}, {"text": "So for an application like that,", "start": 1276.51, "duration": 1.98}, {"text": "you need to model p of x as a joint distribution over,", "start": 1278.49, "duration": 5.79}, {"text": "you know, all of the different senses, right?", "start": 1284.28, "duration": 2.31}, {"text": "If you imagine maybe just in this room,", "start": 1286.59, "duration": 2.655}, {"text": "let say we have 30 sensors in this room,", "start": 1289.245, "duration": 2.46}, {"text": "then the temperatures at", "start": 1291.705, "duration": 2.055}, {"text": "the 30 different points in this room will be highly correlated with each other.", "start": 1293.76, "duration": 2.955}, {"text": "But how do you model this vector of", "start": 1296.715, "duration": 3.12}, {"text": "a 100 - 100 dimensional vector with a relatively small training set?", "start": 1299.835, "duration": 4.915}, {"text": "So it turns out that the problem with applying a Gaussian model.", "start": 1306.92, "duration": 7.285}, {"text": "Well, all right.", "start": 1314.205, "duration": 7.725}, {"text": "So one thing you could do is model this as a single Gaussian.", "start": 1321.93, "duration": 3.48}, {"text": "And say that x is distributed, right?", "start": 1325.41, "duration": 5.535}, {"text": "And if you look in your training set of 30 examples and find", "start": 1330.945, "duration": 4.515}, {"text": "the maximum likelihood estimate parameters, you find that", "start": 1335.46, "duration": 2.73}, {"text": "the maximum likelihood estimate of mu is just the average.", "start": 1338.19, "duration": 3.675}, {"text": "And the maximum likelihood estimate of sigma is this,", "start": 1341.865, "duration": 4.615}, {"text": "but it turns out that if m is less than equal to n,", "start": 1355.79, "duration": 7.45}, {"text": "then sigma, this covariance matrix will be singular.", "start": 1363.24, "duration": 6.61}, {"text": "And singular just means,", "start": 1370.85, "duration": 2.05}, {"text": "uh - non-invertible, okay?", "start": 1372.9, "duration": 6.85}, {"text": "I'll set for another illustration in a second.", "start": 1380.78, "duration": 4.67}, {"text": "But, er, if you look at the formula for", "start": 1395.75, "duration": 2.77}, {"text": "the Gaussian density, right?", "start": 1398.52, "duration": 9.09}, {"text": "So the Gaussian density kind of looks like this, right?", "start": 1407.61, "duration": 4.125}, {"text": "Abstracting away some details.", "start": 1411.735, "duration": 1.815}, {"text": "And when a covariance matrix is singular,", "start": 1413.55, "duration": 2.624}, {"text": "then this term, this determinant term will be 0.", "start": 1416.174, "duration": 6.496}, {"text": "Um, so you end up with one over 0.", "start": 1422.67, "duration": 2.535}, {"text": "Um, and then sigma inverse is also undefined or,", "start": 1425.205, "duration": 4.14}, {"text": "er, blows up to infinity it will depending on how you think about it.", "start": 1429.345, "duration": 3.285}, {"text": "Right but so, you know the inverse of a matrix like,", "start": 1432.63, "duration": 2.67}, {"text": "um, 1, 10, right?", "start": 1435.3, "duration": 3.48}, {"text": "Would be I guess one,", "start": 1438.78, "duration": 1.11}, {"text": "1 over 10, right?", "start": 1439.89, "duration": 3.0}, {"text": "And, er, an example of a non-invertible matrix or singular matrix would be this,", "start": 1442.89, "duration": 4.155}, {"text": "and you can't actually calculate the inverse of that matrix, right?", "start": 1447.045, "duration": 3.63}, {"text": "So it turns out that, um,", "start": 1450.675, "duration": 1.995}, {"text": "if your number of training examples is less than the dimension of the data,", "start": 1452.67, "duration": 4.245}, {"text": "if you use the usual formula to derive the maximum likelihood estimate of Sigma,", "start": 1456.915, "duration": 4.53}, {"text": "you'll end up with a covariance matrix that is singular.", "start": 1461.445, "duration": 2.73}, {"text": "Uh, singular just means non-invertible,", "start": 1464.175, "duration": 1.575}, {"text": "which means our covariance martix looks like this.", "start": 1465.75, "duration": 2.07}, {"text": "And so, you know, the Gaussian density,", "start": 1467.82, "duration": 1.95}, {"text": "if we try to compute p of x you get- you kind of get infinity over 0.", "start": 1469.77, "duration": 3.96}, {"text": "You see, right? Oh, sorry not infinity,", "start": 1473.73, "duration": 6.315}, {"text": "actually 0 over 0.", "start": 1480.045, "duration": 1.155}, {"text": "Sorry, right. It doesn't matter, it's all bad.", "start": 1481.2, "duration": 2.385}, {"text": "Um, and I think- let me just illustrate what this looks like.", "start": 1483.585, "duration": 5.19}, {"text": "Which is, um, let's say m equals 2,", "start": 1488.775, "duration": 4.74}, {"text": "and n equals 2, right?", "start": 1493.515, "duration": 2.76}, {"text": "So you have two-dimensional data x1 and x2,", "start": 1496.275, "duration": 2.805}, {"text": "and, um, uh, so n equals 2,", "start": 1499.08, "duration": 2.64}, {"text": "and the number of training examples is equal to 2.", "start": 1501.72, "duration": 1.845}, {"text": "So it turns out that, um- let's see,", "start": 1503.565, "duration": 2.55}, {"text": "so you see me draw contours of Gaussian densities like this, right?", "start": 1506.115, "duration": 3.405}, {"text": "Like ellipses like that.", "start": 1509.52, "duration": 1.23}, {"text": "It turns out that if you have two examples, a two-dimensional space,", "start": 1510.75, "duration": 3.66}, {"text": "and you compute the most likely- maximum likelihood estimate", "start": 1514.41, "duration": 3.39}, {"text": "of the parameters of the Gaussian to fit to this data,", "start": 1517.8, "duration": 2.025}, {"text": "then it turns out that these contours will look like that, right?", "start": 1519.825, "duration": 5.145}, {"text": "Um, except that, instead of being very thin,", "start": 1524.97, "duration": 5.455}, {"text": "as I'm drawing it, it will be,", "start": 1530.425, "duration": 1.495}, {"text": "it will be infinitely skinny.", "start": 1531.92, "duration": 1.605}, {"text": "So you end up with a Gaussian density where I can't draw lines,", "start": 1533.525, "duration": 3.51}, {"text": "you know, of 0 width on the whiteboard, right?", "start": 1537.035, "duration": 2.28}, {"text": "Um, uh, but it turns out that the contours will be squished infinitely thin.", "start": 1539.315, "duration": 4.395}, {"text": "So you end up with a Gaussian density all- all of whose mass is", "start": 1543.71, "duration": 3.88}, {"text": "on the straight line over there with infinitely thin contours that,", "start": 1547.59, "duration": 4.47}, {"text": "that they're just, you know,", "start": 1552.06, "duration": 1.065}, {"text": "we squish the centers on the,", "start": 1553.125, "duration": 1.815}, {"text": "on the plane that goes on the line,", "start": 1554.94, "duration": 1.95}, {"text": "um, connecting these two points.", "start": 1556.89, "duration": 1.845}, {"text": "And so this is- so first there are,", "start": 1558.735, "duration": 3.225}, {"text": "uh, practical numerical problems, right?", "start": 1561.96, "duration": 2.4}, {"text": "As you end up with 0 over 0 if you try to compute p of x for any example.", "start": 1564.36, "duration": 3.915}, {"text": "And second, um, this is-", "start": 1568.275, "duration": 2.58}, {"text": "this very poorly conditioned Gaussian density puts", "start": 1570.855, "duration": 3.27}, {"text": "all the probability mass on this line segment and so any example, right?", "start": 1574.125, "duration": 4.605}, {"text": "Over there, just a little bit off,", "start": 1578.73, "duration": 1.77}, {"text": "has no probability mass because,", "start": 1580.5, "duration": 1.65}, {"text": "oh, has a probability mass of 0, a probability density of", "start": 1582.15, "duration": 2.4}, {"text": "0 because the Gaussian is squished infinitely thin,", "start": 1584.55, "duration": 3.075}, {"text": "you know, on that, on that line, okay [NOISE].", "start": 1587.625, "duration": 2.805}, {"text": "But, but you can tell, this is just not", "start": 1590.43, "duration": 1.29}, {"text": "a very good just- this is not a very good model, right?", "start": 1591.72, "duration": 2.79}, {"text": "For, for this data.", "start": 1594.51, "duration": 1.74}, {"text": "Um, So what we're gonna do is, ah,", "start": 1596.25, "duration": 8.085}, {"text": "come up with a model that will work even for,", "start": 1604.335, "duration": 4.08}, {"text": "um, for these applications,", "start": 1608.415, "duration": 2.67}, {"text": "even, even for a dataset like this, right?", "start": 1611.085, "duration": 2.31}, {"text": "Um, there's actually a, uh- I think the,", "start": 1613.395, "duration": 2.55}, {"text": "the origins of the factor analysis model, uh,", "start": 1615.945, "duration": 2.88}, {"text": "one of the very early applications was actually a psychological testing.", "start": 1618.825, "duration": 3.375}, {"text": "Um, where, uh, if you have a, you know,", "start": 1622.2, "duration": 4.065}, {"text": "administer a psychology, um, ah,", "start": 1626.265, "duration": 1.935}, {"text": "exam to people to measure different personality attributes, right?", "start": 1628.2, "duration": 4.8}, {"text": "So you might measure- you might have 100 questions or measure 100, uh,", "start": 1633.0, "duration": 4.065}, {"text": "psychological attributes,", "start": 1637.065, "duration": 2.935}, {"text": "um, but have a dataset of 30 persons, right?", "start": 1640.61, "duration": 7.585}, {"text": "And again, you know, doing, doing psych research,", "start": 1648.195, "duration": 1.995}, {"text": "collecting, you know, assembling survey data is hard.", "start": 1650.19, "duration": 1.83}, {"text": "We assume you have a sample of 30 people and each person answers 100 quiz questions.", "start": 1652.02, "duration": 5.325}, {"text": "Um, and so each person is one- gives you one example, right?", "start": 1657.345, "duration": 5.73}, {"text": "X, and the dimension of this is,", "start": 1663.075, "duration": 3.225}, {"text": "um, 100 dimensional, we have only 30 of these.", "start": 1666.3, "duration": 2.895}, {"text": "And so if you want to model p of x,", "start": 1669.195, "duration": 2.28}, {"text": "try to model how correlated are the different psychological attributes of people, right?", "start": 1671.475, "duration": 4.575}, {"text": "Oh, is intelligence correlated with math ability, is that", "start": 1676.05, "duration": 2.61}, {"text": "correlated with language ability, is that correlated with other things,", "start": 1678.66, "duration": 3.66}, {"text": "uh, then how do you build a model for p of x, okay?", "start": 1682.32, "duration": 4.48}, {"text": "All right. So, um,", "start": 1687.47, "duration": 3.505}, {"text": "if the standard Gaussian model doesn't work,", "start": 1690.975, "duration": 1.815}, {"text": "let's look at some alternatives.", "start": 1692.79, "duration": 2.565}, {"text": "Um, one thing you could do is, uh,", "start": 1695.355, "duration": 5.185}, {"text": "constrain", "start": 1701.57, "duration": 2.845}, {"text": "Sigma to be diagonal, right?", "start": 1704.415, "duration": 8.775}, {"text": "So Sigma is a covariance matrix,", "start": 1713.19, "duration": 2.72}, {"text": "is an n by n covariance matrix.", "start": 1715.91, "duration": 2.13}, {"text": "So in this case, it would be a 100 by 100 matrix.", "start": 1718.04, "duration": 2.475}, {"text": "Um, but let's say we constrain it to just", "start": 1720.515, "duration": 3.375}, {"text": "have diagonal entries and 0s on the off diagonals, right?", "start": 1723.89, "duration": 6.49}, {"text": "So these giant 0s, I mean,", "start": 1730.38, "duration": 1.785}, {"text": "the diagonal entries of this square matrix are these values in", "start": 1732.165, "duration": 2.88}, {"text": "all of the entries of the diagonals you set to 0.", "start": 1735.045, "duration": 2.805}, {"text": "So that's one thing you could do.", "start": 1737.85, "duration": 1.53}, {"text": "And this turns out to be,", "start": 1739.38, "duration": 2.94}, {"text": "um- this turns out to correspond to", "start": 1742.32, "duration": 2.64}, {"text": "constraining your Gaussian to have axes align contours.", "start": 1744.96, "duration": 4.845}, {"text": "So this is a Gaussian with 0 off-diagonals.", "start": 1749.805, "duration": 4.2}, {"text": "Um, this would be another one, right?", "start": 1754.005, "duration": 3.085}, {"text": "This would be another one.", "start": 1757.31, "duration": 2.65}, {"text": "So these are examples of Gaussian- of,", "start": 1759.96, "duration": 2.715}, {"text": "of contours of Gaussian densities with,", "start": 1762.675, "duration": 2.76}, {"text": "um, 0 off diagonals.", "start": 1765.435, "duration": 2.115}, {"text": "So the axes here are the X1 and X2, right?", "start": 1767.55, "duration": 2.835}, {"text": "Whereas you cannot model something like", "start": 1770.385, "duration": 3.48}, {"text": "this if your off diagonals are, are 0.", "start": 1773.865, "duration": 5.7}, {"text": "Um, and so you do this,", "start": 1779.565, "duration": 2.565}, {"text": "the maximum likelihood estimate of the parameters Sigma j,", "start": 1782.13, "duration": 3.375}, {"text": "is pretty much what you'd expect actually.", "start": 1785.505, "duration": 2.935}, {"text": "Right. The maximum likelihood estimate of the mean vector mu is the same as before.", "start": 1790.34, "duration": 5.965}, {"text": "And this is maximum likelihood estimate of Sigma j, right?", "start": 1796.305, "duration": 3.255}, {"text": "This kind of knowledge should be no surprise, it's kind of what you'd expect.", "start": 1799.56, "duration": 2.85}, {"text": "Uh, and it turns out that, uh,", "start": 1802.41, "duration": 3.66}, {"text": "right- and, and so the covariance matrix here has n parameters,", "start": 1806.07, "duration": 3.554}, {"text": "instead of n squared or about n squared over two parameters,", "start": 1809.624, "duration": 3.896}, {"text": "the covariance matrix Sigma now just has n parameters,", "start": 1815.54, "duration": 3.49}, {"text": "which is the n diagonal entries.", "start": 1819.03, "duration": 2.535}, {"text": "Now, the problem with this is that,", "start": 1821.565, "duration": 3.075}, {"text": "this modeling assumption assumes that all of your features are uncorrelated, right?", "start": 1824.64, "duration": 4.65}, {"text": "So you know, this just assumes that", "start": 1829.29, "duration": 1.92}, {"text": "any two features they kind of share are, are completely uncorrelated.", "start": 1831.21, "duration": 3.375}, {"text": "And, um, if you have temperature sensors in this room,", "start": 1834.585, "duration": 3.135}, {"text": "it's just not a good assumption to assume", "start": 1837.72, "duration": 2.1}, {"text": "the temperature at all points of this room are completely uncorrelated,", "start": 1839.82, "duration": 3.12}, {"text": "completely independent of each other,", "start": 1842.94, "duration": 1.485}, {"text": "or if you measure, you know,", "start": 1844.425, "duration": 1.215}, {"text": "psychological attributes of people,", "start": 1845.64, "duration": 1.2}, {"text": "it's just not a great assumption to assume that, you know,", "start": 1846.84, "duration": 2.97}, {"text": "the different- different psychological measures you", "start": 1849.81, "duration": 2.16}, {"text": "might have are completely, um, uh, independent.", "start": 1851.97, "duration": 2.535}, {"text": "So while this model would take care of the problem, the,", "start": 1854.505, "duration": 5.22}, {"text": "the technical problem of the covariance matrix", "start": 1859.725, "duration": 2.385}, {"text": "being singular you can fit this model [NOISE],", "start": 1862.11, "duration": 2.46}, {"text": "um you know, on a,", "start": 1864.57, "duration": 1.605}, {"text": "on 100 dimensional dataset with 30 samples.", "start": 1866.175, "duration": 3.225}, {"text": "You can fit this, you won't get this- you could build this model,", "start": 1869.4, "duration": 3.24}, {"text": "you won't run into numerical singular,", "start": 1872.64, "duration": 2.535}, {"text": "um, covariance matrix type problems,", "start": 1875.175, "duration": 1.785}, {"text": "it's just not a very good model where you're just assuming nothing is", "start": 1876.96, "duration": 2.64}, {"text": "correlated to anything else", "start": 1879.6, "duration": 1.23}, {"text": "[NOISE].", "start": 1880.83, "duration": 16.14}, {"text": "Something else that you can do is, um,", "start": 1896.97, "duration": 2.235}, {"text": "uh, make an even stronger assumption.", "start": 1899.205, "duration": 5.325}, {"text": "So this is an even worse model,", "start": 1904.53, "duration": 1.62}, {"text": "but I want to go through it because it will be", "start": 1906.15, "duration": 1.35}, {"text": "a building block for what we'll actually do later,", "start": 1907.5, "duration": 2.31}, {"text": "which is constrain Sigma to be Sigma equals,", "start": 1909.81, "duration": 8.42}, {"text": "um, lowercase Sigma squared times i, right?", "start": 1918.23, "duration": 5.485}, {"text": "And so, um, constrain Sigma to be dia- not only", "start": 1923.715, "duration": 4.965}, {"text": "diagonal but to have the same entry in every single element.", "start": 1928.68, "duration": 5.385}, {"text": "So now you've gone from, um,", "start": 1934.065, "duration": 2.25}, {"text": "I guess n parameters to just one parameter, right?", "start": 1936.315, "duration": 4.935}, {"text": "Uh, and this means that you are constraining", "start": 1941.25, "duration": 2.22}, {"text": "the covariance matrix to- you", "start": 1943.47, "duration": 2.82}, {"text": "are constraining the Gaussian you use to have circular contours.", "start": 1946.29, "duration": 2.835}, {"text": "So this is an example where you can model.", "start": 1949.125, "duration": 2.295}, {"text": "Uh, and this would be another example, right?", "start": 1951.42, "duration": 2.745}, {"text": "And this is- I guess this is another example, okay?", "start": 1954.165, "duration": 2.91}, {"text": "So you can model things like this, where every feature,", "start": 1957.075, "duration": 2.7}, {"text": "not only is every feature uncorrelated but", "start": 1959.775, "duration": 2.085}, {"text": "every feature further has the same variance as every other feature.", "start": 1961.86, "duration": 3.015}, {"text": "Um, and the maximum likelihood is", "start": 1964.875, "duration": 4.845}, {"text": "this, okay?", "start": 1969.72, "duration": 8.745}, {"text": "And again, not, not, not a huge surprise,", "start": 1978.465, "duration": 1.785}, {"text": "just the average over,", "start": 1980.25, "duration": 1.02}, {"text": "uh, the previous values.", "start": 1981.27, "duration": 2.8}, {"text": "So what we'd like to do is,", "start": 1984.29, "duration": 3.49}, {"text": "um, not quite use either of these options, right?", "start": 1987.78, "duration": 3.27}, {"text": "Which assumes- really, the biggest problem is it assumes the features are uncorrelated.", "start": 1991.05, "duration": 3.555}, {"text": "Um, and what I'd like to do is build the model that you can fit even when you", "start": 1994.605, "duration": 4.305}, {"text": "have very high dimensional data and a relatively small number of examples,", "start": 1998.91, "duration": 4.155}, {"text": "um, but that allows you to capture some of the correlations, right?", "start": 2003.065, "duration": 3.465}, {"text": "So if you have 30 temperature sensors in this room,", "start": 2006.53, "duration": 2.325}, {"text": "you know, probably there are some correlations, right?", "start": 2008.855, "duration": 3.105}, {"text": "Probably, this side of the room temperature is gonna be correlated,", "start": 2011.96, "duration": 2.595}, {"text": "and that side of the room temperature is gonna be correlated", "start": 2014.555, "duration": 1.86}, {"text": "and maybe the ambient temperature in this whole building.", "start": 2016.415, "duration": 2.55}, {"text": "The, the temperature of this room really goes up and down as a whole,", "start": 2018.965, "duration": 2.805}, {"text": "but maybe some of the lamps on the side heat up", "start": 2021.77, "duration": 2.04}, {"text": "that side of the room a bit more, so different, the different.", "start": 2023.81, "duration": 2.175}, {"text": "There are correlations but maybe you don't need a full covariance matrix either.", "start": 2025.985, "duration": 4.44}, {"text": "So what [NOISE], what factor analysis will do is, um,", "start": 2030.425, "duration": 3.72}, {"text": "give us a model that you can fit even when you have,", "start": 2034.145, "duration": 3.285}, {"text": "you know, [NOISE] 100 dimensional data and 30 examples.", "start": 2037.43, "duration": 2.43}, {"text": "They capture some of the correlations but that doesn't run into the a,", "start": 2039.86, "duration": 4.32}, {"text": "a- uninvertible, um,", "start": 2044.18, "duration": 1.86}, {"text": "covariance matrixes is that the naive Gaussian model does, okay?", "start": 2046.04, "duration": 5.47}, {"text": "All right. So let me- just check any- let me,", "start": 2056.62, "duration": 6.385}, {"text": "let me describe the model, let me just check,", "start": 2063.005, "duration": 1.695}, {"text": "any questions before I move on? Okay.", "start": 2064.7, "duration": 1.95}, {"text": "[BACKGROUND]", "start": 2066.65, "duration": 7.94}, {"text": "Oh, sure. Yes. Um, yes.", "start": 2074.59, "duration": 2.49}, {"text": "There is one thing you can do.", "start": 2077.08, "duration": 1.68}, {"text": "A common thing to do is apply Wishart prior and what that boils down to is,", "start": 2078.76, "duration": 5.1}, {"text": "um, add a small diagonal value to that- to the maximum likelihood estimate.", "start": 2083.86, "duration": 3.75}, {"text": "Um, it- it kind of, uh,", "start": 2087.61, "duration": 2.31}, {"text": "in a technical sense it takes away the,", "start": 2089.92, "duration": 2.01}, {"text": "uh, non-invertible matrix problem.", "start": 2091.93, "duration": 2.07}, {"text": "Uh, it's actually not the best algorithm for a lot of the types of data.", "start": 2094.0, "duration": 3.78}, {"text": "Um, uh, the- the- the- the Wishart or inverse Wishart prior, yeah.", "start": 2097.78, "duration": 5.22}, {"text": "Others, you know- basically,", "start": 2103.0, "duration": 1.17}, {"text": "take the maximum likelihood for Sigma,", "start": 2104.17, "duration": 2.085}, {"text": "and add, you know,", "start": 2106.255, "duration": 1.635}, {"text": "some constant to the diagonal.", "start": 2107.89, "duration": 2.76}, {"text": "Um, it takes care of the problem in a technical way,", "start": 2110.65, "duration": 2.16}, {"text": "but it is not- it's not the best model for a lot of datasets, I see.", "start": 2112.81, "duration": 3.765}, {"text": "Why do we even think about option two [inaudible]", "start": 2116.575, "duration": 8.055}, {"text": "Oh, yes. Why do you think about option two,", "start": 2124.63, "duration": 1.425}, {"text": "when it's likesemi even worse than option one. Um, yes.", "start": 2126.055, "duration": 2.49}, {"text": "Option two is not a good option,", "start": 2128.545, "duration": 1.455}, {"text": "but I need to use this as a building block for factor analysis.", "start": 2130.0, "duration": 2.85}, {"text": "So you see this is a small component of, uh, of, uh,", "start": 2132.85, "duration": 2.505}, {"text": "see I actually planned these things out,", "start": 2135.355, "duration": 1.215}, {"text": "you know. [LAUGHTER]. Cool, yeah.", "start": 2136.57, "duration": 3.045}, {"text": "And- and- and maybe- actually to- to- to give [inaudible] just- just to mention,", "start": 2139.615, "duration": 3.645}, {"text": "you know, um, just mention some things I see.", "start": 2143.26, "duration": 3.3}, {"text": "Yeah. Actually the- the machine learning work evolves all the time,", "start": 2146.56, "duration": 2.31}, {"text": "which I find fascinating.", "start": 2148.87, "duration": 1.2}, {"text": "But you look at all the big tech companies, um,", "start": 2150.07, "duration": 2.895}, {"text": "a lot of the large tech companies,", "start": 2152.965, "duration": 1.515}, {"text": "they're all like working on exactly the same problems, right?", "start": 2154.48, "duration": 2.415}, {"text": "Every large tech company, you know,", "start": 2156.895, "duration": 2.04}, {"text": "software, AI company, is working on machine translation,", "start": 2158.935, "duration": 2.7}, {"text": "every one of them works on speech recognition,", "start": 2161.635, "duration": 2.22}, {"text": "every one of them works on face recognition,", "start": 2163.855, "duration": 1.935}, {"text": "and I- I- I've been part of these teams myself.", "start": 2165.79, "duration": 1.905}, {"text": "Right? And I think it's great that we have so much progress in machine translation,", "start": 2167.695, "duration": 4.335}, {"text": "because there are so many people,", "start": 2172.03, "duration": 1.29}, {"text": "and so many large companies that work on machine translation.", "start": 2173.32, "duration": 2.46}, {"text": "It's actually really happy to see so much progress in these", "start": 2175.78, "duration": 2.565}, {"text": "problems that every single large tech company,", "start": 2178.345, "duration": 2.58}, {"text": "large software, AI-ish tech company works on.", "start": 2180.925, "duration": 2.88}, {"text": "Um, one of the fascinating things I see is that, um, uh,", "start": 2183.805, "duration": 3.825}, {"text": "because of all this work into large tech companies working on very similar problems,", "start": 2187.63, "duration": 4.62}, {"text": "one of the really overlooked parts of", "start": 2192.25, "duration": 2.01}, {"text": "the machine learning world is small data problems, right?", "start": 2194.26, "duration": 3.06}, {"text": "So there's a lot work in big data if you are Brazilian, English,", "start": 2197.32, "duration": 3.0}, {"text": "and French, and Chinese,", "start": 2200.32, "duration": 1.05}, {"text": "and Spanish sentences is the semi-close models that work.", "start": 2201.37, "duration": 2.385}, {"text": "Um, and I think, uh,", "start": 2203.755, "duration": 1.995}, {"text": "uh, there's actually a lack of attention,", "start": 2205.75, "duration": 2.46}, {"text": "like a disproportionately small amount of attention, on, you know,", "start": 2208.21, "duration": 3.18}, {"text": "small data problems, where instead of,", "start": 2211.39, "duration": 2.115}, {"text": "uh, 100 million images, you maybe have 100 images.", "start": 2213.505, "duration": 2.85}, {"text": "Um, and so, uh,", "start": 2216.355, "duration": 1.905}, {"text": "some of the teams I work with these days,", "start": 2218.26, "duration": 1.5}, {"text": "actually like Landing AI.", "start": 2219.76, "duration": 1.23}, {"text": "Um, I actually spent a lot of my time thinking about small data problems,", "start": 2220.99, "duration": 4.08}, {"text": "because a lot of the practical applications of machine learning,", "start": 2225.07, "duration": 2.535}, {"text": "including a lot of things you see in your class projects,", "start": 2227.605, "duration": 2.595}, {"text": "are actually small data problems.", "start": 2230.2, "duration": 1.665}, {"text": "Right? And I think, um, when- when Annan, uh,", "start": 2231.865, "duration": 2.72}, {"text": "worked with a healthcare system, works at Stanford Hospital,", "start": 2234.585, "duration": 2.925}, {"text": "for some of the problems, you only have 100 examples,", "start": 2237.51, "duration": 2.16}, {"text": "or even 1,000, or even 10,000.", "start": 2239.67, "duration": 1.44}, {"text": "You don't have a million patients with the same medical condition.", "start": 2241.11, "duration": 3.165}, {"text": "And so I think that, um, uh,", "start": 2244.275, "duration": 1.76}, {"text": "a lot of these models- So- and again,", "start": 2246.035, "duration": 3.38}, {"text": "uh, earlier this week,", "start": 2249.415, "duration": 1.62}, {"text": "I was using a slightly modified version of", "start": 2251.035, "duration": 3.465}, {"text": "factor analysis on a manufacturing problem at Landing AI.", "start": 2254.5, "duration": 3.54}, {"text": "Right? And I think a lot of these small data problems are actually", "start": 2258.04, "duration": 2.85}, {"text": "where a lot of the exciting work is to be done in machine learning,", "start": 2260.89, "duration": 3.45}, {"text": "and is somehow- it- it feels like a blind spot of- or if like a- like a- like a gap of,", "start": 2264.34, "duration": 5.19}, {"text": "uh, a lot of the work done in the AI world today. Go ahead.", "start": 2269.53, "duration": 3.39}, {"text": "[inaudible].", "start": 2272.92, "duration": 14.97}, {"text": "Uh, yeah. Why don't we use the same algorithms with this big data?", "start": 2287.89, "duration": 2.475}, {"text": "It turns out that, um,", "start": 2290.365, "duration": 1.575}, {"text": "uh, you know, it turns out- if- if- if you look at the computer vision world, right?", "start": 2291.94, "duration": 5.22}, {"text": "There's a data set that everyone is working on.", "start": 2297.16, "duration": 1.98}, {"text": "Now- now we're past it,", "start": 2299.14, "duration": 1.35}, {"text": "we don't really use it any more,", "start": 2300.49, "duration": 1.02}, {"text": "called ImageNet, which had a million images,", "start": 2301.51, "duration": 2.28}, {"text": "and so there are tons of computer vision architectures that have been heavily", "start": 2303.79, "duration": 3.6}, {"text": "designed for the use case of if you have exactly one million training examples.", "start": 2307.39, "duration": 4.575}, {"text": "Uh, and it turns out that the algorithms that work", "start": 2311.965, "duration": 2.085}, {"text": "best if you have 100 training examples is,", "start": 2314.05, "duration": 2.16}, {"text": "you know, looks like it's different than the best learning algorithm.", "start": 2316.21, "duration": 2.46}, {"text": "I think, um, uh,", "start": 2318.67, "duration": 1.92}, {"text": "uh, and so I think right now,", "start": 2320.59, "duration": 2.295}, {"text": "we actually- I think the machine learning world,", "start": 2322.885, "duration": 2.025}, {"text": "we are not very good at understanding the scaling.", "start": 2324.91, "duration": 2.985}, {"text": "Uh, the best algorithm for one training example,", "start": 2327.895, "duration": 2.625}, {"text": "you know, as far as we are able to invent algorithms as a community,", "start": 2330.52, "duration": 3.885}, {"text": "is different than best algorithm for 1000, best for, for a million,", "start": 2334.405, "duration": 3.375}, {"text": "it's actually different than, um, uh,", "start": 2337.78, "duration": 1.77}, {"text": "uh- actually, and Facebook published a paper recently,", "start": 2339.55, "duration": 2.49}, {"text": "with 3.5 billion images.", "start": 2342.04, "duration": 1.23}, {"text": "The result was cool, it was very large, right?", "start": 2343.27, "duration": 1.47}, {"text": "So I was saying, we don't actually have", "start": 2344.74, "duration": 2.49}, {"text": "a good understanding of how to modify our algorithms,", "start": 2347.23, "duration": 3.059}, {"text": "to have one algorithm work on every single point of this spectrum,", "start": 2350.289, "duration": 3.211}, {"text": "going from one example to,", "start": 2353.5, "duration": 1.515}, {"text": "like, a billion examples.", "start": 2355.015, "duration": 1.62}, {"text": "Uh, and so there's a lot of work optimizing for different points of the spectrum,", "start": 2356.635, "duration": 4.575}, {"text": "uh, and I think there's been, um,", "start": 2361.21, "duration": 2.295}, {"text": "a lot of work optimizing for big data, which is great, you know,", "start": 2363.505, "duration": 2.565}, {"text": "build some of these large systems that handle, like,", "start": 2366.07, "duration": 2.31}, {"text": "whatever, petabytes of data a day, uh, that's great.", "start": 2368.38, "duration": 3.09}, {"text": "But, um, uh, I feel like relative to the number of, um, application opportunities,", "start": 2371.47, "duration": 5.39}, {"text": "there- there's a lot of work on small data well,", "start": 2376.86, "duration": 2.565}, {"text": "that- that I find very exciting,", "start": 2379.425, "duration": 1.71}, {"text": "that- that, uh, and I think of this as an example.", "start": 2381.135, "duration": 2.96}, {"text": "Uh, the reason I was using this,", "start": 2384.095, "duration": 2.12}, {"text": "literally, well, modified version of this,", "start": 2386.215, "duration": 2.1}, {"text": "earlier this week on the manufacturing problem, um,", "start": 2388.315, "duration": 2.625}, {"text": "is because, um, uh,", "start": 2390.94, "duration": 2.085}, {"text": "there isn't that much data in those scenarios, right?", "start": 2393.025, "duration": 3.48}, {"text": "Cool. All right.", "start": 2396.505, "duration": 3.69}, {"text": "That's, um, off-topic.", "start": 2400.195, "duration": 1.245}, {"text": "But let's- let's- let's go and describe- well, hopefully,", "start": 2401.44, "duration": 1.875}, {"text": "maybe so, so this stuff does get used, right?", "start": 2403.315, "duration": 2.295}, {"text": "Uh, so let's- let's talk about the model.", "start": 2405.61, "duration": 2.28}, {"text": "Um, so similar to,", "start": 2407.89, "duration": 3.705}, {"text": "uh, the mixture of Gaussians,", "start": 2411.595, "duration": 1.904}, {"text": "I'm gonna define a model with,", "start": 2413.499, "duration": 2.731}, {"text": "um, P of X,", "start": 2416.23, "duration": 1.71}, {"text": "Z equals P of X,", "start": 2417.94, "duration": 1.935}, {"text": "given Z times P of Z,", "start": 2419.875, "duration": 2.895}, {"text": "uh, and Z is hidden.", "start": 2422.77, "duration": 3.64}, {"text": "Okay? So that's the framework,", "start": 2427.38, "duration": 3.025}, {"text": "same as, um, mixture of Gaussian.", "start": 2430.405, "duration": 2.175}, {"text": "So let me just define the factor analysis model.", "start": 2432.58, "duration": 5.02}, {"text": "So first, um, Z will be drawn- distributed according to the Gaussian density,", "start": 2449.91, "duration": 6.19}, {"text": "where Z is going to be an RD,", "start": 2456.1, "duration": 2.1}, {"text": "where D is less than N. And again,", "start": 2458.2, "duration": 2.16}, {"text": "to think about it, um, maybe you can think of it as,", "start": 2460.36, "duration": 2.34}, {"text": "uh, D equals 3, uh,", "start": 2462.7, "duration": 2.49}, {"text": "uh, N equals 100, M equals 30.", "start": 2465.19, "duration": 6.945}, {"text": "Okay? Um, and- and- but I guess,", "start": 2472.135, "duration": 4.305}, {"text": "ju- just make sure this is a concrete example to think about it.", "start": 2476.44, "duration": 2.94}, {"text": "And what we're going to assume is that X is equal to Mu,", "start": 2479.38, "duration": 3.015}, {"text": "plus, um, Lambda Z.", "start": 2482.395, "duration": 5.235}, {"text": "This is, uh, the capital Greek alphabet Lambda, plus Epsilon,", "start": 2487.63, "duration": 4.83}, {"text": "where Epsilon is just using Gaussian with mean 0, and covariance Psi.", "start": 2492.46, "duration": 7.69}, {"text": "Um, so the parameters of this model are Mu which is N dimensional, um,", "start": 2509.88, "duration": 9.49}, {"text": "Lambda which is N by D,", "start": 2519.37, "duration": 4.11}, {"text": "and Psi which is N by N,", "start": 2523.48, "duration": 4.365}, {"text": "and we're going to assume that Psi is a diagonal.", "start": 2527.845, "duration": 4.465}, {"text": "Okay? Um, and so- let's see.", "start": 2532.5, "duration": 5.515}, {"text": "The second equation, an equivalent way to write that, equivalently,", "start": 2538.015, "duration": 8.385}, {"text": "is that given the value of Z,", "start": 2546.4, "duration": 3.33}, {"text": "the conditional distribution of X, right, X given Z,", "start": 2549.73, "duration": 3.54}, {"text": "this is Gaussian with mean given by Mu plus,", "start": 2553.27, "duration": 5.145}, {"text": "um, Lambda Z, and covariance Psi.", "start": 2558.415, "duration": 5.4}, {"text": "Okay? So once you've given Z- once you sample", "start": 2563.815, "duration": 2.805}, {"text": "Z- so this is P of Z and this is P of- P of Z and this is P of X,", "start": 2566.62, "duration": 4.35}, {"text": "Z- X given Z.", "start": 2570.97, "duration": 1.125}, {"text": "Right? So given Z,", "start": 2572.095, "duration": 1.77}, {"text": "X is computed as Mu plus Lambda Z.", "start": 2573.865, "duration": 2.985}, {"text": "So this is just some constant,", "start": 2576.85, "duration": 1.725}, {"text": "and then you add Gaussian noise to it.", "start": 2578.575, "duration": 2.25}, {"text": "And so this equation, an equivalent way to define this equation,", "start": 2580.825, "duration": 3.555}, {"text": "is to say that the mean of X, uh,", "start": 2584.38, "duration": 2.565}, {"text": "conditioned on Z, is this first term.", "start": 2586.945, "duration": 3.615}, {"text": "Right? Since that's the mean.", "start": 2590.56, "duration": 3.18}, {"text": "And the covariance of X given Z,", "start": 2593.74, "duration": 3.705}, {"text": "is given by this, you know,", "start": 2597.445, "duration": 1.905}, {"text": "additional term Psi, by that noise term that you add to it.", "start": 2599.35, "duration": 5.535}, {"text": "Okay? So let me go through a few examples.", "start": 2604.885, "duration": 5.595}, {"text": "And- and I think the intuition behind this model is, um,", "start": 2610.48, "duration": 4.12}, {"text": "if- if you think that there are", "start": 2614.6, "duration": 1.69}, {"text": "three powerful forces driving temperatures across this room,", "start": 2616.29, "duration": 3.75}, {"text": "maybe one powerful force is just what is the temperature,", "start": 2620.04, "duration": 3.28}, {"text": "you know, here in Palo Alto,", "start": 2623.32, "duration": 1.41}, {"text": "what's the temperature here at Stanford.", "start": 2624.73, "duration": 1.53}, {"text": "And another powerful force is how bright are the lights on the left side of the room,", "start": 2626.26, "duration": 3.96}, {"text": "and how hot does it heat up this side of room,", "start": 2630.22, "duration": 1.74}, {"text": "and another is how hot does is it heat up the right side of the room.", "start": 2631.96, "duration": 2.415}, {"text": "Right? So, you know, let's say there are", "start": 2634.375, "duration": 1.56}, {"text": "three main driving factors affecting the temperature of this room,", "start": 2635.935, "duration": 3.225}, {"text": "then that's when D would be equal to 3.", "start": 2639.16, "duration": 2.79}, {"text": "Then you assume that, you know,", "start": 2641.95, "duration": 1.05}, {"text": "there are three things in the world that drive", "start": 2643.0, "duration": 1.8}, {"text": "the temperature of this room that's three-dimensional,", "start": 2644.8, "duration": 1.95}, {"text": "which is the temperature in Palo Alto,", "start": 2646.75, "duration": 1.98}, {"text": "kind of, around this area, um,", "start": 2648.73, "duration": 2.145}, {"text": "how bright that the light is there,", "start": 2650.875, "duration": 1.245}, {"text": "and how bright that the light is there,", "start": 2652.12, "duration": 1.23}, {"text": "and you try to capture that with three numbers.", "start": 2653.35, "duration": 2.52}, {"text": "Given those three numbers, right?", "start": 2655.87, "duration": 2.76}, {"text": "Given Z, the actual temperature for the 100 sensors we scatter around this room,", "start": 2658.63, "duration": 6.435}, {"text": "will be determined by each sensor, right?", "start": 2665.065, "duration": 4.035}, {"text": "So we plug 30 temperature sensors all over this room.", "start": 2669.1, "duration": 2.655}, {"text": "Each sensor we plant will measure an actual temperature,", "start": 2671.755, "duration": 4.235}, {"text": "that's a linear function of those three powerful forces, um,", "start": 2675.99, "duration": 5.155}, {"text": "and if a sensor is on that side of the room,", "start": 2681.145, "duration": 1.785}, {"text": "it'll be affected more by how bright that the lights are on that side of the room.", "start": 2682.93, "duration": 3.36}, {"text": "Um, uh, if there's a sensor near the door,", "start": 2686.29, "duration": 2.16}, {"text": "it will be more affected by the temperature outside- temperature here in Palo Alto.", "start": 2688.45, "duration": 4.665}, {"text": "Right? But so X will be a linear function,", "start": 2693.115, "duration": 3.405}, {"text": "but this first time I underlined.", "start": 2696.52, "duration": 1.935}, {"text": "Um, but rather than just that term,", "start": 2698.455, "duration": 2.325}, {"text": "there is [inaudible] noise.", "start": 2700.78, "duration": 1.245}, {"text": "Right? So each sensor has its own noise term,", "start": 2702.025, "duration": 2.535}, {"text": "which is governed by this additional noise term Epsilon.", "start": 2704.56, "duration": 4.17}, {"text": "And, um, the assumption that this matrix Psi is diagonal,", "start": 2708.73, "duration": 7.185}, {"text": "it's saying that after you compute the mean,", "start": 2715.915, "duration": 3.585}, {"text": "the noise that you observe at each sensor", "start": 2719.5, "duration": 2.7}, {"text": "is independent of the noise at every other sensor.", "start": 2722.2, "duration": 3.36}, {"text": "Does that make sense? Right? That maybe- maybe the sensor,", "start": 2725.56, "duration": 3.21}, {"text": "you know, up there, right?", "start": 2728.77, "duration": 1.53}, {"text": "Maybe it's just noisy or something, just a gust of wind.", "start": 2730.3, "duration": 2.43}, {"text": "But you assume that the noise of, you observe at different sensors is independent.", "start": 2732.73, "duration": 4.74}, {"text": "The- the additional Epsilon error term has a-", "start": 2737.47, "duration": 2.865}, {"text": "has a diagonal covariance matrix given by Psi.", "start": 2740.335, "duration": 3.12}, {"text": "Okay? So you can- so you can think of that as what,", "start": 2743.455, "duration": 3.165}, {"text": "um, uh, factor analysis is trying to model.", "start": 2746.62, "duration": 4.665}, {"text": "Okay? So let me, um,", "start": 2751.285, "duration": 2.955}, {"text": "just go through a couple of examples of the types of data factor analysis can model.", "start": 2754.24, "duration": 6.67}, {"text": "All right, and again by the constraints of the whiteboard,", "start": 2767.8, "duration": 3.325}, {"text": "I'm going to have to go low-dimensional here, right?", "start": 2771.125, "duration": 1.935}, {"text": "Um, so actually let me- let me go through a couple examples.", "start": 2773.06, "duration": 6.7}, {"text": "So let's say Z is R_1 and X is R_2.", "start": 2781.36, "duration": 5.74}, {"text": "So in this example I guess d is equal to 1,", "start": 2787.1, "duration": 3.28}, {"text": "n is equal to 2 and let's say m is 7, right?", "start": 2791.44, "duration": 7.285}, {"text": "just- just. So what will be a typical example,", "start": 2798.725, "duration": 3.375}, {"text": "generated by- what will be an example of a type of data that this can model?", "start": 2802.1, "duration": 4.695}, {"text": "So this, let me erase this here. All right, so", "start": 2806.795, "duration": 11.445}, {"text": "this would be a typical sample of Z_i right?", "start": 2818.24, "duration": 3.18}, {"text": "which is you know- so this is z is just drawn from a standard Gaussian.", "start": 2821.42, "duration": 7.35}, {"text": "So I guess z is just Gaussian, would mean 0 and unit variance.", "start": 2828.77, "duration": 4.17}, {"text": "So that's the number line and you draw seven points from a Gaussian,", "start": 2832.94, "duration": 3.495}, {"text": "you know, maybe you get a sample like that.", "start": 2836.435, "duration": 1.56}, {"text": "Okay? and now let's say lambda is", "start": 2837.995, "duration": 6.525}, {"text": "2, 1 and let's just say mu is 0, 0, okay?", "start": 2844.52, "duration": 7.65}, {"text": "So now let's compute", "start": 2852.17, "duration": 5.77}, {"text": "lambda x plus mu, right?", "start": 2858.49, "duration": 5.65}, {"text": "so given a typical sample like that um,", "start": 2864.14, "duration": 3.255}, {"text": "if you compute lambda x plus mu,", "start": 2867.395, "duration": 2.625}, {"text": "this will now be the R_2, right?", "start": 2870.02, "duration": 2.385}, {"text": "so here is X_1, here is X_2.", "start": 2872.405, "duration": 2.52}, {"text": "We're gonna take those examples and map them to a line as follows.", "start": 2874.925, "duration": 6.265}, {"text": "Where these examples on R_1.", "start": 2883.33, "duration": 3.445}, {"text": "So- excuse me, lambda z plus mu, okay.", "start": 2886.775, "duration": 3.675}, {"text": "So this is just a real number and so lambda z plus mu is now two-dimensional,", "start": 2890.45, "duration": 6.645}, {"text": "right? Because lambda is a 2 by 1 matrix.", "start": 2897.095, "duration": 4.53}, {"text": "Okay? so you end up with- So this would be", "start": 2901.625, "duration": 2.325}, {"text": "a typical sample- typical random sample of lambda z plus mu and", "start": 2903.95, "duration": 4.11}, {"text": "it's a two-dimensional data-set but all of the examples lie perfectly on a straight line.", "start": 2908.06, "duration": 7.2}, {"text": "Okay? Then finally let's say that psi, the covariance matrix is equal to", "start": 2915.26, "duration": 7.89}, {"text": "this as a diagonal covariance matrix and", "start": 2923.15, "duration": 5.1}, {"text": "so this covariance matrix corresponds to X_2 having a bigger variance than X_1, right?", "start": 2928.25, "duration": 4.935}, {"text": "And so you know this,", "start": 2933.185, "duration": 1.275}, {"text": "this- I guess the density of epsilon has ellipses that look a little bit like this,", "start": 2934.46, "duration": 4.77}, {"text": "it's taller than wide.", "start": 2939.23, "duration": 1.305}, {"text": "The aspect ratio should technically be 1 over root 2 to 1, right?", "start": 2940.535, "duration": 3.135}, {"text": "Because the standard deviations will be root 2, I guess.", "start": 2943.67, "duration": 4.275}, {"text": "And so in the last step of what we are going to do,", "start": 2947.945, "duration": 3.87}, {"text": "x equals lambda z plus mu plus epsilon.", "start": 2951.815, "duration": 4.11}, {"text": "We're going to take each of these points we have and put", "start": 2955.925, "duration": 3.345}, {"text": "a little Gaussian contour. You know there's that shape.", "start": 2959.27, "duration": 6.465}, {"text": "There's this- I'm just drawing one contour of the shape and just put it on top of this,", "start": 2965.735, "duration": 6.015}, {"text": "and if you sample one point from each of these Gaussians,", "start": 2971.75, "duration": 4.155}, {"text": "then maybe you get this example, this example,", "start": 2975.905, "duration": 2.55}, {"text": "this example, this example, okay?", "start": 2978.455, "duration": 4.065}, {"text": "So what I just did was look at each of the Gaussian contours and", "start": 2982.52, "duration": 3.15}, {"text": "sample a point from that Gaussian.", "start": 2985.67, "duration": 3.3}, {"text": "And so the red crosses here are a typical sample drawn from this model.", "start": 2988.97, "duration": 6.3}, {"text": "Okay? and so if you have data that looks like this,", "start": 2995.27, "duration": 3.795}, {"text": "that looks at the red crosses.", "start": 2999.065, "duration": 1.725}, {"text": "The Zs are latent random variables, right?", "start": 3000.79, "duration": 2.28}, {"text": "When you get the dataset you kind of just see Zs.", "start": 3003.07, "duration": 1.95}, {"text": "So what you actually see,", "start": 3005.02, "duration": 1.26}, {"text": "is just you know the red crosses,", "start": 3006.28, "duration": 1.92}, {"text": "that's your training set and if you apply", "start": 3008.2, "duration": 3.3}, {"text": "the factor analysis model with these parameters then you can find EM and so on.", "start": 3011.5, "duration": 4.26}, {"text": "Hopefully you can find parameters that models this dataset pretty well,", "start": 3015.76, "duration": 2.76}, {"text": "but hopefully this gives you sense of the type of dataset this could generate", "start": 3018.52, "duration": 6.659}, {"text": "and so- and so on.", "start": 3025.179, "duration": 8.806}, {"text": "And one way to think of this data is you have", "start": 3033.985, "duration": 3.375}, {"text": "two-dimensional data but most of the data lies on a 1D subspace.", "start": 3037.36, "duration": 4.44}, {"text": "So this is how to think about it,", "start": 3041.8, "duration": 2.04}, {"text": "you have two-dimensional data since n is two.", "start": 3043.84, "duration": 2.685}, {"text": "But most of the data lies on", "start": 3046.525, "duration": 2.205}, {"text": "a roughly one-dimensional subspace meaning it lies roughly on a line,", "start": 3048.73, "duration": 3.255}, {"text": "and then there's a little bit of noise off that line, okay?", "start": 3051.985, "duration": 4.98}, {"text": "All right, let me quickly do one more example", "start": 3056.965, "duration": 3.075}, {"text": "because these are- these are high-dimensional spaces.", "start": 3060.04, "duration": 2.34}, {"text": "I think it's- I think it's useful to build intuition.", "start": 3062.38, "duration": 3.13}, {"text": "All right, so let's go through an example where z is in R_2,", "start": 3065.7, "duration": 10.735}, {"text": "x is in R_3 and let's use m equals 5.", "start": 3076.435, "duration": 5.4}, {"text": "So d equals 2,", "start": 3081.835, "duration": 1.515}, {"text": "n equals 3, okay?", "start": 3083.35, "duration": 4.56}, {"text": "So we have a different set of parameters.", "start": 3090.06, "duration": 2.35}, {"text": "Let's look at the type of data you can generate a factor analysis which is,", "start": 3092.41, "duration": 3.12}, {"text": "here is Z_1 and Z_2.", "start": 3095.53, "duration": 1.53}, {"text": "Z is distributed Gaussian,", "start": 3097.06, "duration": 1.5}, {"text": "standard Gaussian 2D so it would be a circular Gaussian.", "start": 3098.56, "duration": 2.73}, {"text": "So maybe this is what the typical sample, right, looks like.", "start": 3101.29, "duration": 4.695}, {"text": "If you- if you if you sample sort of Z_1 and Z_2 from a standard Gaussian,", "start": 3105.985, "duration": 4.68}, {"text": "right that would be a typical sample in Z_1 and Z_2.", "start": 3110.665, "duration": 4.375}, {"text": "So now- all right, I'm going to do a demo.", "start": 3115.29, "duration": 5.395}, {"text": "Let me take these five examples and just copy them to this piece of paper, okay?", "start": 3120.685, "duration": 4.8}, {"text": "So, all right there, right?", "start": 3125.485, "duration": 7.575}, {"text": "Transferred it from the whiteboard to this piece of paper,", "start": 3133.06, "duration": 3.03}, {"text": "to this brown cardboard.", "start": 3136.09, "duration": 1.455}, {"text": "So now you have Z_1 and Z_2 in a two-dimensional space.", "start": 3137.545, "duration": 4.815}, {"text": "What we're going to do is compute lambda z plus mu,", "start": 3142.36, "duration": 5.715}, {"text": "and this will be 3 by 2,", "start": 3148.075, "duration": 2.94}, {"text": "and this will be 3 by 1.", "start": 3151.015, "duration": 2.88}, {"text": "So what this computation will do as you map from z in two-dimensions to lambda z plus mu,", "start": 3153.895, "duration": 7.11}, {"text": "is you're going to map from two-dimensional data to three-dimensional data.", "start": 3161.005, "duration": 4.035}, {"text": "In other words, you want to take the two-dimensional data lying on the plane in", "start": 3165.04, "duration": 4.02}, {"text": "the whiteboard, and map it, check out", "start": 3169.06, "duration": 2.28}, {"text": "this cool animation into the three-dimensional space", "start": 3171.34, "duration": 2.88}, {"text": "of our classroom", "start": 3174.22, "duration": 0.84}, {"text": "[LAUGHTER].", "start": 3175.06, "duration": 11.49}, {"text": "And then the last step is for each of", "start": 3186.55, "duration": 2.85}, {"text": "these points in this three-dimensional space like X_1 X_2 X_3, right?", "start": 3189.4, "duration": 4.05}, {"text": "We'll have a little Gaussian bump that is axis", "start": 3193.45, "duration": 3.03}, {"text": "aligned because epsilon is the features, the-", "start": 3196.48, "duration": 3.93}, {"text": "the components of epsilon are uncorrelated and", "start": 3200.41, "duration": 3.36}, {"text": "taking each of these five points and add a little bit of fuzziness,", "start": 3203.77, "duration": 3.78}, {"text": "add a little bit of Gaussian noise to it.", "start": 3207.55, "duration": 1.95}, {"text": "And so what you end up with is a set of red crosses and", "start": 3209.5, "duration": 4.32}, {"text": "you end up with a few examples, you know add a little bit of noise,", "start": 3213.82, "duration": 4.14}, {"text": "you end up with- except that they", "start": 3217.96, "duration": 2.58}, {"text": "would have a bit of noise off this plane as well, right?", "start": 3220.54, "duration": 3.96}, {"text": "But so what the factor analysis model can capture is if you have data in 3D, right?", "start": 3224.5, "duration": 5.52}, {"text": "In this 3D space,", "start": 3230.02, "duration": 1.305}, {"text": "but most of the dataset lies on this", "start": 3231.325, "duration": 2.355}, {"text": "maybe roughly two-dimensional pancake but there's a little bit of fuzziness off", "start": 3233.68, "duration": 3.78}, {"text": "the pancake, right, so this would be", "start": 3237.46, "duration": 2.52}, {"text": "an example of the type of data that factor analysis can model.", "start": 3239.98, "duration": 3.69}, {"text": "Okay? All right cool.", "start": 3243.67, "duration": 2.83}, {"text": "Um, and the intuition is really think", "start": 3246.84, "duration": 3.775}, {"text": "of factor analysis can take very high dimensional data,", "start": 3250.615, "duration": 3.375}, {"text": "say, 100 dimensional data and model the data as roughly lying on a three-dimensional,", "start": 3253.99, "duration": 6.63}, {"text": "five dimensional subspace with a little bit of fuzz,", "start": 3260.62, "duration": 2.97}, {"text": "with a little bit of noise off that low dimensional subspace.", "start": 3263.59, "duration": 2.7}, {"text": "Great.", "start": 3266.29, "duration": 5.14}, {"text": "So- [NOISE]", "start": 3271.43, "duration": 23.375}, {"text": "All right. So let's talk about- yeah.", "start": 3294.805, "duration": 3.045}, {"text": "[BACKGROUND]", "start": 3297.85, "duration": 6.21}, {"text": "Oh, right. It does not work as well if the data's not", "start": 3304.06, "duration": 1.98}, {"text": "lying on low dimensional subspace. Um, let's see.", "start": 3306.04, "duration": 2.625}, {"text": "So even in 2D,", "start": 3308.665, "duration": 2.384}, {"text": "if you have, um, this data set, right?", "start": 3311.049, "duration": 3.331}, {"text": "[NOISE] You actually have the freedom to choose Gaussian noises like that,", "start": 3314.38, "duration": 4.215}, {"text": "in which case you can actually model things that are quite far off a subspace.", "start": 3318.595, "duration": 4.065}, {"text": "Uh, but, um, uh,", "start": 3322.66, "duration": 2.055}, {"text": "yeah, I, I, I, you know,", "start": 3324.715, "duration": 1.98}, {"text": "I think when you have a very high dimensional data set,", "start": 3326.695, "duration": 1.815}, {"text": "it's actually very difficult to know what's going on because you", "start": 3328.51, "duration": 2.1}, {"text": "can't visualize these very high dimensional data sets,", "start": 3330.61, "duration": 2.265}, {"text": "uh, and you also don't have enough data to build very sophisticated models.", "start": 3332.875, "duration": 3.57}, {"text": "So, so I feel like yes,", "start": 3336.445, "duration": 1.95}, {"text": "if you have- if the data actually does not roughly lie in a subspace,", "start": 3338.395, "duration": 4.785}, {"text": "then this model, you know,", "start": 3343.18, "duration": 1.545}, {"text": "may not be the best model,", "start": 3344.725, "duration": 1.71}, {"text": "but when you have such high dimensional data in such a small data set, um,", "start": 3346.435, "duration": 4.995}, {"text": "you- is- you can't fit very complex models through it anyway,", "start": 3351.43, "duration": 3.39}, {"text": "so this might be pretty reasonable.", "start": 3354.82, "duration": 2.16}, {"text": "Right. Cool. All right.", "start": 3356.98, "duration": 6.63}, {"text": "So, um- [NOISE] all right.", "start": 3363.61, "duration": 6.69}, {"text": "So it turns out that the derivation of EM for factor analysis is actually,", "start": 3370.3, "duration": 4.935}, {"text": "it's actually one of the trickiest EM derivations,", "start": 3375.235, "duration": 2.925}, {"text": "in terms of how you calculate the e-step,", "start": 3378.16, "duration": 1.89}, {"text": "and how you calculate the m-step.", "start": 3380.05, "duration": 2.04}, {"text": "Um, the whole algorithm is,", "start": 3382.09, "duration": 1.68}, {"text": "you know, describe the- every, every,", "start": 3383.77, "duration": 1.68}, {"text": "every single step, the- step three in great detail in the lecture notes.", "start": 3385.45, "duration": 3.375}, {"text": "But what I want to do is give you the flavor of how to do the derivation,", "start": 3388.825, "duration": 3.675}, {"text": "and to especially draw attention to the trickiest step,", "start": 3392.5, "duration": 2.85}, {"text": "so that if you need to derive an algorithm", "start": 3395.35, "duration": 2.01}, {"text": "like this yourself for maybe a different Gaussian model,", "start": 3397.36, "duration": 2.16}, {"text": "then you know how to do it,", "start": 3399.52, "duration": 1.26}, {"text": "but I won't do every step of the algebra here.", "start": 3400.78, "duration": 2.34}, {"text": "All right? Um, so in order to set ourselves up to derive factor analysis, uh,", "start": 3403.12, "duration": 5.61}, {"text": "EN- ENM for factor analysis,", "start": 3408.73, "duration": 2.58}, {"text": "I wanna describe a few properties of, uh, multivariate Gaussians.", "start": 3411.31, "duration": 5.175}, {"text": "So [NOISE] let's say that X is a vector,", "start": 3416.485, "duration": 4.2}, {"text": "and I'm gonna write this as a partition vector, right?", "start": 3420.685, "duration": 3.795}, {"text": "In which, um, uh,", "start": 3424.48, "duration": 1.83}, {"text": "[NOISE] if there are R components there,", "start": 3426.31, "duration": 3.0}, {"text": "and S components there.", "start": 3429.31, "duration": 1.695}, {"text": "So [NOISE] X_1 is in R_r,", "start": 3431.005, "duration": 2.79}, {"text": "X_2 is in R_S,", "start": 3433.795, "duration": 2.4}, {"text": "and X is in R_ r plus S. Okay?", "start": 3436.195, "duration": 6.66}, {"text": "So if X is Gaussian with mean Mu and covariance Sigma,", "start": 3442.855, "duration": 6.675}, {"text": "then, uh, let- similarly,", "start": 3449.53, "duration": 1.71}, {"text": "let Mu be written as this sort of partition vector.", "start": 3451.24, "duration": 4.35}, {"text": "Right? Just break it up into two sub-vectors,", "start": 3455.59, "duration": 2.79}, {"text": "corresponding to the first R components in the second S components.", "start": 3458.38, "duration": 4.53}, {"text": "And similarly, let the covariance matrix be partitioned into, um,", "start": 3462.91, "duration": 5.89}, {"text": "you know, these four diagonal blocks,", "start": 3470.76, "duration": 2.665}, {"text": "where, I guess, this is R components,", "start": 3473.425, "duration": 2.64}, {"text": "this is S components,", "start": 3476.065, "duration": 1.695}, {"text": "this is R components,", "start": 3477.76, "duration": 1.59}, {"text": "this is S components.", "start": 3479.35, "duration": 1.605}, {"text": "Um, so all this means is,", "start": 3480.955, "duration": 1.635}, {"text": "uh, you take the covariance matrix,", "start": 3482.59, "duration": 2.1}, {"text": "and take the top leftmost R-by-R elements,", "start": 3484.69, "duration": 3.765}, {"text": "and call that Sigma 1, 1.", "start": 3488.455, "duration": 1.77}, {"text": "Right? And, and, uh, and, and,", "start": 3490.225, "duration": 2.325}, {"text": "and then similarly for the other sub-blocks of this, um, covariance matrix.", "start": 3492.55, "duration": 7.9}, {"text": "So in order to derive factor analysis,", "start": 3500.67, "duration": 4.405}, {"text": "one of the things you need to do is compute marginal and,", "start": 3505.075, "duration": 4.53}, {"text": "um, uh, conditional distributions of Gaussians.", "start": 3509.605, "duration": 3.405}, {"text": "So the marginal is,", "start": 3513.01, "duration": 2.28}, {"text": "[NOISE] you know, what is P of X_1.", "start": 3515.29, "duration": 5.32}, {"text": "Right? Um, and so the,", "start": 3520.65, "duration": 3.475}, {"text": "the- if you, you know,", "start": 3524.125, "duration": 1.335}, {"text": "were to derive this, uh,", "start": 3525.46, "duration": 1.755}, {"text": "the way you compute the marginal is to take the joint density [NOISE] of P of X, right?", "start": 3527.215, "duration": 4.65}, {"text": "And you can write this as P of X_1 X_2,", "start": 3531.865, "duration": 2.985}, {"text": "because X can be partitioned into X_1 and X_2,", "start": 3534.85, "duration": 3.12}, {"text": "and integrate out X_2 under P of X_1 X_2, right?", "start": 3537.97, "duration": 4.56}, {"text": "Dx_2, and this will give you P of X_1.", "start": 3542.53, "duration": 2.76}, {"text": "Right? And if you plug in the Gaussian density,", "start": 3545.29, "duration": 3.51}, {"text": "the formula for the Gaussian density,", "start": 3548.8, "duration": 1.995}, {"text": "if you plug in, I guess, you know,", "start": 3550.795, "duration": 1.485}, {"text": "1 over 2 Pi to the N over 2,", "start": 3552.28, "duration": 2.745}, {"text": "is equals to one-half, right?", "start": 3555.025, "duration": 2.25}, {"text": "E to the, you know,", "start": 3557.275, "duration": 1.485}, {"text": "minus one-half, X1 minus Mu 1,", "start": 3558.76, "duration": 3.285}, {"text": "X_2 minus Mu 2,", "start": 3562.045, "duration": 2.905}, {"text": "uh, right?", "start": 3570.84, "duration": 7.36}, {"text": "If you plug this into P of X_1,", "start": 3578.2, "duration": 2.775}, {"text": "X_2, and actually do the integral, um,", "start": 3580.975, "duration": 4.405}, {"text": "then you will find that, um,", "start": 3589.07, "duration": 3.7}, {"text": "the marginal distribution of X_1 [NOISE] is given by;", "start": 3592.77, "duration": 5.22}, {"text": "X_1 is usually a Gaussian,", "start": 3597.99, "duration": 2.105}, {"text": "with mean Mu 1,", "start": 3600.095, "duration": 1.37}, {"text": "and covariance sigma 1, 1.", "start": 3601.465, "duration": 2.88}, {"text": "So it- it's, kind of, not a shocking result,", "start": 3604.345, "duration": 2.175}, {"text": "that the marginal distribution is given just by that and that.", "start": 3606.52, "duration": 5.115}, {"text": "Right? And, and again, the way to show it vigorously is to do this calculation,", "start": 3611.635, "duration": 3.725}, {"text": "[NOISE] but it's actually not shocking,", "start": 3615.36, "duration": 1.65}, {"text": "I guess, that that's what you would get.", "start": 3617.01, "duration": 1.755}, {"text": "Okay? Um, and then the other property you will [NOISE] need to use is a conditional,", "start": 3618.765, "duration": 8.235}, {"text": "which is, um, [NOISE] given the value of X_2,", "start": 3627.0, "duration": 4.86}, {"text": "what is the conditional value of X_1?", "start": 3631.86, "duration": 3.94}, {"text": "Um, and so the way to do that would be,", "start": 3635.8, "duration": 2.1}, {"text": "you know, [NOISE] in theory, you would take P of X_1,", "start": 3637.9, "duration": 2.265}, {"text": "X_2 divide by P of X_2,", "start": 3640.165, "duration": 2.865}, {"text": "right? And then simplify.", "start": 3643.03, "duration": 1.545}, {"text": "And it turns out you can show that, um,", "start": 3644.575, "duration": 2.865}, {"text": "[NOISE] X_1 given X_2, is itself Gaussian,", "start": 3647.44, "duration": 3.99}, {"text": "[NOISE] with some mean and some covariance,", "start": 3651.43, "duration": 5.73}, {"text": "we're just gonna write this Mu of 1 given 2 and Sigma of 1 given 2,", "start": 3657.16, "duration": 4.02}, {"text": "where Mu of 1 given 2 is,", "start": 3661.18, "duration": 2.73}, {"text": "uh- and, and- but this is one of those formulas that I", "start": 3663.91, "duration": 2.85}, {"text": "actually don't- I actually don't manage to remember,", "start": 3666.76, "duration": 2.73}, {"text": "but every time I need it I just look it up.", "start": 3669.49, "duration": 1.515}, {"text": "It's written in the lecture notes as well.", "start": 3671.005, "duration": 1.215}, {"text": "So um, [NOISE] X_2", "start": 3672.22, "duration": 8.55}, {"text": "minus 2 to-", "start": 3680.77, "duration": 5.85}, {"text": "oops.", "start": 3686.62, "duration": 8.64}, {"text": "Okay? [NOISE] So that's how you compute,", "start": 3695.26, "duration": 3.794}, {"text": "um, marginals and conditionals of a Gaussian distribution.", "start": 3699.054, "duration": 5.236}, {"text": "Okay? So [NOISE]", "start": 3704.29, "duration": 12.72}, {"text": "using these properties of,", "start": 3717.01, "duration": 2.07}, {"text": "uh, the multivariate Gaussian density,", "start": 3719.08, "duration": 2.22}, {"text": "let's go through the high-level steps of how you derive the EM algorithm for this.", "start": 3721.3, "duration": 7.8}, {"text": "[NOISE] All right. [NOISE] Um,", "start": 3729.1, "duration": 15.0}, {"text": "step one is, uh,", "start": 3744.1, "duration": 1.68}, {"text": "let's compute- actually, let's, um- excuse me.", "start": 3745.78, "duration": 6.36}, {"text": "[NOISE] Let's derive what is the joint distribution of P of X and Z.", "start": 3752.14, "duration": 8.775}, {"text": "Right? And in particular,", "start": 3760.915, "duration": 2.43}, {"text": "it turns out [NOISE] that if you take Z and X and", "start": 3763.345, "duration": 3.165}, {"text": "stack them up into a vector like so, um,", "start": 3766.51, "duration": 3.645}, {"text": "Z and X viewed as a vector would be Gaussian with mean,", "start": 3770.155, "duration": 4.08}, {"text": "um- [NOISE] with some mean and some covariance,", "start": 3774.235, "duration": 9.765}, {"text": "uh, because X and Z jointly will have a Gaussian density.", "start": 3784.0, "duration": 4.29}, {"text": "And let's try to quickly figure out what are this mean and that covariance matrix.", "start": 3788.29, "duration": 7.38}, {"text": "[NOISE] So that was a definition of these terms.", "start": 3795.67, "duration": 10.08}, {"text": "Um, and so the expected value of Z is equal", "start": 3805.75, "duration": 4.95}, {"text": "to 0 because 0 is- Z is Gaussian with mean 0 and covariance identity,", "start": 3810.7, "duration": 4.68}, {"text": "[NOISE] and the expected value of X is equal to the expected value of Mu plus Lambda Z,", "start": 3815.38, "duration": 7.23}, {"text": "plus epsilon, um, but Z has 0 expected value,", "start": 3822.61, "duration": 4.485}, {"text": "epsilon has 0 expected value,", "start": 3827.095, "duration": 1.365}, {"text": "so that just leaves you with Mu.", "start": 3828.46, "duration": 2.745}, {"text": "And so this mean vector Mu XZ,", "start": 3831.205, "duration": 4.11}, {"text": "is going to equal to 0 Mu.", "start": 3835.315, "duration": 3.345}, {"text": "Right. And so this is D-dimensional,", "start": 3838.66, "duration": 2.49}, {"text": "[NOISE] and this is, uh, N-dimensional.", "start": 3841.15, "duration": 2.76}, {"text": "Okay? Um, and it turns out that,", "start": 3843.91, "duration": 4.955}, {"text": "uh, [NOISE] let's see-", "start": 3848.865, "duration": 3.415}, {"text": "and it turns out that you can [NOISE] similarly compute", "start": 3862.5, "duration": 4.75}, {"text": "the covariance matrix Sigma, right?", "start": 3867.25, "duration": 6.585}, {"text": "Where this is, um,", "start": 3873.835, "duration": 2.13}, {"text": "D dimensions and this is N dimensions.", "start": 3875.965, "duration": 3.75}, {"text": "Um, [NOISE] it turns out that if you take this partition vector,", "start": 3879.715, "duration": 4.44}, {"text": "and compute the covariance matrix,", "start": 3884.155, "duration": 2.144}, {"text": "[NOISE] the four blocks of the covariance matrix can be written as follows. [NOISE] Um-", "start": 3886.299, "duration": 15.851}, {"text": "Okay. And you can,", "start": 3916.05, "duration": 5.155}, {"text": "one at a time, derive what each of these different blocks look like.", "start": 3921.205, "duration": 4.44}, {"text": "Um, and let me just do one of these,", "start": 3925.645, "duration": 5.88}, {"text": "and let me just derive what Sigma 2, 2,", "start": 3931.525, "duration": 2.175}, {"text": "the lower right block is and the rest are", "start": 3933.7, "duration": 2.34}, {"text": "derived similarly and also fleshed out in the lecture notes.", "start": 3936.04, "duration": 3.97}, {"text": "So the way you derive what this block is like is that you say Sigma 2, 2 is x minus Ex,", "start": 3943.23, "duration": 10.49}, {"text": "x minus Ex transpose.", "start": 3954.06, "duration": 4.03}, {"text": "And so if I plug in the definition of x that would be a Lambda z", "start": 3958.09, "duration": 6.72}, {"text": "plus Mu plus Epsilon minus Mu times the same thing.", "start": 3964.81, "duration": 7.485}, {"text": "Right.", "start": 3972.295, "duration": 7.395}, {"text": "Um, so there's x minus Ex.", "start": 3979.69, "duration": 2.16}, {"text": "So there's x minus Ex, okay?", "start": 3981.85, "duration": 2.13}, {"text": "Uh, because the expected value of x is Mu.", "start": 3983.98, "duration": 3.16}, {"text": "So the Mus cancel out.", "start": 3987.72, "duration": 4.78}, {"text": "And then if you do the quadratic expansion,", "start": 3992.5, "duration": 2.85}, {"text": "I guess this becomes expected value of, um, let's see,", "start": 3995.35, "duration": 8.35}, {"text": "Lambda z times", "start": 4004.43, "duration": 4.885}, {"text": "each of these two terms transpose plus- it,", "start": 4009.315, "duration": 6.375}, {"text": "it, it sort of, you know,", "start": 4015.69, "duration": 1.695}, {"text": "a plus b times a plus b, right?", "start": 4017.385, "duration": 3.825}, {"text": "It's a times a times a plus b,", "start": 4021.21, "duration": 1.68}, {"text": "b times a, b plus b.", "start": 4022.89, "duration": 1.05}, {"text": "You get four terms as a result.", "start": 4023.94, "duration": 1.44}, {"text": "And so the first term is Lambda z times Lambda z transpose, which is this,", "start": 4025.38, "duration": 4.98}, {"text": "plus Lambda z Epsilon transpose plus Epsilon,", "start": 4030.36, "duration": 7.275}, {"text": "um, right?", "start": 4037.635, "duration": 8.865}, {"text": "And so, um, this term has 0 expected value because,", "start": 4046.5, "duration": 5.91}, {"text": "uh, Epsilon and, and z,", "start": 4052.41, "duration": 1.47}, {"text": "both have zero expected value uncorrelated.", "start": 4053.88, "duration": 2.535}, {"text": "So this is zero.", "start": 4056.415, "duration": 2.565}, {"text": "This is zero on expectation.", "start": 4058.98, "duration": 2.37}, {"text": "And so you're just left with the expected value of Lambda zz transpose,", "start": 4061.35, "duration": 5.22}, {"text": "Lambda transpose plus the expected value of,", "start": 4066.57, "duration": 3.18}, {"text": "uh, Epsilon Epsilon transpose, right?", "start": 4069.75, "duration": 3.88}, {"text": "Um, and so by the linearity of expectation,", "start": 4073.63, "duration": 4.21}, {"text": "you can take expectation inside a ma- matrix multiplication.", "start": 4077.84, "duration": 3.15}, {"text": "So this Lambda times the expected value of zz transpose times Lambda transpose plus.", "start": 4080.99, "duration": 6.695}, {"text": "And this is just the covariance of Epsilon, right?", "start": 4087.685, "duration": 3.365}, {"text": "Which is- which is Psi.", "start": 4091.05, "duration": 1.725}, {"text": "Um, and then because z is drawn from a standard Gaussian with identity covariance,", "start": 4092.775, "duration": 6.164}, {"text": "that expectation in the middle is just the identity.", "start": 4098.939, "duration": 2.536}, {"text": "So that's Lambda, Lambda transpose plus Psi.", "start": 4101.475, "duration": 8.805}, {"text": "Okay. So that's how you work out what is", "start": 4110.28, "duration": 2.25}, {"text": "this lower right block of this, um, covariance matrix.", "start": 4112.53, "duration": 3.225}, {"text": "I know I did that a little bit quickly,", "start": 4115.755, "duration": 1.605}, {"text": "but every, every step is,", "start": 4117.36, "duration": 1.515}, {"text": "uh, written out, uh,", "start": 4118.875, "duration": 1.65}, {"text": "more slowly in the lecture notes as well.", "start": 4120.525, "duration": 2.67}, {"text": "Okay. And it turns out that if you go through a similar process to figure out,", "start": 4123.195, "duration": 4.56}, {"text": "you know, one at a time using similar process,", "start": 4127.755, "duration": 2.505}, {"text": "one of the other blocks of this covariance matrix,", "start": 4130.26, "duration": 2.52}, {"text": "you find that the other blocks of this covariance matrix are identity,", "start": 4132.78, "duration": 3.615}, {"text": "Lambda, Lambda transpose and the one we just worked out.", "start": 4136.395, "duration": 3.645}, {"text": "Okay. That- that's the one we just worked out.", "start": 4140.04, "duration": 4.47}, {"text": "But so that is the covariance matrix Psi.", "start": 4144.51, "duration": 3.99}, {"text": "[NOISE]", "start": 4148.5, "duration": 33.66}, {"text": "So where we are is that we've figured out that the joint distribution or", "start": 4182.16, "duration": 3.63}, {"text": "the joint density of z x is Gaussian with mean given", "start": 4185.79, "duration": 4.08}, {"text": "by that vector and covariance given", "start": 4189.87, "duration": 3.96}, {"text": "by that matrix, okay?", "start": 4193.83, "duration": 7.71}, {"text": "Um, and so what you could do, uh,", "start": 4201.54, "duration": 4.245}, {"text": "is, um, you write down, right?", "start": 4205.785, "duration": 3.975}, {"text": "P of x_i and try to take the- uh,", "start": 4209.76, "duration": 3.105}, {"text": "so P of x_i will be this Gaussian density.", "start": 4212.865, "duration": 2.865}, {"text": "And what you could do is take derivatives", "start": 4215.73, "duration": 2.339}, {"text": "of the log likelihood with respect to the parameters,", "start": 4218.069, "duration": 2.191}, {"text": "and set the parameters to 0 and solve.", "start": 4220.26, "duration": 1.59}, {"text": "And you find that there is no known closed-form solution.", "start": 4221.85, "duration": 2.475}, {"text": "There is actually no closed-form solution for finding the values of", "start": 4224.325, "duration": 3.825}, {"text": "Lambda and Psi and Mu that maximize this log-likelihood.", "start": 4228.15, "duration": 5.65}, {"text": "So in order to, uh,", "start": 4235.16, "duration": 5.35}, {"text": "fit the parameters of the model,", "start": 4240.51, "duration": 1.83}, {"text": "we're instead going to resort to EM, okay?", "start": 4242.34, "duration": 4.35}, {"text": "And so in the E-step.", "start": 4246.69, "duration": 4.24}, {"text": "Right.", "start": 4255.38, "duration": 7.63}, {"text": "So let's, let's first derive what is the E-step,", "start": 4263.01, "duration": 2.64}, {"text": "which is an E-step, you need to compute this, right?", "start": 4265.65, "duration": 3.825}, {"text": "Now, um, z_i here is a continuous random variable.", "start": 4269.475, "duration": 5.58}, {"text": "When we're fitting a mixture of Gaussian distributions, z_i was discrete,", "start": 4275.055, "duration": 3.795}, {"text": "and so you could have a list of numbers represented by, you know, w_ i_ j,", "start": 4278.85, "duration": 4.47}, {"text": "that just, just at the vector sorting what is", "start": 4283.32, "duration": 2.19}, {"text": "the probability of each of the discrete values of z_i.", "start": 4285.51, "duration": 3.03}, {"text": "But in this case, z_i is a continuous density.", "start": 4288.54, "duration": 3.03}, {"text": "So how do you represent Qi of z_i at a computer?", "start": 4291.57, "duration": 4.935}, {"text": "It turns out that using the formulas we have for the marginal- excuse me,", "start": 4296.505, "duration": 5.475}, {"text": "for the conditional distribution of a Gaussian,", "start": 4301.98, "duration": 2.25}, {"text": "it turns out that if you compute this right hand side,", "start": 4304.23, "duration": 3.345}, {"text": "you find that z_i given x_i,", "start": 4307.575, "duration": 3.045}, {"text": "this is going to be Gaussian with some mean", "start": 4310.62, "duration": 4.45}, {"text": "and some covariance, right?", "start": 4315.62, "duration": 6.79}, {"text": "Where- oh, it's basically those formulas,", "start": 4322.41, "duration": 5.59}, {"text": "Mu of z_i given x_i is", "start": 4328.64, "duration": 4.48}, {"text": "equal to- if you kinda of take that formula and apply it to our thing here,", "start": 4333.12, "duration": 3.51}, {"text": "a zero, uh, plus Lambda transpose.", "start": 4336.63, "duration": 4.51}, {"text": "And, um [NOISE] okay.", "start": 4349.82, "duration": 15.835}, {"text": "So these equations exactly, these two equations,", "start": 4365.655, "duration": 3.555}, {"text": "right, maps to- map to that big Gaussian Density that we have.", "start": 4369.21, "duration": 4.32}, {"text": "Okay. So what you would do in the E-step is,", "start": 4373.53, "duration": 4.77}, {"text": "um, compute this and compute this- compute this vector and compute this matrix,", "start": 4378.3, "duration": 5.325}, {"text": "and store that- store these,", "start": 4383.625, "duration": 1.92}, {"text": "you know, store these as variables,", "start": 4385.545, "duration": 1.8}, {"text": "and your representation of the Q_i is that Q_i is a Gaussian Density,", "start": 4387.345, "duration": 6.975}, {"text": "right, with this mean and this covariance.", "start": 4394.32, "duration": 2.52}, {"text": "So this is what you actually compute to represent Q_i.", "start": 4396.84, "duration": 3.42}, {"text": "Okay.", "start": 4400.26, "duration": 0.75}, {"text": "[NOISE]", "start": 4401.01, "duration": 12.45}, {"text": "All right.", "start": 4413.46, "duration": 0.285}, {"text": "So step two was derive the E-step,", "start": 4413.745, "duration": 2.91}, {"text": "and step three is derive the M-step.", "start": 4416.655, "duration": 2.415}, {"text": "[NOISE] and, um, the derivation of the M-step is,", "start": 4419.07, "duration": 10.575}, {"text": "is quite long and complicated, um,", "start": 4429.645, "duration": 3.345}, {"text": "but I wanna mention", "start": 4432.99, "duration": 1.44}, {"text": "just a key alge- algebraic trick you need to use when deriving the M-step.", "start": 4434.43, "duration": 4.56}, {"text": "Um, so, you know,", "start": 4438.99, "duration": 1.965}, {"text": "we know from the E-step that Q_i of z_i is that Gaussian Density.", "start": 4440.955, "duration": 4.89}, {"text": "Right. So you know, it's 1 over 2 pi to the d over 2,", "start": 4445.845, "duration": 4.215}, {"text": "that thing, and E to the,", "start": 4450.06, "duration": 2.13}, {"text": "right, negative 1.5 dot dot dot.", "start": 4452.19, "duration": 2.475}, {"text": "Right. So tha- that's the formula of a Q_i.", "start": 4454.665, "duration": 2.88}, {"text": "It turns out that,", "start": 4457.545, "duration": 1.785}, {"text": "um, in the M-step,", "start": 4459.33, "duration": 2.205}, {"text": "there will be a few places in the derivation", "start": 4461.535, "duration": 2.265}, {"text": "where you need to compute something like this.", "start": 4463.8, "duration": 2.1}, {"text": "[NOISE] Right.", "start": 4465.9, "duration": 7.755}, {"text": "And one way to approach this would be to plug into the density", "start": 4473.655, "duration": 4.665}, {"text": "for Q_i which is [NOISE] So you'd end up with this.", "start": 4478.32, "duration": 4.77}, {"text": "1 over 2 pi to the d over 2 Sigma, you know, wha- uh,", "start": 4483.09, "duration": 5.205}, {"text": "and so on into the negative 1.5 dot", "start": 4488.295, "duration": 4.095}, {"text": "dot dot times Z_i d_Z_i,", "start": 4492.39, "duration": 7.56}, {"text": "and then try to compute this integral.", "start": 4499.95, "duration": 2.46}, {"text": "Um, it turns out there's a much simpler way to compute this integral.", "start": 4502.41, "duration": 3.75}, {"text": "Anyone know what it is?", "start": 4506.16, "duration": 2.2}, {"text": "All right. Cool. Awesome. Expected value.", "start": 4513.88, "duration": 2.965}, {"text": "So the other way to compute this integral is to notice that this is the expected value of", "start": 4516.845, "duration": 6.09}, {"text": "z_i when z_i is drawn from Q_i, right?", "start": 4522.935, "duration": 6.1}, {"text": "So you know th- the,", "start": 4529.035, "duration": 1.41}, {"text": "the definition of the expected val- value of a random variable is expected value of", "start": 4530.445, "duration": 4.065}, {"text": "z is equal to integral over z probability_z times zdz, right?", "start": 4534.51, "duration": 6.75}, {"text": "That's what the expected value of a random variable is.", "start": 4541.26, "duration": 2.775}, {"text": "And so this integral is", "start": 4544.035, "duration": 3.435}, {"text": "the expected value of z with respect to z drawn from the Q_i distribution.", "start": 4547.47, "duration": 3.975}, {"text": "Um, but we know that Q_i is Gaussian with associated mean and certain variance,", "start": 4551.445, "duration": 5.925}, {"text": "and so the expected value of this- this is just mu of z_i given x_i, right?", "start": 4557.37, "duration": 6.48}, {"text": "It's that thing that you've already computed in the E-step. Makes sense?", "start": 4563.85, "duration": 4.29}, {"text": "And so when students derive the M-step, you know,", "start": 4568.14, "duration": 4.53}, {"text": "for EM implementations of Gaussians,", "start": 4572.67, "duration": 2.294}, {"text": "one of the key things to notice is, uh,", "start": 4574.964, "duration": 2.341}, {"text": "when are you actually taking an expected value with respect to a random variable,", "start": 4577.305, "duration": 3.405}, {"text": "in which case, it's just the value computed already,", "start": 4580.71, "duration": 2.655}, {"text": "and when do you need to plug in this big complicated", "start": 4583.365, "duration": 3.165}, {"text": "integral which can lead to very complicated, very intractable calculations.", "start": 4586.53, "duration": 3.945}, {"text": "Okay. So just when you're- whenever you see this, um, uh,", "start": 4590.475, "duration": 3.675}, {"text": "think about whether you need to be expanding a big complicated integral,", "start": 4594.15, "duration": 3.75}, {"text": "or if it can be interpreted as an expected value.", "start": 4597.9, "duration": 3.405}, {"text": "Okay. Um, and so for the M-step,", "start": 4601.305, "duration": 10.375}, {"text": "it's really, you know, the M-step is [NOISE]", "start": 4613.46, "duration": 29.44}, {"text": "All right. So that's the M-step.", "start": 4642.9, "duration": 2.16}, {"text": "And if you re-write this term as sum over i,", "start": 4645.06, "duration": 5.175}, {"text": "the expected value of z_i, uh,", "start": 4650.235, "duration": 4.125}, {"text": "drawn from Q_i of", "start": 4654.36, "duration": 2.07}, {"text": "this- all right.", "start": 4656.43, "duration": 9.955}, {"text": "It turns out that, um,", "start": 4666.385, "duration": 1.97}, {"text": "if you go ahead and,", "start": 4668.355, "duration": 1.665}, {"text": "uh, plug in the Gaussian density,", "start": 4670.02, "duration": 3.04}, {"text": "here [NOISE] actually on-", "start": 4673.66, "duration": 9.875}, {"text": "one rule of thumb for whether or not you should plug in", "start": 4683.535, "duration": 2.355}, {"text": "a complicated integral or plug in a Gaussian density,", "start": 4685.89, "duration": 2.61}, {"text": "um, this is just a rule of thumb after doing this type of math a long time,", "start": 4688.5, "duration": 3.18}, {"text": "is that see if there's a log in front.", "start": 4691.68, "duration": 1.605}, {"text": "If there's a log in front of a Gaussian density,", "start": 4693.285, "duration": 2.355}, {"text": "basically Gaussian density has an exponentiation, right?", "start": 4695.64, "duration": 3.03}, {"text": "The Gaussian density is 1 over e to the something.", "start": 4698.67, "duration": 3.03}, {"text": "So whenever there's a log in front,", "start": 4701.7, "duration": 1.56}, {"text": "the log exponentiation cancel out,", "start": 4703.26, "duration": 1.829}, {"text": "and this equation simplifies.", "start": 4705.089, "duration": 1.231}, {"text": "So one trick as you're doing", "start": 4706.32, "duration": 1.56}, {"text": "these derivations is just see if there's a log in front of a Gaussian density.", "start": 4707.88, "duration": 3.33}, {"text": "And when there is a plug in,", "start": 4711.21, "duration": 1.665}, {"text": "go ahead and plug in the formula for your Gaussian density,", "start": 4712.875, "duration": 2.46}, {"text": "the log will simplify that,", "start": 4715.335, "duration": 1.845}, {"text": "and what you end up with is the log of", "start": 4717.18, "duration": 2.31}, {"text": "a Gaussian density ends up being a quadratic function,", "start": 4719.49, "duration": 3.55}, {"text": "a quadratic function of the parameters.", "start": 4724.19, "duration": 2.98}, {"text": "And if you take the expected value with respect to a Gaussian density,", "start": 4727.17, "duration": 3.45}, {"text": "respect to quadratic function,", "start": 4730.62, "duration": 1.605}, {"text": "this whole thing ends up being a quadratic function.", "start": 4732.225, "duration": 3.69}, {"text": "Um, and then you can take derivatives of that equation with respect to the parameters.", "start": 4735.915, "duration": 5.67}, {"text": "With respect to mu of that whole thing,", "start": 4741.585, "duration": 2.94}, {"text": "set it to 0,", "start": 4744.525, "duration": 1.83}, {"text": "and then solve and they'll be roughly, um,", "start": 4746.355, "duration": 3.105}, {"text": "level of complexity of,", "start": 4749.46, "duration": 1.29}, {"text": "of maximizing quadratic function.", "start": 4750.75, "duration": 2.475}, {"text": "Okay. Hope that makes sense.", "start": 4753.225, "duration": 2.04}, {"text": "Um, the actual formulas are a little bit complicated.", "start": 4755.265, "duration": 2.475}, {"text": "So I don't- I'll,", "start": 4757.74, "duration": 1.02}, {"text": "I'll leave you to look at the actual formulas in the lecture notes,", "start": 4758.76, "duration": 2.19}, {"text": "but I think the take away is, uh,", "start": 4760.95, "duration": 1.965}, {"text": "don't expand this integral, um,", "start": 4762.915, "duration": 2.715}, {"text": "and when you are deriving this,", "start": 4765.63, "duration": 2.7}, {"text": "plug in the Gaussian densities here because the log will simplify.", "start": 4768.33, "duration": 3.03}, {"text": "Okay. And details of it in the lecture notes.", "start": 4771.36, "duration": 2.91}, {"text": "So let's break for today.", "start": 4774.27, "duration": 2.565}, {"text": "Uh, best of luck with the mid-term and- seriously, I hope you guys do well.", "start": 4776.835, "duration": 3.915}, {"text": "All right. I- I'll see you guys in a, in a few days.", "start": 4780.75, "duration": 3.43}]