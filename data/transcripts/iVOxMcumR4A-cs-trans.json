[{"text": "Okay. Welcome everyone. So, um,", "start": 3.47, "duration": 4.21}, {"text": "today we'll be going over learning theory.", "start": 7.68, "duration": 2.07}, {"text": "Um, this is, um,", "start": 9.75, "duration": 1.275}, {"text": "this used to be taught in the main lectures in- and in previous offerings.", "start": 11.025, "duration": 4.065}, {"text": "Ah, this year we're gonna cover it as,", "start": 15.09, "duration": 2.46}, {"text": "ah, as a Friday section.", "start": 17.55, "duration": 1.92}, {"text": "Um, however, some of the concepts here are,", "start": 19.47, "duration": 2.85}, {"text": "ah, we gonna be covering today are- are,", "start": 22.32, "duration": 2.295}, {"text": "um, important in the sense that they kind of deepen", "start": 24.615, "duration": 4.065}, {"text": "your understanding of how machine learning kind of works under the covers.", "start": 28.68, "duration": 3.54}, {"text": "What are the assumptions that we're making and you know,", "start": 32.22, "duration": 3.06}, {"text": "um, why do things generalize,", "start": 35.28, "duration": 2.009}, {"text": "um, and- and so forth.", "start": 37.289, "duration": 1.636}, {"text": "So here's the rough agenda for today.", "start": 38.925, "duration": 1.785}, {"text": "So, ah, we're going to quickly start off with, ah,", "start": 40.71, "duration": 2.76}, {"text": "framing the learning problem and, ah,", "start": 43.47, "duration": 3.33}, {"text": "we'll go deep into bias-variance, um, trade off.", "start": 46.8, "duration": 3.675}, {"text": "We'll go- we'll spend some time over there and we look at, uh,", "start": 50.475, "duration": 4.78}, {"text": "some other ways where you can kind of, ah,", "start": 55.255, "duration": 2.815}, {"text": "decompose the error, ah,", "start": 58.07, "duration": 1.455}, {"text": "as approximation error and estimation error.", "start": 59.525, "duration": 2.835}, {"text": "Um, we'll see what empirical, ah,", "start": 62.36, "duration": 2.505}, {"text": "risk minimization is and then we'll spend some time", "start": 64.865, "duration": 2.955}, {"text": "on uniform convergence and, um, VC dimensions.", "start": 67.82, "duration": 3.69}, {"text": "So, ah, let's jump right in.", "start": 71.51, "duration": 3.67}, {"text": "Right. So the, um,", "start": 75.58, "duration": 7.82}, {"text": "so the assumptions under which we are going to be operating, um,", "start": 83.4, "duration": 4.39}, {"text": "for- for this lecture and in fact for most of- most of", "start": 87.79, "duration": 3.48}, {"text": "the- the algorithms that we'll be covering in this course,", "start": 91.27, "duration": 3.6}, {"text": "um, is that there are two main assumptions.", "start": 94.87, "duration": 4.26}, {"text": "One is that there exists a data distribution,", "start": 99.13, "duration": 4.84}, {"text": "distribution D from which x y pairs are sampled.", "start": 107.13, "duration": 8.07}, {"text": "So this is, ah, this makes sense in the supervised learning setting where,", "start": 115.2, "duration": 5.02}, {"text": "um, you're expected to learn a mapping from x to y.", "start": 120.22, "duration": 3.285}, {"text": "But, ah, the assumption also actually holds", "start": 123.505, "duration": 3.355}, {"text": "more generally even in the unsupervised, ah, setting case.", "start": 126.86, "duration": 3.34}, {"text": "The- the main assumption is that there is", "start": 130.2, "duration": 2.295}, {"text": "a ge- data-generating distribution and the examples that we have in our training set,", "start": 132.495, "duration": 6.635}, {"text": "and the ones we will be encountering when we test it,", "start": 139.13, "duration": 5.24}, {"text": "ah, are all coming from the same distribution.", "start": 144.37, "duration": 2.51}, {"text": "Right. That's- that's like the core assumption.", "start": 146.88, "duration": 2.255}, {"text": "Um, without this, um,", "start": 149.135, "duration": 2.3}, {"text": "coming up with any theory is- is- is gonna be much harder.", "start": 151.435, "duration": 3.745}, {"text": "So the assumption here is that you know, um,", "start": 155.18, "duration": 2.04}, {"text": "there is some kind of a data ge-, ah, generating process.", "start": 157.22, "duration": 2.97}, {"text": "And we have a few samples from the data generating", "start": 160.19, "duration": 2.88}, {"text": "process that becomes our training set and that is a finite number.", "start": 163.07, "duration": 3.89}, {"text": "Um, you can get an infinite number of samples from this data generating process,", "start": 166.96, "duration": 5.61}, {"text": "and the examples that we're gonna encounter,", "start": 172.57, "duration": 2.845}, {"text": "ah, at test-time are also samples from the same process.", "start": 175.415, "duration": 3.875}, {"text": "Right. That's- that's the assumption.", "start": 179.29, "duration": 1.455}, {"text": "And there is a second assumption.", "start": 180.745, "duration": 2.565}, {"text": "Um, which is that all the samples are sampled independently.", "start": 183.31, "duration": 5.56}, {"text": "Um, so, um, with these two assumptions, ah,", "start": 195.03, "duration": 4.65}, {"text": "we can imagine a learning,", "start": 199.68, "duration": 2.52}, {"text": "ah, the process of learning to look something like this.", "start": 202.2, "duration": 3.07}, {"text": "So, we have a set of x y pairs which we call as s. Um,", "start": 205.27, "duration": 9.77}, {"text": "these are just x 1, y 1,", "start": 215.04, "duration": 3.315}, {"text": "x m y m. So we have m samples from- from- sample from the data", "start": 218.355, "duration": 10.075}, {"text": "generating process and we feed this into", "start": 228.43, "duration": 5.565}, {"text": "a learning algorithm and", "start": 233.995, "duration": 10.935}, {"text": "the output of the learning algorithm is what we call as a hypothesis.", "start": 244.93, "duration": 4.845}, {"text": "Hypothesis, ah, is- is a function, um,", "start": 249.775, "duration": 4.045}, {"text": "which accepts an input- a new input x and makes a prediction about- about y for that x.", "start": 253.82, "duration": 6.42}, {"text": "So, ah, this hypothesis is sometimes also in the form of Theta hat.", "start": 260.24, "duration": 6.055}, {"text": "So if we- if we restrict ourselves to a class of hypothesis.", "start": 266.295, "duration": 3.815}, {"text": "For example, ah, all possible logistic regression models of,", "start": 270.11, "duration": 3.84}, {"text": "ah, of dimension n, for example,", "start": 273.95, "duration": 2.13}, {"text": "then, um, it's, you know, um,", "start": 276.08, "duration": 2.715}, {"text": "obtaining those parameters is equivalent to obtaining the hypothesis function itself.", "start": 278.795, "duration": 5.16}, {"text": "So a key thing to note here is that this s is a random variable.", "start": 283.955, "duration": 7.275}, {"text": "All right.", "start": 291.53, "duration": 4.23}, {"text": "This is a random variable.", "start": 295.76, "duration": 1.415}, {"text": "This is a deterministic function.", "start": 297.175, "duration": 3.295}, {"text": "And what happens when you feed a random variable through", "start": 306.53, "duration": 3.86}, {"text": "a deterministic function you get a? Random variable.", "start": 310.39, "duration": 4.35}, {"text": "Exactly. So, um, the hypothesis that we get is also a random variable.", "start": 314.74, "duration": 6.43}, {"text": "Right. So all random variables have a distribution associated with them.", "start": 325.16, "duration": 5.72}, {"text": "The distribution associated with the data is the distribution of- of capital D. Um,", "start": 330.88, "duration": 5.84}, {"text": "this just a fixed, ah, deterministic function.", "start": 336.72, "duration": 3.33}, {"text": "And there is a distribution associated with the,", "start": 340.05, "duration": 4.73}, {"text": "um, um, with the- with the parameters that we obtain.", "start": 344.78, "duration": 3.87}, {"text": "That has a certain distribution as well.", "start": 348.65, "duration": 2.7}, {"text": "In, um, in the sta- in- in a more statistical setting,", "start": 351.35, "duration": 7.29}, {"text": "um, we call this an estimator.", "start": 358.64, "duration": 3.0}, {"text": "So if you take some advanced statistics courses you will call,", "start": 361.64, "duration": 3.03}, {"text": "ah, what you will come across as an estimator.", "start": 364.67, "duration": 2.85}, {"text": "Here we call it a learning algorithm.", "start": 367.52, "duration": 2.295}, {"text": "Right, and the distribution of Theta,", "start": 369.815, "duration": 3.314}, {"text": "um, is also called the sampling distribution.", "start": 373.129, "duration": 4.271}, {"text": "And the, um, what's implied in this process is that there exists some Theta star,", "start": 382.34, "duration": 8.564}, {"text": "ah, or in A star.", "start": 390.904, "duration": 3.456}, {"text": "However you want to view it which is in a sense a true parameter.", "start": 394.36, "duration": 5.28}, {"text": "A true parameter that we wish,", "start": 399.64, "duration": 7.385}, {"text": "ah, to be the output of the learning algorithm, ah,", "start": 407.025, "duration": 2.695}, {"text": "but of course, we never know- we never know what, ah,", "start": 409.72, "duration": 3.18}, {"text": "Theta star is, um, and when, um,", "start": 412.9, "duration": 3.97}, {"text": "what we get out of the learning algorithm, um,", "start": 416.87, "duration": 2.905}, {"text": "is- is going to be just a- a sample from a random, um, random variable.", "start": 419.775, "duration": 6.675}, {"text": "Now, a thing to note is that this the Theta star or A star is not random.", "start": 426.45, "duration": 6.86}, {"text": "It's just an unknown constant.", "start": 433.31, "duration": 2.62}, {"text": "Not a- when we say it's not random it means there is", "start": 438.55, "duration": 4.03}, {"text": "no probability distribution associated with it.", "start": 442.58, "duration": 3.06}, {"text": "It's just a constant which we don't know,", "start": 445.64, "duration": 2.16}, {"text": "that- that's- that's the assumption under which you operate.", "start": 447.8, "duration": 2.925}, {"text": "Right. Now, um, let- let's see what's,", "start": 450.725, "duration": 5.245}, {"text": "ah, let's see what's- what's- what are some properties about this Theta- Theta-hat.", "start": 455.97, "duration": 5.445}, {"text": "So all the, um,", "start": 461.415, "duration": 2.355}, {"text": "all- all the- all the entities that we estimate are generally,", "start": 463.77, "duration": 3.995}, {"text": "um, decorated with a hat on top,", "start": 467.765, "duration": 2.815}, {"text": "which- which indicates that it's- it's something that we estimated.", "start": 470.58, "duration": 3.285}, {"text": "Um, and anything with a star is like, you know,", "start": 473.865, "duration": 3.11}, {"text": "the true or the right answer which we don't have access to it generally.", "start": 476.975, "duration": 4.125}, {"text": "So any questions with this so far?", "start": 481.1, "duration": 4.675}, {"text": "Yeah. [BACKGROUND]", "start": 485.775, "duration": 6.11}, {"text": "Yeah. So, yeah, this could be, uh, um,", "start": 491.885, "duration": 3.83}, {"text": "in case of like, uh, uh,", "start": 495.715, "duration": 2.235}, {"text": "linear or, or logi- logistic regression", "start": 497.95, "duration": 2.25}, {"text": "or linear regression generally happens to be a vector.", "start": 500.2, "duration": 2.07}, {"text": "It could be a scalar,", "start": 502.27, "duration": 1.2}, {"text": "it could be, you know,", "start": 503.47, "duration": 1.095}, {"text": "a matrix, it could be anything.", "start": 504.565, "duration": 1.56}, {"text": "Right. Uh, it's just an entity that we estimate.", "start": 506.125, "duration": 2.625}, {"text": "Um, and sometimes, uh,", "start": 508.75, "duration": 2.55}, {"text": "H star can also be so generic that it,", "start": 511.3, "duration": 2.52}, {"text": "it need not even be parameterized.", "start": 513.82, "duration": 1.53}, {"text": "It's just some function that you estimate.", "start": 515.35, "duration": 1.98}, {"text": "So, uh, yeah, so it could,", "start": 517.33, "duration": 3.84}, {"text": "it could be a vector or a scalar or,", "start": 521.17, "duration": 1.95}, {"text": "or a matrix, it could be anything.", "start": 523.12, "duration": 1.935}, {"text": "Right? So, uh, let's see what happens when we- so in the lecture,", "start": 525.055, "duration": 9.525}, {"text": "we saw, uh, this diagram for in,", "start": 534.58, "duration": 2.34}, {"text": "in the - when we were talking about bias-variance.", "start": 536.92, "duration": 2.13}, {"text": "So in case of, uh, regression,", "start": 539.05, "duration": 8.97}, {"text": "[NOISE] and, um, we saw that this was one fit,", "start": 548.02, "duration": 14.88}, {"text": "this was just, uh,", "start": 562.9, "duration": 2.46}, {"text": "let me use a different color,", "start": 565.36, "duration": 2.08}, {"text": "straight line, and, right?", "start": 568.14, "duration": 12.295}, {"text": "And we saw this as, uh,", "start": 580.435, "duration": 2.205}, {"text": "the concepts of [NOISE] sorry,", "start": 582.64, "duration": 4.065}, {"text": "underfitting and this is overfit and this is like just right.", "start": 586.705, "duration": 10.165}, {"text": "Right, so the concept of underfitting and overfitting are,", "start": 598.35, "duration": 3.25}, {"text": "kind of, closely related to bias and variance.", "start": 601.6, "duration": 3.27}, {"text": "Uh, so this is how you would view it from the data.", "start": 604.87, "duration": 4.575}, {"text": "So this is from the data view, right?", "start": 609.445, "duration": 2.775}, {"text": "Cause this is x, this is y.", "start": 612.22, "duration": 2.115}, {"text": "You know this is your data.", "start": 614.335, "duration": 1.92}, {"text": "Um, and if, if you look at, you know, um,", "start": 616.255, "duration": 3.465}, {"text": "look at it from a data point of view,", "start": 619.72, "duration": 2.31}, {"text": "these are the kind of, uh,", "start": 622.03, "duration": 1.44}, {"text": "different algorithms that you might get, right?", "start": 623.47, "duration": 2.85}, {"text": "However, uh, to get a more formal sense,", "start": 626.32, "duration": 2.64}, {"text": "uh, formal view into what's,", "start": 628.96, "duration": 1.635}, {"text": "what's bias and variance,", "start": 630.595, "duration": 1.23}, {"text": "it's more useful to see it from the parameter view.", "start": 631.825, "duration": 4.005}, {"text": "[NOISE].", "start": 635.83, "duration": 5.58}, {"text": "So let's imagine we have four different learning algorithms, right?", "start": 641.41, "duration": 4.86}, {"text": "I'm just going to plot four different.", "start": 646.27, "duration": 8.16}, {"text": "And here this is the parameter space,", "start": 654.43, "duration": 2.115}, {"text": "let's say theta 1, theta 2.", "start": 656.545, "duration": 1.56}, {"text": "Let's imagine, you know,", "start": 658.105, "duration": 1.41}, {"text": "uh, we have just two parameters.", "start": 659.515, "duration": 2.475}, {"text": "It's easier to visualize theta 1 and theta 2, right.", "start": 661.99, "duration": 5.805}, {"text": "And this corresponds to algorithm A,", "start": 667.795, "duration": 2.31}, {"text": "algorithm B, C, and D. Right.", "start": 670.105, "duration": 5.115}, {"text": "There is, there is a true theta star.", "start": 675.22, "duration": 3.465}, {"text": "Let's, let's- which is unknown, right?", "start": 678.685, "duration": 7.675}, {"text": "Now, let's imagine we run through this,", "start": 690.33, "duration": 3.25}, {"text": "this process of sampling m examples running it through the algorithm,", "start": 693.58, "duration": 5.655}, {"text": "obtain a theta hat, right?", "start": 699.235, "duration": 2.4}, {"text": "And then we start with a new sample- sample from", "start": 701.635, "duration": 3.675}, {"text": "D run it through the algorithm we get a different theta hat, right?", "start": 705.31, "duration": 3.54}, {"text": "And theta hat is going to be different for different learning algorithms.", "start": 708.85, "duration": 3.735}, {"text": "So, so let's imagine first we,", "start": 712.585, "duration": 4.35}, {"text": "we sample some data that's our training set,", "start": 716.935, "duration": 2.82}, {"text": "run it through algorithm A and let's", "start": 719.755, "duration": 3.255}, {"text": "say this is the parameter we got and then we run it through", "start": 723.01, "duration": 3.33}, {"text": "Algorithm B and let's say this is the parameter we got and through", "start": 726.34, "duration": 3.57}, {"text": "C here and through D over here.", "start": 729.91, "duration": 4.515}, {"text": "And we're gonna repeat this, you know,", "start": 734.425, "duration": 1.8}, {"text": "second one maybe here,", "start": 736.225, "duration": 2.4}, {"text": "maybe here, here, here and so on and you repeat this process over and over and over.", "start": 738.625, "duration": 7.125}, {"text": "The, the key is that the number of samples per input is m,", "start": 745.75, "duration": 4.575}, {"text": "that is fixed, right?", "start": 750.325, "duration": 1.155}, {"text": "But we're gonna repeat this process and over and over and for every time we repeat it,", "start": 751.48, "duration": 4.515}, {"text": "we get a different point over here.", "start": 755.995, "duration": 2.175}, {"text": "[NOISE]", "start": 758.17, "duration": 15.6}, {"text": "Right? So, uh,", "start": 773.77, "duration": 2.18}, {"text": "each point each dot corresponds to a sample of size M, right?", "start": 775.95, "duration": 5.34}, {"text": "The number of points is basically the number of times we repeated the experiment, right?", "start": 781.29, "duration": 4.44}, {"text": "And what we see is that", "start": 785.73, "duration": 2.385}, {"text": "these dots are basically samples from the sampling distribution, right?", "start": 788.115, "duration": 5.33}, {"text": "Now, the concept of,", "start": 793.445, "duration": 4.7}, {"text": "of bias and variance is kind of visible over here.", "start": 798.145, "duration": 3.39}, {"text": "So if we were to classify this,", "start": 801.535, "duration": 2.1}, {"text": "now we would call this as", "start": 803.635, "duration": 2.73}, {"text": "bias and variance, right?", "start": 806.365, "duration": 10.8}, {"text": "So these two are algorithms that have low bias,", "start": 817.165, "duration": 3.915}, {"text": "these two are- have high variance,", "start": 821.08, "duration": 3.15}, {"text": "these two have low varia- I'm so- these two have low bias,", "start": 824.23, "duration": 2.58}, {"text": "high bias low variance, high variance.", "start": 826.81, "duration": 3.24}, {"text": "So what does this mean?", "start": 830.05, "duration": 1.695}, {"text": "Uh, so bias is basically, um,", "start": 831.745, "duration": 4.635}, {"text": "checking are the- is,", "start": 836.38, "duration": 2.355}, {"text": "is the sampling distribution kind of centered around the true parameter,", "start": 838.735, "duration": 3.48}, {"text": "the true unknown parameter?", "start": 842.215, "duration": 1.35}, {"text": "Is it centered around the true parameter?", "start": 843.565, "duration": 1.92}, {"text": "Right? And variance is, um, is,", "start": 845.485, "duration": 2.67}, {"text": "is measuring basically how dispersed the,", "start": 848.155, "duration": 3.765}, {"text": "the sampling distribution is, right?", "start": 851.92, "duration": 3.3}, {"text": "So, so formally speaking,", "start": 855.22, "duration": 1.62}, {"text": "this is bias and variance and it becomes, uh,", "start": 856.84, "duration": 2.46}, {"text": "you know pretty clear when we see it in the parameter view instead of in,", "start": 859.3, "duration": 3.69}, {"text": "uh, uh, uh, the data view.", "start": 862.99, "duration": 2.025}, {"text": "And essentially bias and variance are basically", "start": 865.015, "duration": 2.955}, {"text": "just properties of the first and second moments of your sampling distribution.", "start": 867.97, "duration": 3.75}, {"text": "So you're asking the first moment that's the mean, is it centered around the true parameter", "start": 871.72, "duration": 4.305}, {"text": "and the second moment that variance - that's", "start": 876.025, "duration": 2.475}, {"text": "literally variance of the bias-variance trade-off. Yeah.", "start": 878.5, "duration": 2.46}, {"text": "[inaudible].", "start": 880.96, "duration": 16.14}, {"text": "Yeah.", "start": 897.1, "duration": 0.1}, {"text": "[inaudible].", "start": 897.2, "duration": 0.1}, {"text": "Um, so this is, a,", "start": 897.3, "duration": 1.405}, {"text": "a diagram where I am using only two thetas just to fit,", "start": 898.705, "duration": 4.11}, {"text": "you know write on a whiteboard.", "start": 902.815, "duration": 1.695}, {"text": "So you, you would imagine something that has high variance, for example,", "start": 904.51, "duration": 3.36}, {"text": "this one to probably be of a much,", "start": 907.87, "duration": 2.565}, {"text": "much higher dimension, not just two,", "start": 910.435, "duration": 1.815}, {"text": "but it would still be spread out.", "start": 912.25, "duration": 1.47}, {"text": "It would still have like high variance.", "start": 913.72, "duration": 2.04}, {"text": "There would be points in a higher-dimensional space,", "start": 915.76, "duration": 2.415}, {"text": "you know but more spread out.", "start": 918.175, "duration": 2.275}, {"text": "Right, so, so the question was,", "start": 925.14, "duration": 2.815}, {"text": "um, the question was,", "start": 927.955, "duration": 2.535}, {"text": "um, in over here we, uh,", "start": 930.49, "duration": 2.295}, {"text": "we actually had more number of thetas but, uh,", "start": 932.785, "duration": 2.58}, {"text": "here with the higher variance,", "start": 935.365, "duration": 1.619}, {"text": "um, uh, plots we are having the same number of thetas.", "start": 936.984, "duration": 3.556}, {"text": "So, uh, yeah so you could imagine this to be higher-dimensional.", "start": 940.54, "duration": 4.245}, {"text": "And also, different algorithms could have different,", "start": 944.785, "duration": 4.755}, {"text": "uh, bias and variance even though they have the same number of parameters.", "start": 949.54, "duration": 5.13}, {"text": "For example, if you had regularization,", "start": 954.67, "duration": 1.89}, {"text": "the variance would come down, for example.", "start": 956.56, "duration": 2.31}, {"text": "Let me go over that, um, um, um,", "start": 958.87, "duration": 3.0}, {"text": "a few observations that we want to make, uh,", "start": 961.87, "duration": 2.085}, {"text": "is that as we increase the size of the data,", "start": 963.955, "duration": 3.81}, {"text": "every time we feed in, so if this were to,", "start": 967.765, "duration": 2.835}, {"text": "to be made bigger,", "start": 970.6, "duration": 1.725}, {"text": "if you take a bigger sample for every, um,", "start": 972.325, "duration": 3.21}, {"text": "every time we learn, uh,", "start": 975.535, "duration": 2.535}, {"text": "the variance of theta hat would become small, right?", "start": 978.07, "duration": 5.7}, {"text": "So if we repeat the same thing but with,", "start": 983.77, "duration": 2.62}, {"text": "with larger number of examples,", "start": 986.39, "duration": 2.595}, {"text": "this would be more- all of these would be more,", "start": 988.985, "duration": 2.815}, {"text": "um, tightly concentrated, right?", "start": 991.8, "duration": 2.17}, {"text": "So, so the spread is, uh, uh,", "start": 993.97, "duration": 2.685}, {"text": "so the spread is a function of how many examples we have in each,", "start": 996.655, "duration": 3.55}, {"text": "um, in each, uh, uh, iteration.", "start": 1000.205, "duration": 3.175}, {"text": "Right? So, uh, as m tends to infinity, right?", "start": 1005.16, "duration": 7.225}, {"text": "The variance tends to zero, right?", "start": 1012.385, "duration": 5.545}, {"text": "If you were to collect an infinite number of samples,", "start": 1017.93, "duration": 3.985}, {"text": "run it through the algorithm,", "start": 1021.915, "duration": 1.679}, {"text": "you would get some particular, um, um, theta-hat.", "start": 1023.594, "duration": 4.136}, {"text": "And if you were to repeat that with an infinite number of examples", "start": 1027.73, "duration": 2.85}, {"text": "we'll always keep getting the same, um, uh, theta hat.", "start": 1030.58, "duration": 3.675}, {"text": "Now the rate at which the variance goes,", "start": 1034.255, "duration": 4.515}, {"text": "goes to 0 as you increase m,", "start": 1038.77, "duration": 2.16}, {"text": "is you can think of it as what's also,", "start": 1040.93, "duration": 2.37}, {"text": "uh, called the statistical efficiency.", "start": 1043.3, "duration": 3.16}, {"text": "It's basically a measure of how efficient your algorithm", "start": 1051.18, "duration": 3.88}, {"text": "is in squeezing out information from a given amount of data.", "start": 1055.06, "duration": 3.51}, {"text": "And if theta hat tends", "start": 1058.57, "duration": 5.51}, {"text": "to theta star as m tends to infinity,", "start": 1064.08, "duration": 6.375}, {"text": "you call such algorithms as consistent.", "start": 1070.455, "duration": 3.865}, {"text": "So, um, consistent and if", "start": 1076.79, "duration": 7.52}, {"text": "the expected value of your theta hat is equal to theta star for all m, right?", "start": 1084.31, "duration": 9.455}, {"text": "So no matter how big your, um, sample size is,", "start": 1093.765, "duration": 3.535}, {"text": "if you always end up with", "start": 1097.3, "duration": 2.295}, {"text": "a sampling distribution that's centered around the true parameter,", "start": 1099.595, "duration": 4.605}, {"text": "then your estimator is called an unbiased estimator. Yes.", "start": 1104.2, "duration": 3.35}, {"text": "[inaudible].", "start": 1107.55, "duration": 2.61}, {"text": "So efficiency is, is, uh,", "start": 1110.16, "duration": 1.455}, {"text": "basically the rate at which, uh,", "start": 1111.615, "duration": 2.46}, {"text": "the variance drops to 0 as m tends to 0.", "start": 1114.075, "duration": 7.575}, {"text": "So for example, you may have one algorithm which, uh,", "start": 1121.65, "duration": 4.08}, {"text": "which, which, where the variance is a function of 1 over M square.", "start": 1125.73, "duration": 4.585}, {"text": "Another algorithm where the variance is a function of e to the,", "start": 1130.315, "duration": 4.435}, {"text": "uh, uh minus m. You,", "start": 1134.75, "duration": 2.56}, {"text": "you can have- the variance can, uh,", "start": 1137.31, "duration": 2.235}, {"text": "drive down at different rates, uh,", "start": 1139.545, "duration": 2.095}, {"text": "relative to m. So that's kind of captures, um, uh,", "start": 1141.64, "duration": 2.61}, {"text": "what- what's efficiency here.", "start": 1144.25, "duration": 1.26}, {"text": "[NOISE] Right? Yeah.", "start": 1145.51, "duration": 5.31}, {"text": "[inaudible]", "start": 1150.82, "duration": 8.12}, {"text": "Yeah. So uh, theta-theta hat approaches um,", "start": 1158.94, "duration": 5.98}, {"text": "so um, this is a random variable here so so here's one thing to be clear about here.", "start": 1165.77, "duration": 7.12}, {"text": "This is ah, a number, a constant,", "start": 1172.89, "duration": 2.985}, {"text": "and this is a constant but here this is a random variable, right?", "start": 1175.875, "duration": 4.785}, {"text": "So what we're seeing is that as m tends to infinity, theta hat,", "start": 1180.66, "duration": 5.1}, {"text": "that is the distribution,", "start": 1185.76, "duration": 1.605}, {"text": "converges towards being a constant and that constant is going to be a theta star.", "start": 1187.365, "duration": 4.995}, {"text": "Which means at smaller values of m,", "start": 1192.36, "duration": 3.03}, {"text": "your algorithm might be centered elsewhere,", "start": 1195.39, "duration": 2.505}, {"text": "but as you get more and more data,", "start": 1197.895, "duration": 2.145}, {"text": "your sampling distribution variance reduces and also gets", "start": 1200.04, "duration": 3.21}, {"text": "centered around the true theta star eventually.", "start": 1203.25, "duration": 4.54}, {"text": "Okay. So um, informally speaking,", "start": 1208.16, "duration": 4.885}, {"text": "if your algorithm has high bias,", "start": 1213.045, "duration": 2.999}, {"text": "it essentially means no matter how much data or evidence you provided,", "start": 1216.044, "duration": 5.296}, {"text": "it kind of always keeps away from from theta star, right?", "start": 1221.34, "duration": 3.795}, {"text": "You cannot change its mind no matter how much data you feed it,", "start": 1225.135, "duration": 3.09}, {"text": "it's never going to center itself around theta star.", "start": 1228.225, "duration": 2.85}, {"text": "That's like a high biased algorithm,", "start": 1231.075, "duration": 1.275}, {"text": "it's biased away from the true parameter.", "start": 1232.35, "duration": 2.805}, {"text": "And variance is, you can think of it as your algorithm", "start": 1235.155, "duration": 4.185}, {"text": "that's kind of highly distracted by", "start": 1239.34, "duration": 3.06}, {"text": "the noise in the data and kind of easily get swayed away,", "start": 1242.4, "duration": 3.57}, {"text": "you know, far away depending on the noise in your data.", "start": 1245.97, "duration": 4.065}, {"text": "So uh, these algorithms you would call them as those having high variance,", "start": 1250.035, "duration": 4.86}, {"text": "because they can easily get swayed by noise in the data.", "start": 1254.895, "duration": 3.96}, {"text": "And as we are seeing here,", "start": 1258.855, "duration": 3.21}, {"text": "bias and variance are kind of independent of each other.", "start": 1262.065, "duration": 3.045}, {"text": "You can have algorithms that have,", "start": 1265.11, "duration": 2.04}, {"text": "you know, an independent amount of bias and variance in them,", "start": 1267.15, "duration": 3.57}, {"text": "you know, there is there is no um,", "start": 1270.72, "duration": 2.13}, {"text": "um correlation between ah,", "start": 1272.85, "duration": 2.37}, {"text": "ah bias and variance.", "start": 1275.22, "duration": 2.26}, {"text": "And one way- so the- how do we how- do we kind of fight variance?", "start": 1278.15, "duration": 5.515}, {"text": "So first let's look at how we can address variance. Yes.", "start": 1283.665, "duration": 7.455}, {"text": "[BACKGROUND].", "start": 1291.12, "duration": 4.38}, {"text": "So bias and variance are properties of the algorithm at a given size m. Right?", "start": 1295.5, "duration": 7.785}, {"text": "So these plots were from um,", "start": 1303.285, "duration": 3.795}, {"text": "were from a fixed size m and for that fixed size data,", "start": 1307.08, "duration": 4.865}, {"text": "this algorithm has high bias, low variance,", "start": 1311.945, "duration": 4.41}, {"text": "this algorithm has high variance and high bias and so on.", "start": 1316.355, "duration": 4.84}, {"text": "Yeah. Yeah. You can you can um,", "start": 1321.195, "duration": 2.07}, {"text": "you can think of it as yeah, it,", "start": 1323.265, "duration": 1.64}, {"text": "you- you assume like a fixed data size.", "start": 1324.905, "duration": 2.655}, {"text": "Right? So uh, fighting variance.", "start": 1327.56, "duration": 11.3}, {"text": "Okay. So uh, one way to kind of ah,", "start": 1338.86, "duration": 5.075}, {"text": "address if you're in a high variance situation,", "start": 1343.935, "duration": 4.29}, {"text": "this will just increase the amount of data that you have,", "start": 1348.225, "duration": 2.73}, {"text": "and that would naturally just reduce the variance in your algorithm. Yes.", "start": 1350.955, "duration": 4.575}, {"text": "[BACKGROUND].", "start": 1355.53, "duration": 4.56}, {"text": "That is true. So you don't know upfront what uh,", "start": 1360.09, "duration": 2.7}, {"text": "whether you're you're uh,", "start": 1362.79, "duration": 1.515}, {"text": "in a in a high bias or high variance um, um, scenario.", "start": 1364.305, "duration": 4.23}, {"text": "One way to kind of um- one way to kind of uh, uh,", "start": 1368.535, "duration": 5.49}, {"text": "test that is by looking at your training performance versus test performance uh,", "start": 1374.025, "duration": 6.96}, {"text": "we'll go- we'll go over that um.", "start": 1380.985, "duration": 2.01}, {"text": "In fact we're gonna go into um, you know,", "start": 1382.995, "duration": 2.265}, {"text": "much more detail in the main lectures of how do you identify bias and variance,", "start": 1385.26, "duration": 3.615}, {"text": "here we're just going over the concepts of what are bias and what are variance.", "start": 1388.875, "duration": 4.35}, {"text": "So one way to um,", "start": 1393.225, "duration": 3.195}, {"text": "address variance is you just get more data, right?", "start": 1396.42, "duration": 2.145}, {"text": "As you as you get more data,", "start": 1398.565, "duration": 1.53}, {"text": "the- your sampling distributions kind of tend to get more concentrated.", "start": 1400.095, "duration": 4.89}, {"text": "Um, the other way is what's called as regularization.", "start": 1404.985, "duration": 4.915}, {"text": "So when you- when you um,", "start": 1412.07, "duration": 2.68}, {"text": "add regularization like L2 regularization or L1 regularization um,", "start": 1414.75, "duration": 5.61}, {"text": "what we're effectively doing is let's say we have", "start": 1420.36, "duration": 4.53}, {"text": "an algorithm with high variance maybe low bias,", "start": 1424.89, "duration": 5.89}, {"text": "low bias, high variance and you add regularization, right?", "start": 1432.17, "duration": 8.97}, {"text": "What you end up with is an algorithm that", "start": 1441.14, "duration": 5.34}, {"text": "has maybe a small bias,", "start": 1446.48, "duration": 5.325}, {"text": "you increase the bias by adding regularization but low variance.", "start": 1451.805, "duration": 5.965}, {"text": "So if what you care about is your predictive accuracy,", "start": 1459.46, "duration": 4.585}, {"text": "you're probably better off trading off", "start": 1464.045, "duration": 4.08}, {"text": "high variance to some bias and getting down- reducing your your um,", "start": 1468.125, "duration": 4.935}, {"text": "variance ah, to a large extent. Yeah.", "start": 1473.06, "duration": 2.4}, {"text": "[BACKGROUND].", "start": 1475.46, "duration": 3.75}, {"text": "Yeah. We'll- we- we- we're gonna uh,", "start": 1479.21, "duration": 2.35}, {"text": "uh, look into that next.", "start": 1481.56, "duration": 2.98}, {"text": "Right. So in order to kind of um,", "start": 1491.69, "duration": 4.99}, {"text": "get a better understanding of this uh, let's imagine um.", "start": 1496.68, "duration": 5.68}, {"text": "So think of this as the space of hypothesis, space of, right?", "start": 1502.82, "duration": 6.62}, {"text": "So um, let's assume there is a true- there exists, this hypothesis.", "start": 1512.51, "duration": 9.04}, {"text": "Let's call it g, right?", "start": 1521.55, "duration": 2.775}, {"text": "Which is like the best possible hypothesis you can think of.", "start": 1524.325, "duration": 4.44}, {"text": "By best possible hypothesis,", "start": 1528.765, "duration": 2.025}, {"text": "I mean if you were to kind of take this uh, um, um,", "start": 1530.79, "duration": 6.63}, {"text": "take this hypothesis and take the expected value of the loss", "start": 1537.42, "duration": 4.62}, {"text": "with respect to the data generating distribution across an infinite amount of data,", "start": 1542.04, "duration": 4.11}, {"text": "you kind of have the lowest error with this.", "start": 1546.15, "duration": 2.145}, {"text": "So this is, you know, um,", "start": 1548.295, "duration": 1.545}, {"text": "you know the best possible hypothesis.", "start": 1549.84, "duration": 2.175}, {"text": "And then, there is this class of hypotheses.", "start": 1552.015, "duration": 4.695}, {"text": "Let's call this classes h, right?", "start": 1556.71, "duration": 4.515}, {"text": "So this, for example,", "start": 1561.225, "duration": 1.245}, {"text": "can be the set of all logistic regression ah,", "start": 1562.47, "duration": 4.59}, {"text": "hypotheses, or the set of all ah, SVMs you know.", "start": 1567.06, "duration": 4.065}, {"text": "So this is a class of hypotheses and what we,", "start": 1571.125, "duration": 6.255}, {"text": "what we end up with when we ah,", "start": 1577.38, "duration": 2.505}, {"text": "take a finite amount of data,", "start": 1579.885, "duration": 1.89}, {"text": "is some member over here, right?", "start": 1581.775, "duration": 2.745}, {"text": "So let me call h star.", "start": 1584.52, "duration": 2.835}, {"text": "Okay. There is also some hypothesis in this class,", "start": 1587.355, "duration": 7.485}, {"text": "let me call it kind of h star,", "start": 1594.84, "duration": 3.735}, {"text": "which is the best in-class hypotheses.", "start": 1598.575, "duration": 3.015}, {"text": "So within the set of all logistic regression functions,", "start": 1601.59, "duration": 3.089}, {"text": "there exists some, you know,", "start": 1604.679, "duration": 1.891}, {"text": "some model which would give you the lowest um,", "start": 1606.57, "duration": 4.035}, {"text": "lowest error if you were to ah,", "start": 1610.605, "duration": 1.8}, {"text": "test it on the full data distribution, right?", "start": 1612.405, "duration": 2.82}, {"text": "Um, the best possible hypothesis may not be inside ah, your ah, um,", "start": 1615.225, "duration": 6.285}, {"text": "inside your hypothesis class,", "start": 1621.51, "duration": 1.29}, {"text": "it's just some, you know,", "start": 1622.8, "duration": 1.53}, {"text": "some hypothesis that that's um, um,", "start": 1624.33, "duration": 2.535}, {"text": "that's conceptually something outside the class, right?", "start": 1626.865, "duration": 2.535}, {"text": "Now g is not the best possible hypothesis,", "start": 1629.4, "duration": 8.47}, {"text": "h star is best in-class h,", "start": 1641.81, "duration": 9.385}, {"text": "and h hat is one you learned from finite data, right?", "start": 1651.195, "duration": 9.535}, {"text": "So, uh, we also introduce some new notation.", "start": 1666.53, "duration": 4.87}, {"text": "Um, so epsilon of H is,", "start": 1671.4, "duration": 5.535}, {"text": "you will call this the risk or generalization error.", "start": 1676.935, "duration": 5.205}, {"text": "[NOISE] Right?", "start": 1682.14, "duration": 6.45}, {"text": "And it is defined to be equal to the expectation of xy sampled from", "start": 1688.59, "duration": 7.47}, {"text": "E of indicator of h of x not equal to y.", "start": 1696.06, "duration": 8.76}, {"text": "Right? So you sample examples from the data-generating process,", "start": 1704.82, "duration": 6.3}, {"text": "run it through the hypothesis,", "start": 1711.12, "duration": 2.775}, {"text": "check whether it matches with,", "start": 1713.895, "duration": 2.295}, {"text": "uh, with your output and if it matches,", "start": 1716.19, "duration": 2.94}, {"text": "you get a 1, if it does, uh,", "start": 1719.13, "duration": 1.53}, {"text": "if it- if it, uh,", "start": 1720.66, "duration": 1.71}, {"text": "doesn't match you get a 1,", "start": 1722.37, "duration": 1.26}, {"text": "if it matches you get a 0.", "start": 1723.63, "duration": 1.485}, {"text": "So on average, this is, you know,", "start": 1725.115, "duration": 2.685}, {"text": "roughly speaking the fraction of all examples on which you make a mistake.", "start": 1727.8, "duration": 5.415}, {"text": "And here we are kind of thinking about this, um,", "start": 1733.215, "duration": 3.315}, {"text": "from a classification point of view to check if, you know,", "start": 1736.53, "duration": 2.625}, {"text": "the class of your output matches the true class or not.", "start": 1739.155, "duration": 3.06}, {"text": "But you can also extend this to,", "start": 1742.215, "duration": 2.145}, {"text": "uh, the regression setting.", "start": 1744.36, "duration": 2.04}, {"text": "Uh, but that's a little harder to analyze but, you know,", "start": 1746.4, "duration": 2.685}, {"text": "the generalization holds to, um uh,", "start": 1749.085, "duration": 2.49}, {"text": "the regression setting as well but we'll stick to classification for now.", "start": 1751.575, "duration": 3.39}, {"text": "And we have an epsilon hat,", "start": 1754.965, "duration": 4.485}, {"text": "s of h and this is called the empirical risk.", "start": 1759.45, "duration": 7.21}, {"text": "This is the empirical risk or empirical error.", "start": 1769.79, "duration": 3.34}, {"text": "And this over here is 1 over m,", "start": 1773.13, "duration": 5.65}, {"text": "i equal to 1 to m,", "start": 1779.15, "duration": 3.79}, {"text": "indicator of h of x_i not equal to y_i, right?", "start": 1782.94, "duration": 9.22}, {"text": "The difference here is that here this is like an infinite process.", "start": 1793.61, "duration": 4.09}, {"text": "You're- you're- you're, um,", "start": 1797.7, "duration": 1.44}, {"text": "sampling from D forever and calculating like the long-term average.", "start": 1799.14, "duration": 3.735}, {"text": "Whereas this is you have a finite number that's given to you", "start": 1802.875, "duration": 3.105}, {"text": "and what's the fraction of examples on which you make - you make an error.", "start": 1805.98, "duration": 4.44}, {"text": "Right. All right, uh,", "start": 1810.42, "duration": 3.765}, {"text": "before we go further, uh,", "start": 1814.185, "duration": 1.38}, {"text": "there was a question of how,", "start": 1815.565, "duration": 1.695}, {"text": "um, adding regularization reduces your variance.", "start": 1817.26, "duration": 4.77}, {"text": "So what you can see,", "start": 1822.03, "duration": 1.635}, {"text": "um, or actually let me- let me get back to that,", "start": 1823.665, "duration": 4.905}, {"text": "um - um, in a- in a bit.", "start": 1828.57, "duration": 2.34}, {"text": "Uh, so E of g and this is called the Bayes error.", "start": 1830.91, "duration": 10.26}, {"text": "[NOISE] So this essentially means if you take the best possible hypothesis,", "start": 1841.17, "duration": 8.355}, {"text": "what's the fraction, uh,", "start": 1849.525, "duration": 1.635}, {"text": "what's - what's the rate at which you make errors?", "start": 1851.16, "duration": 2.1}, {"text": "You know, uh, and that can be non-zero, right?", "start": 1853.26, "duration": 2.325}, {"text": "Even if you take the best possible hypothesis ever and that can still -", "start": 1855.585, "duration": 4.05}, {"text": "still make some - some mistakes and - and this is also called irreducible error.", "start": 1859.635, "duration": 4.305}, {"text": "[NOISE] For example if your data-generating process you know, uh,", "start": 1863.94, "duration": 10.065}, {"text": "spits out examples where for the same x you have different y's, uh,", "start": 1874.005, "duration": 4.77}, {"text": "in two different examples then no - no learning algorithm can,", "start": 1878.775, "duration": 5.25}, {"text": "you know, uh - uh,", "start": 1884.025, "duration": 2.295}, {"text": "do well in such cases.", "start": 1886.32, "duration": 2.115}, {"text": "That's just one- one kind of irreducible error,", "start": 1888.435, "duration": 2.745}, {"text": "they can be other kinds of irreducible, uh, errors as well.", "start": 1891.18, "duration": 3.88}, {"text": "And epsilon of h_*,", "start": 1895.88, "duration": 6.73}, {"text": "epsilon of g is called the approximation error.", "start": 1902.61, "duration": 4.11}, {"text": "[NOISE] So this essentially", "start": 1906.72, "duration": 7.92}, {"text": "means what is the price that we are paying for limiting ourselves to some class, right?", "start": 1914.64, "duration": 6.045}, {"text": "So it's the - it's the error between - it's the difference between", "start": 1920.685, "duration": 4.215}, {"text": "the best possible error that you can get and", "start": 1924.9, "duration": 2.25}, {"text": "the best possible error you can get from h_*.", "start": 1927.15, "duration": 2.895}, {"text": "Right, so this is, um,", "start": 1930.045, "duration": 2.01}, {"text": "this is an attribute of the class.", "start": 1932.055, "duration": 3.825}, {"text": "So what's the cost we are paying for restricting yourself to a class?", "start": 1935.88, "duration": 4.305}, {"text": "Then you have, uh,", "start": 1940.185, "duration": 2.355}, {"text": "epsilon of h_i minus epsilon h_* and this you call it the estimation error.", "start": 1942.54, "duration": 7.14}, {"text": "[NOISE] The estimation error is,", "start": 1949.68, "duration": 6.39}, {"text": "given the data that we got,", "start": 1956.07, "duration": 2.355}, {"text": "the m examples that we got and we estimated,", "start": 1958.425, "duration": 4.23}, {"text": "you know, using our estimator sum h - h - h_i.", "start": 1962.655, "duration": 3.075}, {"text": "What's the - what's", "start": 1965.73, "duration": 7.685}, {"text": "the - what's the error due to estimation and this is like approximation.", "start": 1973.415, "duration": 10.675}, {"text": "All right. So, this - this,", "start": 1984.16, "duration": 3.825}, {"text": "uh, the error on G is the Bayes error.", "start": 1987.985, "duration": 4.01}, {"text": "The gap between this error and the best in class is the approximation error and the gap", "start": 1991.995, "duration": 6.105}, {"text": "between the best in class and the hypothesis that you end", "start": 1998.1, "duration": 2.85}, {"text": "up with is called the estimation error, right?", "start": 2000.95, "duration": 3.15}, {"text": "And, uh, it's easy to see that, um,", "start": 2004.1, "duration": 4.18}, {"text": "h hat is actually equal to", "start": 2008.5, "duration": 5.3}, {"text": "estimation error", "start": 2014.89, "duration": 3.35}, {"text": "plus approximation error plus irreducible error.", "start": 2020.5, "duration": 9.29}, {"text": "Right? It's pretty easy.", "start": 2033.16, "duration": 2.785}, {"text": "You know, if you just add them up all these cancel out and you're just left with,", "start": 2035.945, "duration": 2.94}, {"text": "uh um, epsilon of H hat.", "start": 2038.885, "duration": 3.255}, {"text": "Um, so it's - it's kind of useful to think about", "start": 2042.14, "duration": 3.675}, {"text": "your generalization error as different components.", "start": 2045.815, "duration": 4.365}, {"text": "Um, some error which you just cannot,", "start": 2050.18, "duration": 3.585}, {"text": "you know, uh - uh, reduce it no matter what - no matter", "start": 2053.765, "duration": 2.565}, {"text": "what hypothesis you pick no matter how much of training data you have.", "start": 2056.33, "duration": 2.64}, {"text": "There's no way you can get rid of the irreducible error.", "start": 2058.97, "duration": 2.925}, {"text": "And then you make some - some decisions about", "start": 2061.895, "duration": 2.984}, {"text": "- that you're going to limit yourself to neural networks or", "start": 2064.879, "duration": 3.286}, {"text": "Logistic regression or whatever and thereby you're defining a class of", "start": 2068.165, "duration": 4.005}, {"text": "all possible models and that has a cost itself and that's your approximation error.", "start": 2072.17, "duration": 4.17}, {"text": "And then you are working with limited data.", "start": 2076.34, "duration": 2.535}, {"text": "And this is generally due to data, right?", "start": 2078.875, "duration": 3.33}, {"text": "And with the limited data that you have and", "start": 2082.205, "duration": 1.905}, {"text": "possibly due to some nuances of your algorithm,", "start": 2084.11, "duration": 2.28}, {"text": "you also have an estimation error, right?", "start": 2086.39, "duration": 3.04}, {"text": "We can further see that the estimation error can be broken down into", "start": 2089.98, "duration": 5.485}, {"text": "estimation variance and the estimation bias, right?", "start": 2095.465, "duration": 7.95}, {"text": "Um, and, uh, you can not, therefore,", "start": 2103.415, "duration": 5.01}, {"text": "write this as approximation error plus irreducible error.", "start": 2108.425, "duration": 8.725}, {"text": "And what we commonly call as bias and variance are - this we call it as variance", "start": 2118.75, "duration": 6.685}, {"text": "and this we call it as bias and this is just irreducible.", "start": 2125.435, "duration": 7.135}, {"text": "So sometimes you see", "start": 2133.0, "duration": 3.43}, {"text": "the bias-variance decomposition and", "start": 2136.43, "duration": 3.66}, {"text": "sometimes you see the estimation approximation error decomposition.", "start": 2140.09, "duration": 2.955}, {"text": "There are somewhat related, they're not exactly the same.", "start": 2143.045, "duration": 2.55}, {"text": "So, uh, the bias is basically why is,", "start": 2145.595, "duration": 6.524}, {"text": "you know, bias is basically trying to capture why is H hat far from a - from G, right?", "start": 2152.119, "duration": 5.791}, {"text": "Why is it staying away from G? You know.", "start": 2157.91, "duration": 1.755}, {"text": "Why did our hypothesis stay away from the true hypotheses?", "start": 2159.665, "duration": 3.51}, {"text": "And that could be because your classes, uh,", "start": 2163.175, "duration": 3.0}, {"text": "is- is kind of too small or it could be due to other reasons,", "start": 2166.175, "duration": 3.795}, {"text": "uh, such as, you know,", "start": 2169.97, "duration": 1.2}, {"text": "um - um, as we'll see", "start": 2171.17, "duration": 2.175}, {"text": "maybe regularization that kind of keeps you away from a certain- certain,", "start": 2173.345, "duration": 4.275}, {"text": "uh - uh, hypothesis, right?", "start": 2177.62, "duration": 1.995}, {"text": "And the variance is generally due to it like", "start": 2179.615, "duration": 2.745}, {"text": "- it's almost always due to having small data.", "start": 2182.36, "duration": 2.58}, {"text": "It could be due to other,", "start": 2184.94, "duration": 1.285}, {"text": "uh - uh, reasons as well.", "start": 2186.225, "duration": 2.185}, {"text": "But these are two different ways of,", "start": 2188.41, "duration": 2.985}, {"text": "uh, of decomposing your, um, your error.", "start": 2191.395, "duration": 3.525}, {"text": "So now, um, if you have high bias,", "start": 2194.92, "duration": 3.42}, {"text": "how do you fight high bias?", "start": 2198.34, "duration": 2.045}, {"text": "Fight high bias.", "start": 2200.385, "duration": 10.925}, {"text": "So how would you fight high bias?", "start": 2211.31, "duration": 2.205}, {"text": "Any guesses. [inaudible] Yeah exactly.", "start": 2213.515, "duration": 7.995}, {"text": "So one way is to just,", "start": 2221.51, "duration": 2.43}, {"text": "you know make your h bigger, right.", "start": 2223.94, "duration": 6.81}, {"text": "Make your h bigger. And also you can - you can try,", "start": 2230.75, "duration": 2.835}, {"text": "you know different algorithms,", "start": 2233.585, "duration": 2.025}, {"text": "um - um uh, after making your h bigger.", "start": 2235.61, "duration": 3.945}, {"text": "And what this generally means is what we saw there was regularization kind of,", "start": 2239.555, "duration": 6.03}, {"text": "you know reduces your - your, um,", "start": 2245.585, "duration": 3.66}, {"text": "variance by paying a small cost in bias and over here,", "start": 2249.245, "duration": 4.53}, {"text": "you know, um. [NOISE]", "start": 2253.775, "duration": 15.9}, {"text": "So let's say your algorithm has some bias, right.", "start": 2269.675, "duration": 7.365}, {"text": "So it has a high bias and some variance, right,", "start": 2277.04, "duration": 9.465}, {"text": "and you make H bigger, your,", "start": 2286.505, "duration": 6.24}, {"text": "your class bigger right and this generally results in", "start": 2292.745, "duration": 3.615}, {"text": "something which reduces your bias but also increases your variance, right?", "start": 2296.36, "duration": 5.2}, {"text": "So, with, with this picture you can,", "start": 2303.61, "duration": 2.605}, {"text": "you can also see, you know,", "start": 2306.215, "duration": 1.365}, {"text": "what's the effect of, um,", "start": 2307.58, "duration": 1.665}, {"text": "how, how does variance come into the picture?", "start": 2309.245, "duration": 2.175}, {"text": "Now just by having a bigger class,", "start": 2311.42, "duration": 2.355}, {"text": "there is a higher probability that", "start": 2313.775, "duration": 2.04}, {"text": "the hypothesis that you estimate can vary a lot, right,", "start": 2315.815, "duration": 4.095}, {"text": "if you reduce your- the space of hypothesis,", "start": 2319.91, "duration": 5.745}, {"text": "you may be increasing your bias because you may be moving away from g,", "start": 2325.655, "duration": 4.11}, {"text": "but you're also effectively reducing your variance, right.", "start": 2329.765, "duration": 3.06}, {"text": "So that's, that's the,", "start": 2332.825, "duration": 1.44}, {"text": "the one of the, you know,", "start": 2334.265, "duration": 1.71}, {"text": "trade off that you observe that any step,", "start": 2335.975, "duration": 2.295}, {"text": "you- a step that you take for example in,", "start": 2338.27, "duration": 3.6}, {"text": "um, reducing bias by making it", "start": 2341.87, "duration": 3.27}, {"text": "bigger also makes it possible for your h hat to land at much,", "start": 2345.14, "duration": 4.17}, {"text": "you know, a- at a wider space and increases your variance.", "start": 2349.31, "duration": 3.36}, {"text": "And if you take a step to reducing your variance by maybe making your,", "start": 2352.67, "duration": 4.665}, {"text": "um, your, your class smaller,", "start": 2357.335, "duration": 2.325}, {"text": "you may end up making it smaller by being away from the end thereby increase your,", "start": 2359.66, "duration": 4.26}, {"text": "your, um, um, increase your bias.", "start": 2363.92, "duration": 3.12}, {"text": "So, when you, when you add regularization,", "start": 2367.04, "duration": 4.185}, {"text": "you know, th- the question, uh, uh,", "start": 2371.225, "duration": 2.235}, {"text": "somebody asked before of how does, um, in,", "start": 2373.46, "duration": 3.795}, {"text": "how does adding regularization decrease the variance?", "start": 2377.255, "duration": 5.1}, {"text": "By adding regularization, you are effectively,", "start": 2382.355, "duration": 2.745}, {"text": "kind of shrinking the class of hypothesis that you have.", "start": 2385.1, "duration": 3.09}, {"text": "You start penalizing those hypotheses whose Theta is very,", "start": 2388.19, "duration": 4.29}, {"text": "is very large, and in a way you're kind of,", "start": 2392.48, "duration": 2.055}, {"text": "you know, shrinking the class of hypothesis that you have.", "start": 2394.535, "duration": 2.7}, {"text": "So, if you shrink the class of hypothesis your,", "start": 2397.235, "duration": 3.06}, {"text": "your variance is kind of reduced because, you know,", "start": 2400.295, "duration": 2.775}, {"text": "there's much smaller wiggles room for your estimator to place your h hat.", "start": 2403.07, "duration": 4.83}, {"text": "And, you know, if you shrink it by going away from,", "start": 2407.9, "duration": 4.035}, {"text": "from, uh, from g, you,", "start": 2411.935, "duration": 1.86}, {"text": "you also introduce bias.", "start": 2413.795, "duration": 1.14}, {"text": "That's like, you know, the bias variance,", "start": 2414.935, "duration": 2.565}, {"text": "uh, um, trade off.", "start": 2417.5, "duration": 1.995}, {"text": "Any questions on this so far?", "start": 2419.495, "duration": 2.665}, {"text": "Yeah. [BACKGROUND].Yeah, you, you probably wanna think of each of these,", "start": 2431.8, "duration": 4.81}, {"text": "you probably wanna think of this as a generalized version of this,", "start": 2436.61, "duration": 3.765}, {"text": "right, so here we have, like, fixed Theta 1,", "start": 2440.375, "duration": 1.995}, {"text": "Theta 2, but you know,", "start": 2442.37, "duration": 1.89}, {"text": "uh, because you could parameterize them into,", "start": 2444.26, "duration": 2.595}, {"text": "uh, uh, a few parameters you can kind of plot it in", "start": 2446.855, "duration": 2.055}, {"text": "a metric space but that's like a more general, um,", "start": 2448.91, "duration": 2.91}, {"text": "um, like a bag of hypotheses, and, you know,", "start": 2451.82, "duration": 3.555}, {"text": "but in any case in both of- both those diagrams,", "start": 2455.375, "duration": 3.584}, {"text": "a point here  is one hypothesis,", "start": 2458.959, "duration": 2.371}, {"text": "a point there is one hypothesis.", "start": 2461.33, "duration": 1.35}, {"text": "Here it's parameterized, here it's not parameterized.", "start": 2462.68, "duration": 2.7}, {"text": "Yes. [BACKGROUND]. The thing", "start": 2465.38, "duration": 10.59}, {"text": "is we differ,", "start": 2475.97, "duration": 1.365}, {"text": "d, um, so the question is,", "start": 2477.335, "duration": 2.205}, {"text": "how- what if we,", "start": 2479.54, "duration": 1.5}, {"text": "we shrink it towards h star, right.", "start": 2481.04, "duration": 2.265}, {"text": "The thing is, uh,", "start": 2483.305, "duration": 1.515}, {"text": "we don't know where h star is, right.", "start": 2484.82, "duration": 1.86}, {"text": "If we knew it, we didn't even need to learn anything.", "start": 2486.68, "duration": 2.52}, {"text": "We could just go straight there,", "start": 2489.2, "duration": 1.335}, {"text": "right. So, um, yeah.", "start": 2490.535, "duration": 2.385}, {"text": "[BACKGROUND].", "start": 2492.92, "duration": 12.33}, {"text": "With regularization? So the question is,", "start": 2505.25, "duration": 2.43}, {"text": "when we add regularization,", "start": 2507.68, "duration": 1.455}, {"text": "are we sure that the bias is going up?", "start": 2509.135, "duration": 2.145}, {"text": "No, we, we don't know and,", "start": 2511.28, "duration": 1.53}, {"text": "and this is a common scenario what happens, right.", "start": 2512.81, "duration": 2.535}, {"text": "You, when you add regularization, you, you,", "start": 2515.345, "duration": 2.865}, {"text": "you reduce the variance for sure but you're", "start": 2518.21, "duration": 3.045}, {"text": "very likely gonna introduce some bias in that process.", "start": 2521.255, "duration": 3.105}, {"text": "[BACKGROUND].", "start": 2524.36, "duration": 5.49}, {"text": "So if you add regularization,", "start": 2529.85, "duration": 1.5}, {"text": "you're shrinking your hypothesis space in some ways.", "start": 2531.35, "duration": 3.12}, {"text": "So you're kind of moving away from 2g. So you're kind of adding a little bit of bias.", "start": 2534.47, "duration": 4.65}, {"text": "You're very likely to add some bias in that process.", "start": 2539.12, "duration": 3.34}, {"text": "Yes, so, it's, uh, so I, I,", "start": 2543.25, "duration": 3.985}, {"text": "I would encourage you to, you know, kind of,", "start": 2547.235, "duration": 2.04}, {"text": "after this lecture to think about this a little more slowly, it's, it's,", "start": 2549.275, "duration": 3.18}, {"text": "it takes a while to kind of internalize this,", "start": 2552.455, "duration": 2.025}, {"text": "the concept of bias and variance and, and, um,", "start": 2554.48, "duration": 3.03}, {"text": "uh, It's not very intuitive but,", "start": 2557.51, "duration": 2.385}, {"text": "but, uh thinking about it more definitely helps.", "start": 2559.895, "duration": 4.545}, {"text": "All right, an- any other questions before we move on?", "start": 2565.71, "duration": 3.31}, {"text": "[BACKGROUND].", "start": 2569.02, "duration": 4.03}, {"text": "So an example for a hypothesis class, right?", "start": 2573.05, "duration": 2.01}, {"text": "So the- an example would be, um,", "start": 2575.06, "duration": 2.31}, {"text": "the set of all logistic regression models, right?", "start": 2577.37, "duration": 4.785}, {"text": "And, uh, when you do gradient descent on your,", "start": 2582.155, "duration": 2.82}, {"text": "you know, logistic regression class,", "start": 2584.975, "duration": 1.905}, {"text": "you're kind of implicitly restricting yourself to set", "start": 2586.88, "duration": 2.79}, {"text": "up possible logistic regression models, that's kind of implicit.", "start": 2589.67, "duration": 2.82}, {"text": "[BACKGROUND].", "start": 2592.49, "duration": 8.67}, {"text": "So, the h is the output of the learning algorithm, right?", "start": 2601.16, "duration": 5.58}, {"text": "So you feed and input your algorithm.", "start": 2606.74, "duration": 2.055}, {"text": "Like this is not the model.", "start": 2608.795, "duration": 1.47}, {"text": "This the learning algorithm like, this is,", "start": 2610.265, "duration": 1.845}, {"text": "like gradient descent for example.", "start": 2612.11, "duration": 2.385}, {"text": "And the output of that is the parameters that you learned that converge to.", "start": 2614.495, "duration": 5.1}, {"text": "Right. So d- so, yeah, you,", "start": 2619.595, "duration": 3.585}, {"text": "you probably don't wanna think about this as the model that you learned but this as the,", "start": 2623.18, "duration": 4.635}, {"text": "like the training process and the output of the training process is a model that you learn.", "start": 2627.815, "duration": 5.64}, {"text": "And that is a point in your,", "start": 2633.455, "duration": 2.565}, {"text": "in the class of hypotheses.", "start": 2636.02, "duration": 1.77}, {"text": "[BACKGROUND]. Yes, so, so,", "start": 2637.79, "duration": 8.19}, {"text": "you fix, um, that, uh,", "start": 2645.98, "duration": 2.4}, {"text": "th- the class of learning models, you, you,", "start": 2648.38, "duration": 2.01}, {"text": "say I'm gonna only gonna learn logistic regression models, right?", "start": 2650.39, "duration": 3.03}, {"text": "For different, different samples of data that you feed it as your training set,", "start": 2653.42, "duration": 4.125}, {"text": "you're gonna get, learn a different Theta hat.", "start": 2657.545, "duration": 3.195}, {"text": "[BACKGROUND].", "start": 2660.74, "duration": 2.49}, {"text": "Yes, the- they have to be within the class of hypotheses.", "start": 2663.23, "duration": 3.94}, {"text": "All right, so let's move on.", "start": 2668.98, "duration": 4.16}, {"text": "Next, we come across this concept called empirical risk minimization.", "start": 2696.62, "duration": 4.33}, {"text": "[NOISE].", "start": 2700.95, "duration": 21.63}, {"text": "ERM. So this is the Empirical Risk Minimizer.", "start": 2722.58, "duration": 8.08}, {"text": "Right. So, so the empirical risk minimizer is a learning algorithm.", "start": 2735.56, "duration": 5.92}, {"text": "Right. It is one of those kind of boxes that we drew.", "start": 2741.48, "duration": 4.47}, {"text": "It is, you know, ah- so in the box", "start": 2745.95, "duration": 8.91}, {"text": "that we drew earlier as learning algorithm, right.", "start": 2754.86, "duration": 9.18}, {"text": "So the- the- the diagram that we drew earlier based on which we- we ah,", "start": 2764.04, "duration": 3.87}, {"text": "reasoned everything so far,", "start": 2767.91, "duration": 1.725}, {"text": "didn't actually tell you what actually happens inside.", "start": 2769.635, "duration": 2.89}, {"text": "It could be doing gradient descent, it could just do something else.", "start": 2772.525, "duration": 3.025}, {"text": "It could be, you know,", "start": 2775.55, "duration": 1.29}, {"text": "some- some, you know,", "start": 2776.84, "duration": 1.725}, {"text": "smart programmer who's written a whole bunch of if,", "start": 2778.565, "duration": 2.325}, {"text": "else and just returns a theta, it could be anything.", "start": 2780.89, "duration": 2.825}, {"text": "Right. Uh, and no matter what kind of algorithm was used,", "start": 2783.715, "duration": 4.13}, {"text": "the- the bias-variance theory still holds.", "start": 2787.845, "duration": 3.54}, {"text": "Right. Now we're going to look at, ah,", "start": 2791.385, "duration": 2.34}, {"text": "a very specific type of learning algorithms called the empirical risk minimizer.", "start": 2793.725, "duration": 5.605}, {"text": "Right. So, um, and this was feed into your algorithm and", "start": 2799.85, "duration": 6.67}, {"text": "you get h star,", "start": 2806.52, "duration": 2.805}, {"text": "h hat ERM.", "start": 2809.325, "duration": 8.96}, {"text": "Right? Now, h, um,", "start": 2818.285, "duration": 4.205}, {"text": "h hat ERM is equal to- what is ERM, empirical risk minimization?", "start": 2823.04, "duration": 8.425}, {"text": "It's what we've been doing so far in the course.", "start": 2831.465, "duration": 3.685}, {"text": "Right? We, we tried to find", "start": 2835.15, "duration": 4.55}, {"text": "a minimizer in a class of hypotheses that minimizes the average training error.", "start": 2839.7, "duration": 6.1}, {"text": "Right. Um, so for example, um,", "start": 2856.25, "duration": 3.445}, {"text": "this is trying to minimize the training error from a classification, ah, ah, perspective.", "start": 2859.695, "duration": 6.345}, {"text": "This is kind of minimizing the- or increasing the training accuracy,", "start": 2866.04, "duration": 5.31}, {"text": "which is different from what actually logistic regression did with,", "start": 2871.35, "duration": 2.835}, {"text": "where we were doing the maximum likelihood or minimizing the negative log-likelihood.", "start": 2874.185, "duration": 3.69}, {"text": "It can be shown that, ah,", "start": 2877.875, "duration": 1.455}, {"text": "losses like the logistic loss are - are can be well approximated by,", "start": 2879.33, "duration": 4.605}, {"text": "um, by the ERM.", "start": 2883.935, "duration": 1.365}, {"text": "And, and, and this theory should- should, ah, ah,", "start": 2885.3, "duration": 2.85}, {"text": "hold nonetheless. Um, All right.", "start": 2888.15, "duration": 4.98}, {"text": "So if- if we are limiting", "start": 2893.13, "duration": 4.62}, {"text": "ourselves to do that class of algorithms which,", "start": 2897.75, "duration": 6.09}, {"text": "which worked by minimizing the training loss, right, um,", "start": 2903.84, "duration": 4.92}, {"text": "as opposed to something that say returns a", "start": 2908.76, "duration": 2.49}, {"text": "constant all the time or- or- or does something else.", "start": 2911.25, "duration": 2.805}, {"text": "If we limit ourselves to, um,", "start": 2914.055, "duration": 3.495}, {"text": "empirical risk minimizers, then we can come up with more theoretical results.", "start": 2917.55, "duration": 5.595}, {"text": "For example, uniform convergence,", "start": 2923.145, "duration": 2.28}, {"text": "which we are gonna look at right now.", "start": 2925.425, "duration": 1.695}, {"text": "[NOISE].", "start": 2927.12, "duration": 15.84}, {"text": "Right. So, so we're limiting ourselves to", "start": 2942.96, "duration": 4.875}, {"text": "empirical risk minimizers and starting off, er, uniform convergence.", "start": 2947.835, "duration": 7.585}, {"text": "Right. So there are two central questions that we are kind of interested in.", "start": 2965.3, "duration": 5.785}, {"text": "So, ah, one question is,", "start": 2971.085, "duration": 4.08}, {"text": "if we do empirical risk minimization,", "start": 2975.165, "duration": 2.775}, {"text": "that is if we just reduce the training loss, right,", "start": 2977.94, "duration": 3.6}, {"text": "what- what does that say about the generalization of an effect?", "start": 2981.54, "duration": 4.165}, {"text": "So that is basically, um,", "start": 2985.705, "duration": 2.69}, {"text": "e hat of h versus h. So for,", "start": 2988.395, "duration": 8.495}, {"text": "you know, consider some hypotheses.", "start": 2996.89, "duration": 1.755}, {"text": "Right. And that gives you some amount of training error.", "start": 2998.645, "duration": 3.255}, {"text": "Right. What does that say about its generalization error?", "start": 3001.9, "duration": 3.84}, {"text": "And that's one central question we wanna, um, um, consider.", "start": 3005.74, "duration": 4.935}, {"text": "And the second one is,", "start": 3010.675, "duration": 2.415}, {"text": "how does the generalization error of our learned hypothesis", "start": 3013.09, "duration": 6.25}, {"text": "compare to the best possible generalization error in that class?", "start": 3019.34, "duration": 7.425}, {"text": "Right. Note we're- you know, we're only talking about h star and not, g um, there.", "start": 3026.765, "duration": 5.76}, {"text": "So h star is- is- is the best in class um, um.", "start": 3032.525, "duration": 4.23}, {"text": "So these are- these are two central questions that we wanna- we wanna um, explore.", "start": 3036.755, "duration": 5.505}, {"text": "And for this, we're gonna use our two tools.", "start": 3042.26, "duration": 4.15}, {"text": "Right. So one is called the union bound.", "start": 3047.41, "duration": 3.97}, {"text": "[NOISE] Right.", "start": 3051.38, "duration": 4.38}, {"text": "What's the union bound?", "start": 3055.76, "duration": 1.35}, {"text": "Um, if we have um,", "start": 3057.11, "duration": 2.79}, {"text": "k different events A_2, A_K.", "start": 3059.9, "duration": 5.37}, {"text": "Then, ah, these need not be independent.", "start": 3065.27, "duration": 4.84}, {"text": "Independent. Then the probability of A_1 union A_2 union A_k,", "start": 3073.75, "duration": 9.865}, {"text": "is less than equal to the sum of-", "start": 3083.615, "duration": 5.035}, {"text": "If this looks trivial, it is trivial.", "start": 3097.84, "duration": 2.68}, {"text": "It's- it's um, it's probably one of the axioms in- in- in your,", "start": 3100.52, "duration": 3.72}, {"text": "ah, undergrad probability class.", "start": 3104.24, "duration": 2.235}, {"text": "But the, the probability of any one of these events happening", "start": 3106.475, "duration": 3.855}, {"text": "is less than or equal to the sum of the probabilities of,", "start": 3110.33, "duration": 4.335}, {"text": "ah, each of them, ah, happening.", "start": 3114.665, "duration": 2.475}, {"text": "Right. And then we have a second tool.", "start": 3117.14, "duration": 3.81}, {"text": "Right. It's called the Hoeffding's inequality.", "start": 3120.95, "duration": 8.37}, {"text": "[NOISE].", "start": 3129.32, "duration": 7.44}, {"text": "We're only going to state the- the inequality here,", "start": 3136.76, "duration": 2.67}, {"text": "ah, there is ah, um,", "start": 3139.43, "duration": 1.83}, {"text": "a supplemental notes on the website that actually proves the Hoeffding inequality.", "start": 3141.26, "duration": 3.87}, {"text": "You can, ah, go through that,", "start": 3145.13, "duration": 1.68}, {"text": "um, but here we're only going to state the result.", "start": 3146.81, "duration": 3.42}, {"text": "In fact, throughout this session, we are going to state results.", "start": 3150.23, "duration": 2.31}, {"text": "We're not gonna prove anything.", "start": 3152.54, "duration": 1.605}, {"text": "Um, so, ah, let Z_1, Z_2, Z_m,", "start": 3154.145, "duration": 11.49}, {"text": "be sampled from some Bernoulli distribution of parameter phi.", "start": 3165.635, "duration": 6.24}, {"text": "And let's call well,", "start": 3171.875, "duration": 3.42}, {"text": "phi hat to be the average of them,", "start": 3175.295, "duration": 5.005}, {"text": "m of z_i,", "start": 3181.9, "duration": 3.8}, {"text": "and let there be a Gamma greater than zero,", "start": 3186.76, "duration": 7.135}, {"text": "which we call it as the margin.", "start": 3193.895, "duration": 2.935}, {"text": "So the Hoeffding Inequality basically says,", "start": 3197.41, "duration": 3.025}, {"text": "the probability that the absolute difference between", "start": 3200.435, "duration": 6.705}, {"text": "the estimated phi parameter and the true phi parameter is greater than some margin,", "start": 3207.14, "duration": 9.36}, {"text": "can be bounded by 2 times the exponential", "start": 3216.5, "duration": 6.21}, {"text": "of minus 2 gamma square m. Right?", "start": 3222.71, "duration": 8.59}, {"text": "Not very obvious but you know,", "start": 3231.52, "duration": 2.71}, {"text": "you can, you can show this.", "start": 3234.23, "duration": 1.59}, {"text": "What, what it's basically saying,", "start": 3235.82, "duration": 1.08}, {"text": "is there is some- there is some- some ber- ah,", "start": 3236.9, "duration": 4.605}, {"text": "parameter between 0 and 1 of a Bernoulli distribution.", "start": 3241.505, "duration": 4.515}, {"text": "The fact that it is between 0 and 1 means it's- it's bounded.", "start": 3246.02, "duration": 4.755}, {"text": "And- and that's a key requirement for,", "start": 3250.775, "duration": 2.685}, {"text": "ah, the Hoeffding's inequality.", "start": 3253.46, "duration": 1.65}, {"text": "And now, we take samples from this Bernoulli distribution,", "start": 3255.11, "duration": 3.795}, {"text": "and the estimator for this is basically- and these are just 0s or 1s.", "start": 3258.905, "duration": 5.835}, {"text": "Z- Z- each of the Z is either a 0 or 1.", "start": 3264.74, "duration": 2.55}, {"text": "The sample of 0 or a 1 with probability, um, um,", "start": 3267.29, "duration": 3.195}, {"text": "Phi, and the estimator is basically just the averages of your samples.", "start": 3270.485, "duration": 5.47}, {"text": "Right. And, um, the absolute difference between the estimated value and the true value,", "start": 3275.955, "duration": 7.625}, {"text": "the probability that this difference becomes greater", "start": 3283.58, "duration": 3.36}, {"text": "than some margin Gamma is bounded by this expression.", "start": 3286.94, "duration": 5.145}, {"text": "Right. So there are a lot of things happening here.", "start": 3292.085, "duration": 1.815}, {"text": "So you probably want to, um, um,", "start": 3293.9, "duration": 1.62}, {"text": "you know, slowly think through this.", "start": 3295.52, "duration": 1.92}, {"text": "So this is a margin.", "start": 3297.44, "duration": 4.455}, {"text": "All right.", "start": 3301.895, "duration": 2.025}, {"text": "And this is like- basically like the deviation of the error.", "start": 3303.92, "duration": 5.39}, {"text": "[NOISE] Right.", "start": 3309.31, "duration": 4.005}, {"text": "Um, the absolute value of how- how- how far away", "start": 3313.315, "duration": 2.725}, {"text": "your estimated values from- from the true.", "start": 3316.04, "duration": 2.325}, {"text": "And you'd like it to be small- closer.", "start": 3318.365, "duration": 4.095}, {"text": "So you- you- you probably want, ah,", "start": 3322.46, "duration": 2.415}, {"text": "your -your Phi hat and phi,", "start": 3324.875, "duration": 2.13}, {"text": "to be not more than,", "start": 3327.005, "duration": 1.215}, {"text": "I don't know, 0.001.", "start": 3328.22, "duration": 1.57}, {"text": "Right. So in which case,", "start": 3329.79, "duration": 1.615}, {"text": "if the absolute value between,", "start": 3331.405, "duration": 2.925}, {"text": "ah, ah, the estimated and,", "start": 3334.33, "duration": 1.65}, {"text": "um, the true parameter is greater than 0.01,", "start": 3335.98, "duration": 4.765}, {"text": "if that's the margin your- that you're interested in.", "start": 3340.745, "duration": 2.205}, {"text": "Then this, ah, the Hoeffding's inequality proves", "start": 3342.95, "duration": 3.99}, {"text": "that if you were to repeat this process over and over and over,", "start": 3346.94, "duration": 4.695}, {"text": "the number of times phi hat is going to be", "start": 3351.635, "duration": 3.675}, {"text": "great- is going to be farther than 0.001 from the true parameter,", "start": 3355.31, "duration": 3.825}, {"text": "it's going to be less than this expression,", "start": 3359.135, "duration": 2.22}, {"text": "which is a function of m. Right.", "start": 3361.355, "duration": 1.95}, {"text": "And that is- you- you- you can kind of, ah,", "start": 3363.305, "duration": 2.175}, {"text": "believe it because as m increases, this becomes smaller,", "start": 3365.48, "duration": 3.87}, {"text": "which means the probability of, um,", "start": 3369.35, "duration": 3.075}, {"text": "your estimate deviating more than a certain margin only reduces as you increase m. Right.", "start": 3372.425, "duration": 6.21}, {"text": "So this is Hoeffding's inequality and we're gonna use this.", "start": 3378.635, "duration": 2.445}, {"text": "[inaudible].", "start": 3381.08, "duration": 6.12}, {"text": "Oh, yeah. Questions?", "start": 3387.2, "duration": 1.29}, {"text": "[inaudible].", "start": 3388.49, "duration": 7.635}, {"text": "Not, so, so the question is,", "start": 3396.125, "duration": 2.13}, {"text": "is h star, uh,", "start": 3398.255, "duration": 1.83}, {"text": "the limit of h_r as M goes to infinity?", "start": 3400.085, "duration": 3.255}, {"text": "Uh, it is h star in,", "start": 3403.34, "duration": 4.065}, {"text": "in the limit as M goes to infinity,", "start": 3407.405, "duration": 2.039}, {"text": "if it is a consistent estimator, right?", "start": 3409.444, "duration": 3.346}, {"text": "So we, we, we went over the concept of consistency.", "start": 3412.79, "duration": 2.76}, {"text": "Given infinite data, will you eventually get to the right answer?", "start": 3415.55, "duration": 3.045}, {"text": "And if your estimator is not consistent,", "start": 3418.595, "duration": 2.205}, {"text": "then it will- it need not be.", "start": 3420.8, "duration": 1.23}, {"text": "So, uh, in general h hat", "start": 3422.03, "duration": 2.655}, {"text": "need not converge to h star as you get an infinite amount of data.", "start": 3424.685, "duration": 3.105}, {"text": "[NOISE] All right?", "start": 3427.79, "duration": 3.18}, {"text": "So, uh, now we wanna use, um, uh,", "start": 3430.97, "duration": 3.51}, {"text": "these tools, tool 1 and tool 2 to answer our- like the central questions.", "start": 3434.48, "duration": 5.76}, {"text": "[NOISE] Any other questions?", "start": 3440.24, "duration": 5.715}, {"text": "Yeah.", "start": 3445.955, "duration": 0.345}, {"text": "[BACKGROUND]", "start": 3446.3, "duration": 12.12}, {"text": "This is a more limited version of Hoeffding's inequality and yes,", "start": 3458.42, "duration": 3.21}, {"text": "uh, if we limit ourselves to a Bernoulli variable, uh,", "start": 3461.63, "duration": 3.51}, {"text": "ba- um, which has some parameter phi and you take samples from it.", "start": 3465.14, "duration": 4.515}, {"text": "And you construct an estimator which", "start": 3469.655, "duration": 3.465}, {"text": "is the average of th- the samples of the 0s and 1s,", "start": 3473.12, "duration": 3.75}, {"text": "then, um, this inequality holds.", "start": 3476.87, "duration": 3.345}, {"text": "That's- thi- this inequality is called the Hoeffding's inequality.", "start": 3480.215, "duration": 3.825}, {"text": "Yes.", "start": 3484.04, "duration": 5.4}, {"text": "[BACKGROUND] So if you're, um,", "start": 3489.44, "duration": 4.62}, {"text": "in general, there, there are- there are- there", "start": 3494.06, "duration": 3.9}, {"text": "is this class of algorithms called maximum likelihood algorithms,", "start": 3497.96, "duration": 3.239}, {"text": "maximum likelihood estimators and a pure", "start": 3501.199, "duration": 2.281}, {"text": "maximum likelihood estimator is generally consistent.", "start": 3503.48, "duration": 2.79}, {"text": "If you include regularization,", "start": 3506.27, "duration": 3.0}, {"text": "then it need not be- it need not be,", "start": 3509.27, "duration": 4.245}, {"text": "uh, uh, uh, consistent though,", "start": 3513.515, "duration": 2.19}, {"text": "uh, I'm not very sure about that.", "start": 3515.705, "duration": 1.38}, {"text": "I'm not very sure about that. [NOISE] Yeah, sure.", "start": 3517.085, "duration": 5.415}, {"text": "Yeah. So basically like- if you think about", "start": 3522.5, "duration": 3.36}, {"text": "a neural net where you have something that's completely", "start": 3525.86, "duration": 1.83}, {"text": "[inaudible] neural net is not always consistent.", "start": 3527.69, "duration": 20.37}, {"text": "Yeah. So the- basically, um, um,", "start": 3548.11, "duration": 4.9}, {"text": "um, I know for the mic, uh,", "start": 3553.01, "duration": 1.725}, {"text": "wha- what, what, what he responded was, um,", "start": 3554.735, "duration": 2.79}, {"text": "if you have an algorithm like a neural net which is, um,", "start": 3557.525, "duration": 3.645}, {"text": "which is non-convex, you may actually not end up with the same,", "start": 3561.17, "duration": 3.66}, {"text": "uh, uh, result even if you, uh, um,", "start": 3564.83, "duration": 2.745}, {"text": "increase, um, increase like,", "start": 3567.575, "duration": 2.19}, {"text": "uh, the number of, um,", "start": 3569.765, "duration": 2.265}, {"text": "uh- though I would probably call the,", "start": 3572.03, "duration": 2.475}, {"text": "uh, uh, the fact- I,", "start": 3574.505, "duration": 1.995}, {"text": "I would probably think of the non-convexity to be part of an estimation bias,", "start": 3576.5, "duration": 4.905}, {"text": "um, because you could in theory always", "start": 3581.405, "duration": 2.625}, {"text": "find like the global minima of a neural network.", "start": 3584.03, "duration": 2.58}, {"text": "It's just that there's some bias in our estimator that", "start": 3586.61, "duration": 1.95}, {"text": "we are using gradient descent and we cannot solve it.", "start": 3588.56, "duration": 2.98}, {"text": "Okay. So now, uh,", "start": 3594.61, "duration": 2.305}, {"text": "let's- let's use these two tools, uh,", "start": 3596.915, "duration": 2.4}, {"text": "and for that, uh, we're gonna start [NOISE] how do we look at this diagram, right?", "start": 3599.315, "duration": 5.85}, {"text": "So, [NOISE] so over here,", "start": 3605.165, "duration": 9.27}, {"text": "um, we have hypotheses.", "start": 3614.435, "duration": 3.225}, {"text": "[NOISE] Here we have error,", "start": 3617.66, "duration": 3.33}, {"text": "[NOISE] and let's think of this.", "start": 3620.99, "duration": 5.19}, {"text": "[NOISE] There's actually one,", "start": 3626.18, "duration": 10.08}, {"text": "one curve which I'm trying to make it thick", "start": 3636.26, "duration": 2.37}, {"text": "and probably make it look like multiple curves,", "start": 3638.63, "duration": 2.145}, {"text": "this is just one curve and this we will call it as.", "start": 3640.775, "duration": 4.035}, {"text": "[NOISE] So this is the generalization risk or the,", "start": 3644.81, "duration": 8.835}, {"text": "uh, uh, the generalization error of every possible hypothesis,", "start": 3653.645, "duration": 5.565}, {"text": "uh, in our class, right?", "start": 3659.21, "duration": 2.85}, {"text": "So pick one hypothesis that's gonna be somewhere on this axis,", "start": 3662.06, "duration": 4.83}, {"text": "calculate the generalization error,", "start": 3666.89, "duration": 3.93}, {"text": "not the empirical, the generalization error", "start": 3670.82, "duration": 3.01}, {"text": "and- no that's the height of that curve, right?", "start": 3673.83, "duration": 3.37}, {"text": "And we also have something like this.", "start": 3677.2, "duration": 4.23}, {"text": "[NOISE] Right?", "start": 3681.43, "duration": 7.725}, {"text": "So this dotted line now corresponds to sum each of s_h.", "start": 3689.155, "duration": 10.805}, {"text": "Now let's, let's sample a set of m examples and calculate", "start": 3701.11, "duration": 6.73}, {"text": "the empirical error of all our hypotheses in our class and plot it as a curve, right?", "start": 3707.84, "duration": 7.605}, {"text": "Any questions on what, what, what these two are?", "start": 3715.445, "duration": 2.22}, {"text": "Yeah. [BACKGROUND] It need not meet.", "start": 3717.665, "duration": 4.125}, {"text": "I'm, I'm just, uh, uh, in fact,", "start": 3721.79, "duration": 2.43}, {"text": "thi- this is very likely not even a straight line,", "start": 3724.22, "duration": 1.98}, {"text": "you're just thinking of all, all possible hypotheses.", "start": 3726.2, "duration": 1.95}, {"text": "It may not be convex.", "start": 3728.15, "duration": 1.455}, {"text": "Um, this just to, to, um,", "start": 3729.605, "duration": 2.64}, {"text": "get some ideas, um,", "start": 3732.245, "duration": 2.34}, {"text": "um, get, get better intuitions on some of these ideas.", "start": 3734.585, "duration": 2.61}, {"text": "Yes. [BACKGROUND] So, uh, the black line,", "start": 3737.195, "duration": 4.035}, {"text": "the thick black line is the generalization error of all your hypotheses, right?", "start": 3741.23, "duration": 5.97}, {"text": "And let's say you sample some,", "start": 3747.2, "duration": 1.875}, {"text": "some, some data, right?", "start": 3749.075, "duration": 1.74}, {"text": "Let's call it S. On that sample,", "start": 3750.815, "duration": 2.52}, {"text": "you have training error for all possible hypotheses, right?", "start": 3753.335, "duration": 3.525}, {"text": "[NOISE] We haven't not learned anything, right?", "start": 3756.86, "duration": 2.535}, {"text": "It's, it's, uh, uh,", "start": 3759.395, "duration": 1.89}, {"text": "this is the generalization error and this is the empirical error for the given S, right?", "start": 3761.285, "duration": 6.42}, {"text": "Now, uh, in order to app- apply", "start": 3767.705, "duration": 4.215}, {"text": "Hoeffding's Inequality here, right?", "start": 3771.92, "duration": 7.155}, {"text": "So let's consider some h_i, right?", "start": 3779.075, "duration": 4.555}, {"text": "This is some hypothesis. We- we don't know.", "start": 3785.74, "duration": 3.37}, {"text": "So we start with some random hypotheses, right?", "start": 3789.11, "duration": 3.21}, {"text": "And- so by starting", "start": 3792.32, "duration": 4.86}, {"text": "with some hypotheses like think of this as you start with some parameter, [NOISE] right?", "start": 3797.18, "duration": 4.56}, {"text": "And, uh, let's- right.", "start": 3801.74, "duration": 8.26}, {"text": "So the height of this line up to the,", "start": 3810.79, "duration": 5.395}, {"text": "the thick black curve is basically,", "start": 3816.185, "duration": 4.005}, {"text": "um, the generalization error of h_i is the height to the thick black curve.", "start": 3820.19, "duration": 9.525}, {"text": "So let me call this Epsilon of h_i, right?", "start": 3829.715, "duration": 7.005}, {"text": "And the height to the dotted curve until here.", "start": 3836.72, "duration": 4.42}, {"text": "And this is Epsilon hat of h_i.", "start": 3841.57, "duration": 6.49}, {"text": "I'm gonna ignore the S for now, right?", "start": 3848.06, "duration": 2.98}, {"text": "And this corresponds to like the,", "start": 3854.02, "duration": 3.115}, {"text": "the sample that we obtain.", "start": 3857.135, "duration": 2.07}, {"text": "Now one thing, ah, you can,", "start": 3859.205, "duration": 2.76}, {"text": "you can check is that the expected value of- [NOISE]", "start": 3861.965, "duration": 23.145}, {"text": "where the expectation is with respect to the data's sample.", "start": 3885.11, "duration": 2.82}, {"text": "So what this means is that, ah,", "start": 3887.93, "duration": 1.875}, {"text": "for one particular sample you,", "start": 3889.805, "duration": 2.745}, {"text": "ah, -this is the generalization error you got.", "start": 3892.55, "duration": 2.37}, {"text": "Take another set of samples,", "start": 3894.92, "duration": 1.755}, {"text": "that curve might look som- some,", "start": 3896.675, "duration": 1.605}, {"text": "you know, some other way, and,", "start": 3898.28, "duration": 1.485}, {"text": "you know, the height of the dotted line would be there.", "start": 3899.765, "duration": 2.49}, {"text": "So in general on average,", "start": 3902.255, "duration": 2.025}, {"text": "if you sum average across all possible training samples that you can get, ah, the,", "start": 3904.28, "duration": 5.4}, {"text": "the expected value of the height to the dotted line is gonna be the height to the,", "start": 3909.68, "duration": 6.765}, {"text": "the, the thick line.", "start": 3916.445, "duration": 1.32}, {"text": "Right? That's, that's justified.", "start": 3917.765, "duration": 1.815}, {"text": "Now here if you apply Hoeffding's inequality,", "start": 3919.58, "duration": 2.715}, {"text": "you basically get probability of absolute difference between the empirical error versus", "start": 3922.295, "duration": 9.915}, {"text": "the generalization error to be greater than Gamma is", "start": 3932.21, "duration": 7.17}, {"text": "less than equal to 2 minus 2 Gamma square.", "start": 3939.38, "duration": 7.74}, {"text": "And- this is basically,", "start": 3947.12, "duration": 2.115}, {"text": "you know, Hoeffding's inequality.", "start": 3949.235, "duration": 1.965}, {"text": "We have right here except in place of phi and phi hat,", "start": 3951.2, "duration": 4.125}, {"text": "we have the true generalization error,", "start": 3955.325, "duration": 3.375}, {"text": "and the empirical error.", "start": 3958.7, "duration": 2.415}, {"text": "Any questions on this so far?", "start": 3961.115, "duration": 2.455}, {"text": "So what we are saying is essentially", "start": 3964.84, "duration": 4.21}, {"text": "the gap between the generalization error and the empirical error. All right.", "start": 3969.05, "duration": 7.68}, {"text": "Right. The gap being greater than", "start": 3976.73, "duration": 5.43}, {"text": "some margin Gamma is gonna be bounded by this expression.", "start": 3982.16, "duration": 7.275}, {"text": "Right? So loosely speaking what this means is,", "start": 3989.435, "duration": 4.35}, {"text": "as we increase the size M,", "start": 3993.785, "duration": 3.735}, {"text": "if our training is up- if we plot the set of all dotted lines for a larger M,", "start": 3997.52, "duration": 6.6}, {"text": "they are gonna be more concentrated around the black line.", "start": 4004.12, "duration": 4.15}, {"text": "Does that make sense? So, so take a moment and think about it.", "start": 4008.64, "duration": 5.23}, {"text": "This dotted line corresponds to S of some particular size", "start": 4013.87, "duration": 3.78}, {"text": "M. We could take another sample of,", "start": 4017.65, "duration": 4.575}, {"text": "you know, a fixed set of examples,", "start": 4022.225, "duration": 1.98}, {"text": "and that might look kinda something like this.", "start": 4024.205, "duration": 4.795}, {"text": "Right. And take another sample of size M,", "start": 4029.34, "duration": 3.67}, {"text": "and that might look something like [NOISE] this.", "start": 4033.01, "duration": 3.99}, {"text": "Now- and now, consider the set of", "start": 4037.0, "duration": 3.99}, {"text": "all deviations from the black line", "start": 4040.99, "duration": 3.84}, {"text": "to every possible dotted line along the vertical line of HI.", "start": 4044.83, "duration": 3.66}, {"text": "Right? Now this gap is greater than some margin", "start": 4048.49, "duration": 6.03}, {"text": "Gamma with probability less than this term over here.", "start": 4054.52, "duration": 6.18}, {"text": "Right? So, so it essentially means that if", "start": 4060.7, "duration": 3.63}, {"text": "you start plotting dotted lines with a bigger M, right,", "start": 4064.33, "duration": 3.72}, {"text": "where the set of all those dotted lines correspond to a bigger M,", "start": 4068.05, "duration": 4.05}, {"text": "they are gonna be much more tightly", "start": 4072.1, "duration": 3.21}, {"text": "concentrated around the true generalization of that, of that edge.", "start": 4075.31, "duration": 4.39}, {"text": "That make sense? And you're basically applying", "start": 4080.49, "duration": 3.805}, {"text": "Hoeffding's inequality to this gap over here instead of some phi.", "start": 4084.295, "duration": 4.08}, {"text": "That's basically what you're doing.", "start": 4088.375, "duration": 1.74}, {"text": "Right? Now, that's good.", "start": 4090.115, "duration": 4.77}, {"text": "But there's a problem here.", "start": 4094.885, "duration": 1.815}, {"text": "The problem here is that,", "start": 4096.7, "duration": 2.01}, {"text": "we started with some hypotheses,", "start": 4098.71, "duration": 2.685}, {"text": "and then averaged across all possible data that you could sample.", "start": 4101.395, "duration": 3.72}, {"text": "But in practice, this is useless.", "start": 4105.115, "duration": 2.01}, {"text": "Because in practice we start with some data,", "start": 4107.125, "duration": 3.045}, {"text": "and run the empirical risk minimizer to find the lowest H for that particular data.", "start": 4110.17, "duration": 5.385}, {"text": "Right? And when you, when,", "start": 4115.555, "duration": 2.04}, {"text": "when- which means that H,", "start": 4117.595, "duration": 3.135}, {"text": "and the data that you have are not really independent.", "start": 4120.73, "duration": 2.745}, {"text": "Right? You, you chose the H to minimize, ah,", "start": 4123.475, "duration": 3.165}, {"text": "minimize the risk for the empirical risk for", "start": 4126.64, "duration": 3.06}, {"text": "th- the particular data that you are given in the first place.", "start": 4129.7, "duration": 3.63}, {"text": "Right? So to, to fix this,", "start": 4133.33, "duration": 3.885}, {"text": "what we wanna do is basically extend this result that we got", "start": 4137.215, "duration": 6.255}, {"text": "to account for all H. Right.", "start": 4143.47, "duration": 7.62}, {"text": "Now if we want to get a bound on the gap between the probabilistic bound,", "start": 4151.09, "duration": 8.01}, {"text": "and the gap between the generalization error,", "start": 4159.1, "duration": 4.665}, {"text": "and the empirical error for all H. You know what's that bound gonna look like.", "start": 4163.765, "duration": 8.31}, {"text": "Right. And this is basically called uniform, uniform convergence.", "start": 4172.075, "duration": 3.945}, {"text": "This is- I'll just call uniform convergence because we are trying to, ah,", "start": 4176.02, "duration": 4.275}, {"text": "we are trying to see how the risk curve converges uniformly to the generalization risk curve,", "start": 4180.295, "duration": 6.765}, {"text": "or how the empirical risk curve uniformly converges to the generalization risk curve.", "start": 4187.06, "duration": 5.355}, {"text": "And, ah, it's, ah,", "start": 4192.415, "duration": 1.95}, {"text": "that's called uniform convergence which you can apply to functions in general,", "start": 4194.365, "duration": 2.985}, {"text": "but here we are applying to the risk curves across our hypotheses.", "start": 4197.35, "duration": 4.74}, {"text": "And we can show- I'm gonna, ah,", "start": 4202.09, "duration": 2.745}, {"text": "just, um, skip the math.", "start": 4204.835, "duration": 2.535}, {"text": "So, um, this we showed using Hoeffding's inequality,", "start": 4207.37, "duration": 3.854}, {"text": "and you can apply the union bound for unioning across all H.", "start": 4211.224, "duration": 6.481}, {"text": "Except we can- first we're going to limit ourselves to, um- all right.", "start": 4217.705, "duration": 7.14}, {"text": "So let me start over.", "start": 4224.845, "duration": 2.145}, {"text": "So we got this bound for a fixed H. Right?", "start": 4226.99, "duration": 3.735}, {"text": "But we are interested in getting the bound for any possible H. Right?", "start": 4230.725, "duration": 5.685}, {"text": "So that's our next step. Right? And the way we're gonna,", "start": 4236.41, "duration": 2.985}, {"text": "gonna extend this pointwise result to across all of them,", "start": 4239.395, "duration": 4.47}, {"text": "is gonna look different for two possible cases.", "start": 4243.865, "duration": 2.505}, {"text": "One is a case of a finite hypothesis class,", "start": 4246.37, "duration": 3.045}, {"text": "and the other case is gonna be the case for infinite hypothesis classes.", "start": 4249.415, "duration": 4.47}, {"text": "So what does it look like?", "start": 4253.885, "duration": 1.575}, {"text": "So, [NOISE]", "start": 4255.46, "duration": 12.33}, {"text": "so let's first consider finite hypothesis classes.", "start": 4267.79, "duration": 4.33}, {"text": "So first we're gonna assume that the class of H has a finite number of hypotheses.", "start": 4279.42, "duration": 9.385}, {"text": "The result by itself is not very useful,", "start": 4288.805, "duration": 2.265}, {"text": "but it's gonna be like a building block for,", "start": 4291.07, "duration": 2.595}, {"text": "for the, for the other case.", "start": 4293.665, "duration": 1.53}, {"text": "So let's assume that the number of hypotheses in this class is some number K. Right?", "start": 4295.195, "duration": 8.415}, {"text": "Ah, we can show that- I'm not gonna go over the,", "start": 4303.61, "duration": 5.625}, {"text": "the derivation, but I'm just gonna,", "start": 4309.235, "duration": 2.19}, {"text": "um, write out the result.", "start": 4311.425, "duration": 2.085}, {"text": "It's, it's pretty intuitive.", "start": 4313.51, "duration": 1.38}, {"text": "So basically what we do is, ah,", "start": 4314.89, "duration": 2.235}, {"text": "we apply the union bound for all K hypotheses,", "start": 4317.125, "duration": 3.525}, {"text": "and we end up just multiplying that by a factor of K. Right?", "start": 4320.65, "duration": 4.02}, {"text": "So what we get is the probability that there exists some hypotheses", "start": 4324.67, "duration": 10.335}, {"text": "in H such that the empirical error", "start": 4335.005, "duration": 7.56}, {"text": "minus generalization error is greater than Gamma,", "start": 4342.565, "duration": 9.9}, {"text": "is less than equal to K times,", "start": 4352.465, "duration": 4.525}, {"text": "K times the probability of any 1 which is equal to K times,", "start": 4357.6, "duration": 5.064}, {"text": "ah, 2 minus xp 2 Gamma square M. And this we flip it over,", "start": 4362.664, "duration": 11.76}, {"text": "we negate it, and we get the probability that for all hypotheses in our class,", "start": 4374.424, "duration": 8.156}, {"text": "the empirical risk minus generalization risk is less than Gamma,", "start": 4384.54, "duration": 9.685}, {"text": "is gonna be greater than equal to 1 minus 2K,", "start": 4394.225, "duration": 9.765}, {"text": "minus 2 Gamma square.", "start": 4403.99, "duration": 2.5}, {"text": "Okay. So with probability at least 1 minus,", "start": 4407.37, "duration": 6.385}, {"text": "you know this expression,", "start": 4413.755, "duration": 1.425}, {"text": "which we can, we can call this Delta with probability at least so much.", "start": 4415.18, "duration": 8.295}, {"text": "For all hypotheses, our margin is gonna be less than some Gamma.", "start": 4423.475, "duration": 6.735}, {"text": "Right? This is, this is just, um,", "start": 4430.21, "duration": 4.26}, {"text": "Hoeffding's inequality plus union bound,", "start": 4434.47, "duration": 3.39}, {"text": "and just negate the two sides, you get this.", "start": 4437.86, "duration": 2.49}, {"text": "And you, you can go with this slowly, um, um,", "start": 4440.35, "duration": 3.165}, {"text": "you know later from the notes,", "start": 4443.515, "duration": 1.665}, {"text": "the notes goes over this,", "start": 4445.18, "duration": 1.245}, {"text": "um, in more detail.", "start": 4446.425, "duration": 2.385}, {"text": "Right? Now, basically now what we have is, you know,", "start": 4448.81, "duration": 4.875}, {"text": "now let's let Delta equals to", "start": 4453.685, "duration": 4.23}, {"text": "K exp minus 2 Gamma squared M. So we basically now have,", "start": 4457.915, "duration": 7.47}, {"text": "um, a relation between Delta which is like the probability of error.", "start": 4465.385, "duration": 8.695}, {"text": "By here, um, ah, by error I mean that the,", "start": 4477.18, "duration": 4.465}, {"text": "um, empirical risk, and the generalization risk are farther than some, some margin.", "start": 4481.645, "duration": 6.315}, {"text": "And Gamma is called the margin of error.", "start": 4487.96, "duration": 3.61}, {"text": "And M is your sample size.", "start": 4494.07, "duration": 3.68}, {"text": "So, so what this basically tells us, um,", "start": 4500.97, "duration": 4.025}, {"text": "if your algorithm is the empirical risk minimizer,", "start": 4504.995, "duration": 4.135}, {"text": "it could have been any kind of algorithm.", "start": 4509.13, "duration": 1.86}, {"text": "But if it is the kind that minimizes the training error,", "start": 4510.99, "duration": 3.075}, {"text": "then you can get by, by,", "start": 4514.065, "duration": 3.675}, {"text": "by just changing the sample size,", "start": 4517.74, "duration": 3.985}, {"text": "you can get a relation between the margin of error and", "start": 4521.725, "duration": 3.435}, {"text": "the probability of error and relate it to the sample size, right?", "start": 4525.16, "duration": 3.285}, {"text": "So, um, what we can do with this relation is basically", "start": 4528.445, "duration": 8.295}, {"text": "fix any two and solve for the third,", "start": 4536.74, "duration": 2.295}, {"text": "and that gives us,", "start": 4539.035, "duration": 2.955}, {"text": "nope, some actionable results.", "start": 4541.99, "duration": 4.47}, {"text": "For example, you can fix any two and solve for the third from this relationship, right?", "start": 4546.46, "duration": 6.945}, {"text": "And what, what, what that could, uh,", "start": 4553.405, "duration": 3.165}, {"text": "mean is for example,", "start": 4556.57, "duration": 2.895}, {"text": "so you, you can choose any two and solve for the third.", "start": 4559.465, "duration": 2.445}, {"text": "Um, I'm only gonna go over one, one of those.", "start": 4561.91, "duration": 2.7}, {"text": "So let, let's fix,", "start": 4564.61, "duration": 2.62}, {"text": "fix uh, Gamma and Delta to be greater than zero.", "start": 4567.96, "duration": 8.5}, {"text": "And we solve for m, and we get m [NOISE] weighted to, equal 1 over 2 Gamma square,", "start": 4576.46, "duration": 11.174}, {"text": "log 2K over Delta.", "start": 4587.634, "duration": 5.326}, {"text": "So what this means is with probability at least 1 minus", "start": 4592.96, "duration": 4.35}, {"text": "Delta which means probably at least 99% or 99.9%.", "start": 4597.31, "duration": 4.71}, {"text": "For example, with probability at least,", "start": 4602.02, "duration": 2.61}, {"text": "uh, 1 minus delta,", "start": 4604.63, "duration": 1.86}, {"text": "the margin of error between", "start": 4606.49, "duration": 3.165}, {"text": "the empirical risk and the true generalization risk is gonna be less", "start": 4609.655, "duration": 5.085}, {"text": "than Gamma as long as your training size is bigger than this expression, right.", "start": 4614.74, "duration": 8.265}, {"text": "That's something actionable for us, right.", "start": 4623.005, "duration": 1.755}, {"text": "Now, theory can be useful.", "start": 4624.76, "duration": 1.305}, {"text": "So this is also called the sample complexity dessert.", "start": 4626.065, "duration": 2.835}, {"text": "[NOISE] right? And uh, basically,", "start": 4628.9, "duration": 7.58}, {"text": "what this means is as you increase m and you,", "start": 4636.48, "duration": 3.18}, {"text": "you sample different [NOISE] sets of, uh, uh, data-sets,", "start": 4639.66, "duration": 4.005}, {"text": "your dotted lines are gonna get closer and closer to, to, uh,", "start": 4643.665, "duration": 4.895}, {"text": "the thick line which means,", "start": 4648.56, "duration": 3.215}, {"text": "minimizing your- minimizing on", "start": 4651.775, "duration": 2.205}, {"text": "the dotted line will also get you closer to the generalization error.", "start": 4653.98, "duration": 4.85}, {"text": "So this, this is basically telling you how minimizing on, on, um,", "start": 4658.83, "duration": 4.53}, {"text": "minimizing on the empirical risk gets you closer to,", "start": 4663.36, "duration": 4.005}, {"text": "uh, gen- generalization, right?", "start": 4667.365, "duration": 3.55}, {"text": "Okay, so that- so we started off with two questions,", "start": 4670.915, "duration": 4.02}, {"text": "relating the empirical risk to generalization risk.", "start": 4674.935, "duration": 3.375}, {"text": "Now, let's, let's explore the second question.", "start": 4678.31, "duration": 2.7}, {"text": "What about, uh, the generalization error [NOISE] of [NOISE] our minimizer with the,", "start": 4681.01, "duration": 9.72}, {"text": "uh, um, best possible in class?", "start": 4690.73, "duration": 3.54}, {"text": "So let's look at this diagram again.", "start": 4694.27, "duration": 3.075}, {"text": "Let's say we started with this dotted curve, right.", "start": 4697.345, "duration": 3.945}, {"text": "And the minimizer of that would be h-star.", "start": 4701.29, "duration": 3.525}, {"text": "This is h-star. Sorry the diagram is a little, uh,", "start": 4704.815, "duration": 4.215}, {"text": "[NOISE] let me erase the previous one [NOISE] right?", "start": 4709.03, "duration": 8.04}, {"text": "So this is h-hat.", "start": 4717.07, "duration": 2.43}, {"text": "Sorry, this is h-hat.", "start": 4719.5, "duration": 2.53}, {"text": "And this has a particular generalization error, right?", "start": 4723.54, "duration": 4.39}, {"text": "That is the point of, of- uh,", "start": 4727.93, "duration": 1.755}, {"text": "let- let- let's assume we got this data-set.", "start": 4729.685, "duration": 2.355}, {"text": "We ran the empirical [NOISE] risk minimizer and we obtained this hypothesis.", "start": 4732.04, "duration": 4.305}, {"text": "And when we deploy this in the world- in the real world,", "start": 4736.345, "duration": 3.285}, {"text": "its error is gonna be so much, right?", "start": 4739.63, "duration": 3.21}, {"text": "Now, how does this compare [NOISE] to the performance of the minimizer of the, the,", "start": 4742.84, "duration": 6.555}, {"text": "the best possible [NOISE] ,", "start": 4749.395, "duration": 7.875}, {"text": "so this is h-star,", "start": 4757.27, "duration": 2.97}, {"text": "best in class, right?", "start": 4760.24, "duration": 2.595}, {"text": "Now, we want to get a relation between this error level and this error level.", "start": 4762.835, "duration": 5.52}, {"text": "We got one bound that relates this to this,", "start": 4768.355, "duration": 4.32}, {"text": "and now we want something that relates this to this.", "start": 4772.675, "duration": 3.045}, {"text": "Now, how do we do that?", "start": 4775.72, "duration": 2.13}, {"text": "It's pretty straightforward.", "start": 4777.85, "duration": 1.5}, {"text": "Um, so the em- generalization error of h-hat,", "start": 4779.35, "duration": 10.095}, {"text": "that's this dot over here,", "start": 4789.445, "duration": 2.395}, {"text": "is less than equal to the empirical risk of h-hat plus Gamma.", "start": 4791.84, "duration": 9.68}, {"text": "So we got the result, um,", "start": 4801.52, "duration": 2.28}, {"text": "using a Hoeffding and union bound that the gap between the dotted line and the,", "start": 4803.8, "duration": 5.985}, {"text": "the thick black line is always less than Gamma, right?", "start": 4809.785, "duration": 3.495}, {"text": "And it's the absolute value.", "start": 4813.28, "duration": 1.44}, {"text": "So we can, we can, uh, um,", "start": 4814.72, "duration": 2.22}, {"text": "write it this way as well.", "start": 4816.94, "duration": 1.74}, {"text": "And this, right?", "start": 4818.68, "duration": 2.73}, {"text": "So basically, we, we start it from the,", "start": 4821.41, "duration": 2.235}, {"text": "the thick black line, drop down to the dotted line.", "start": 4823.645, "duration": 5.91}, {"text": "And this is gonna be less than the empirical error of h-star plus Gamma. Why is that?", "start": 4829.555, "duration": 9.885}, {"text": "Because em- empirical error,", "start": 4839.44, "duration": 2.865}, {"text": "um, the empirical error, uh, uh,", "start": 4842.305, "duration": 3.915}, {"text": "of h-hat by definition,", "start": 4846.22, "duration": 2.565}, {"text": "is less than or equal to the empirical error on any other hypotheses,", "start": 4848.785, "duration": 4.38}, {"text": "including the best in class.", "start": 4853.165, "duration": 1.29}, {"text": "Because this is the training error,", "start": 4854.455, "duration": 1.875}, {"text": "not, not, not the generalization error, right?", "start": 4856.33, "duration": 2.715}, {"text": "So which means, um- and,", "start": 4859.045, "duration": 5.685}, {"text": "and this is less than or equal to.", "start": 4864.73, "duration": 2.28}, {"text": "So we, we dropped from the generalization to the test.", "start": 4867.01, "duration": 4.515}, {"text": "And we said, this test is,", "start": 4871.525, "duration": 2.19}, {"text": "thi- this training error is always gonna to be less", "start": 4873.715, "duration": 4.185}, {"text": "than the empirical error of the best-in-class.", "start": 4877.9, "duration": 5.01}, {"text": "You see that the best-in-class is higher for the trade to be empirical error.", "start": 4882.91, "duration": 4.14}, {"text": "And this again, is now- this gap is also bounded because we,", "start": 4887.05, "duration": 5.055}, {"text": "we proved uniform convergence.", "start": 4892.105, "duration": 1.365}, {"text": "That the gap between the dotted line and thick line is bounded by Gamma for any h, right?", "start": 4893.47, "duration": 5.73}, {"text": "And this is therefore h-star plus 2 Gamma,", "start": 4899.2, "duration": 10.275}, {"text": "because we added the extra margin.", "start": 4909.475, "duration": 1.86}, {"text": "So we wanted the relation between the, uh,", "start": 4911.335, "duration": 3.33}, {"text": "the- our, our hypothesis", "start": 4914.665, "duration": 3.645}, {"text": "generalization error to the generalization error of the best in class hypotheses.", "start": 4918.31, "duration": 3.945}, {"text": "So we dropped from the generalization error to the empirical error of our hypotheses,", "start": 4922.255, "duration": 7.44}, {"text": "related that to the empirical one of the best in", "start": 4929.695, "duration": 2.355}, {"text": "class and again bounded by the gap between these two.", "start": 4932.05, "duration": 3.66}, {"text": "So we- we've got a gap between the generalization bound,", "start": 4935.71, "duration": 3.24}, {"text": "the generalized error for hypothesis to the best in", "start": 4938.95, "duration": 2.25}, {"text": "class generalization. Any questions on this?", "start": 4941.2, "duration": 3.73}, {"text": "So the result basically says,", "start": 4949.2, "duration": 5.815}, {"text": "with probability, 1 minus Delta,", "start": 4955.015, "duration": 3.855}, {"text": "and for training size m,", "start": 4958.87, "duration": 2.61}, {"text": "[NOISE] the generalization error of [NOISE] the hypothesis from", "start": 4961.48, "duration": 9.51}, {"text": "the empirical risk minimizer is going to be within the", "start": 4970.99, "duration": 6.57}, {"text": "best in class generalization error plus 2 times 1 over,", "start": 4977.56, "duration": 7.38}, {"text": "1 over 2m plus log 2K over Delta.", "start": 4984.94, "duration": 7.14}, {"text": "So this was basically uh,", "start": 4992.08, "duration": 2.535}, {"text": "so you can get this, uh, when you,", "start": 4994.615, "duration": 3.405}, {"text": "when you- so in this expression,", "start": 4998.02, "duration": 6.21}, {"text": "if you set this equal to Delta and solve for Gamma, you will get this.", "start": 5004.23, "duration": 4.18}, {"text": "Any questions? [NOISE] I think we're already over time.", "start": 5010.79, "duration": 9.295}, {"text": "So, uh, the case for infinite classes is an extension to this.", "start": 5020.085, "duration": 6.015}, {"text": "Maybe I'll just write the results.", "start": 5026.1, "duration": 1.29}, {"text": "So there is a concept called VC dimension,", "start": 5027.39, "duration": 2.085}, {"text": "which is a pretty simple concept but [NOISE] we won't be going over it today.", "start": 5029.475, "duration": 6.075}, {"text": "VC dimension basically says,", "start": 5035.55, "duration": 2.31}, {"text": "um, what is the- so VC dimension is,", "start": 5037.86, "duration": 4.005}, {"text": "you can think of it as trying to assign a size to an infinitely,", "start": 5041.865, "duration": 5.865}, {"text": "uh, to an infinite size hypothesis class.", "start": 5047.73, "duration": 2.07}, {"text": "For a fixed size hypothesis class,", "start": 5049.8, "duration": 1.44}, {"text": "we had like, you know, K to be the size of the hypothesis class.", "start": 5051.24, "duration": 2.46}, {"text": "So VC [NOISE] of some hypothesis class is gonna be some number, right?", "start": 5053.7, "duration": 7.38}, {"text": "Some number which, which kind of,", "start": 5061.08, "duration": 2.145}, {"text": "um, which is like the size of the hypothesis.", "start": 5063.225, "duration": 2.49}, {"text": "It's basically, telling you how,", "start": 5065.715, "duration": 1.215}, {"text": "how expressive it is um, and, and, uh,", "start": 5066.93, "duration": 4.0}, {"text": "on using, using the VC dimension,", "start": 5070.93, "duration": 3.445}, {"text": "uh, there are very nice uh,", "start": 5074.375, "duration": 1.605}, {"text": "geometrical meanings of VC dimension.", "start": 5075.98, "duration": 1.92}, {"text": "You can, you can get a bound, similar bound.", "start": 5077.9, "duration": 3.54}, {"text": "But now, it's not for, uh, uh, um,", "start": 5081.44, "duration": 2.98}, {"text": "it's not for uh,", "start": 5084.42, "duration": 2.26}, {"text": "uh, finite classes anymore.", "start": 5086.68, "duration": 3.68}, {"text": "Some big O of [NOISE]", "start": 5092.57, "duration": 27.7}, {"text": "right? So in place of this margin, we ended up with,", "start": 5120.27, "duration": 4.44}, {"text": "uh, a different margin that is, uh,", "start": 5124.71, "duration": 2.295}, {"text": "a function of the, the VC dimension.", "start": 5127.005, "duration": 3.105}, {"text": "And the, the key takeaway from this is that uh,", "start": 5130.11, "duration": 5.76}, {"text": "the number of data examples,", "start": 5135.87, "duration": 4.125}, {"text": "that the sample complexity that you want is generally,", "start": 5139.995, "duration": 4.335}, {"text": "uh, an order of the VC dimension to get good results.", "start": 5144.33, "duration": 3.405}, {"text": "That's basically, the, uh, uh,", "start": 5147.735, "duration": 2.085}, {"text": "main result from that, right?", "start": 5149.82, "duration": 2.1}, {"text": "From, uh- with that,", "start": 5151.92, "duration": 1.02}, {"text": "I guess we'll, we'll, uh,", "start": 5152.94, "duration": 1.56}, {"text": "we'll break for the day and,", "start": 5154.5, "duration": 1.545}, {"text": "uh, we'll take more questions.", "start": 5156.045, "duration": 2.785}]