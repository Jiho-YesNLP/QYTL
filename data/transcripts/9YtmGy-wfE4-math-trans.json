[{"text": "The following content is\nprovided under a Creative", "start": 0.06, "duration": 2.44}, {"text": "Commons license.", "start": 2.5, "duration": 1.51}, {"text": "Your support will help\nMIT OpenCourseWare", "start": 4.01, "duration": 2.35}, {"text": "continue to offer high quality\neducational resources for free.", "start": 6.36, "duration": 4.37}, {"text": "To make a donation or\nview additional materials", "start": 10.73, "duration": 2.6}, {"text": "from hundreds of MIT courses,\nvisit MIT OpenCourseWare", "start": 13.33, "duration": 3.887}, {"text": "at ocw.mit.edu.", "start": 17.217, "duration": 0.625}, {"text": "PROFESSOR: So let's begin.", "start": 21.46, "duration": 1.65}, {"text": "Today, I'm going to\nreview linear algebra.", "start": 23.11, "duration": 3.49}, {"text": "So I'm assuming that you\nalready took some linear algebra", "start": 26.6, "duration": 4.14}, {"text": "course.", "start": 30.74, "duration": 0.65}, {"text": "And I'm going to just review\nthe relevant content that", "start": 31.39, "duration": 3.77}, {"text": "will appear again and again\nthroughout the course.", "start": 35.16, "duration": 3.3}, {"text": "But do interrupt me if some\nconcepts are not clear,", "start": 38.46, "duration": 3.61}, {"text": "if you don't remember some\nconcept from linear algebra.", "start": 42.07, "duration": 5.08}, {"text": "I hope you do.", "start": 47.15, "duration": 1.76}, {"text": "But please let me know.", "start": 48.91, "duration": 1.56}, {"text": "I just don't know.", "start": 50.47, "duration": 2.98}, {"text": "You have very different\nbackground knowledge.", "start": 53.45, "duration": 3.4}, {"text": "So it's hard to tune\nto one special group.", "start": 56.85, "duration": 3.54}, {"text": "So I tailored this\nlecture notes so that it's", "start": 60.39, "duration": 3.2}, {"text": "a review for those who took\nthe most basic linear algebra", "start": 63.59, "duration": 3.21}, {"text": "course.", "start": 66.8, "duration": 2.17}, {"text": "So if you already\nhave that experience,", "start": 68.97, "duration": 1.69}, {"text": "and don't understand it, please\nfeel free to interrupt me.", "start": 70.66, "duration": 2.92}, {"text": "So I'm going to start by\ntalking about matrices.", "start": 76.49, "duration": 2.08}, {"text": "A matrix, in a very\nsimple form, is just", "start": 81.354, "duration": 2.826}, {"text": "a collection of numbers.", "start": 84.18, "duration": 2.64}, {"text": "For example\n[1, 2, 3; 2, 3, 4;  4, 5, 10].", "start": 86.82, "duration": 6.8}, {"text": "You can pick any number of\nrows, any number of columns.", "start": 93.62, "duration": 2.87}, {"text": "You just write down\nnumbers in a square format.", "start": 96.49, "duration": 3.19}, {"text": "And that's the matrix.", "start": 99.68, "duration": 2.64}, {"text": "What's special about it?", "start": 102.32, "duration": 2.49}, {"text": "So what kind of data can\nyou arrange in a matrix?", "start": 104.81, "duration": 2.67}, {"text": "So I'll take an example,\nwhich looks relevant to us.", "start": 107.48, "duration": 4.36}, {"text": "So for example, we can index the\nrows by stocks, by companies,", "start": 111.84, "duration": 5.01}, {"text": "like Apple.", "start": 116.85, "duration": 0.74}, {"text": "Morgan Stanley should be\nthere, and then Google.", "start": 120.17, "duration": 5.18}, {"text": "And then maybe we can\nindex the column by dates.", "start": 128.81, "duration": 2.955}, {"text": "I'll say July 1st, October\n1st, September 1st.", "start": 134.29, "duration": 6.63}, {"text": "And the numbers, you can\npick whatever data you want.", "start": 140.92, "duration": 3.01}, {"text": "But probably the\nsensible data will", "start": 143.93, "duration": 1.7}, {"text": "be the stock price on that day.", "start": 145.63, "duration": 2.68}, {"text": "I don't know for example\n400, 500, and 5,000.", "start": 148.31, "duration": 5.44}, {"text": "That would be great.", "start": 153.75, "duration": 2.18}, {"text": "So these kind of data,\nthat's just the matrix.", "start": 155.93, "duration": 5.02}, {"text": "So defining a matrix\nis really simple.", "start": 160.95, "duration": 2.28}, {"text": "But why is it so powerful?", "start": 163.23, "duration": 4.64}, {"text": "So that's an application\npoint of view,", "start": 167.87, "duration": 2.21}, {"text": "just as a collection of data.", "start": 170.08, "duration": 1.9}, {"text": "But from a theoretical\npoint of view,", "start": 171.98, "duration": 9.63}, {"text": "a matrix, an m by n\nmatrix, is an operator.", "start": 181.61, "duration": 9.25}, {"text": "It defines a linear\ntransformation.", "start": 190.86, "duration": 2.1}, {"text": "A defines a linear\ntransformation", "start": 192.96, "duration": 3.29}, {"text": "from the vector space,\nn-dimensional vector", "start": 196.25, "duration": 2.41}, {"text": "space to the m-dimensional\nvector space.", "start": 198.66, "duration": 4.119}, {"text": "That sounds a lot more\nabstract than this.", "start": 202.779, "duration": 1.749}, {"text": "So for example, let's just\ntake a very small example.", "start": 207.84, "duration": 2.89}, {"text": "If I use a 2 by 2\nmatrix, [2, 0;  0, 3].", "start": 210.73, "duration": 8.81}, {"text": "Then [2, 0; 0, 3] times, let's\nsay [1, 1] is just [2, 3].", "start": 219.54, "duration": 8.049}, {"text": "Does that makes sense?", "start": 233.122, "duration": 1.278}, {"text": "It's just matrix multiplication.", "start": 234.4, "duration": 2.66}, {"text": "So now try to combine\nthe point of view.", "start": 237.06, "duration": 3.8}, {"text": "What does it mean to have a\nlinear transformation defined", "start": 240.86, "duration": 2.64}, {"text": "by a data set?", "start": 243.5, "duration": 3.19}, {"text": "And things start\nto get confusing.", "start": 246.69, "duration": 1.68}, {"text": "What is it?", "start": 248.37, "duration": 2.67}, {"text": "Why does a data set define\na linear transformation?", "start": 251.04, "duration": 2.75}, {"text": "And does it have any\nsensible meaning?", "start": 253.79, "duration": 3.71}, {"text": "So that's a good question\nto have in mind today.", "start": 257.5, "duration": 3.709}, {"text": "And try to remember\nthis question.", "start": 261.209, "duration": 3.201}, {"text": "Because today I'll\ntry to really develop", "start": 264.41, "duration": 2.63}, {"text": "a theory of eigenvalues and\neigenvectors in a purely", "start": 267.04, "duration": 4.49}, {"text": "theoretical language.", "start": 271.53, "duration": 2.33}, {"text": "But it can still be\napplied to these data sets,", "start": 273.86, "duration": 4.28}, {"text": "and give very\nimportant properties", "start": 278.14, "duration": 5.89}, {"text": "and very important quantities.", "start": 284.03, "duration": 2.1}, {"text": "You can get some useful\ninformation out of it.", "start": 286.13, "duration": 3.9}, {"text": "Try to make sense out\nof why it happens.", "start": 290.03, "duration": 4.61}, {"text": "So that will be the goal today,\nto really treat linear algebra", "start": 294.64, "duration": 4.19}, {"text": "as a theoretical thing.", "start": 298.83, "duration": 2.29}, {"text": "But remember that there's some\ndata set, like really data set", "start": 301.12, "duration": 3.696}, {"text": "underlying.", "start": 304.816, "duration": 0.499}, {"text": "This doesn't go up.", "start": 308.06, "duration": 1.462}, {"text": "That was a bad choice\nfor my first board.", "start": 309.522, "duration": 3.708}, {"text": "Sorry.", "start": 313.23, "duration": 0.5}, {"text": "So the most important concepts\nfor us are the eigenvalues", "start": 322.15, "duration": 8.73}, {"text": "and eigenvectors\nof a matrix, which", "start": 330.88, "duration": 4.51}, {"text": "is defined as a real number,\nlambda, and vector v,", "start": 335.39, "duration": 12.35}, {"text": "is an eigenvalue, and\neigenvector of a matrix A,", "start": 347.74, "duration": 14.51}, {"text": "if A times v is equal to\nlambda times V. We also", "start": 362.25, "duration": 6.87}, {"text": "say that v is an eigenvector\ncorresponding to lambda.", "start": 369.12, "duration": 8.805}, {"text": "So remember eigenvalues\nand eigenvectors always", "start": 381.64, "duration": 2.93}, {"text": "come in pairs.", "start": 384.57, "duration": 2.01}, {"text": "And they are defined by the\nproperty that A*v = lambda*v.", "start": 386.58, "duration": 8.13}, {"text": "First question, does all\nmatrix have eigenvalues", "start": 394.71, "duration": 3.15}, {"text": "and eigenvectors?", "start": 397.86, "duration": 0.744}, {"text": "Nope?", "start": 401.508, "duration": 2.422}, {"text": "So Av-- It looks like this\nis a very strange equation", "start": 403.93, "duration": 6.69}, {"text": "to satisfy.", "start": 410.62, "duration": 1.08}, {"text": "But if you change it in this\nform, (A - lambda I)v = 0.", "start": 411.7, "duration": 5.94}, {"text": "That still looks strange.", "start": 417.64, "duration": 3.4}, {"text": "But at least you\nunderstand that-- it's", "start": 421.04, "duration": 2.44}, {"text": "an only if, this can happen\nonly if this can happen.", "start": 423.48, "duration": 5.33}, {"text": "Happens only if A - lambda\nI does not have full rank.", "start": 428.81, "duration": 7.8}, {"text": "So determinant of (A - lambda I)\nis equal to 0, if and only if,", "start": 436.61, "duration": 7.455}, {"text": "in fact.", "start": 444.065, "duration": 0.615}, {"text": "So now comes a very\ninteresting observation.", "start": 449.02, "duration": 3.893}, {"text": "det(A - lambda I) is a\npolynomial of degree n.", "start": 455.44, "duration": 9.96}, {"text": "I made a mistake.", "start": 468.587, "duration": 0.894}, {"text": "I should have said, this is\nonly for n by n matrices.", "start": 469.481, "duration": 3.164}, {"text": "This is only for\nsquare matrices.", "start": 480.95, "duration": 1.57}, {"text": "Sorry.", "start": 482.52, "duration": 2.11}, {"text": "It's a polynomial of degree n.", "start": 484.63, "duration": 1.8}, {"text": "That means it has a solution.", "start": 486.43, "duration": 2.014}, {"text": "It has to give n\nin terms of lambda.", "start": 488.444, "duration": 2.991}, {"text": "So it has a solution.", "start": 495.17, "duration": 1.94}, {"text": "It might be a complex number.", "start": 497.11, "duration": 1.32}, {"text": "I'm really sorry.", "start": 506.25, "duration": 0.965}, {"text": "I'm nervous in\nfront of the video.", "start": 507.215, "duration": 1.575}, {"text": "I understand why you were saying\nthat is doesn't necessarily", "start": 512.974, "duration": 2.961}, {"text": "exist.", "start": 515.935, "duration": 1.595}, {"text": "Let me repeat.", "start": 517.53, "duration": 0.8}, {"text": "I made a few mistakes here.", "start": 518.33, "duration": 1.31}, {"text": "So let me repeat here.", "start": 519.64, "duration": 1.87}, {"text": "For n by n matrix A, a complex\nnumber lambda, and the vector", "start": 521.51, "duration": 8.14}, {"text": "v, is an eigenvalue\nand eigenvector", "start": 529.65, "duration": 3.539}, {"text": "if it satisfies this condition.", "start": 533.189, "duration": 1.291}, {"text": "It doesn't have to be real.", "start": 534.48, "duration": 1.3}, {"text": "Sorry about that.", "start": 535.78, "duration": 1.72}, {"text": "And now if we\nrephrase it this way,", "start": 537.5, "duration": 2.38}, {"text": "because this is a\npolynomial, it always", "start": 539.88, "duration": 3.25}, {"text": "has at least one solution.", "start": 543.13, "duration": 1.49}, {"text": "That was just a side point.", "start": 547.64, "duration": 2.03}, {"text": "Very theoretical.", "start": 549.67, "duration": 1.3}, {"text": "So we see that there\nalways exists at least one", "start": 550.97, "duration": 2.36}, {"text": "eigenvalue and eigenvector.", "start": 553.33, "duration": 1.125}, {"text": "Now we saw its existence, what\nis the geometrical meaning", "start": 557.42, "duration": 3.81}, {"text": "of it?", "start": 561.23, "duration": 0.978}, {"text": "Now let's go back to the linear\ntransformation point of view.", "start": 574.94, "duration": 4.28}, {"text": "So suppose A is a 3 by 3 matrix.", "start": 579.22, "duration": 4.77}, {"text": "Then A takes the vector in R^3\nand transforms it into another", "start": 588.23, "duration": 10.28}, {"text": "vector in R^3.", "start": 598.51, "duration": 2.81}, {"text": "But if you have this\nrelation, what's", "start": 604.23, "duration": 2.93}, {"text": "going to happen is\nA, when applied to v,", "start": 607.16, "duration": 4.19}, {"text": "it will just scale the vector\nv. If this was the original v,", "start": 611.35, "duration": 4.81}, {"text": "A of v will just be\nlambda times this vector.", "start": 616.16, "duration": 3.43}, {"text": "That will be our Av, which\nis equal to lambda v.", "start": 619.59, "duration": 4.85}, {"text": "So eigenvectors are\nthose special vectors", "start": 624.44, "duration": 3.72}, {"text": "which when applied this\nlinear transformation just", "start": 628.16, "duration": 3.71}, {"text": "get scaled by some amount, where\nthat amount is exactly lambda.", "start": 631.87, "duration": 7.75}, {"text": "So what we established so\nfar, what we recall so far", "start": 639.62, "duration": 3.24}, {"text": "is every n by n matrix has\nat least one such direction.", "start": 642.86, "duration": 4.79}, {"text": "There is some vector where the\nlinear transformation defined", "start": 647.65, "duration": 4.18}, {"text": "by A just scales that vector.", "start": 651.83, "duration": 3.175}, {"text": "Which is quite\ninteresting, if you ever", "start": 655.005, "duration": 1.625}, {"text": "thought about it before.", "start": 656.63, "duration": 2.2}, {"text": "There's no reason such\nvector should exist.", "start": 658.83, "duration": 2.28}, {"text": "Of course I'm\nlying a little bit.", "start": 661.11, "duration": 1.53}, {"text": "Because these might\nbe complex vectors.", "start": 662.64, "duration": 2.805}, {"text": "But at least in the\ncomplex world it's true.", "start": 665.445, "duration": 4.965}, {"text": "So if you think about\nthis, this is very helpful.", "start": 673.21, "duration": 5.86}, {"text": "It gives you the vectors-- from\nthese vectors' point of view,", "start": 679.07, "duration": 4.45}, {"text": "this linear transformation\nis really easy to understand.", "start": 683.52, "duration": 4.07}, {"text": "That's why eigenvalues and\neigenvector are so good.", "start": 687.59, "duration": 2.41}, {"text": "It breaks down the\nlinear transformation", "start": 690.0, "duration": 1.97}, {"text": "into really simple operations.", "start": 691.97, "duration": 1.45}, {"text": "Let me formalize that\na little bit more.", "start": 696.18, "duration": 3.93}, {"text": "So in an extreme case a\nmatrix, an n by n matrix A,", "start": 700.11, "duration": 10.0}, {"text": "we call it\ndiagonalizable, if there", "start": 710.11, "duration": 8.26}, {"text": "exists an orthonormal\nmatrix, I'll", "start": 718.37, "duration": 8.43}, {"text": "call what it is, U, such that\nA is equal to U times D times U", "start": 726.8, "duration": 14.12}, {"text": "inverse for a diagonal matrix D.", "start": 740.92, "duration": 12.62}, {"text": "Let me iterate through\nthis a little bit.", "start": 753.54, "duration": 2.214}, {"text": "What is an orthonormal matrix?", "start": 759.93, "duration": 2.21}, {"text": "It's a matrix defined by the\nrelation U times U transposed", "start": 762.14, "duration": 3.39}, {"text": "is equal to the identity.", "start": 765.53, "duration": 2.95}, {"text": "What is a diagonal matrix?", "start": 768.48, "duration": 1.85}, {"text": "It's a matrix whose\nnonzero entries are all", "start": 770.33, "duration": 3.76}, {"text": "on the diagonal.", "start": 774.09, "duration": 2.102}, {"text": "All the rest are zero.", "start": 776.192, "duration": 1.434}, {"text": "Why is it so good to\nhave this decomposition?", "start": 781.36, "duration": 3.44}, {"text": "What does it mean to have an\northonormal matrix like this?", "start": 784.8, "duration": 3.47}, {"text": "It means basically I'll just\nexplain what's happening.", "start": 788.27, "duration": 7.87}, {"text": "If that happens, if a\nmatrix is diagonalizable,", "start": 796.14, "duration": 2.58}, {"text": "if this A is\ndiagonalizable, there", "start": 798.72, "duration": 2.085}, {"text": "will be three directions,\nv_1, v_2, v_3,", "start": 800.805, "duration": 7.755}, {"text": "such that when you apply this\nA, v_1 scales by some lambda_1.", "start": 808.56, "duration": 6.01}, {"text": "v_2 scales by some lambda_2.", "start": 814.57, "duration": 3.67}, {"text": "And v_3 scales by some lambda_3.", "start": 818.24, "duration": 2.004}, {"text": "So we can completely understand\nthe transformation A,", "start": 823.41, "duration": 5.0}, {"text": "just in terms of\nthese three vectors.", "start": 828.41, "duration": 1.57}, {"text": "So this, the stuff here will\nbe the most important things", "start": 838.6, "duration": 7.01}, {"text": "you'll use in linear algebra\nthroughout this course.", "start": 845.61, "duration": 3.64}, {"text": "So let me repeat\nit really slowly.", "start": 849.25, "duration": 4.14}, {"text": "So an eigenvalue and eigenvector\nis defined by this relation.", "start": 853.39, "duration": 5.34}, {"text": "We know that there are at least\none eigenvalue for each matrix,", "start": 858.73, "duration": 2.93}, {"text": "and there is an eigenvector\ncorresponding to it.", "start": 861.66, "duration": 3.65}, {"text": "And eigenvectors have\nthis geometrical meaning", "start": 865.31, "duration": 3.26}, {"text": "where-- a vector\nis an eigenvector,", "start": 868.57, "duration": 4.36}, {"text": "if the linear\ntransformation defined", "start": 872.93, "duration": 1.85}, {"text": "by A just scales that vector.", "start": 874.78, "duration": 3.52}, {"text": "So for our setting,\nthe real good matrices", "start": 878.3, "duration": 4.37}, {"text": "are the matrices which\ncan be broken down", "start": 882.67, "duration": 2.76}, {"text": "into these directions.", "start": 885.43, "duration": 2.76}, {"text": "And those directions\nare defined by this U.", "start": 888.19, "duration": 3.99}, {"text": "And D defines how\nmuch it will scale.", "start": 892.18, "duration": 2.84}, {"text": "So in this case U will\nbe our v_1, v_2, v_3.", "start": 895.02, "duration": 7.09}, {"text": "And D will be our lambda_1,\nlambda_2, lambda_3 all 0.", "start": 902.11, "duration": 5.777}, {"text": "Any questions so far?", "start": 917.0, "duration": 0.89}, {"text": "So that is abstract.", "start": 922.93, "duration": 1.72}, {"text": "Now remember the question\nI posed in the beginning.", "start": 924.65, "duration": 3.0}, {"text": "So remember that matrix where we\nhad stocks and dates and stock", "start": 927.65, "duration": 5.85}, {"text": "prices in the entries?", "start": 933.5, "duration": 3.02}, {"text": "What will an eigenvector\nof that matrix mean?", "start": 936.52, "duration": 3.625}, {"text": "What will an eigenvalue mean?", "start": 940.145, "duration": 1.591}, {"text": "So try to think\nabout that question.", "start": 945.05, "duration": 1.57}, {"text": "It's not like it will have\nsome physical counterpart.", "start": 949.49, "duration": 3.26}, {"text": "But there's some really\ninteresting things", "start": 952.75, "duration": 2.85}, {"text": "going on there.", "start": 955.6, "duration": 0.78}, {"text": "The bad news is that not all\nmatrices are diagonalizable.", "start": 969.81, "duration": 4.7}, {"text": "If a matrix is diagonalizable,\nit's really easy", "start": 974.51, "duration": 2.95}, {"text": "to understand what it does.", "start": 977.46, "duration": 2.14}, {"text": "Because it really breaks down\ninto these three directions,", "start": 979.6, "duration": 3.35}, {"text": "if it's a 3 by 3.", "start": 982.95, "duration": 1.055}, {"text": "If it's an n by n, it breaks\ndown into n directions.", "start": 984.005, "duration": 3.275}, {"text": "Unfortunately, not all\nmatrices are diagonalizable.", "start": 987.28, "duration": 4.81}, {"text": "But there is a\nvery special class", "start": 992.09, "duration": 1.88}, {"text": "of matrices which are\nalways diagonalizable.", "start": 993.97, "duration": 4.36}, {"text": "And fortunately we\nwill see those matrices", "start": 998.33, "duration": 3.31}, {"text": "throughout the course.", "start": 1001.64, "duration": 1.7}, {"text": "Most of the matrices,\nn by n matrices,", "start": 1003.34, "duration": 1.92}, {"text": "we will study, fall\ninto this category.", "start": 1005.26, "duration": 2.81}, {"text": "So an n by n matrix\nA is symmetric", "start": 1011.9, "duration": 10.07}, {"text": "if A is equal to A transpose.", "start": 1021.97, "duration": 3.58}, {"text": "Before proceeding,\nplease raise your hand", "start": 1025.55, "duration": 4.45}, {"text": "if you're familiar with\nall the concepts so far.", "start": 1030.0, "duration": 3.9}, {"text": "OK.", "start": 1033.9, "duration": 0.5}, {"text": "Good feeling.", "start": 1034.4, "duration": 1.78}, {"text": "So a matrix is symmetric if\nit's equal to its transpose.", "start": 1042.5, "duration": 3.109}, {"text": "A transpose is obtained\nby taking the mirror", "start": 1045.609, "duration": 2.101}, {"text": "image across the diagonal.", "start": 1047.71, "duration": 1.515}, {"text": "And then it is known that\nall symmetric matrices", "start": 1052.19, "duration": 12.53}, {"text": "are diagonalizable.", "start": 1064.72, "duration": 2.397}, {"text": "Ah, I've made another mistake.", "start": 1067.117, "duration": 3.7}, {"text": "Orthonormally.", "start": 1070.817, "duration": 0.583}, {"text": "So with this I missed matrices\northonormally diagonalizable.", "start": 1075.558, "duration": 11.412}, {"text": "So it's diagonalizable if\nwe drop this condition,", "start": 1086.97, "duration": 6.22}, {"text": "and replace it\nwith an invertible.", "start": 1093.19, "duration": 1.6}, {"text": "So symmetric matrices\nare really good.", "start": 1105.15, "duration": 4.15}, {"text": "And fortunately most of the n\nby n matrices that we will study", "start": 1109.3, "duration": 4.62}, {"text": "are symmetric.", "start": 1113.92, "duration": 0.96}, {"text": "Just by the nature of\nit, it will be symmetric.", "start": 1114.88, "duration": 3.76}, {"text": "The one I gave as an\nexample is not symmetric.", "start": 1118.64, "duration": 3.39}, {"text": "It's not symmetric.", "start": 1122.03, "duration": 2.506}, {"text": "But I will address\nthat issue in a minute.", "start": 1124.536, "duration": 5.234}, {"text": "And another important\nthing is symmetric matrices", "start": 1129.77, "duration": 10.556}, {"text": "have real eigenvalues.", "start": 1140.326, "duration": 2.802}, {"text": "So really this geometrical\npicture just the--", "start": 1152.34, "duration": 4.61}, {"text": "for symmetric\nmatrices, this picture", "start": 1156.95, "duration": 1.941}, {"text": "is really the picture\nyou should have in mind.", "start": 1158.891, "duration": 1.916}, {"text": "So proof of Theorem 2.", "start": 1170.87, "duration": 6.0}, {"text": "Suppose lambda is an eigenvalue\nwith eigenvector v. Then", "start": 1185.03, "duration": 13.58}, {"text": "by definition we have this.", "start": 1198.61, "duration": 1.58}, {"text": "Now multiply v\ntransposed on both sides.", "start": 1204.72, "duration": 3.99}, {"text": "It is lambda times the norm v.", "start": 1212.07, "duration": 8.08}, {"text": "Now take the complex\nconjugate-- Real symmetric.", "start": 1220.15, "duration": 14.582}, {"text": "And then first A conjugate,\nwe have v^T A^T v,", "start": 1240.71, "duration": 6.83}, {"text": "and then take the\nconjugate of it.", "start": 1247.54, "duration": 3.425}, {"text": "Then we get lambda...", "start": 1250.965, "duration": 6.055}, {"text": "v. And this side is\nequal to v^T A^T v.", "start": 1257.02, "duration": 22.47}, {"text": "But because A is real symmetric,\nwe see that A is equal", "start": 1279.49, "duration": 8.27}, {"text": "to the conjugate of\ncomplex conjugate of A.", "start": 1287.76, "duration": 4.1}, {"text": "So this expression and this\nexpression is the same.", "start": 1291.86, "duration": 3.9}, {"text": "The right side should\nalso be the same.", "start": 1295.76, "duration": 3.97}, {"text": "That means lambda is equal\nto the conjugate of lambda.", "start": 1299.73, "duration": 3.406}, {"text": "So lambda has to be a real.", "start": 1303.136, "duration": 1.644}, {"text": "So Theorem 1 is a little\nbit more complicated,", "start": 1319.48, "duration": 3.18}, {"text": "and it involves more\nadvanced concepts", "start": 1322.66, "duration": 3.83}, {"text": "like basis and linear\nsubspace, and so on.", "start": 1326.49, "duration": 7.01}, {"text": "And those concepts\nare not really", "start": 1333.5, "duration": 1.71}, {"text": "important for this class.", "start": 1335.21, "duration": 1.23}, {"text": "So I'll just skip the proof.", "start": 1336.44, "duration": 2.47}, {"text": "But it's really important to\nremember these two theorems.", "start": 1338.91, "duration": 2.99}, {"text": "Wherever you see\na symmetric matrix", "start": 1341.9, "duration": 3.86}, {"text": "you should really feel like\nyou have control on it.", "start": 1345.76, "duration": 2.14}, {"text": "Because you can diagonalize it.", "start": 1347.9, "duration": 2.83}, {"text": "And moreover, all\neigenvalues are real,", "start": 1354.37, "duration": 3.682}, {"text": "and you have really good\ncontrol on symmetric matrices.", "start": 1358.052, "duration": 2.51}, {"text": "That's good.", "start": 1364.891, "duration": 3.159}, {"text": "That was when\neverything went well.", "start": 1368.05, "duration": 3.12}, {"text": "We can diagonalize it.", "start": 1371.17, "duration": 1.97}, {"text": "So, so far we saw that if\nfor a symmetric matrix,", "start": 1373.14, "duration": 5.62}, {"text": "we can diagonalize it.", "start": 1378.76, "duration": 1.33}, {"text": "It's really easy to understand.", "start": 1380.09, "duration": 1.36}, {"text": "But what about general matrices?", "start": 1381.45, "duration": 1.88}, {"text": "In general, not all matrices are\ndiagonalizable, first of all.", "start": 1396.69, "duration": 2.9}, {"text": "But sometimes we still want\nto decomposition like this.", "start": 1417.5, "duration": 4.41}, {"text": "So diagonalization was A equals\nU times D times U inverse.", "start": 1421.91, "duration": 11.68}, {"text": "But we want something similar.", "start": 1439.91, "duration": 1.59}, {"text": "We want to understand.", "start": 1441.5, "duration": 2.78}, {"text": "So our goal, we want to\nstill understand the matrix,", "start": 1444.28, "duration": 10.74}, {"text": "give a matrix A through simple\noperations, such as scaling.", "start": 1455.02, "duration": 7.925}, {"text": "When the matrix was a\ndiagonalizable matrix this", "start": 1467.81, "duration": 2.99}, {"text": "was done, this was possible.", "start": 1470.8, "duration": 2.754}, {"text": "Unfortunately, it's not\nalways diagonalizable.", "start": 1473.554, "duration": 1.916}, {"text": "So we have to do something else.", "start": 1478.56, "duration": 2.97}, {"text": "So that's what I\nwant to talk about.", "start": 1485.46, "duration": 2.32}, {"text": "And luckily the\ngood news is there", "start": 1487.78, "duration": 2.08}, {"text": "is a nice tool we can\nuse for all matrices,", "start": 1489.86, "duration": 2.74}, {"text": "even those slightly weaker,\nin fact, a little bit more", "start": 1492.6, "duration": 3.76}, {"text": "weaker than this\ndiagonalization.", "start": 1496.36, "duration": 2.4}, {"text": "But still it distills some\nvery important information", "start": 1498.76, "duration": 3.82}, {"text": "of the matrix.", "start": 1502.58, "duration": 0.827}, {"text": "So it's called singular\nvalue decomposition.", "start": 1503.407, "duration": 1.833}, {"text": "So this will be our second\ntool of understanding matrices.", "start": 1517.22, "duration": 5.13}, {"text": "It's very similar to\nthis diagonalization,", "start": 1522.35, "duration": 2.53}, {"text": "or in other words I call this\neigenvalue decomposition.", "start": 1524.88, "duration": 2.33}, {"text": "But it has a slightly\ndifferent form.", "start": 1534.4, "duration": 2.0}, {"text": "So what is its form?", "start": 1536.4, "duration": 2.91}, {"text": "So theorem.", "start": 1539.31, "duration": 2.46}, {"text": "Let A be an m by n matrix.", "start": 1541.77, "duration": 4.104}, {"text": "Then there always exists\northonormal matrices", "start": 1551.35, "duration": 21.05}, {"text": "U and V such that A is\nequal to U times sigma times", "start": 1572.4, "duration": 13.13}, {"text": "V transpose.", "start": 1585.53, "duration": 1.81}, {"text": "For some diagonal matrix sigma.", "start": 1587.34, "duration": 9.54}, {"text": "Let me parse through the\ntheorem a little bit more.", "start": 1596.88, "duration": 4.1}, {"text": "Whenever you're\ngiven a matrix, it", "start": 1600.98, "duration": 1.69}, {"text": "doesn't even have to be\na square matrix anymore.", "start": 1602.67, "duration": 2.39}, {"text": "It can be non-symmetric.", "start": 1605.06, "duration": 1.98}, {"text": "So whenever we're given an\nm by n matrix, in general,", "start": 1607.04, "duration": 3.36}, {"text": "there always exists\ntwo matrices, U and V,", "start": 1610.4, "duration": 4.71}, {"text": "which are orthonormal,\nsuch that A", "start": 1615.11, "duration": 3.4}, {"text": "can be decomposed as U times\nsigma times V transposed, where", "start": 1618.51, "duration": 4.87}, {"text": "sigma is a diagonal matrix.", "start": 1623.38, "duration": 1.91}, {"text": "But now the size of the\nmatrix are important", "start": 1625.29, "duration": 3.05}, {"text": "so U is an m by n matrix,\nsigma is an m by n matrix,", "start": 1628.34, "duration": 5.4}, {"text": "and V is an n by n matrix.", "start": 1633.74, "duration": 2.17}, {"text": "That just denotes the size,\nthe dimensions of the matrix.", "start": 1635.91, "duration": 5.1}, {"text": "So what does it mean for an\nm by n matrix to be diagonal?", "start": 1641.01, "duration": 4.12}, {"text": "It just means the same thing.", "start": 1645.13, "duration": 2.28}, {"text": "So only the (i,i) entries\nare allowed to be nonzero.", "start": 1647.41, "duration": 3.23}, {"text": "So that was just\na bunch of words.", "start": 1659.76, "duration": 1.89}, {"text": "So let me rephrase this.", "start": 1661.65, "duration": 1.46}, {"text": "So let me compare now eigenvalue\ndecomposition, with singular", "start": 1672.37, "duration": 3.8}, {"text": "value decomposition.", "start": 1676.17, "duration": 1.89}, {"text": "So this is EVD, what\nwe just saw before.", "start": 1678.06, "duration": 5.31}, {"text": "It only-- SVD.", "start": 1683.37, "duration": 2.92}, {"text": "This only works for\nn by n matrices,", "start": 1686.29, "duration": 2.969}, {"text": "which are diagonalizable.", "start": 1689.259, "duration": 1.041}, {"text": "SVD works for all\ngeneral m by n matrices.", "start": 1695.26, "duration": 2.533}, {"text": "However, this is powerful.", "start": 1703.47, "duration": 1.36}, {"text": "Because it gives you one frame.", "start": 1704.83, "duration": 4.12}, {"text": "So v_1 with a v_2, v_3 for which\nA acts as a scaling operator.", "start": 1708.95, "duration": 12.558}, {"text": "Kind of like that.", "start": 1721.508, "duration": 2.522}, {"text": "That's what A does,\nA does, A does.", "start": 1724.03, "duration": 1.942}, {"text": "That's because these U on\nthe both sides are equal.", "start": 1729.14, "duration": 4.98}, {"text": "However, for singular\nvalue decomposition,", "start": 1734.12, "duration": 3.646}, {"text": "this is called singular\nvalue decomposition.", "start": 1737.766, "duration": 2.34}, {"text": "I just erased It.", "start": 1740.106, "duration": 1.224}, {"text": "What you have instead\nis first of all,", "start": 1748.75, "duration": 2.73}, {"text": "the spaces are different.", "start": 1751.48, "duration": 1.19}, {"text": "If you take a vector in\nR^m, and bring it to R^n,", "start": 1752.67, "duration": 9.688}, {"text": "apply this operation A. What's\ngoing to happen here is there", "start": 1762.358, "duration": 3.332}, {"text": "will be one frame in here,\nand one frame in here.", "start": 1765.69, "duration": 3.1}, {"text": "So there will be vectors\nv_1, v_2, v_3, v_4 like this.", "start": 1768.79, "duration": 7.64}, {"text": "And there will be vectors\nu_1, u_2, u_3 like this here.", "start": 1776.43, "duration": 7.87}, {"text": "And what's going to happen\nis when you take v_1,", "start": 1784.3, "duration": 4.12}, {"text": "A will take v_1 to u_1\nand scale it a little bit", "start": 1788.42, "duration": 4.38}, {"text": "according to that diagonal.", "start": 1792.8, "duration": 1.49}, {"text": "A will take v_2 to\nu_2, it will scale it.", "start": 1794.29, "duration": 4.13}, {"text": "It'll take v_3 to u_3, scale it.", "start": 1798.42, "duration": 3.57}, {"text": "Wait a minute.", "start": 1801.99, "duration": 0.63}, {"text": "But for v_4, we don't have u_4.", "start": 1802.62, "duration": 2.48}, {"text": "What's going to happen is this\nis just going to disappear.", "start": 1805.1, "duration": 2.97}, {"text": "u_4, when applied\nA, will disappear.", "start": 1808.07, "duration": 3.44}, {"text": "So I know it's a very\nvague explanation,", "start": 1811.51, "duration": 2.42}, {"text": "but this geometric picture,\ntry to compare them.", "start": 1813.93, "duration": 4.39}, {"text": "A diagonalization,\neigenvalue decomposition,", "start": 1818.32, "duration": 2.88}, {"text": "works within its frame, so\nit's very, very powerful.", "start": 1821.2, "duration": 4.15}, {"text": "You just have some directions\nand you scale those directions.", "start": 1825.35, "duration": 4.13}, {"text": "But the singular\nvalue composition", "start": 1829.48, "duration": 1.97}, {"text": "it's applicable to a more\ngeneral class of matrices,", "start": 1831.45, "duration": 3.39}, {"text": "but it's rather more restricted.", "start": 1834.84, "duration": 1.91}, {"text": "You have two frames, one\nfor the original space, one", "start": 1836.75, "duration": 3.0}, {"text": "for the target space.", "start": 1839.75, "duration": 1.65}, {"text": "And what the linear\ntransformation does is,", "start": 1841.4, "duration": 1.92}, {"text": "it just sends one\nvector to another vector", "start": 1843.32, "duration": 3.92}, {"text": "and scales it a little bit.", "start": 1847.24, "duration": 2.53}, {"text": "So now is another\ngood time to go back", "start": 1854.08, "duration": 5.069}, {"text": "to that matrix in\nthe very beginning.", "start": 1859.149, "duration": 1.541}, {"text": "So remember that example where\nwe had a vector of companies,", "start": 1872.52, "duration": 10.194}, {"text": "and dates, and the\nentry was stock prices.", "start": 1882.714, "duration": 4.89}, {"text": "So if it's an n by\nn matrix, you can", "start": 1897.4, "duration": 3.6}, {"text": "try to apply both\neigenvalue decomposition,", "start": 1901.0, "duration": 1.94}, {"text": "and singular value\ndecomposition.", "start": 1902.94, "duration": 2.14}, {"text": "But what will be more sensible\nis singular value decomposition", "start": 1905.08, "duration": 3.15}, {"text": "in this case.", "start": 1908.23, "duration": 1.98}, {"text": "I won't explain why, and\nwhat's happening here.", "start": 1910.21, "duration": 2.92}, {"text": "Peter will probably.", "start": 1913.13, "duration": 3.04}, {"text": "You will come to it later.", "start": 1916.17, "duration": 1.93}, {"text": "But just try to do some\nimagining before listening", "start": 1918.1, "duration": 3.44}, {"text": "what's really happening\nin real world.", "start": 1921.54, "duration": 2.65}, {"text": "So try to use your own\nimagination, your own language", "start": 1924.19, "duration": 3.19}, {"text": "to express.", "start": 1927.38, "duration": 0.86}, {"text": "See what happens for\nthis matrix, what", "start": 1928.24, "duration": 2.71}, {"text": "this decomposition is doing.", "start": 1930.95, "duration": 1.48}, {"text": "It just looks like\ntotally nonsense.", "start": 1940.01, "duration": 4.05}, {"text": "Why does this have\neven a geometry?", "start": 1944.06, "duration": 2.57}, {"text": "Why does it define a linear\ntransformation and so on?", "start": 1946.63, "duration": 2.53}, {"text": "It's just a beautiful\ntheory, which just", "start": 1952.44, "duration": 2.15}, {"text": "gives many useful information.", "start": 1954.59, "duration": 2.4}, {"text": "I can't really emphasize more.", "start": 1956.99, "duration": 1.76}, {"text": "Because-- emphasize\nenough, because really", "start": 1958.75, "duration": 3.79}, {"text": "this is just universal, being\nused in all science, these.", "start": 1962.54, "duration": 3.47}, {"text": "I think the eigenvalue\ndecomposition, and the singular", "start": 1966.01, "duration": 2.25}, {"text": "value decomposition.", "start": 1968.26, "duration": 1.81}, {"text": "Not just for this\ncourse, but pretty much", "start": 1970.07, "duration": 3.49}, {"text": "it's safe to say in\nevery engineering,", "start": 1973.56, "duration": 2.06}, {"text": "you'll encounter\none of the forms.", "start": 1975.62, "duration": 2.0}, {"text": "So let me talk about the\nproof of the singular value", "start": 1980.15, "duration": 5.41}, {"text": "decomposition.", "start": 1985.56, "duration": 1.32}, {"text": "And I will show you an\nexample of what singular value", "start": 1986.88, "duration": 4.24}, {"text": "decomposition does for some\nexample matrix, the matrix", "start": 1991.12, "duration": 4.29}, {"text": "that I chose.", "start": 1995.41, "duration": 2.35}, {"text": "Proof of singular\nvalue decomposition,", "start": 1997.76, "duration": 7.905}, {"text": "which is interesting.", "start": 2005.665, "duration": 0.875}, {"text": "It relies on eigenvalue\ndecomposition.", "start": 2006.54, "duration": 1.583}, {"text": "So given a matrix A, consider\nthe eigenvalues of A times", "start": 2011.03, "duration": 26.095}, {"text": "A transpose.", "start": 2037.125, "duration": 1.155}, {"text": "Oh, A transpose A. First\nobservation: that's", "start": 2044.91, "duration": 12.114}, {"text": "a symmetric matrix.", "start": 2057.024, "duration": 0.791}, {"text": "So if you remember, it\nwill have real eigenvalues,", "start": 2066.17, "duration": 3.04}, {"text": "and it's diagonalizable.", "start": 2069.21, "duration": 1.0}, {"text": "So A^T of A has eigenvalues\nlambda_1, lambda_2,", "start": 2075.11, "duration": 16.246}, {"text": "up to, it's an n by n\nmatrix, so lambda_n.", "start": 2091.356, "duration": 5.97}, {"text": "And corresponding eigenvectors\nv_1, v_2, up to v_n.", "start": 2100.08, "duration": 9.307}, {"text": "And so for convenience, I\nwill cut it at lambda_r,", "start": 2113.79, "duration": 4.32}, {"text": "and assume all rest is 0.", "start": 2118.11, "duration": 4.04}, {"text": "So there might be\nnone which are 0.", "start": 2122.15, "duration": 1.54}, {"text": "In that case we use\nall the eigenvalues.", "start": 2123.69, "duration": 2.88}, {"text": "But I only am interested\nin nonzero eigenvalues.", "start": 2126.57, "duration": 3.28}, {"text": "So I'll say up to\nlambda_r, they're nonzero.", "start": 2129.85, "duration": 3.16}, {"text": "Afterwards it's 0.", "start": 2133.01, "duration": 2.7}, {"text": "It's just a notational choice.", "start": 2135.71, "duration": 1.25}, {"text": "And now I'm just\ngoing to make a claim", "start": 2140.367, "duration": 1.583}, {"text": "that they're all positive.", "start": 2141.95, "duration": 2.3}, {"text": "This part is kind\nof just believe me.", "start": 2144.25, "duration": 5.05}, {"text": "Then if that's the case, we\ncan rewrite the eigenvalues.", "start": 2153.73, "duration": 2.67}, {"text": "Rewrite eigenvalues as\nsigma_1^2, sigma_2^2,", "start": 2156.4, "duration": 10.21}, {"text": "sigma_r^2, and 0.", "start": 2166.61, "duration": 2.336}, {"text": "That was my first step.", "start": 2175.61, "duration": 2.92}, {"text": "My second step,\nthat was step one,", "start": 2178.53, "duration": 3.326}, {"text": "step two is to define\nu_1 as A*v_1 / sigma_1,", "start": 2181.856, "duration": 8.914}, {"text": "u_2 as A*v_2 / sigma_2.", "start": 2190.77, "duration": 1.63}, {"text": "And u_r as A*V_r / sigma_r.", "start": 2195.2, "duration": 2.75}, {"text": "And then u times r+1\nas-- up to u times m,", "start": 2201.46, "duration": 7.78}, {"text": "as complete the\nabove into a basis.", "start": 2209.24, "duration": 7.15}, {"text": "So for those who\ndon't understand,", "start": 2222.59, "duration": 1.49}, {"text": "just think of we pick u_1\nup to u_r first, and then", "start": 2224.08, "duration": 3.62}, {"text": "arbitrarily pick the rest.", "start": 2227.7, "duration": 3.05}, {"text": "And you'll see why I only care\nabout the nonzero eigenvalues.", "start": 2230.75, "duration": 4.14}, {"text": "Because I have to divide by\nsigmas, the sigma values.", "start": 2234.89, "duration": 4.61}, {"text": "And if it's zero, I\ncan't do the division.", "start": 2239.5, "duration": 2.83}, {"text": "So that's why I identified\nthose which are not zero.", "start": 2242.33, "duration": 2.18}, {"text": "And then we're done.", "start": 2246.817, "duration": 0.833}, {"text": "So it doesn't look at\nall like we're done.", "start": 2250.68, "duration": 2.005}, {"text": "But I'm going to let my U be\nthis, u_1, u_2, up to u_n.", "start": 2252.685, "duration": 8.725}, {"text": "Sorry, it has to be n.", "start": 2264.012, "duration": 1.976}, {"text": "My V I will pick as\nv_1, v_2, up to v_r.", "start": 2268.65, "duration": 8.46}, {"text": "And then v_(r+1) up to v_n.", "start": 2277.11, "duration": 3.55}, {"text": "So this again just\ncomplete into a basis.", "start": 2280.66, "duration": 2.47}, {"text": "Now let's see what happens.", "start": 2295.575, "duration": 1.125}, {"text": "So A times U transpose\ntimes V. Oh, ah.", "start": 2307.96, "duration": 5.812}, {"text": "That's why it's a problem.", "start": 2313.772, "duration": 1.288}, {"text": "You have to do U times\nA times V transpose.", "start": 2319.137, "duration": 4.303}, {"text": "So I would write V\nis n, and this is m.", "start": 2323.44, "duration": 5.85}, {"text": "Ah yes, so U times A\ntimes V transpose here.", "start": 2360.5, "duration": 4.66}, {"text": "That will be u_1, u_2, u_m.", "start": 2365.16, "duration": 5.92}, {"text": "A. V transpose will be v_1\ntranspose, v_2 transpose,", "start": 2371.08, "duration": 6.785}, {"text": "to v_n transpose.", "start": 2377.865, "duration": 1.485}, {"text": "I messed up something.", "start": 2403.605, "duration": 1.48}, {"text": "Sorry.", "start": 2405.085, "duration": 0.5}, {"text": "Oh, that's the\nform I want, right?", "start": 2416.18, "duration": 2.592}, {"text": "Yeah.", "start": 2418.772, "duration": 1.778}, {"text": "So I have to transpose\nU and V there.", "start": 2420.55, "duration": 3.21}, {"text": "OK, sorry.", "start": 2423.76, "duration": 0.995}, {"text": "Thank you.", "start": 2427.28, "duration": 0.83}, {"text": "Thank you for the correction.", "start": 2428.11, "duration": 1.7}, {"text": "I know this looks\ndifferent from that.", "start": 2429.81, "duration": 1.81}, {"text": "But I mean if you flip the\ndefinition it will be the same.", "start": 2431.62, "duration": 4.76}, {"text": "So I'll just not--\nstop making mistakes.", "start": 2436.38, "duration": 2.86}, {"text": "Do you have a question?", "start": 2439.24, "duration": 1.96}, {"text": "So, yeah.", "start": 2441.2, "duration": 0.97}, {"text": "Thank you.", "start": 2442.17, "duration": 0.5}, {"text": "Yeah.", "start": 2448.55, "duration": 0.98}, {"text": "That will make more sense.", "start": 2449.53, "duration": 1.47}, {"text": "Thank you very much.", "start": 2455.9, "duration": 2.84}, {"text": "And then you're going\nto have u_1 transpose up", "start": 2458.74, "duration": 2.863}, {"text": "to u_n transpose.", "start": 2461.603, "duration": 2.857}, {"text": "A times V, because of\nthe definition of V,", "start": 2464.46, "duration": 3.84}, {"text": "will be lambda_1 of v_1.", "start": 2468.3, "duration": 3.37}, {"text": "A times v_2 will\nbe lambda_2 of v_2.", "start": 2471.67, "duration": 2.32}, {"text": "Up to lambda_r of v_r,\nand the rest will be zero.", "start": 2473.99, "duration": 5.02}, {"text": "These all define the columns.", "start": 2479.01, "duration": 1.418}, {"text": "Now let's do a few computations.", "start": 2493.874, "duration": 13.326}, {"text": "So u_1^T times lambda_1 v_1.", "start": 2507.2, "duration": 3.48}, {"text": "u_1^T, and lambda_1 v_1.", "start": 2510.68, "duration": 3.37}, {"text": "When you take the dot product,\nwhat you're going to get is", "start": 2514.05, "duration": 5.09}, {"text": "v_1^T A transpose\nof v_1 lambda_1.", "start": 2519.14, "duration": 6.301}, {"text": "I'm missing something.", "start": 2533.393, "duration": 1.491}, {"text": "Ah, sorry about that.", "start": 2545.85, "duration": 1.05}, {"text": "This is not right.", "start": 2546.9, "duration": 3.021}, {"text": "These are As.", "start": 2549.921, "duration": 0.934}, {"text": "I defined the eigenvalues\nfor A transpose A.", "start": 2553.66, "duration": 14.14}, {"text": "Then that's u_1 transpose\ntimes sigma_1 times u_1.", "start": 2567.8, "duration": 4.34}, {"text": "That will be sigma_1.", "start": 2572.14, "duration": 2.62}, {"text": "And then if you look at the\nsecond entry, u_1 transpose", "start": 2579.669, "duration": 2.291}, {"text": "times A v_2, you get u_1\ntranspose times sigma_2 of u_2.", "start": 2581.96, "duration": 9.41}, {"text": "But I claim that\nthis is equal to 0.", "start": 2594.01, "duration": 4.42}, {"text": "So why is that the case?", "start": 2598.43, "duration": 1.91}, {"text": "u_1 transpose is\nequal to V_1 transpose", "start": 2600.34, "duration": 2.742}, {"text": "A transpose over sigma_1.", "start": 2603.082, "duration": 3.234}, {"text": "And we have sigma_2.", "start": 2606.316, "duration": 1.844}, {"text": "u_2 is equal to A\ntimes v_2 over sigma_2.", "start": 2608.16, "duration": 4.716}, {"text": "So those two cancel.", "start": 2612.876, "duration": 2.774}, {"text": "And we have v_1^T A^T\nA v_2 over sigma_1.", "start": 2615.65, "duration": 6.58}, {"text": "But v_1 and v_2 are two\ndifferent eigenvectors", "start": 2622.23, "duration": 3.45}, {"text": "of this matrix.", "start": 2625.68, "duration": 2.48}, {"text": "At the beginning we can have an\northonormal decomposition of A", "start": 2628.16, "duration": 4.41}, {"text": "transpose A. That means v_1^T\ntimes v_2 times that has to be", "start": 2632.57, "duration": 6.67}, {"text": "equal to zero.", "start": 2639.24, "duration": 0.982}, {"text": "Because that's an eigenvalue.", "start": 2640.222, "duration": 2.918}, {"text": "We have v_1^T times\nlambda_2 v_2 over sigma_1.", "start": 2643.14, "duration": 6.518}, {"text": "So we have lambda_2 over\nsigma_1 times v_1 transpose v_2.", "start": 2649.658, "duration": 4.837}, {"text": "These two are\northogonal so give 0.", "start": 2654.495, "duration": 3.915}, {"text": "So if you do the\ncomputation, what", "start": 2658.41, "duration": 2.35}, {"text": "you're going to have\nis sigma_1, sigma_2", "start": 2660.76, "duration": 2.8}, {"text": "on the diagonal, up to\nsigma_r, and then 0, 0 rest.", "start": 2663.56, "duration": 4.987}, {"text": "And 0 the rest.", "start": 2668.547, "duration": 3.903}, {"text": "Sorry for the confusion.", "start": 2672.45, "duration": 3.18}, {"text": "Actually the process\nis quite simple.", "start": 2675.63, "duration": 1.56}, {"text": "I was just lost in the\ncomputation in the middle.", "start": 2677.19, "duration": 2.69}, {"text": "So process is first\nlook at A transpose A.", "start": 2679.88, "duration": 4.67}, {"text": "Find the eigenvalues\nand eigenvectors.", "start": 2684.55, "duration": 2.9}, {"text": "And using those, they\ndefine the matrix V.", "start": 2687.45, "duration": 5.62}, {"text": "And you can define the\nmatrix U by applying A times", "start": 2693.07, "duration": 3.51}, {"text": "V over sigma.", "start": 2696.58, "duration": 1.7}, {"text": "Each of those will\ndefine the entries of U.", "start": 2698.28, "duration": 3.58}, {"text": "The reason I wanted to\ngo through this proof", "start": 2701.86, "duration": 2.04}, {"text": "is because this gives you a\nprocess of finding a singular", "start": 2703.9, "duration": 3.59}, {"text": "value decomposition.", "start": 2707.49, "duration": 2.34}, {"text": "It was a little\nbit painful for me.", "start": 2709.83, "duration": 2.09}, {"text": "But if you have a matrix\nthere's just these simple steps", "start": 2711.92, "duration": 5.15}, {"text": "you can follow to find the\nsingular value decomposition.", "start": 2717.07, "duration": 4.53}, {"text": "So look at this matrix, find its\neigenvalues and eigenvectors.", "start": 2721.6, "duration": 3.86}, {"text": "Just arrange it\nin the right way.", "start": 2725.46, "duration": 2.56}, {"text": "Of course, the right\nway needs some practice", "start": 2728.02, "duration": 2.47}, {"text": "to be done correctly.", "start": 2730.49, "duration": 1.49}, {"text": "But once you do that, you\njust obtain a singular value", "start": 2731.98, "duration": 2.37}, {"text": "composition.", "start": 2734.35, "duration": 2.4}, {"text": "And really I can't explain\nhow powerful it is.", "start": 2736.75, "duration": 2.924}, {"text": "You will only later\nsee it in the course", "start": 2739.674, "duration": 1.666}, {"text": "how powerful this\ndecomposition will be.", "start": 2741.34, "duration": 2.38}, {"text": "And only then you'll\nmore appreciate", "start": 2743.72, "duration": 2.13}, {"text": "how good it is to have\nthis decomposition,", "start": 2745.85, "duration": 3.05}, {"text": "and be able to\ncompute it so simply.", "start": 2748.9, "duration": 4.2}, {"text": "So let's try to do it by hand.", "start": 2753.1, "duration": 3.498}, {"text": "Yes?", "start": 2756.598, "duration": 1.535}, {"text": "STUDENT: So when you\ncompute the [INAUDIBLE].", "start": 2758.133, "duration": 1.874}, {"text": "PROFESSOR: Yes.", "start": 2765.155, "duration": 0.625}, {"text": "STUDENT: [INAUDIBLE]", "start": 2765.78, "duration": 0.833}, {"text": "PROFESSOR: It would have\nto be orthonormal, yeah.", "start": 2768.7, "duration": 3.79}, {"text": "It should be orthonormal.", "start": 2772.49, "duration": 1.437}, {"text": "These should be orthonormal.", "start": 2773.927, "duration": 1.437}, {"text": "These also.", "start": 2778.238, "duration": 0.732}, {"text": "And that's a good point,\nbecause that can be annoying", "start": 2783.07, "duration": 2.41}, {"text": "when you want to do it by hand.", "start": 2785.48, "duration": 1.44}, {"text": "Actually this decomposition.", "start": 2786.92, "duration": 1.61}, {"text": "You have to do some Gram-Schmidt\nprocess or something like that.", "start": 2788.53, "duration": 3.27}, {"text": "What I mean by\nhand, I don't really", "start": 2795.38, "duration": 1.705}, {"text": "mean by hand, other than\nwhen you're doing homework.", "start": 2797.085, "duration": 3.915}, {"text": "Because you can use\nthe computer to do it.", "start": 2801.0, "duration": 3.03}, {"text": "And in fact, if you\nuse computer there", "start": 2804.03, "duration": 2.66}, {"text": "are much better algorithms\nthan this that are known,", "start": 2806.69, "duration": 2.624}, {"text": "which can do this a lot more\nquickly and more efficiently.", "start": 2809.314, "duration": 2.416}, {"text": "So let's try to do it by hand.", "start": 2815.14, "duration": 1.937}, {"text": "So let A be this matrix:\n[3, 2  2; 2, 3, -2].", "start": 2825.85, "duration": 10.43}, {"text": "And we want to make the\neigenvalue decomposition", "start": 2836.28, "duration": 3.505}, {"text": "of this.", "start": 2839.785, "duration": 2.295}, {"text": "A transpose A, we\nhave to compute that,", "start": 2842.08, "duration": 2.72}, {"text": "is [3, 2, 2; 2, 3, -2].", "start": 2844.8, "duration": 4.38}, {"text": "And you will get [13, 12, 2; 12,\n 13, -2; 2, -2, 8].", "start": 2859.044, "duration": 13.78}, {"text": "And let me just say that the\neigenvalues are 0, 9, and 25.", "start": 2883.92, "duration": 8.752}, {"text": "So in this algorithm,\nsigma_1^2 will be 25.", "start": 2895.57, "duration": 5.33}, {"text": "Sigma_2^2 squared will be 9.", "start": 2900.9, "duration": 2.76}, {"text": "And sigma_3^2 squared will be 0.", "start": 2903.66, "duration": 3.59}, {"text": "So we can take sigma_1\nto be 5, sigma_2 to be 3,", "start": 2907.25, "duration": 2.89}, {"text": "sigma_3 to be 0.", "start": 2910.14, "duration": 1.09}, {"text": "Now we have to find the\ncorresponding eigenvectors", "start": 2916.93, "duration": 4.72}, {"text": "to find the singular\nvalue decomposition.", "start": 2921.65, "duration": 3.19}, {"text": "And I'll just do one\njust to remind you", "start": 2924.84, "duration": 3.42}, {"text": "how to find an eigenvector.", "start": 2928.26, "duration": 1.86}, {"text": "So A transpose A,\nminus 25I is equal to,", "start": 2930.12, "duration": 6.225}, {"text": "if you subtract 25\nfrom these entries,", "start": 2936.345, "duration": 2.665}, {"text": "you're going to get [-12, 12, 2;\n 12, -12, -2; 2, -2, -13].", "start": 2939.01, "duration": 10.774}, {"text": "And then you have to\nfind the vector which", "start": 2957.0, "duration": 3.11}, {"text": "annihilates this matrix.", "start": 2960.11, "duration": 1.95}, {"text": "And that will be, I can take one\nof those vectors to be just 1", "start": 2962.06, "duration": 5.153}, {"text": "over square root of 2, 1\nover square root of two, 0,", "start": 2967.213, "duration": 4.28}, {"text": "after normalizing.", "start": 2971.493, "duration": 0.982}, {"text": "And then just do it\nfor other vectors.", "start": 2976.41, "duration": 1.85}, {"text": "You find v_2 to be 1 over\nsquare root 18, negative 1", "start": 2983.43, "duration": 8.7}, {"text": "over square root 18,\n4 over square root 18.", "start": 2992.13, "duration": 5.614}, {"text": "Now then find v_3 to be the\none that annihilates this.", "start": 3011.22, "duration": 7.8}, {"text": "But I'll just say it's x, y, z.", "start": 3019.02, "duration": 1.86}, {"text": "This will not be important.", "start": 3020.88, "duration": 2.37}, {"text": "I'll explain why it's\nnot that important.", "start": 3023.25, "duration": 2.02}, {"text": "Then our v as written\nabove, actually", "start": 3035.52, "duration": 9.29}, {"text": "there it was transposed.", "start": 3044.81, "duration": 1.14}, {"text": "So I will transpose it.", "start": 3045.95, "duration": 1.355}, {"text": "That will be 1 over\nsquare root of 2,", "start": 3047.305, "duration": 1.86}, {"text": "1 over square root of 2, 0.", "start": 3049.165, "duration": 4.185}, {"text": "v_2 is that.", "start": 3053.35, "duration": 1.31}, {"text": "So we can write 1 over\nsquare root 18, negative 1", "start": 3054.66, "duration": 3.95}, {"text": "over square root 18,\n4 over square root 18.", "start": 3058.61, "duration": 5.225}, {"text": "And here just write x, y, z.", "start": 3063.835, "duration": 2.375}, {"text": "And U will be defined\nas u_1 and u_2,", "start": 3070.485, "duration": 6.725}, {"text": "where u_1 is A times\nv_1 over sigma_1.", "start": 3077.21, "duration": 4.39}, {"text": "u_2 is A times v_2 over sigma_2.", "start": 3081.6, "duration": 5.01}, {"text": "So multiply A by this\nvector, divide by sigma_1", "start": 3086.61, "duration": 3.54}, {"text": "to get U. I already did\nthe computation for you.", "start": 3090.15, "duration": 4.265}, {"text": "It's going to be-- and\nthis is going to be-- yes?", "start": 3094.415, "duration": 10.395}, {"text": "STUDENT: How did you get v_1?", "start": 3104.81, "duration": 1.51}, {"text": "PROFESSOR: v_1?", "start": 3106.32, "duration": 1.55}, {"text": "So if you did the computation\nright in the beginning to get", "start": 3107.87, "duration": 3.11}, {"text": "the eigenvalues, then A^T\nA - 25I, this has to be--", "start": 3110.98, "duration": 7.58}, {"text": "has to not have full rank.", "start": 3118.56, "duration": 2.29}, {"text": "So there has to be a vector v,\nwhich when multiplied by this", "start": 3120.85, "duration": 2.92}, {"text": "gives [0, 0, 0] vector.", "start": 3123.77, "duration": 2.49}, {"text": "And then you say [a, b, c]\nand set it equal to [0, 0, 0].", "start": 3126.26, "duration": 7.41}, {"text": "And just solve the system\nof linear equations.", "start": 3133.67, "duration": 3.371}, {"text": "There will be several of them.", "start": 3137.041, "duration": 1.249}, {"text": "For example, we can\ntake [1, 1,  0] as well.", "start": 3138.29, "duration": 2.55}, {"text": "But I just normalized\nit to have [INAUDIBLE].", "start": 3140.84, "duration": 4.33}, {"text": "So there's a lot\nof work involved", "start": 3145.17, "duration": 2.18}, {"text": "if you want to do it by hand,\neven though you can do it.", "start": 3147.35, "duration": 3.2}, {"text": "You have to find eigenvalues,\nfind eigenvectors.", "start": 3150.55, "duration": 2.08}, {"text": "In this case, you have\nto find three of them.", "start": 3152.63, "duration": 2.64}, {"text": "And then you have to do\nmore work, and more work.", "start": 3155.27, "duration": 2.07}, {"text": "But it can be done.", "start": 3157.34, "duration": 2.27}, {"text": "And we are done now.", "start": 3159.61, "duration": 4.71}, {"text": "So now this decomposes A into\nU sigma V transformation.", "start": 3164.32, "duration": 7.7}, {"text": "So U is given as [1 over square\nroot 2, 1 over square root 2;", "start": 3172.02, "duration": 5.75}, {"text": "1 over square root 2, minus\n1 over square root 2].", "start": 3177.77, "duration": 4.55}, {"text": "Sigma was 5, 3, 0.", "start": 3182.32, "duration": 5.396}, {"text": "And V is this.", "start": 3192.07, "duration": 3.42}, {"text": "So V transpose is just\ntranspose of that.", "start": 3195.49, "duration": 3.344}, {"text": "I'll just write it like\nthat, where V is that.", "start": 3198.834, "duration": 3.956}, {"text": "So we have this decomposition.", "start": 3202.79, "duration": 2.41}, {"text": "And so let me actually write\nit, because I want to show you", "start": 3205.2, "duration": 3.17}, {"text": "why x, y, z is not important.", "start": 3208.37, "duration": 1.626}, {"text": "1 over square root 2,\n1 over square root 2,", "start": 3213.272, "duration": 3.738}, {"text": "0; 1 over square root 18,\nminus 1 over square root 18,", "start": 3217.01, "duration": 6.24}, {"text": "4 over square root 18; x, y, z.", "start": 3223.25, "duration": 2.94}, {"text": "The reason I'm\nsaying this is not", "start": 3230.6, "duration": 1.81}, {"text": "important is because I can just\ndrop-- oh what did I do here?", "start": 3232.41, "duration": 4.57}, {"text": "It has to be 2 by 3.", "start": 3236.98, "duration": 3.72}, {"text": "I can just drop this column,\nand drop this column together.", "start": 3240.7, "duration": 3.67}, {"text": "It has to be that form.", "start": 3246.89, "duration": 1.627}, {"text": "Drop this and drop\nthis altogether.", "start": 3265.51, "duration": 3.65}, {"text": "So the message here is that\nthe eigenvectors corresponding", "start": 3269.16, "duration": 4.71}, {"text": "to eigenvalue zero\nare not important.", "start": 3273.87, "duration": 4.47}, {"text": "The only relevant ones\nare nonzero eigenvalues.", "start": 3278.34, "duration": 3.3}, {"text": "So drop this, and drop this.", "start": 3281.64, "duration": 1.65}, {"text": "That will save you\nsome computation.", "start": 3283.29, "duration": 2.83}, {"text": "So let me state a different\nform of singular value", "start": 3286.12, "duration": 4.177}, {"text": "decomposition.", "start": 3290.297, "duration": 0.583}, {"text": "So this works in general.", "start": 3297.94, "duration": 1.82}, {"text": "There's a corollary.", "start": 3299.76, "duration": 1.18}, {"text": "We get a simplified form of SVD.", "start": 3300.94, "duration": 2.135}, {"text": "Where A becomes equal to U\ntimes sigma times V transpose.", "start": 3310.73, "duration": 5.316}, {"text": "And A was an m by n matrix.", "start": 3318.71, "duration": 2.55}, {"text": "U is still an m by m matrix.", "start": 3321.26, "duration": 2.96}, {"text": "But now sigma is\nalso m by m matrix.", "start": 3324.22, "duration": 3.1}, {"text": "This only works when m is\nless than or equal to n.", "start": 3327.32, "duration": 2.432}, {"text": "And V is a m by n matrix.", "start": 3333.19, "duration": 2.795}, {"text": "So the proof is\nexactly the same.", "start": 3338.96, "duration": 2.44}, {"text": "And the last step is just\nto drop the irrelevant", "start": 3341.4, "duration": 3.06}, {"text": "information.", "start": 3344.46, "duration": 2.13}, {"text": "So I will not write\ndown why it works.", "start": 3346.59, "duration": 2.22}, {"text": "But you can see if\nyou go through it,", "start": 3348.81, "duration": 3.02}, {"text": "you'll see that\ndropping this part", "start": 3351.83, "duration": 2.45}, {"text": "just corresponds to\nexactly that information.", "start": 3354.28, "duration": 1.93}, {"text": "So that's the reduced form.", "start": 3359.5, "duration": 3.16}, {"text": "So let's see.", "start": 3362.66, "duration": 1.54}, {"text": "In the beginning\nwe had A. I erased", "start": 3364.2, "duration": 2.45}, {"text": "A. A was the 2 by 3\nmatrix in the beginning.", "start": 3366.65, "duration": 3.04}, {"text": "And we obtained the\ndecomposition into 2", "start": 3369.69, "duration": 1.86}, {"text": "by 2, 2 by 2, and 2 by 3 matrix.", "start": 3371.55, "duration": 3.85}, {"text": "If we didn't delete the\nfifth column and fifth row,", "start": 3375.4, "duration": 3.27}, {"text": "we would have obtained a 2\nby 2, times 2 by 3, times 3", "start": 3378.67, "duration": 2.65}, {"text": "by 3 matrix.", "start": 3381.32, "duration": 1.76}, {"text": "But now we can simplify\nit by removing those.", "start": 3383.08, "duration": 2.492}, {"text": "And it might not look that\nmuch different on this board.", "start": 3388.91, "duration": 4.11}, {"text": "Because I just erased one row.", "start": 3393.02, "duration": 2.06}, {"text": "But many matrices that you'll\nsee in real application", "start": 3395.08, "duration": 3.84}, {"text": "have a lot lower rank than the\nnumber of columns and rows.", "start": 3398.92, "duration": 4.43}, {"text": "So if r is a lot more smaller\nthan both m and n, then", "start": 3403.35, "duration": 6.16}, {"text": "this part really--\nit's not obvious here.", "start": 3409.51, "duration": 4.14}, {"text": "But if m and n has\na big gap here,", "start": 3413.65, "duration": 2.75}, {"text": "really the number of\ncolumns that you're saving,", "start": 3416.4, "duration": 4.2}, {"text": "it can be enormous.", "start": 3420.6, "duration": 0.96}, {"text": "So to illustrate an\nexample, look at this.", "start": 3426.24, "duration": 3.7}, {"text": "Now look at the\nstock prices, where", "start": 3429.94, "duration": 2.725}, {"text": "you have companies and dates.", "start": 3432.665, "duration": 6.105}, {"text": "Previously I just gave an\nexample of a 3 by 3 matrix.", "start": 3438.77, "duration": 3.08}, {"text": "But it's more sensible\nto have dates, a lot", "start": 3441.85, "duration": 3.14}, {"text": "more dates than companies.", "start": 3444.99, "duration": 1.96}, {"text": "So let's say you recorded\n365 days of a year,", "start": 3446.95, "duration": 4.87}, {"text": "even though the market is\nnot open all days, and just", "start": 3451.82, "duration": 3.07}, {"text": "like five companies.", "start": 3454.89, "duration": 3.24}, {"text": "If you did a decomposition this\nthis, you'll have a 5 by 5,", "start": 3458.13, "duration": 3.21}, {"text": "5 by 365, 365 by 365 here.", "start": 3461.34, "duration": 4.5}, {"text": "But now in the reduced form,\nyou're saving a lot of space.", "start": 3465.84, "duration": 3.048}, {"text": "So if you just\nlook at the board,", "start": 3471.726, "duration": 1.374}, {"text": "it doesn't look like\nit's so powerful.", "start": 3473.1, "duration": 1.84}, {"text": "But in fact it is.", "start": 3474.94, "duration": 1.19}, {"text": "So that's the reduced form.", "start": 3476.13, "duration": 2.16}, {"text": "And that will be the\nform that you'll see most", "start": 3478.29, "duration": 2.64}, {"text": "of the time, this reduced form.", "start": 3480.93, "duration": 1.58}, {"text": "So I made lot of mistakes today.", "start": 3487.35, "duration": 2.05}, {"text": "I have one more topic, but\na totally irrelevant topic.", "start": 3489.4, "duration": 4.44}, {"text": "So any questions before I\nmove on to the next topic?", "start": 3493.84, "duration": 3.774}, {"text": "Yes?", "start": 3502.594, "duration": 0.661}, {"text": "STUDENT: [INAUDIBLE]", "start": 3503.255, "duration": 0.833}, {"text": "PROFESSOR: Can you\npress the button?", "start": 3510.295, "duration": 1.5}, {"text": "STUDENT: [INAUDIBLE]", "start": 3527.792, "duration": 0.833}, {"text": "PROFESSOR: Oh, so in\nthis data, what it means.", "start": 3537.04, "duration": 2.6}, {"text": "You're asking what the\neigenvectors will mean over", "start": 3539.64, "duration": 3.13}, {"text": "this data?", "start": 3542.77, "duration": 2.51}, {"text": "It will give you some stocks.", "start": 3545.28, "duration": 5.68}, {"text": "It will give you\nlike the correlation.", "start": 3550.96, "duration": 3.86}, {"text": "So each eigenvector\nwill give you", "start": 3554.82, "duration": 2.79}, {"text": "a group of companies that\nare correlated somehow.", "start": 3557.61, "duration": 3.38}, {"text": "It measures their\ncorrelation with each other.", "start": 3560.99, "duration": 2.56}, {"text": "So I don't have a\nvery good explanation", "start": 3563.55, "duration": 3.33}, {"text": "what its physical meaning is.", "start": 3566.88, "duration": 1.4}, {"text": "Maybe you can give\njust a little bit more.", "start": 3568.28, "duration": 3.76}, {"text": "GUEST SPEAKER: Possibly.", "start": 3572.04, "duration": 2.01}, {"text": "We will get into this\nin later lectures.", "start": 3574.05, "duration": 1.82}, {"text": "But in the singular\nvalue decomposition,", "start": 3575.87, "duration": 5.41}, {"text": "what you want to think of is\nthese orthonormal matrices", "start": 3581.28, "duration": 4.36}, {"text": "are really defining a new basis,\nsort of an orthogonal basis.", "start": 3585.64, "duration": 4.86}, {"text": "So you're taking the\noriginal coordinate system,", "start": 3590.5, "duration": 2.39}, {"text": "then you're rotating it.", "start": 3592.89, "duration": 2.14}, {"text": "And without changing\nor stretching", "start": 3595.03, "duration": 2.41}, {"text": "or squeezing the data.", "start": 3597.44, "duration": 0.97}, {"text": "You're just rotating the axes.", "start": 3598.41, "duration": 1.96}, {"text": "So an orthonormal\nmatrix gives you", "start": 3600.37, "duration": 2.79}, {"text": "the cosines of the\nnew coordinate system", "start": 3603.16, "duration": 3.15}, {"text": "with respect to the old one.", "start": 3606.31, "duration": 1.57}, {"text": "And so the singular\nvalue decomposition", "start": 3607.88, "duration": 2.122}, {"text": "then is simply sort\nof rotating the data", "start": 3610.002, "duration": 3.358}, {"text": "into a different orientation.", "start": 3613.36, "duration": 2.66}, {"text": "And the orthonormal basis\nthat you're transforming to,", "start": 3616.02, "duration": 7.96}, {"text": "is essentially the coordinates\nof the original data", "start": 3623.98, "duration": 4.35}, {"text": "in the transformed system.", "start": 3628.33, "duration": 1.6}, {"text": "So as Choongbum was\ncommenting, you're essentially", "start": 3629.93, "duration": 4.98}, {"text": "looking at a representation\nof the original data", "start": 3634.91, "duration": 3.45}, {"text": "points in a linearly\ntransformed space,", "start": 3638.36, "duration": 5.12}, {"text": "and the correlations\nbetween different stocks,", "start": 3643.48, "duration": 3.34}, {"text": "say, is represented by how those\npoints are oriented in the new,", "start": 3646.82, "duration": 5.17}, {"text": "in the transformed space.", "start": 3651.99, "duration": 1.88}, {"text": "PROFESSOR: So you'll have to see\nreal data to really make sense", "start": 3657.16, "duration": 3.56}, {"text": "out of it.", "start": 3660.72, "duration": 0.5}, {"text": "But another way to think of\nit is where it comes from.", "start": 3663.812, "duration": 3.618}, {"text": "So all this singular\nvalue decomposition,", "start": 3667.43, "duration": 1.772}, {"text": "if you remember\nthe proof, it comes", "start": 3669.202, "duration": 1.458}, {"text": "from eigenvectors and\neigenvalues of A transpose A.", "start": 3670.66, "duration": 5.19}, {"text": "Now if you look at A\ntranspose A, or I'll just say", "start": 3675.85, "duration": 4.12}, {"text": "it's A times A transposed.", "start": 3679.97, "duration": 2.49}, {"text": "It's pretty much the same.", "start": 3682.46, "duration": 1.49}, {"text": "If you look at A\ntimes A transpose,", "start": 3683.95, "duration": 2.58}, {"text": "you're going to get\nan m by n matrix.", "start": 3686.53, "duration": 1.895}, {"text": "And it'll be indexed\nboth by these companies.", "start": 3692.79, "duration": 3.642}, {"text": "And the numbers\nhere will represent", "start": 3700.92, "duration": 1.99}, {"text": "how much the\ncompanies are related", "start": 3702.91, "duration": 1.63}, {"text": "to each other, how\nmuch correlation they", "start": 3704.54, "duration": 1.71}, {"text": "have between each other.", "start": 3706.25, "duration": 2.44}, {"text": "So by looking at the\neigenvectors of this matrix,", "start": 3708.69, "duration": 3.08}, {"text": "you're looking at the\ncorrelation between these stock", "start": 3711.77, "duration": 3.18}, {"text": "prices, let's say, these\ncompany stock prices.", "start": 3714.95, "duration": 3.34}, {"text": "And that information is\nrepresented inside the singular", "start": 3718.29, "duration": 3.1}, {"text": "value decomposition.", "start": 3721.39, "duration": 2.8}, {"text": "But again, it's a lot\nbetter to understand", "start": 3724.19, "duration": 2.42}, {"text": "if you have real\nnumbers and real data,", "start": 3726.61, "duration": 2.64}, {"text": "which you will have later.", "start": 3729.25, "duration": 1.63}, {"text": "So please be excited and wait.", "start": 3730.88, "duration": 6.151}, {"text": "You're going to see\nsome cool stuff.", "start": 3737.031, "duration": 1.499}, {"text": "So that was all for eigenvalue\ndecomposition and singular", "start": 3746.11, "duration": 4.01}, {"text": "value decomposition.", "start": 3750.12, "duration": 2.53}, {"text": "And the last thing I\nwant to mention today", "start": 3752.65, "duration": 2.65}, {"text": "is something called\nPerron-Frobenius theorem.", "start": 3755.3, "duration": 4.67}, {"text": "This one even looks a lot\nmore theoretical than the ones", "start": 3759.97, "duration": 3.16}, {"text": "I showed you.", "start": 3763.13, "duration": 2.19}, {"text": "But surprisingly a few\nyears ago, Steve Ross,", "start": 3765.32, "duration": 4.76}, {"text": "he's a faculty in the\nbusiness school here,", "start": 3770.08, "duration": 3.47}, {"text": "found a very interesting result\ncalled Steve Ross recovery", "start": 3773.55, "duration": 3.28}, {"text": "theorem that makes\nuse of this theorem,", "start": 3776.83, "duration": 4.42}, {"text": "makes use of\nPerron-Frobenius theorem", "start": 3781.25, "duration": 1.62}, {"text": "that I will tell you today.", "start": 3782.87, "duration": 3.54}, {"text": "Unfortunately you will\nonly see a lecture", "start": 3786.41, "duration": 2.38}, {"text": "on Steve Ross recovery\ntheorem towards the end", "start": 3788.79, "duration": 2.92}, {"text": "of the semester.", "start": 3791.71, "duration": 2.02}, {"text": "So I will try to recall\nwhat it is later.", "start": 3793.73, "duration": 2.83}, {"text": "But since we're talking\nabout linear algebra today,", "start": 3796.56, "duration": 2.55}, {"text": "let me introduce the theorem.", "start": 3799.11, "duration": 3.43}, {"text": "This is called Perron-Frobenius.", "start": 3802.54, "duration": 1.5}, {"text": "And you really won't believe\nthat it has any applications", "start": 3808.04, "duration": 2.43}, {"text": "in finance because it\njust looks so theoretical.", "start": 3810.47, "duration": 3.485}, {"text": "I'm just stating a\nreally weak form.", "start": 3817.32, "duration": 3.51}, {"text": "Weak form.", "start": 3820.83, "duration": 3.11}, {"text": "Let A be an n by n symmetric\nmatrix, whose entries are all", "start": 3823.94, "duration": 10.39}, {"text": "positive, with positive entries.", "start": 3834.33, "duration": 3.03}, {"text": "Then there are a few\nproperties that they have.", "start": 3843.79, "duration": 7.12}, {"text": "First there exists\nan eigenvalue,", "start": 3850.91, "duration": 3.88}, {"text": "there exists a largest\neigenvalue, lambda_0, such", "start": 3854.79, "duration": 5.82}, {"text": "that lambda is\nless than lambda_0.", "start": 3860.61, "duration": 4.35}, {"text": "Well that's true for\nall other lambda.", "start": 3864.96, "duration": 6.222}, {"text": "So this statement is really\neasy for symmetric matrix.", "start": 3871.182, "duration": 3.378}, {"text": "So forget about-- you\ncan drop symmetric,", "start": 3874.56, "duration": 2.149}, {"text": "but I'm just stated it,\nbecause I'm going to prove only", "start": 3876.709, "duration": 2.291}, {"text": "for this weak case.", "start": 3879.0, "duration": 1.41}, {"text": "Just think about the statement\nwhen it's not symmetric.", "start": 3880.41, "duration": 4.14}, {"text": "So if you have an n by n matrix\nwhose entries are all positive,", "start": 3884.55, "duration": 4.18}, {"text": "then there exists an eigenvalue,\nlambda_0, a real eigenvalue", "start": 3888.73, "duration": 4.56}, {"text": "such that the absolute value\nof all of other eigenvalues", "start": 3893.29, "duration": 5.88}, {"text": "are strictly smaller\nthan this eigenvalue.", "start": 3899.17, "duration": 3.69}, {"text": "So remember that if it's\nnot a symmetric matrix,", "start": 3902.86, "duration": 2.68}, {"text": "they can be complex values.", "start": 3905.54, "duration": 2.56}, {"text": "This is saying that there's\na unique eigenvalue which", "start": 3908.1, "duration": 2.4}, {"text": "has largest absolute value, and\nmoreover, it's a real number.", "start": 3910.5, "duration": 3.29}, {"text": "Second part, there\nexists an eigenvector,", "start": 3916.81, "duration": 6.45}, {"text": "a positive eigenvector\nwith positive entries,", "start": 3923.26, "duration": 11.55}, {"text": "corresponding to lambda 0.", "start": 3934.81, "duration": 5.31}, {"text": "So the eigenvector\ncorresponding to this lambda 0", "start": 3940.12, "duration": 3.54}, {"text": "has positive entries.", "start": 3943.66, "duration": 3.03}, {"text": "And the third part\nis lambda_0 is", "start": 3946.69, "duration": 4.63}, {"text": "an eigenvalue of multiplicity 1,\nfor those who know what it is.", "start": 3951.32, "duration": 13.74}, {"text": "So this really is\na unique eigenvalue", "start": 3965.06, "duration": 3.01}, {"text": "with a unique eigenvector,\nwhich has positive entries.", "start": 3968.07, "duration": 3.51}, {"text": "And it's larger, really\nlarger than other eigenvalues.", "start": 3971.58, "duration": 2.781}, {"text": "So from the mathematician\npoint of view,", "start": 3977.66, "duration": 2.0}, {"text": "this has many applications.", "start": 3979.66, "duration": 1.38}, {"text": "It's probability theory.", "start": 3981.04, "duration": 2.02}, {"text": "My main research area\nis combinatorics,", "start": 3983.06, "duration": 2.23}, {"text": "discrete mathematics.", "start": 3985.29, "duration": 1.8}, {"text": "It's also used in there.", "start": 3987.09, "duration": 2.99}, {"text": "So from the theoretical\npoint of view,", "start": 3990.08, "duration": 1.71}, {"text": "this has been used\nin many contexts.", "start": 3991.79, "duration": 3.68}, {"text": "It's not a standard theorem\ntaught in linear algebra.", "start": 3995.47, "duration": 3.52}, {"text": "So I don't think probably most\nof you haven't seen it before.", "start": 3998.99, "duration": 4.0}, {"text": "But it's a well known\nresult, with many uses,", "start": 4002.99, "duration": 4.43}, {"text": "theoretical uses.", "start": 4007.42, "duration": 1.65}, {"text": "But you also see one use\nin, later, as I mentioned,", "start": 4009.07, "duration": 5.63}, {"text": "in finance, which\nis quite surprising.", "start": 4014.7, "duration": 2.221}, {"text": "So let me just give you some\nfeeling of why it happens.", "start": 4023.74, "duration": 3.58}, {"text": "I won't give you the full\ndetail of the proof, but just", "start": 4027.32, "duration": 2.98}, {"text": "a very brief description.", "start": 4030.3, "duration": 1.29}, {"text": "Sketch when A is symmetric, just\na simple case, A is symmetric.", "start": 4036.257, "duration": 9.957}, {"text": "In this case, this\nstatement, if you look at it.", "start": 4052.69, "duration": 5.85}, {"text": "First of all A has\nreal eigenvalues.", "start": 4062.8, "duration": 1.85}, {"text": "I'll say it's lambda_1,\nlambda_2, up to lambda_n.", "start": 4073.54, "duration": 5.99}, {"text": "And at some point, I'll\nsay up to lambda_i,", "start": 4079.53, "duration": 3.26}, {"text": "it's greater than\nzero, pass to where", "start": 4082.79, "duration": 1.72}, {"text": "this is smaller than zero.", "start": 4084.51, "duration": 2.16}, {"text": "There are some\npositive eigenvalues.", "start": 4086.67, "duration": 1.5}, {"text": "There are some\nnegative eigenvalues.", "start": 4088.17, "duration": 3.32}, {"text": "So that's observation one.", "start": 4091.49, "duration": 2.285}, {"text": "Things are more easy to control,\nbecause they are all real.", "start": 4098.05, "duration": 4.04}, {"text": "The first statement says that--\nmaybe I should have indexed it", "start": 4102.09, "duration": 3.5}, {"text": "as lambda_0.", "start": 4105.59, "duration": 1.794}, {"text": "I'll just call this\nlambda 0 instead.", "start": 4107.384, "duration": 3.345}, {"text": "This lambda_0 is in fact\nlarger in absolute value", "start": 4110.729, "duration": 3.451}, {"text": "than lambda_n.", "start": 4114.18, "duration": 1.45}, {"text": "That's the content\nof the first bullet.", "start": 4115.63, "duration": 6.38}, {"text": "So if they all have all\npositive entries, then", "start": 4122.01, "duration": 3.63}, {"text": "the positive, largest\npositive eigenvalue", "start": 4125.64, "duration": 2.15}, {"text": "dominates the smallest negative\neigenvalue, which yeah.", "start": 4127.79, "duration": 10.739}, {"text": "So why is that the case?", "start": 4138.529, "duration": 2.781}, {"text": "First of all, to\nsee that you have", "start": 4141.31, "duration": 1.61}, {"text": "to go through different steps.", "start": 4142.92, "duration": 2.69}, {"text": "So we go into observation two.", "start": 4145.61, "duration": 1.249}, {"text": "Lambda_1, so look at lambda_1.", "start": 4150.09, "duration": 1.86}, {"text": "lambda_1 has an eigenvector\nwith positive entries.", "start": 4151.95, "duration": 9.284}, {"text": "Why is that the case?", "start": 4167.529, "duration": 2.351}, {"text": "That's because if\nyou look at A times v", "start": 4169.88, "duration": 6.059}, {"text": "equals lambda times v. If\nv-- let me state it this way.", "start": 4175.939, "duration": 13.246}, {"text": "Lambda_0 is the maximum\nof all lambda, lambda_0.", "start": 4189.185, "duration": 4.95}, {"text": "That's not entirely correct.", "start": 4201.56, "duration": 1.975}, {"text": "Lambda_1.", "start": 4203.535, "duration": 0.5}, {"text": "Sorry about that.", "start": 4208.985, "duration": 0.99}, {"text": "So If you look at this, if\nv has non-positive entries,", "start": 4209.975, "duration": 4.635}, {"text": "if it has a negative entry,\nif v has a negative entry,", "start": 4214.61, "duration": 9.14}, {"text": "then flip it.", "start": 4223.75, "duration": 1.22}, {"text": "Flip the sign, and in this\nway obtain new vector v prime.", "start": 4224.97, "duration": 9.376}, {"text": "Since A has positive entries,\nA has positive entries.", "start": 4238.18, "duration": 4.89}, {"text": "What we conclude\nis that A times v", "start": 4247.99, "duration": 1.97}, {"text": "prime will be larger than A\ntimes v. You have to look.", "start": 4249.96, "duration": 8.63}, {"text": "Think about, because it\nhas positive entries,", "start": 4258.59, "duration": 2.22}, {"text": "if it had some negative\npart somewhere,", "start": 4260.81, "duration": 2.01}, {"text": "the magnitude will decrease.", "start": 4262.82, "duration": 2.3}, {"text": "So if you flip the sign it\nshould increase the magnitude.", "start": 4265.12, "duration": 5.0}, {"text": "And this cannot happen.", "start": 4270.12, "duration": 1.6}, {"text": "This shouldn't happen.", "start": 4271.72, "duration": 1.61}, {"text": "This should not happen.", "start": 4273.33, "duration": 1.122}, {"text": "That's where the positive\nentries part is used.", "start": 4280.33, "duration": 2.615}, {"text": "If you have positive\nentries, then it should have,", "start": 4282.945, "duration": 6.385}, {"text": "the eigenvector should have\npositive entries as well.", "start": 4289.33, "duration": 3.29}, {"text": "So I will not work through\nthe details of the rest.", "start": 4292.62, "duration": 5.8}, {"text": "I will post it on\nthe lecture notes.", "start": 4298.42, "duration": 2.41}, {"text": "But really this\ntheorem, in fact,", "start": 4300.83, "duration": 3.539}, {"text": "can be stated in a lot\nmore generality than this.", "start": 4304.369, "duration": 2.041}, {"text": "I'm stating only\na very weak form.", "start": 4306.41, "duration": 1.54}, {"text": "It doesn't have to have\nall positive entries.", "start": 4307.95, "duration": 2.68}, {"text": "It has to only be something\ncalled irreducible,", "start": 4310.63, "duration": 3.06}, {"text": "which is a concept from\nprobability theory,", "start": 4313.69, "duration": 2.95}, {"text": "from Markov chains.", "start": 4316.64, "duration": 0.99}, {"text": "But here we will only\nuse it in this setting.", "start": 4320.15, "duration": 4.66}, {"text": "So I will review it later,\nbefore it's really being used.", "start": 4324.81, "duration": 3.26}, {"text": "But just remember that how\nthese positive entries kick", "start": 4328.07, "duration": 3.15}, {"text": "into this kind of\nstatement, where", "start": 4331.22, "duration": 1.73}, {"text": "there is an eigenvalue,\nlargest eigenvalue, why", "start": 4332.95, "duration": 3.34}, {"text": "there has to be a vector\nwhich is all positive entries.", "start": 4336.29, "duration": 5.09}, {"text": "Those will all come\ninto play later.", "start": 4341.38, "duration": 3.21}, {"text": "So I think that's it for today.", "start": 4344.59, "duration": 2.682}, {"text": "If you have any last\nminute questions?", "start": 4347.272, "duration": 1.583}, {"text": "If not, I will see\nyou on Thursday.", "start": 4353.45, "duration": 3.2}]