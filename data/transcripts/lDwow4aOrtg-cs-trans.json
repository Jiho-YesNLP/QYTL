[{"text": "All right. Hey, everyone.", "start": 3.47, "duration": 2.23}, {"text": "Morning and welcome back.", "start": 5.7, "duration": 1.98}, {"text": "Um, so what I'd like to do today", "start": 7.68, "duration": 4.005}, {"text": "is continue our discussion of Naive Bayes and in particular,", "start": 11.685, "duration": 4.11}, {"text": "um, we've described how to use Naive Bayes in a generative learning algorithm,", "start": 15.795, "duration": 4.905}, {"text": "to build a spam classifier that will almost work, right?", "start": 20.7, "duration": 3.375}, {"text": "And, and, and so today you see how Laplace smoothing is one other idea, uh,", "start": 24.075, "duration": 4.77}, {"text": "you need to add to the Naive Bayes algorithm we described on Monday,", "start": 28.845, "duration": 4.005}, {"text": "to really make it work, um, for, say,", "start": 32.85, "duration": 2.46}, {"text": "email spam classification, or,", "start": 35.31, "duration": 1.92}, {"text": "or for text classification.", "start": 37.23, "duration": 1.62}, {"text": "Uh, and then we'll talk about the different version of", "start": 38.85, "duration": 1.89}, {"text": "Naive Bayes that's even better than the one we've been discussing so far.", "start": 40.74, "duration": 3.5}, {"text": "Um, talk a little bit about,", "start": 44.24, "duration": 2.345}, {"text": "ah, advice for applying machine-learning algorithms.", "start": 46.585, "duration": 2.785}, {"text": "So this would be useful to you as you get started on your,", "start": 49.37, "duration": 3.45}, {"text": "ah, CS229 class projects as well.", "start": 52.82, "duration": 1.845}, {"text": "This is a strategy of how to choose an algorithm and what to do first, what to do second,", "start": 54.665, "duration": 3.525}, {"text": "uh, and then we'll start with,", "start": 58.19, "duration": 2.45}, {"text": "um, intro to support vector machines.", "start": 60.64, "duration": 2.78}, {"text": "Okay? Um, so to recap, uh,", "start": 63.42, "duration": 4.41}, {"text": "the Naive Bayes Algorithm is", "start": 67.83, "duration": 2.36}, {"text": "a generative learning algorithm in which given a piece of email,", "start": 70.19, "duration": 3.93}, {"text": "or Twitter message or some piece of text, um,", "start": 74.12, "duration": 3.42}, {"text": "take a dictionary and put in zeros and ones depending on", "start": 77.54, "duration": 3.765}, {"text": "whether different words appear in", "start": 81.305, "duration": 2.745}, {"text": "a particular email and so this becomes your feature representation for,", "start": 84.05, "duration": 3.33}, {"text": "say, an email that you're trying to classify as spam or not spam.", "start": 87.38, "duration": 3.06}, {"text": "Um, so using the indicator function notation, um, X_j-, uh,", "start": 90.44, "duration": 6.705}, {"text": "X_j- I've been trying to use the subscript J not consistently", "start": 97.145, "duration": 4.815}, {"text": "to denote the indexes and the features", "start": 101.96, "duration": 1.9}, {"text": "and ith index in the training examples and you'll see I'm not being consistent with that.", "start": 103.86, "duration": 3.675}, {"text": "So X_j is whether or not the indicator for whether words j appears in an email.", "start": 107.535, "duration": 6.8}, {"text": "And so, um, to build a generative model for this,", "start": 114.335, "duration": 4.255}, {"text": "uh, we need to model these two terms p of x given y and p of y.", "start": 118.59, "duration": 5.82}, {"text": "Uh, so Gaussian distribution analysis models these two terms", "start": 124.41, "duration": 4.22}, {"text": "with a Gaussian and the Bernoulli respectively and Naive Bayes uses a different model.", "start": 128.63, "duration": 4.455}, {"text": "And with Naive Bayes in particular p of x given y is modeled as a, um,", "start": 133.085, "duration": 4.515}, {"text": "product of the conditional probabilities of the individual features given the class label y.", "start": 137.6, "duration": 6.76}, {"text": "And so the parameters that Naive Bayes model are,", "start": 144.36, "duration": 2.85}, {"text": "um, phi subscript y is the class prior.", "start": 147.21, "duration": 2.97}, {"text": "What's the chance that y is equal to 1,", "start": 150.18, "duration": 1.815}, {"text": "before you've seen any features?", "start": 151.995, "duration": 1.62}, {"text": "As well as phi subscript J given y equals 0,", "start": 153.615, "duration": 4.205}, {"text": "which is a chance of that word appearing in a non-spam,", "start": 157.82, "duration": 4.005}, {"text": "as well as phi subscript J given y equals 1 which is a chance of", "start": 161.825, "duration": 3.315}, {"text": "that word appearing in spam email.", "start": 165.14, "duration": 3.49}, {"text": "Okay? Um, and so if you derive the maximum likelihood estimates,", "start": 168.63, "duration": 7.445}, {"text": "you will find that the maximum likelihood estimates of,", "start": 176.075, "duration": 3.135}, {"text": "you know, phi y is this.", "start": 179.21, "duration": 3.38}, {"text": "Right? Just a fraction of training examples, um,", "start": 185.81, "duration": 4.885}, {"text": "that was equal to spam and maximum likelihood estimates of this-", "start": 190.695, "duration": 8.145}, {"text": "and this is just an indicator function notation,", "start": 216.53, "duration": 4.84}, {"text": "way of writing, um,", "start": 221.37, "duration": 1.665}, {"text": "look, at all of your,", "start": 223.035, "duration": 1.305}, {"text": "uh, emails with label y equals 0 and contact y fraction of them,", "start": 224.34, "duration": 4.935}, {"text": "did this feature X_j appear?", "start": 229.275, "duration": 2.175}, {"text": "Did this word X_j appear?", "start": 231.45, "duration": 1.875}, {"text": "Right? Um, and then finally at prediction time,", "start": 233.325, "duration": 6.505}, {"text": "um, let's see,", "start": 245.378, "duration": 5.272}, {"text": "you will calculate p of y equals 1 given X.", "start": 250.65, "duration": 6.73}, {"text": "This is kinda according to Bayes rule.", "start": 261.58, "duration": 3.955}, {"text": "Okay?", "start": 265.535, "duration": 1.465}, {"text": "Um, all right.", "start": 280.02, "duration": 2.955}, {"text": "So it turns out this algorithm will almost work and here's where it breaks down,", "start": 282.975, "duration": 6.45}, {"text": "which is, um, you know,", "start": 289.425, "duration": 2.07}, {"text": "so actually eve- every year,", "start": 291.495, "duration": 1.935}, {"text": "there are some CS229 students and some machine learning students,", "start": 293.43, "duration": 2.76}, {"text": "they will do a class project and some of you will", "start": 296.19, "duration": 2.36}, {"text": "end up submitting this to an academic conference.", "start": 298.55, "duration": 2.565}, {"text": "Right? Some, some- actually some,", "start": 301.115, "duration": 1.35}, {"text": "some of CS229 class projects get submitted,", "start": 302.465, "duration": 2.475}, {"text": "you know, as conference papers pretty much every year.", "start": 304.94, "duration": 2.79}, {"text": "One of the top machine learning conferences,", "start": 307.73, "duration": 2.745}, {"text": "is the conference NIPS.", "start": 310.475, "duration": 1.365}, {"text": "NIPS stands for Neural Information Processing Systems, um,", "start": 311.84, "duration": 2.64}, {"text": "ah, and let's say that in your dictionary,", "start": 314.48, "duration": 4.375}, {"text": "you know, you have 10,000 words in your dictionary.", "start": 318.855, "duration": 1.805}, {"text": "Let's say that the NIPS conference,", "start": 320.66, "duration": 2.04}, {"text": "the word NIPS corresponds to word number 6017, right?", "start": 322.7, "duration": 4.62}, {"text": "In your, in your 10,000 word dictionary.", "start": 327.32, "duration": 3.325}, {"text": "But up until now,", "start": 330.645, "duration": 2.395}, {"text": "presumably you've not had a lot of emails from your friends asking,", "start": 333.04, "duration": 4.45}, {"text": "\"Hey, do you want to submit the paper to the NIPS Conference or not.\"", "start": 337.49, "duration": 3.735}, {"text": "Um, and so if you use your current, you know,", "start": 341.225, "duration": 4.36}, {"text": "email, set of emails to find these maximum likelihood estimates of parameters,", "start": 345.585, "duration": 4.415}, {"text": "you will probably estimate that, um,", "start": 350.0, "duration": 2.895}, {"text": "probability of seeing this word given that it's spam email, is probably zero.", "start": 352.895, "duration": 9.215}, {"text": "Right? Zero over the number of, ah,", "start": 362.24, "duration": 3.275}, {"text": "examples that you've labeled as spam in your email.", "start": 365.515, "duration": 4.195}, {"text": "So if, if you train up this model using your personal email,", "start": 369.71, "duration": 2.935}, {"text": "probably none of the emails you've received for", "start": 372.645, "duration": 2.015}, {"text": "the last few ones had the word NIPS in it, um, maybe.", "start": 374.66, "duration": 3.09}, {"text": "Uh, and so if you plug in this formula for maximum likelihood estimate,", "start": 377.75, "duration": 4.02}, {"text": "the numerator is 0 and so your estimate of this is probably 0.", "start": 381.77, "duration": 3.21}, {"text": "Um, and then similarly,", "start": 384.98, "duration": 2.59}, {"text": "this is also 0 over,", "start": 390.86, "duration": 2.695}, {"text": "you know, the number of non-spam emails I guess.", "start": 393.555, "duration": 2.355}, {"text": "Right. So that's what this is,", "start": 395.91, "duration": 1.905}, {"text": "is just this formula.", "start": 397.815, "duration": 1.845}, {"text": "Right? And, um, statistically it's just a bad idea to say that", "start": 399.66, "duration": 8.21}, {"text": "the chance of something is 0 just because you haven't seen it", "start": 407.87, "duration": 3.015}, {"text": "yet and where this will cause the Naive Bayes algorithm to break down is,", "start": 410.885, "duration": 5.205}, {"text": "if you use these as estimates of the, of the parameters,", "start": 416.09, "duration": 3.93}, {"text": "so this is your estimates parameter phi subscript 6017 given y equals 1.", "start": 420.02, "duration": 6.445}, {"text": "This is phi subscript 6017 given y equals 0.", "start": 426.465, "duration": 4.265}, {"text": "Yes? And if you ever calculate this probability,", "start": 430.73, "duration": 3.47}, {"text": "that is equal to a product from I equals 1 through n. Let's say you", "start": 434.2, "duration": 5.33}, {"text": "have 10,000 words appear of X_i equals 1,", "start": 439.53, "duration": 6.525}, {"text": "p of X_i given y, right?", "start": 446.055, "duration": 6.225}, {"text": "And so if, um,", "start": 452.28, "duration": 1.83}, {"text": "you train your spam classifier on the emails you've gotten up until today,", "start": 454.11, "duration": 3.23}, {"text": "and then after CS229, your project teammates sen- starts sending you emails saying,", "start": 457.34, "duration": 4.6}, {"text": "hey, you know, we like the class project.", "start": 461.94, "duration": 2.365}, {"text": "Shall we consider submitting this class project to the NIPS conference?", "start": 464.305, "duration": 4.075}, {"text": "The NIPS conference deadline is usually in,", "start": 468.38, "duration": 1.68}, {"text": "um, sort of May or June most years so, you know,", "start": 470.06, "duration": 4.135}, {"text": "finish your class project this December,", "start": 474.195, "duration": 1.905}, {"text": "work on it some more by January, February,", "start": 476.1, "duration": 2.4}, {"text": "March, April next year and then maybe submit it to the conference May or June of 2019.", "start": 478.5, "duration": 4.605}, {"text": "When you start getting emails from your friends saying,", "start": 483.105, "duration": 1.875}, {"text": "let's submit our papers to NIPS conference,", "start": 484.98, "duration": 2.22}, {"text": "then when you start to see the word NIPS in your email maybe in March of next year,", "start": 487.2, "duration": 5.6}, {"text": "um, this product of probabilities will have a 0 in it, right?", "start": 492.8, "duration": 6.445}, {"text": "And so this thing that I've just circled will evaluate to", "start": 499.245, "duration": 2.925}, {"text": "0  because you multiply a lot numbers, one of which is 0.", "start": 502.17, "duration": 3.14}, {"text": "Um, and in the same way this,", "start": 505.31, "duration": 3.15}, {"text": "well, this is also 0, right?", "start": 508.46, "duration": 2.735}, {"text": "And this is also 0 because there'll be that one term in that product over there.", "start": 511.195, "duration": 7.465}, {"text": "And so what that means is if you train a spam classifier", "start": 518.66, "duration": 2.67}, {"text": "today using all the data you have in your email inbox so far,", "start": 521.33, "duration": 5.13}, {"text": "and if tomorrow or- or,", "start": 526.46, "duration": 1.815}, {"text": "you know, or two months from now, whenever.", "start": 528.275, "duration": 2.07}, {"text": "The first time you get an email from your teammates that has the word NIPS in it,", "start": 530.345, "duration": 4.825}, {"text": "your spam classifier will estimate this probability as 0 over 0 plus 0, okay?", "start": 535.17, "duration": 7.645}, {"text": "Now, apart from the divide by 0 error, uh,", "start": 542.815, "duration": 3.755}, {"text": "it turns out that, um,", "start": 546.57, "duration": 2.04}, {"text": "statistically, it's just a bad idea, right?", "start": 548.61, "duration": 2.07}, {"text": "To estimate the probability of something as 0 just", "start": 550.68, "duration": 3.08}, {"text": "because you have not seen it once yet, right?", "start": 553.76, "duration": 3.415}, {"text": "Um, so [NOISE] what I want to do is describe to you Laplace smoothing,", "start": 557.175, "duration": 6.56}, {"text": "which is a technique that helps,", "start": 563.735, "duration": 2.58}, {"text": "um, address this problem.", "start": 566.315, "duration": 2.075}, {"text": "Okay? And, um, let's- let's- In order to motivate Laplace smoothing, let me, um,", "start": 568.39, "duration": 6.515}, {"text": "use a- a- a- Yeah,", "start": 574.905, "duration": 4.365}, {"text": "Let me use a different example for now. Right? Um.", "start": 579.27, "duration": 4.63}, {"text": "Let's see. All right.", "start": 587.25, "duration": 2.44}, {"text": "So, you know, several years ago,", "start": 589.69, "duration": 1.86}, {"text": "this is- this is all the data,", "start": 591.55, "duration": 1.32}, {"text": "but several years ago- so- so let me put aside Naive Bayes,", "start": 592.87, "duration": 2.31}, {"text": "I want to talk about Laplace smoothing.", "start": 595.18, "duration": 1.05}, {"text": "We will come back to apply Laplace smoothing in Naive Bayes.", "start": 596.23, "duration": 2.13}, {"text": "So several years ago,", "start": 598.36, "duration": 1.245}, {"text": "I was tracking the progress of the Stanford football team,", "start": 599.605, "duration": 4.275}, {"text": "um, just a few years ago now.", "start": 603.88, "duration": 2.31}, {"text": "But that year on 9/12, um,", "start": 606.19, "duration": 3.69}, {"text": "our football team played to Wake Forest and,", "start": 609.88, "duration": 5.19}, {"text": "you know, actually these are all the, uh,", "start": 615.07, "duration": 2.22}, {"text": "all the stay games we played that year, right?", "start": 617.29, "duration": 2.58}, {"text": "And, um, uh, we did not win that game.", "start": 619.87, "duration": 4.425}, {"text": "Then on 10/10, we played Oregon State and we did not win that game.", "start": 624.295, "duration": 7.855}, {"text": "Arizona, we did not win that game.", "start": 634.17, "duration": 4.43}, {"text": "We played Caltech, we did not win that game.", "start": 639.75, "duration": 3.64}, {"text": "[LAUGHTER].", "start": 643.39, "duration": 3.03}, {"text": "And the question is,", "start": 646.42, "duration": 2.265}, {"text": "these are all the away games- almost all the out of state games we played that year.", "start": 648.685, "duration": 4.365}, {"text": "And so you're, you know, Stanford football team's biggest fan.", "start": 653.05, "duration": 2.145}, {"text": "You followed them to every single out of state game and watched all these games.", "start": 655.195, "duration": 2.94}, {"text": "The question is, after this unfortunate streak,", "start": 658.135, "duration": 3.06}, {"text": "when you go on- there's actually a game on New Year's Eve,", "start": 661.195, "duration": 3.945}, {"text": "you follow them to their over home game,", "start": 665.14, "duration": 2.79}, {"text": "what's your estimate of the chances of their winning or losing?", "start": 667.93, "duration": 4.155}, {"text": "Right? Now, if you use maximum likelihood,", "start": 672.085, "duration": 3.614}, {"text": "so let's say this is the variable x,", "start": 675.699, "duration": 1.876}, {"text": "you would estimate the probability of their winning.", "start": 677.575, "duration": 3.51}, {"text": "Well, maximum likelihood is really count up the number of wins, right,", "start": 681.085, "duration": 4.92}, {"text": "and divide that by the number of wins plus the number of losses.", "start": 686.005, "duration": 7.62}, {"text": "And so in this case, um,", "start": 693.625, "duration": 2.565}, {"text": "you estimate this as 0 divided by number of wins with 0,", "start": 696.19, "duration": 5.895}, {"text": "number of losses was 4, right?", "start": 702.085, "duration": 3.03}, {"text": "Which is equal to 0, okay?", "start": 705.115, "duration": 1.965}, {"text": "Um, that's kinda mean, right?", "start": 707.08, "duration": 2.4}, {"text": "[LAUGHTER].", "start": 709.48, "duration": 0.96}, {"text": "They lost 4 games, but you say, no, the chances of their winning is 0.", "start": 710.44, "duration": 3.33}, {"text": "Absolute certainty.", "start": 713.77, "duration": 1.14}, {"text": "And- and- and just statistically, this is not,", "start": 714.91, "duration": 2.055}, {"text": "um, this is not a good idea.", "start": 716.965, "duration": 3.03}, {"text": "Um, and so what Laplace smoothing,", "start": 719.995, "duration": 3.925}, {"text": "what we're going to do is, uh,", "start": 727.71, "duration": 2.575}, {"text": "imagine that we saw the positive outcomes,", "start": 730.285, "duration": 4.545}, {"text": "the number of wins, you know,", "start": 734.83, "duration": 2.145}, {"text": "just add 1 to the number of wins we actually saw and also the number of losses add 1.", "start": 736.975, "duration": 7.65}, {"text": "Right? So if you actually saw 0 wins, pretend you saw one and if you saw 4 losses,", "start": 744.625, "duration": 4.365}, {"text": "pretend you saw 1 more than you actually saw.", "start": 748.99, "duration": 2.28}, {"text": "And so Laplace smoothing,", "start": 751.27, "duration": 1.41}, {"text": "you're gonna end up adding 1 to the numerator and adding 2 to the denominator.", "start": 752.68, "duration": 5.85}, {"text": "And so this ends up being 1 over 6, right?", "start": 758.53, "duration": 3.945}, {"text": "And that's actually a more reasonable may- maybe", "start": 762.475, "duration": 3.255}, {"text": "it is a more reasonable estimate for the chance of,", "start": 765.73, "duration": 2.73}, {"text": "uh, them winning or losing the next game.", "start": 768.46, "duration": 2.955}, {"text": "Um, uh, and the- there's actually", "start": 771.415, "duration": 2.325}, {"text": "a cert- certain- certain set of circumstances under which there's more estimates.", "start": 773.74, "duration": 3.24}, {"text": "I didn't just make this up in thin air.", "start": 776.98, "duration": 1.44}, {"text": "Uh, Laplace, um, uh, you know, uh,", "start": 778.42, "duration": 3.12}, {"text": "it's an ancient that -- well known,", "start": 781.54, "duration": 2.655}, {"text": "uh, very influential mathematician.", "start": 784.195, "duration": 3.075}, {"text": "He actually tried to estimated the chance of the sun rising the next day.", "start": 787.27, "duration": 2.85}, {"text": "And the reasoning was, well, we've seen the sunrise all times and so, uh,", "start": 790.12, "duration": 4.395}, {"text": "but tha- that doesn't mean we should be absolutely certain", "start": 794.515, "duration": 2.295}, {"text": "the sun will still rise tomorrow, right?", "start": 796.81, "duration": 2.325}, {"text": "And so his reasoning was, well,", "start": 799.135, "duration": 1.395}, {"text": "we've seen the sunrise 10,000 times, you know,", "start": 800.53, "duration": 2.43}, {"text": "we can be really certain the sun will rise again tomorrow but maybe not absolutely", "start": 802.96, "duration": 3.51}, {"text": "certain because maybe something will go wrong", "start": 806.47, "duration": 1.8}, {"text": "or who- who knows what will happen in this galaxy?", "start": 808.27, "duration": 2.415}, {"text": "Um , uh, uh, and so his reasoning", "start": 810.685, "duration": 3.21}, {"text": "was- he derived the optimal estimate- way of estimating,", "start": 813.895, "duration": 3.225}, {"text": "you know, really the chance the sun will rise tomorrow.", "start": 817.12, "duration": 2.355}, {"text": "And this is actually an optimal estimate under I'll say- I'll say the same assumptions,", "start": 819.475, "duration": 4.515}, {"text": "we don't need to worry about it.", "start": 823.99, "duration": 1.05}, {"text": "But it turns out that if you assume that you are Bayesian,", "start": 825.04, "duration": 2.685}, {"text": "where the uniform Bayesian prior on the chance of the sun rising tomorrow.", "start": 827.725, "duration": 4.455}, {"text": "So if the chance the sun rising tomorrow is uniformly distributed,", "start": 832.18, "duration": 3.555}, {"text": "you know, in the unit interval anywhere from 0 to 1,", "start": 835.735, "duration": 2.61}, {"text": "then after a set of observations of this coin toss of whether the sun rises,", "start": 838.345, "duration": 4.725}, {"text": "this is actually a Bayesian optimal estimate of the chances of the sun rising tomorrow, okay?", "start": 843.07, "duration": 3.735}, {"text": "If you don't understand what I just said in the last 30 seconds, don't worry about it.", "start": 846.805, "duration": 3.0}, {"text": "Um, uh, it's taught in sort of", "start": 849.805, "duration": 1.395}, {"text": "a Bayesian statistics- advanced Bayesian statistics classes.", "start": 851.2, "duration": 2.94}, {"text": "But mechanically, what you should do is, uh,", "start": 854.14, "duration": 2.37}, {"text": "take this formula and add 1 to the number of", "start": 856.51, "duration": 3.06}, {"text": "counts you actually saw for each of the possible outcomes.", "start": 859.57, "duration": 3.315}, {"text": "Um, and more generally,", "start": 862.885, "duration": 1.995}, {"text": "uh, if y, er, excuse me.", "start": 864.88, "duration": 7.41}, {"text": "If- if you're estimating probabilities for a k way random variable, um,", "start": 872.29, "duration": 8.26}, {"text": "then you estimate the chance that X being i to be equal to,", "start": 880.56, "duration": 7.76}, {"text": "um, so- so that's", "start": 894.011, "duration": 9.259}, {"text": "the maximum likelihood estimate.", "start": 903.27, "duration": 3.2}, {"text": "And for the fast-moving,", "start": 906.47, "duration": 1.97}, {"text": "you'd add one to the numerator and,", "start": 908.44, "duration": 3.045}, {"text": "um, you add k to the denominator.", "start": 911.485, "duration": 3.42}, {"text": "Okay? So for Naive Bayes,", "start": 914.905, "duration": 9.6}, {"text": "the way this mod- modifies your parameter estimates is this.", "start": 924.505, "duration": 5.25}, {"text": "Um, I'm just gonna copy over the formula from above.", "start": 929.755, "duration": 4.62}, {"text": "Right?", "start": 934.375, "duration": 8.285}, {"text": "Um, so that's the maximum likely estimate.", "start": 944.37, "duration": 5.95}, {"text": "And with Laplace smoothing,", "start": 950.32, "duration": 1.74}, {"text": "you add one to the numerator and add two to the denominator and this", "start": 952.06, "duration": 5.13}, {"text": "means that your estimates are probably- these probabilities they're never", "start": 957.19, "duration": 2.43}, {"text": "exactly 0 or exactly 1,", "start": 959.62, "duration": 2.61}, {"text": "which takes away that problem of,", "start": 962.23, "duration": 1.98}, {"text": "you know, the 0 over 0.", "start": 964.21, "duration": 2.1}, {"text": "Okay. Um, and so if you implement this algorithm,", "start": 966.31, "duration": 3.96}, {"text": "it's not- it's not like a great spam classifier but it's not terrible either.", "start": 970.27, "duration": 3.945}, {"text": "And one nice thing about this algorithm is is so simple, right?", "start": 974.215, "duration": 3.765}, {"text": "Estimated parameters is just counting ,um,", "start": 977.98, "duration": 2.33}, {"text": "uh, uh can be done, you know,", "start": 980.31, "duration": 2.08}, {"text": "very efficiently, right, just- just by counting,", "start": 982.39, "duration": 3.0}, {"text": "uh, and then- and classification time is just multiplying a bunch probabilities together.", "start": 985.39, "duration": 4.305}, {"text": "Uh, this is very confusing first algorithm. All right.", "start": 989.695, "duration": 4.38}, {"text": "Any questions about this? Yeah.", "start": 994.075, "duration": 6.135}, {"text": "[inaudible]?", "start": 1000.21, "duration": 3.3}, {"text": "Oh sorry. This is y. Er, oh yes.", "start": 1003.51, "duration": 3.06}, {"text": "Thank you. Er, yes thank you.", "start": 1006.57, "duration": 3.34}, {"text": "All right. Oh, by the way,", "start": 1018.02, "duration": 2.575}, {"text": "I- I was actually following the Stanford football team that year so,", "start": 1020.595, "duration": 3.66}, {"text": "you know, they lost.", "start": 1024.255, "duration": 1.665}, {"text": "[LAUGHTER].", "start": 1025.92, "duration": 1.74}, {"text": "Because, okay, I love our football team.", "start": 1027.66, "duration": 1.185}, {"text": "They're doing much better right now. That was a few years ago.", "start": 1028.845, "duration": 1.935}, {"text": "[LAUGHTER].", "start": 1030.78, "duration": 1.451}, {"text": "[NOISE].", "start": 1032.231, "duration": 6.859}, {"text": "All right.", "start": 1039.09, "duration": 0.51}, {"text": "Um,", "start": 1039.6, "duration": 0.88}, {"text": "[NOISE]", "start": 1040.48, "duration": 9.417}, {"text": "So, um, in- in the- examples we've talked about so far,", "start": 1049.897, "duration": 5.573}, {"text": "the features were binary valued.", "start": 1055.47, "duration": 2.355}, {"text": "Um, and so, um,", "start": 1057.825, "duration": 3.225}, {"text": "actually one quick generalization, uh,", "start": 1061.05, "duration": 2.82}, {"text": "when the features are multinomial valued,", "start": 1063.87, "duration": 5.955}, {"text": "um, then the generalization- actually here's one example.", "start": 1069.825, "duration": 4.275}, {"text": "We talked about predicting housing prices, right?", "start": 1074.1, "duration": 2.52}, {"text": "That was our very first world meaning example.", "start": 1076.62, "duration": 2.04}, {"text": "Let's say you have a classification problem instead,", "start": 1078.66, "duration": 3.03}, {"text": "which is you're listing a house you want to sell,", "start": 1081.69, "duration": 1.845}, {"text": "what is the chance of this house to be sold within the next 30 days?", "start": 1083.535, "duration": 2.85}, {"text": "So it's a classification problem.", "start": 1086.385, "duration": 1.605}, {"text": "Um, so if one of the features is the size of the house x, right,", "start": 1087.99, "duration": 5.595}, {"text": "then one way to turn the feature into", "start": 1093.585, "duration": 2.955}, {"text": "a discrete feature would be to choose a few buckets,", "start": 1096.54, "duration": 3.93}, {"text": "assert the size is less than 400 square feet, uh, versus,", "start": 1100.47, "duration": 6.285}, {"text": "you know, 400 to 800 or 800 to 1200 or greater than 1200 square feet.", "start": 1106.755, "duration": 6.825}, {"text": "Then you can set the feature XI to one of four values, right?", "start": 1113.58, "duration": 5.07}, {"text": "So that is how you discretize a continuous valued feature to a discrete value feature.", "start": 1118.65, "duration": 4.215}, {"text": "Um, and if you want to apply Naive Bayes to this problem,", "start": 1122.865, "duration": 4.335}, {"text": "then probability of x given y,", "start": 1127.2, "duration": 2.91}, {"text": "this is just the same as before.", "start": 1130.11, "duration": 2.31}, {"text": "Product from i equals 1 through n of p of", "start": 1132.42, "duration": 7.98}, {"text": "xj given Y where now this can be a multinomial probability.", "start": 1140.4, "duration": 8.925}, {"text": "Right? Where if- if X now takes on one of four values there then,", "start": 1149.325, "duration": 4.05}, {"text": "um, this can be a,", "start": 1153.375, "duration": 1.905}, {"text": "uh, estimators and multinomial problem.", "start": 1155.28, "duration": 1.89}, {"text": "So instead of a Bernoulli distribution over two possible outcomes,", "start": 1157.17, "duration": 3.96}, {"text": "this can be a probably, uh,", "start": 1161.13, "duration": 1.365}, {"text": "probability mass function probably over four possible outcomes if you", "start": 1162.495, "duration": 3.045}, {"text": "discretize the size of a house into four values.", "start": 1165.54, "duration": 3.885}, {"text": "Um, and if you ever discretized variables, a", "start": 1169.425, "duration": 2.805}, {"text": "typical rule of thumb in machine learning often we", "start": 1172.23, "duration": 2.67}, {"text": "discretize variables into 10 values, into 10 buckets.", "start": 1174.9, "duration": 3.105}, {"text": "Uh, just as a- it often seems to work well enough.", "start": 1178.005, "duration": 3.105}, {"text": "I- I drew 4  here so I don't have to write all 10 buckets.", "start": 1181.11, "duration": 2.46}, {"text": "But if you ever discretize var- variables,", "start": 1183.57, "duration": 2.61}, {"text": "you know, most people will start off with discretizing things into 10 values.", "start": 1186.18, "duration": 6.46}, {"text": "All right. Now, uh, right.", "start": 1200.0, "duration": 9.82}, {"text": "And so this is how you can apply Naive Bayes on other problems as well including cost line,", "start": 1209.82, "duration": 3.67}, {"text": "for example, if a house is likely to be sold in the next 30 days.", "start": 1213.49, "duration": 3.855}, {"text": "Now, um, there's, uh,", "start": 1217.345, "duration": 3.975}, {"text": "there's a different variation on Naive Bayes that I want to describe to", "start": 1221.32, "duration": 5.34}, {"text": "you that is actually much better for the specific problem of text classification.", "start": 1226.66, "duration": 5.22}, {"text": "Uh and so our feature representation for x so far was the following, right?", "start": 1231.88, "duration": 6.65}, {"text": "With a dictionary a, aardvark, buy,", "start": 1238.53, "duration": 5.95}, {"text": "So let's say you get an email that's,", "start": 1250.22, "duration": 3.505}, {"text": "you know, a very spammy email that's \"Drugs, buy drugs now\",", "start": 1253.725, "duration": 3.885}, {"text": "[LAUGHTER] This is meant as an illustrative example,", "start": 1257.61, "duration": 5.88}, {"text": "I'm not telling any of you to buy drugs.", "start": 1263.49, "duration": 1.5}, {"text": "[LAUGHTER] Um, so if,", "start": 1264.99, "duration": 8.13}, {"text": "uh, if you have a dictionary of 10,000 words,", "start": 1273.12, "duration": 2.22}, {"text": "then I guess- let's say a is worth 1,", "start": 1275.34, "duration": 1.86}, {"text": "aardvark is worth 2,", "start": 1277.2, "duration": 1.695}, {"text": "uh, just to, you know, make this example concrete.", "start": 1278.895, "duration": 2.49}, {"text": "Let's say the word buy is word 800,", "start": 1281.385, "duration": 2.46}, {"text": "drugs is word 1,600,", "start": 1283.845, "duration": 2.085}, {"text": "and let's say now is the word- is the 6,200th word in your,", "start": 1285.93, "duration": 3.93}, {"text": "uh, 10,000 words in the sorted dictionary.", "start": 1289.86, "duration": 2.745}, {"text": "Um, then the representation for x will be,", "start": 1292.605, "duration": 3.795}, {"text": "you know, 0, 0, [NOISE] right?", "start": 1296.4, "duration": 1.95}, {"text": "And they put a 1 there, and a 1 there, and a 1 there.", "start": 1298.35, "duration": 2.565}, {"text": "Okay? Now, one, one- so, um,", "start": 1300.915, "duration": 2.745}, {"text": "one interesting thing about Naive Bayes is that it throws away", "start": 1303.66, "duration": 3.015}, {"text": "the fact that the word drugs has appeared twice, right?", "start": 1306.675, "duration": 3.27}, {"text": "So that's losing a little bit of information, um, uh, and,", "start": 1309.945, "duration": 4.515}, {"text": "and in this feature representation, um,", "start": 1314.46, "duration": 2.73}, {"text": "you know, each feature is either 0 or 1, right?", "start": 1317.19, "duration": 2.91}, {"text": "And that's part of why it throws away the information that's, uh,", "start": 1320.1, "duration": 3.705}, {"text": "where the one-word drugs appear twice,", "start": 1323.805, "duration": 1.875}, {"text": "and maybe should be given more weight for your- in your classifier.", "start": 1325.68, "duration": 3.76}, {"text": "Um, [NOISE] there's a different representation,", "start": 1329.87, "duration": 2.71}, {"text": "uh, which is specific to text.", "start": 1332.58, "duration": 7.44}, {"text": "And I think text data has a,", "start": 1340.02, "duration": 1.695}, {"text": "has a property that they can be very long or very short.", "start": 1341.715, "duration": 2.52}, {"text": "You can have a five-word email,", "start": 1344.235, "duration": 1.845}, {"text": "or a 1,000-word email, um,", "start": 1346.08, "duration": 2.385}, {"text": "and somehow you're taking very short or very long", "start": 1348.465, "duration": 2.985}, {"text": "emails and just mapping them to a feature vector that's always the same length.", "start": 1351.45, "duration": 3.465}, {"text": "Just a different representation [NOISE] for, um,", "start": 1354.915, "duration": 4.5}, {"text": "this email, which is, uh,", "start": 1359.415, "duration": 2.25}, {"text": "for that email that says,", "start": 1361.665, "duration": 1.335}, {"text": "\"Drugs, buy drugs now\",", "start": 1363.0, "duration": 1.095}, {"text": "we're gonna represent it as a four-dimensional feature vector, [NOISE] right?", "start": 1364.095, "duration": 9.195}, {"text": "And so this is going to be, um,", "start": 1373.29, "duration": 2.43}, {"text": "n-dimensional for an email of length n.", "start": 1375.72, "duration": 3.405}, {"text": "So rather than a 10,000-dimensional feature vector,", "start": 1379.125, "duration": 3.555}, {"text": "we now have a four-dimensional feature vector,", "start": 1382.68, "duration": 3.315}, {"text": "but now xj is,", "start": 1385.995, "duration": 6.805}, {"text": "um, an index from 1 to 10,000 instead of just being 0 or 1.", "start": 1395.3, "duration": 5.77}, {"text": "Okay? And, uh, n is- and I guess n varies by training example.", "start": 1401.07, "duration": 6.84}, {"text": "So ni is the, uh,", "start": 1407.91, "duration": 2.685}, {"text": "length of email i.", "start": 1410.595, "duration": 9.855}, {"text": "So the longer email,", "start": 1420.45, "duration": 1.35}, {"text": "this vector, the feature vector x will be longer,", "start": 1421.8, "duration": 4.035}, {"text": "and the shorter email,", "start": 1425.835, "duration": 1.41}, {"text": "this feature vector will be shorter, okay?", "start": 1427.245, "duration": 5.7}, {"text": "So, um, let's see.", "start": 1432.945, "duration": 4.755}, {"text": "Uh, just to give names to the algorithms we're gonna develop,", "start": 1437.7, "duration": 4.11}, {"text": "these are- these are really very confusing, very horrible names.", "start": 1441.81, "duration": 3.54}, {"text": "But this is what the community calls them.", "start": 1445.35, "duration": 2.535}, {"text": "That the, the model we've talked about so far is sometimes", "start": 1447.885, "duration": 3.135}, {"text": "called the Multivariate Bernoulli.", "start": 1451.02, "duration": 8.98}, {"text": "And that model, uh,", "start": 1460.67, "duration": 2.17}, {"text": "so Bernoulli means coin tosses,", "start": 1462.84, "duration": 1.725}, {"text": "so multivariate means, you know,", "start": 1464.565, "duration": 1.755}, {"text": "there are 10,000 Bernoulli random variables in", "start": 1466.32, "duration": 2.7}, {"text": "this model whereas as a Multivariate Bernoulli event model.", "start": 1469.02, "duration": 2.835}, {"text": "An event comes with statistics I guess.", "start": 1471.855, "duration": 1.725}, {"text": "Um, and the new representation we're gonna talk about is called", "start": 1473.58, "duration": 3.75}, {"text": "the [NOISE] Multinomial Event Model.", "start": 1477.33, "duration": 7.62}, {"text": "Uh, these two names are- are- are- frankly,", "start": 1484.95, "duration": 2.55}, {"text": "these two names are quite confusing.", "start": 1487.5, "duration": 1.605}, {"text": "But these are the names that, uh, I think- actually,", "start": 1489.105, "duration": 2.655}, {"text": "one of my friends Andrew McCallum, uh, as far as I know,", "start": 1491.76, "duration": 2.49}, {"text": "wrote the paper that named these two algorithms.", "start": 1494.25, "duration": 2.64}, {"text": "But- but I think these are- these are the names we seem to use.", "start": 1496.89, "duration": 6.04}, {"text": "Um, and so, with this new model,", "start": 1512.63, "duration": 5.53}, {"text": "um, we're gonna build a generative model,", "start": 1518.16, "duration": 3.885}, {"text": "and because it's a generative model,", "start": 1522.045, "duration": 2.535}, {"text": "or model p of x,", "start": 1524.58, "duration": 1.275}, {"text": "y which can be factored as follows and using the Naive Bayes assumption,", "start": 1525.855, "duration": 8.05}, {"text": "we're going to assume that p of x given y is product from i equals 1 through n,", "start": 1533.905, "duration": 6.985}, {"text": "of j equals 1 through n,", "start": 1540.89, "duration": 2.19}, {"text": "of p of xj,", "start": 1543.08, "duration": 2.1}, {"text": "given y, and then times, you know, p of y.", "start": 1545.18, "duration": 4.39}, {"text": "Is that second term, right?", "start": 1549.57, "duration": 2.145}, {"text": "Now, one of the, uh, uh, one,", "start": 1551.715, "duration": 2.775}, {"text": "one of the reasons these two models were very-", "start": 1554.49, "duration": 2.595}, {"text": "were frankly actually very confusing to the machine learning community,", "start": 1557.085, "duration": 3.0}, {"text": "is because this is exactly the equation [NOISE] that,", "start": 1560.085, "duration": 2.985}, {"text": "you know, you saw on Monday,", "start": 1563.07, "duration": 1.44}, {"text": "when we described Naive Bayes for the first time, um,", "start": 1564.51, "duration": 3.27}, {"text": "that, you know, this, you know,", "start": 1567.78, "duration": 1.575}, {"text": "p of x given y is part of probabilities.", "start": 1569.355, "duration": 1.995}, {"text": "Right? So this is exactly, uh, so this,", "start": 1571.35, "duration": 1.62}, {"text": "this equation looks cosmetically identical,", "start": 1572.97, "duration": 3.075}, {"text": "but with this new model, the second model,", "start": 1576.045, "duration": 2.505}, {"text": "the confusingly named Multinomial Event Model, um,", "start": 1578.55, "duration": 3.93}, {"text": "the definition of xj and the definition of n is very different, right?", "start": 1582.48, "duration": 5.835}, {"text": "So instead of a product from 1 through 10,000,", "start": 1588.315, "duration": 3.51}, {"text": "there's a product from 1 through the number of words in the email,", "start": 1591.825, "duration": 2.85}, {"text": "and this is now instead a multinomial probability.", "start": 1594.675, "duration": 2.955}, {"text": "Rather than a binary or Bernoulli probability.", "start": 1597.63, "duration": 3.345}, {"text": "Okay? Um, and it turns out that, uh, well,", "start": 1600.975, "duration": 5.055}, {"text": "[NOISE] with this model,", "start": 1606.03, "duration": 3.36}, {"text": "the parameters are same as before.", "start": 1609.39, "duration": 3.15}, {"text": "Phi y is probability of y equals 1,", "start": 1612.54, "duration": 2.13}, {"text": "and also, um, the other parameters of this model, phi k,", "start": 1614.67, "duration": 5.49}, {"text": "given y equals 0,", "start": 1620.16, "duration": 1.755}, {"text": "is a chance of xj equals k,", "start": 1621.915, "duration": 5.395}, {"text": "given y equals 0.", "start": 1628.88, "duration": 2.425}, {"text": "Right? And- and just to make sure you understand the notation.", "start": 1631.305, "duration": 2.355}, {"text": "See if this makes sense.", "start": 1633.66, "duration": 1.2}, {"text": "So this probability is the chance of word", "start": 1634.86, "duration": 5.355}, {"text": "blank being blank if label y equals 0.", "start": 1640.215, "duration": 9.24}, {"text": "So what goes into those two blanks?", "start": 1649.455, "duration": 2.515}, {"text": "Actually, what goes in the second blank?", "start": 1658.88, "duration": 2.575}, {"text": "Uh, let's see.", "start": 1661.455, "duration": 1.545}, {"text": "Well- well, yeah?", "start": 1663.0, "duration": 19.78}, {"text": "[inaudible].", "start": 1682.78, "duration": 2.6}, {"text": "Yes. Right. So it's the chance of the third word in the email,", "start": 1685.38, "duration": 4.215}, {"text": "being the word drugs,", "start": 1689.595, "duration": 1.035}, {"text": "or the chance of the second in the email being buy, or whatever.", "start": 1690.63, "duration": 3.615}, {"text": "And one part of, um,", "start": 1694.245, "duration": 2.31}, {"text": "why we implicitly assume,", "start": 1696.555, "duration": 1.44}, {"text": "mainly why this is tricky,", "start": 1697.995, "duration": 1.565}, {"text": "is that, uh, we assume that this probability doesn't depend on j, right?", "start": 1699.56, "duration": 5.875}, {"text": "That for every position in the email,", "start": 1705.435, "duration": 2.07}, {"text": "for the- the chance that the first word being", "start": 1707.505, "duration": 1.935}, {"text": "drugs is same as chance of the second word being drugs,", "start": 1709.44, "duration": 2.1}, {"text": "is same as the third word being drugs,", "start": 1711.54, "duration": 1.44}, {"text": "which is why, um,", "start": 1712.98, "duration": 1.29}, {"text": "on the left-hand side j doesn't actually appear on the left-hand side, right.", "start": 1714.27, "duration": 4.3}, {"text": "Makes sense? Any questions about this?", "start": 1719.27, "duration": 2.47}, {"text": "No?", "start": 1721.74, "duration": 3.03}, {"text": "Okay. All right.", "start": 1724.77, "duration": 1.545}, {"text": "Um, and so the way you calculate the probability, the way you would,", "start": 1726.315, "duration": 3.57}, {"text": "um, uh, and, and so the way that,", "start": 1729.885, "duration": 4.695}, {"text": "uh, given a new email,", "start": 1734.58, "duration": 1.305}, {"text": "a test email, um, uh,", "start": 1735.885, "duration": 2.235}, {"text": "you would calculate this probability is by, you know,", "start": 1738.12, "duration": 3.405}, {"text": "plugging these parameters that you estimate from the data into this formula.", "start": 1741.525, "duration": 4.29}, {"text": "Okay?", "start": 1745.815, "duration": 0.705}, {"text": "[NOISE]", "start": 1746.52, "duration": 20.31}, {"text": "Um, oh, and then, um, I wrote down, uh, [NOISE] right.", "start": 1766.83, "duration": 6.945}, {"text": "And then, and then the other set of the parameters is this.", "start": 1773.775, "duration": 2.205}, {"text": "[NOISE] Right.", "start": 1775.98, "duration": 2.43}, {"text": "Kind of just with y equals 1,", "start": 1778.41, "duration": 1.8}, {"text": "is that y equals 0.", "start": 1780.21, "duration": 1.62}, {"text": "And then for the maximum likelihood estimate of the parameters,", "start": 1781.83, "duration": 2.775}, {"text": "I'll just write out one of them.", "start": 1784.605, "duration": 1.305}, {"text": "[NOISE] Your estimate of, uh,", "start": 1785.91, "duration": 4.245}, {"text": "the chance of a given word is really anywhere in any position,", "start": 1790.155, "duration": 4.995}, {"text": "being word k. What's the chance of some word in", "start": 1795.15, "duration": 3.0}, {"text": "a non-spam email being the word drugs, let's say?", "start": 1798.15, "duration": 3.57}, {"text": "Um, the chance of that is equal to [NOISE]", "start": 1801.72, "duration": 21.33}, {"text": "I find that- well, this indicates a function notation. It looks complex.", "start": 1823.05, "duration": 3.975}, {"text": "I'll just say in a second,", "start": 1827.025, "duration": 2.965}, {"text": "uh, what this actually means.", "start": 1830.72, "duration": 5.155}, {"text": "So the denominator, um,", "start": 1835.875, "duration": 2.04}, {"text": "so this space means- so- and so if you figure out what", "start": 1837.915, "duration": 3.105}, {"text": "the English meaning of this complicated formula is, this basically says,", "start": 1841.02, "duration": 3.93}, {"text": "\"Look at all the words in all of your non-spam emails,", "start": 1844.95, "duration": 3.63}, {"text": "all the emails of y equals 0,", "start": 1848.58, "duration": 2.115}, {"text": "and look at all of the words in all of the emails,", "start": 1850.695, "duration": 2.355}, {"text": "and so all of those words,", "start": 1853.05, "duration": 1.245}, {"text": "what fraction of those words is the word drugs?\"", "start": 1854.295, "duration": 2.565}, {"text": "And that's, uh, your estimate of the chance of the word drugs", "start": 1856.86, "duration": 3.48}, {"text": "appearing in the non-spam email in some position in that email, right?", "start": 1860.34, "duration": 3.735}, {"text": "And so, um, in math,", "start": 1864.075, "duration": 2.175}, {"text": "the denominator is sum of your training set, indicator is not spam,", "start": 1866.25, "duration": 5.295}, {"text": "times the number of words in that email.", "start": 1871.545, "duration": 2.685}, {"text": "So the denominator ends up being", "start": 1874.23, "duration": 3.0}, {"text": "the total number of words in all of your non-spam emails in your training set,", "start": 1877.23, "duration": 4.095}, {"text": "um, and the numerator as some of your training set,", "start": 1881.325, "duration": 3.525}, {"text": "sum from i equals 1 through m,", "start": 1884.85, "duration": 2.34}, {"text": "indicates a y equals 0.", "start": 1887.19, "duration": 2.76}, {"text": "So, you know, count up only the things for non-spam email,", "start": 1889.95, "duration": 4.17}, {"text": "and for the non-spam email j equals 1 through ni,", "start": 1894.12, "duration": 3.75}, {"text": "go over the words in that email and see how many words are that word k. Right.", "start": 1897.87, "duration": 5.52}, {"text": "And so, uh, uh, if in your training set you have,", "start": 1903.39, "duration": 3.285}, {"text": "um, uh, ah, you know,", "start": 1906.675, "duration": 2.31}, {"text": "100,000 words in your non-spam emails and 200 of them are the word drugs,", "start": 1908.985, "duration": 6.105}, {"text": "that occurs, uh, you know,", "start": 1915.09, "duration": 1.455}, {"text": "200 times, then this ratio will be 200 over 100,000.", "start": 1916.545, "duration": 3.39}, {"text": "Okay? Oh, and then lastly, um,", "start": 1919.935, "duration": 6.825}, {"text": "[NOISE] to implement Laplace smoothing with this, you would,", "start": 1926.76, "duration": 5.25}, {"text": "um, add 1 to the numerator as usual,", "start": 1932.01, "duration": 3.78}, {"text": "and then, um, let's see.", "start": 1935.79, "duration": 3.105}, {"text": "Actually, what- what- what- what would you add to the denominator? Uh-", "start": 1938.895, "duration": 12.03}, {"text": "Uh, wait. But what is k?", "start": 1950.925, "duration": 2.205}, {"text": "Not k, right? k is a variable.", "start": 1953.13, "duration": 1.98}, {"text": "So k indexes into, ah,", "start": 1955.11, "duration": 2.1}, {"text": "the words? What do you have?", "start": 1957.21, "duration": 4.57}, {"text": "About 10,000.", "start": 1963.62, "duration": 1.645}, {"text": "10,000. Cool. How come? Why 10,000?", "start": 1965.265, "duration": 2.595}, {"text": "[inaudible].", "start": 1967.86, "duration": 3.87}, {"text": "Cool. Yeah. Yeah. All right.", "start": 1971.73, "duration": 2.19}, {"text": "Yeah, Right.", "start": 1973.92, "duration": 3.255}, {"text": "Oh, I think I just realized why you say k I think, uh, overloading notation.", "start": 1977.175, "duration": 3.405}, {"text": "When defining the possibility,", "start": 1980.58, "duration": 1.305}, {"text": "I think I used k as the number of possible outcomes.", "start": 1981.885, "duration": 2.37}, {"text": "Yeah, but here k is an index.", "start": 1984.255, "duration": 2.13}, {"text": "Yeah. Right? So, um, uh,", "start": 1986.385, "duration": 2.355}, {"text": "see I want a numerator and add to number of the", "start": 1988.74, "duration": 2.88}, {"text": "possible outcomes in the denominator which in this case was there 10,000.", "start": 1991.62, "duration": 3.495}, {"text": "So, um, uh, so this is the probability of, um,", "start": 1995.115, "duration": 5.25}, {"text": "X being equal to the value of k,", "start": 2000.365, "duration": 3.72}, {"text": "where k ranges from 1-10,000 if you have a dictionary size.", "start": 2004.085, "duration": 6.69}, {"text": "If you have a list of 10,000 words you're modeling.", "start": 2010.775, "duration": 2.07}, {"text": "And so the number of possible values for X is 10,000,", "start": 2012.845, "duration": 4.125}, {"text": "so you add 10,000 to the denominator.", "start": 2016.97, "duration": 2.625}, {"text": "Makes sense? Cool. Yeah. Question?", "start": 2019.595, "duration": 2.625}, {"text": "[inaudible].", "start": 2022.22, "duration": 2.73}, {"text": "Oh, what do you do if the word's not in your dictionary?", "start": 2024.95, "duration": 2.145}, {"text": "So, um, uh, there are two approaches to that.", "start": 2027.095, "duration": 3.09}, {"text": "One is, um, just throw it away.", "start": 2030.185, "duration": 2.34}, {"text": "Just ignore it, disregard it, that's one.", "start": 2032.525, "duration": 2.265}, {"text": "Uh, second approach, is to take the rare words and map them to", "start": 2034.79, "duration": 3.39}, {"text": "a special token which traditionally is denoted UNK for unknown words.", "start": 2038.18, "duration": 4.755}, {"text": "So, um, if in your training set, uh,", "start": 2042.935, "duration": 3.225}, {"text": "you decide to take just the top 10,000 words in- into your dictionary,", "start": 2046.16, "duration": 3.6}, {"text": "then everything that's not in the top 10,000 words can map to", "start": 2049.76, "duration": 2.955}, {"text": "your unknown word token or the unknown words special symbol. Yeah.", "start": 2052.715, "duration": 3.885}, {"text": "[inaudible].", "start": 2056.6, "duration": 5.97}, {"text": "Oh, why did I write the run before?", "start": 2062.57, "duration": 1.365}, {"text": "Oh, this is an indicator function notation.", "start": 2063.935, "duration": 3.18}, {"text": "Uh, uh, so indicator function uh,", "start": 2067.115, "duration": 3.06}, {"text": "boy- so if- if,", "start": 2070.175, "duration": 2.835}, {"text": "um, and so this is- this notation, right?", "start": 2073.01, "duration": 3.84}, {"text": "Means uh- well, so indicator of, you know,", "start": 2076.85, "duration": 3.24}, {"text": "2 equals 1 plus 1. This is true.", "start": 2080.09, "duration": 3.63}, {"text": "An indicator of, you know,", "start": 2083.72, "duration": 2.115}, {"text": "3 equals 5 is- is 0, is false.", "start": 2085.835, "duration": 4.26}, {"text": "So that's the- yeah, um, cool.", "start": 2090.095, "duration": 3.345}, {"text": "Yes, uh, but this is a- this is a little formula", "start": 2093.44, "duration": 4.35}, {"text": "that's either true or false depending on whether y-i is 0.", "start": 2097.79, "duration": 4.14}, {"text": "Uh, I guess if y-i is 01 this- this is the same as not y-i I guess,", "start": 2101.93, "duration": 4.53}, {"text": "so 1 minus y-i will give us 0- yeah.", "start": 2106.46, "duration": 1.98}, {"text": "Cool. Okay great.", "start": 2108.44, "duration": 4.57}, {"text": "Um, all right.", "start": 2113.74, "duration": 2.635}, {"text": "So I think both of the models, ah, ah,", "start": 2116.375, "duration": 2.295}, {"text": "including the details that maximum likelihood estimate are written out in,", "start": 2118.67, "duration": 3.63}, {"text": "um, more detail in the lecture notes.", "start": 2122.3, "duration": 4.275}, {"text": "Um, so, you know,", "start": 2126.575, "duration": 3.915}, {"text": "when would you use the Naive Bayes algorithm.", "start": 2130.49, "duration": 2.67}, {"text": "It turns out Naive Bayes algorithm is actually not", "start": 2133.16, "duration": 1.95}, {"text": "very competitive with other learning algorithms.", "start": 2135.11, "duration": 2.25}, {"text": "Uh, so for most problems you find that logistic regression,um,", "start": 2137.36, "duration": 4.365}, {"text": "will work better in terms of delivering a higher accuracy than Naive Bayes.", "start": 2141.725, "duration": 6.225}, {"text": "But the- the- the advantages of Naive Bayes is, uh,", "start": 2147.95, "duration": 4.05}, {"text": "first it's computationally very efficient,", "start": 2152.0, "duration": 2.01}, {"text": "and second it's relatively quick to implement, right?", "start": 2154.01, "duration": 2.985}, {"text": "And it also doesn't require an iterative gradient descent thing,", "start": 2156.995, "duration": 2.985}, {"text": "and the number of lines of code needed to implement Naive Bayes is relatively small.", "start": 2159.98, "duration": 4.14}, {"text": "So if you are, uh, facing a problem,", "start": 2164.12, "duration": 3.99}, {"text": "way you go is to implement something quick and dirty,", "start": 2168.11, "duration": 3.36}, {"text": "then Naive Bayes is- is maybe a reasonable choice.", "start": 2171.47, "duration": 3.525}, {"text": "Um, and I think,", "start": 2174.995, "duration": 1.845}, {"text": "um, you know as you work on your class projects,", "start": 2176.84, "duration": 3.315}, {"text": "I think some of you", "start": 2180.155, "duration": 1.215}, {"text": "probably a minority will try to invent a new machine learning algorithm,", "start": 2181.37, "duration": 4.619}, {"text": "and write a research paper.", "start": 2185.989, "duration": 1.786}, {"text": "Um, and I think, you know,", "start": 2187.775, "duration": 1.98}, {"text": "inventing the machine learning algorithm is a great thing to do.", "start": 2189.755, "duration": 2.265}, {"text": "It helps a lot of people on a lot different applications so that's one.", "start": 2192.02, "duration": 3.675}, {"text": "Um, the majority of class projects in CS229 will try", "start": 2195.695, "duration": 3.975}, {"text": "to apply a learning algorithm to a project that you care about.", "start": 2199.67, "duration": 5.25}, {"text": "Apply to a research project you're working on somewhere in", "start": 2204.92, "duration": 2.19}, {"text": "Stanford or apply to a fun application you wanna", "start": 2207.11, "duration": 2.85}, {"text": "build or apply to a business application for some of you", "start": 2209.96, "duration": 2.4}, {"text": "taking this on SCPD, taking this remotely.", "start": 2212.36, "duration": 3.015}, {"text": "And if your goal is not to invent a brand new learning algorithm,", "start": 2215.375, "duration": 4.11}, {"text": "but to take the existing algorithms and apply them,", "start": 2219.485, "duration": 2.355}, {"text": "then rule of thumb that's suggested here is, um, ah,", "start": 2221.84, "duration": 3.945}, {"text": "when you get started on a machine learning project,", "start": 2225.785, "duration": 3.165}, {"text": "start by implementing something quick and dirty.", "start": 2228.95, "duration": 2.85}, {"text": "That's been implemented in most complicated possible learning algorithms.", "start": 2231.8, "duration": 2.82}, {"text": "Start by implementing something quickly,", "start": 2234.62, "duration": 1.935}, {"text": "and, uh, train the algorithm,", "start": 2236.555, "duration": 2.055}, {"text": "look at how it performs,", "start": 2238.61, "duration": 1.26}, {"text": "and then use that to deep out the algorithm,", "start": 2239.87, "duration": 2.04}, {"text": "and keep iterating on- on that.", "start": 2241.91, "duration": 2.16}, {"text": "So I think, you know,", "start": 2244.07, "duration": 1.605}, {"text": "we're- we're- that's at Stanford.", "start": 2245.675, "duration": 1.215}, {"text": "So we're very good at coming up with very complicated algorithms.", "start": 2246.89, "duration": 3.225}, {"text": "But if your goal is to make something,", "start": 2250.115, "duration": 2.625}, {"text": "um, work for an application,", "start": 2252.74, "duration": 1.56}, {"text": "rather than inventing a new learning algorithm and publishing a paper", "start": 2254.3, "duration": 2.7}, {"text": "on a new technical, you know, contribution.", "start": 2257.0, "duration": 2.94}, {"text": "If you- if your main goal is, uh,", "start": 2259.94, "duration": 1.425}, {"text": "you're working on an application on- on", "start": 2261.365, "duration": 3.525}, {"text": "understanding news better or improving the environment or estimating prices or whatever.", "start": 2264.89, "duration": 5.01}, {"text": "Uh, and your primary objective is just make an algorithm work.", "start": 2269.9, "duration": 3.54}, {"text": "Then rather than, uh,", "start": 2273.44, "duration": 2.43}, {"text": "building a very complicated algorithm at the onset,", "start": 2275.87, "duration": 3.085}, {"text": "um, I would recommend implementing something quickly,", "start": 2278.955, "duration": 3.245}, {"text": "uh, so that you can then better understand how it's performing,", "start": 2282.2, "duration": 3.929}, {"text": "and then do error analysis which we'll talk about later,", "start": 2286.129, "duration": 2.671}, {"text": "and use that to drive your development.", "start": 2288.8, "duration": 2.25}, {"text": "Um, you know one- one- one analogy I sometimes make is that,", "start": 2291.05, "duration": 5.52}, {"text": "um, if you are,", "start": 2296.57, "duration": 2.61}, {"text": "uh, uh, let's see.", "start": 2299.18, "duration": 2.625}, {"text": "So if you're writing a new computer program with 10,000 lines of code, right?", "start": 2301.805, "duration": 4.98}, {"text": "One approach is to write all 10,000 lines of code first,", "start": 2306.785, "duration": 3.72}, {"text": "and then to try compiling it for the first time, right.", "start": 2310.505, "duration": 3.15}, {"text": "And that's clearly a bad idea, right?", "start": 2313.655, "duration": 1.695}, {"text": "And it's a, you know, you should write small modules, run it,", "start": 2315.35, "duration": 2.94}, {"text": "it test it- unit testing,", "start": 2318.29, "duration": 1.65}, {"text": "and then build up a program incrementally.", "start": 2319.94, "duration": 1.62}, {"text": "Rather than write 10,000 lines of code,", "start": 2321.56, "duration": 1.65}, {"text": "and then start to see what syntax errors you're getting for the first time.", "start": 2323.21, "duration": 3.285}, {"text": "Um, and I think it's similar for machine learning.", "start": 2326.495, "duration": 2.79}, {"text": "Uh, instead of building a very complicated algorithm from the get-go, um,", "start": 2329.285, "duration": 4.26}, {"text": "you build a simpler algorithm, test it,", "start": 2333.545, "duration": 1.89}, {"text": "and then- and then use the- see what it's doing wrong,", "start": 2335.435, "duration": 3.36}, {"text": "see what it's doing wrong to improve from there.", "start": 2338.795, "duration": 2.385}, {"text": "You often end up, um, uh,", "start": 2341.18, "duration": 2.28}, {"text": "getting to a better performing algorithm faster.", "start": 2343.46, "duration": 3.915}, {"text": "Um, so here's- here's- here's one example.", "start": 2347.375, "duration": 2.43}, {"text": "This is actually something I used to work on.", "start": 2349.805, "duration": 1.8}, {"text": "I- I actually started a conference on email and anti-spam.", "start": 2351.605, "duration": 3.975}, {"text": "My student worked on spam classification many years ago.", "start": 2355.58, "duration": 2.97}, {"text": "And, um, it turns out that when your'e starting out on a new application problem,", "start": 2358.55, "duration": 6.045}, {"text": "um, it's hard to know what's the hardest part of the problem, right.", "start": 2364.595, "duration": 4.395}, {"text": "So if you want to build an anti-spam classifier,", "start": 2368.99, "duration": 2.37}, {"text": "there are lots of you could work on.", "start": 2371.36, "duration": 1.38}, {"text": "For example, spammers will deliberately misspell words.", "start": 2372.74, "duration": 3.735}, {"text": "Uh, you know, a lot of mortgage spam, right,", "start": 2376.475, "duration": 2.25}, {"text": "refinance your mortgage or whatever.", "start": 2378.725, "duration": 1.785}, {"text": "But instead of writing th- the words uh,", "start": 2380.51, "duration": 3.795}, {"text": "mortgage spammers will write M-0-R-T-G-A-G-E.", "start": 2384.305, "duration": 8.605}, {"text": "Right. Or instead of G-A-G-E, maybe,", "start": 2393.01, "duration": 3.85}, {"text": "uh, slash slash, right.", "start": 2396.86, "duration": 3.03}, {"text": "But all of us as people have no trouble reading this as a word mortgage but uh,", "start": 2399.89, "duration": 4.245}, {"text": "this will trip up a spam filter.", "start": 2404.135, "duration": 1.68}, {"text": "This might map the word to- to an unknown word.", "start": 2405.815, "duration": 2.88}, {"text": "There it was off by just a letter and it hasn't seen this before,", "start": 2408.695, "duration": 2.385}, {"text": "and that's the lightest way to slip by this spam filter.", "start": 2411.08, "duration": 2.415}, {"text": "So that's one idea for improving, um,", "start": 2413.495, "duration": 2.88}, {"text": "spam or- actually one of our PhD students [inaudible]", "start": 2416.375, "duration": 3.105}, {"text": "actually wrote a paper mapping this back to words like that.", "start": 2419.48, "duration": 3.51}, {"text": "So the spam filter can see the words the way that humans see them, right.", "start": 2422.99, "duration": 3.33}, {"text": "So- so that's one idea.", "start": 2426.32, "duration": 1.455}, {"text": "Um, another idea might be a lot of spam email spoofs email headers.", "start": 2427.775, "duration": 4.785}, {"text": "[NOISE] You know, uh,", "start": 2432.56, "duration": 4.335}, {"text": "spam has often tried to hide where the email truly came from, uh,", "start": 2436.895, "duration": 4.845}, {"text": "by spoofing the email header that,", "start": 2441.74, "duration": 2.22}, {"text": "you know, address and other information.", "start": 2443.96, "duration": 2.7}, {"text": "Um, ah, an- an- another thing you might do is, ah,", "start": 2446.66, "duration": 3.885}, {"text": "try to fetch the URLs that are referred to in the email,", "start": 2450.545, "duration": 3.015}, {"text": "and then analyze the web pages that you get to.", "start": 2453.56, "duration": 2.175}, {"text": "Right, there are a lot of things that you could do to improve a spam filter.", "start": 2455.735, "duration": 4.035}, {"text": "And any one of these topics could easily be three months or six months of research.", "start": 2459.77, "duration": 5.085}, {"text": "But when you are building say a new spam filter for the first time,", "start": 2464.855, "duration": 3.015}, {"text": "how do you actually know which of these is the best investments of your time.", "start": 2467.87, "duration": 3.555}, {"text": "So my advice to, ah,", "start": 2471.425, "duration": 2.255}, {"text": "those who work on projects,", "start": 2473.68, "duration": 1.35}, {"text": "if your primary goal is to just get this thing to work,", "start": 2475.03, "duration": 2.325}, {"text": "is to not so-somewhat arbitrarily dive in,", "start": 2477.355, "duration": 3.75}, {"text": "and spend six months on improving this or spend,", "start": 2481.105, "duration": 3.675}, {"text": "you know, six months on trying to analyze email headers.", "start": 2484.78, "duration": 4.015}, {"text": "But you instead implement a more basic algorithm.", "start": 2488.795, "duration": 2.46}, {"text": "Almost implement something quick and dirty.", "start": 2491.255, "duration": 1.995}, {"text": "And then look at the examples that your learning algorithm is still misclassifying.", "start": 2493.25, "duration": 4.2}, {"text": "And you'll find that, if after you've implemented a quick and dirty algorithm,", "start": 2497.45, "duration": 3.54}, {"text": "you find that your sp- anti-spam algorithm is", "start": 2500.99, "duration": 2.745}, {"text": "misclassifying a lot of examples with these deliberately misspelled words.", "start": 2503.735, "duration": 3.39}, {"text": "It's only then that you have more evidence that it's worth", "start": 2507.125, "duration": 2.82}, {"text": "spending a bunch of time solving the misspelled words,", "start": 2509.945, "duration": 3.045}, {"text": "the deliberately misspelled words problem.", "start": 2512.99, "duration": 2.025}, {"text": "Right. When you implement a spam filter,", "start": 2515.015, "duration": 1.71}, {"text": "and you see that it's not misclassifying a lot of examples of these misspelled words,", "start": 2516.725, "duration": 3.945}, {"text": "then I would say don't bother.", "start": 2520.67, "duration": 1.155}, {"text": "Go work on something else instead or at least- at least treat that as a low priority.", "start": 2521.825, "duration": 4.185}, {"text": "Okay. So one of the uses of, um,", "start": 2526.01, "duration": 3.51}, {"text": "GDA Gaussian discriminant analysis as well as Naive Bayes is that- is,", "start": 2529.52, "duration": 3.945}, {"text": "uh, they're not going to be the most accurate algorithms.", "start": 2533.465, "duration": 2.955}, {"text": "If you want the highest classification accuracy,", "start": 2536.42, "duration": 2.78}, {"text": "their are other algorithms like logistic regression or SVM", "start": 2539.2, "duration": 3.09}, {"text": "which we talked about, or neural networks we'll talk about later,", "start": 2542.29, "duration": 3.09}, {"text": "which will almost always give you higher classification accuracy than these algorithms.", "start": 2545.38, "duration": 3.9}, {"text": "But the advantage of Gaussian discriminant analysis,", "start": 2549.28, "duration": 2.784}, {"text": "and Naive Bayes is that, um,", "start": 2552.064, "duration": 2.206}, {"text": "they are very quick to train or it's non-iterative.", "start": 2554.27, "duration": 4.05}, {"text": "Uh, uh, this is just counting,", "start": 2558.32, "duration": 1.455}, {"text": "and GDA is just computing means and co-variances, right.", "start": 2559.775, "duration": 3.395}, {"text": "So it's very competition efficient,", "start": 2563.17, "duration": 1.8}, {"text": "and also they are- they are simple to implement.", "start": 2564.97, "duration": 2.64}, {"text": "So it can help you implement that quick and dirty thing that helps you,", "start": 2567.61, "duration": 4.245}, {"text": "um, get going more quickly.", "start": 2571.855, "duration": 3.005}, {"text": "And so I think for your project as well,", "start": 2574.86, "duration": 2.495}, {"text": "I would advise most of you to uh, uh,", "start": 2577.355, "duration": 3.465}, {"text": "you know, as you start working on your project,", "start": 2580.82, "duration": 2.115}, {"text": "I would advise most of you to, um,", "start": 2582.935, "duration": 2.445}, {"text": "don't spend weeks designing exactly what you're going to do.", "start": 2585.38, "duration": 3.465}, {"text": "Uh, if you have an applicant- if- if you- if you- if you have an applied project,", "start": 2588.845, "duration": 2.925}, {"text": "but instead get a data set,", "start": 2591.77, "duration": 2.085}, {"text": "uh, and apply something simple.", "start": 2593.855, "duration": 1.62}, {"text": "Start with logistic regression not- not", "start": 2595.475, "duration": 2.145}, {"text": "a neural network or not- not something more complicated.", "start": 2597.62, "duration": 2.61}, {"text": "Or start with Naive Bayes,", "start": 2600.23, "duration": 1.485}, {"text": "and then see how that performs,", "start": 2601.715, "duration": 1.455}, {"text": "and then- and then go from there.", "start": 2603.17, "duration": 2.175}, {"text": "Okay? All right.", "start": 2605.345, "duration": 3.66}, {"text": "So that's it for,", "start": 2609.005, "duration": 1.245}, {"text": "uh, Naive Bayes, um,", "start": 2610.25, "duration": 2.325}, {"text": "and generative learning algorithms.", "start": 2612.575, "duration": 3.195}, {"text": "The next thing I wanna do is move on to a different cla- type of classifier, ah,", "start": 2615.77, "duration": 4.71}, {"text": "which is a support vector machine.", "start": 2620.48, "duration": 1.634}, {"text": "Um, let me just check any questions about this before I move on. Yeah.", "start": 2622.114, "duration": 5.026}, {"text": "[inaudible].", "start": 2627.14, "duration": 19.71}, {"text": "Sorry you can use logistic regression with.", "start": 2646.85, "duration": 2.37}, {"text": "[OVERLAPPING] Discrete variables [inaudible]", "start": 2649.22, "duration": 12.46}, {"text": "Oh I see yeah right yes so yes, uh, right.", "start": 2661.68, "duration": 4.38}, {"text": "So one of the weaknesses of", "start": 2666.06, "duration": 1.65}, {"text": "the Naive Bayes Algorithm is that it treats all of the words as completely,", "start": 2667.71, "duration": 3.27}, {"text": "you know, separate from each other.", "start": 2670.98, "duration": 2.175}, {"text": "And so the words one and two are quite similar and the words,", "start": 2673.155, "duration": 3.315}, {"text": "you know, like mother and father are quite similar.", "start": 2676.47, "duration": 2.88}, {"text": "Uh, and so wi- wi-with this, uh,", "start": 2679.35, "duration": 3.285}, {"text": "feature representation, it doesn't know the relationship between these words.", "start": 2682.635, "duration": 3.345}, {"text": "So, um, in machine learning there are other ways of representing words,", "start": 2685.98, "duration": 3.75}, {"text": "uh, there's a technique called word embeddings,", "start": 2689.73, "duration": 2.07}, {"text": "um-[NOISE] In which you", "start": 2691.8, "duration": 6.06}, {"text": "choose the feature representation that encodes", "start": 2697.86, "duration": 2.25}, {"text": "the fact that the words one and two are quite similar to each other.", "start": 2700.11, "duration": 2.94}, {"text": "Uh, the words mother and father are quite similar to each other.", "start": 2703.05, "duration": 2.835}, {"text": "Yeah the words, um,", "start": 2705.885, "duration": 1.875}, {"text": "whatever London and Tokyo are", "start": 2707.76, "duration": 2.43}, {"text": "quite similar to each other because they are both city names.", "start": 2710.19, "duration": 2.13}, {"text": "Uh, and so, uh,", "start": 2712.32, "duration": 1.785}, {"text": "this is a technique that I was not planning to teach here but that is taught in CS 230.", "start": 2714.105, "duration": 6.24}, {"text": "So in- in- in neural networks [NOISE] , right,", "start": 2720.345, "duration": 4.11}, {"text": "but you can also read up on word embeddings or look at some of", "start": 2724.455, "duration": 3.105}, {"text": "the videos and resources from CS 230 if you want to learn about that.", "start": 2727.56, "duration": 3.81}, {"text": "Uh, so the word embeddings techniques.", "start": 2731.37, "duration": 2.37}, {"text": "These are techniques from neural networks really.", "start": 2733.74, "duration": 1.695}, {"text": "Will reduce the number of training examples you need so they are", "start": 2735.435, "duration": 2.325}, {"text": "a good text classifier because it comes in with", "start": 2737.76, "duration": 2.34}, {"text": "more knowledge baked in, right. Cool. Anything else?", "start": 2740.1, "duration": 5.28}, {"text": "[NOISE] Cool.", "start": 2745.38, "duration": 5.19}, {"text": "By the way I do this in the other classes too.", "start": 2750.57, "duration": 2.49}, {"text": "In some of the other classes,", "start": 2753.06, "duration": 1.11}, {"text": "somebody's got a question they go,", "start": 2754.17, "duration": 1.29}, {"text": "no we don't do that we just covered that in", "start": 2755.46, "duration": 1.665}, {"text": "CS 229 so", "start": 2757.125, "duration": 1.035}, {"text": "[LAUGHTER].", "start": 2758.16, "duration": 13.5}, {"text": "Actually CS224N I think also covers this.", "start": 2771.66, "duration": 2.64}, {"text": "Yeah, The NLP class, yeah, pretty sure, actually I am sure they do.", "start": 2774.3, "duration": 5.05}, {"text": "Okay so,", "start": 2782.45, "duration": 3.01}, {"text": "[NOISE] su-support vector machines, SVMs.", "start": 2785.46, "duration": 9.21}, {"text": "Um, let's say the classification problem,", "start": 2794.67, "duration": 6.33}, {"text": "[NOISE].", "start": 2801.0, "duration": 8.94}, {"text": "Right, where the data set looks like this, uh,", "start": 2809.94, "duration": 2.595}, {"text": "and so you want an algorithm to find, you know,", "start": 2812.535, "duration": 3.27}, {"text": "like a nonlinear decision boundary, right?", "start": 2815.805, "duration": 3.255}, {"text": "So the support vector machine will be an algorithm to help us", "start": 2819.06, "duration": 3.48}, {"text": "find potentially very very non-linear decision boundaries like this.", "start": 2822.54, "duration": 3.885}, {"text": "Now one way to build a classifier like this would be to use logistic regression.", "start": 2826.425, "duration": 5.13}, {"text": "But if this is X 1, this is X 2, right,", "start": 2831.555, "duration": 3.885}, {"text": "so logistic regression will fit the three lines of data,", "start": 2835.44, "duration": 3.93}, {"text": "Gaussian discriminant analysis will end up with a straight line decision boundary.", "start": 2839.37, "duration": 3.135}, {"text": "So one way to apply logistic regression like this would be to take", "start": 2842.505, "duration": 3.015}, {"text": "your feature vector X 1 X 2 and map it to a high dimensional feature vector with,", "start": 2845.52, "duration": 5.97}, {"text": "you know, X 1, X 2, X 1 squared,", "start": 2851.49, "duration": 3.09}, {"text": "X 2 squared X 1, X 2 maybe X 1 cubed,", "start": 2854.58, "duration": 3.975}, {"text": "X 2 cubed and so on.", "start": 2858.555, "duration": 1.455}, {"text": "And have a new feature vector which we would call phi of", "start": 2860.01, "duration": 3.0}, {"text": "x. That- that has these high-dimensional features right, now, um,", "start": 2863.01, "duration": 5.355}, {"text": "it turns out if you do this and then apply", "start": 2868.365, "duration": 2.715}, {"text": "logistic regression to this augmented feature vector, uh,", "start": 2871.08, "duration": 4.02}, {"text": "then logistic regression can learn non-linear decision boundaries.", "start": 2875.1, "duration": 3.611}, {"text": "Uh, with these other features it's just regression", "start": 2878.711, "duration": 2.329}, {"text": "and you actually learn the decision boundary.", "start": 2881.04, "duration": 1.77}, {"text": "This is- there's a- there's a shape of an ellipse, right.", "start": 2882.81, "duration": 2.88}, {"text": "Um, but randomly choosing", "start": 2885.69, "duration": 2.85}, {"text": "these features is little bit of a pain right. I- I- I don't know.", "start": 2888.54, "duration": 3.105}, {"text": "What I- I- I actually don't know what,", "start": 2891.645, "duration": 2.64}, {"text": "you know, type of a,", "start": 2894.285, "duration": 1.095}, {"text": "uh, set of features could get you a decision boundary like that right.", "start": 2895.38, "duration": 5.025}, {"text": "Rather than just an ellipse and more complex as your boundary.", "start": 2900.405, "duration": 2.925}, {"text": "Um, and what we will see with support vector machines is that we", "start": 2903.33, "duration": 5.04}, {"text": "will be able to derive an algorithm that can take say input features X 1, X 2,", "start": 2908.37, "duration": 5.235}, {"text": "map them to a much higher dimensional set of features.", "start": 2913.605, "duration": 4.665}, {"text": "Uh, and then apply a linear classifier,", "start": 2918.27, "duration": 2.835}, {"text": "uh, in a way similar to logistic regression.", "start": 2921.105, "duration": 2.415}, {"text": "But different in details that allows you to learn very non-linear decision boundaries.", "start": 2923.52, "duration": 5.46}, {"text": "Okay. Um, and I think,", "start": 2928.98, "duration": 2.16}, {"text": "uh, you know, a support vector machine,", "start": 2931.14, "duration": 2.01}, {"text": "one of the- actually one of the reasons, uh,", "start": 2933.15, "duration": 2.235}, {"text": "support vector machines are used today is- is a relatively turn-key algorithm.", "start": 2935.385, "duration": 4.365}, {"text": "And what I mean by that is it doesn't have too many parameters to fiddle with.", "start": 2939.75, "duration": 3.9}, {"text": "Uh, even for logistic regression or for linear regression.", "start": 2943.65, "duration": 4.15}, {"text": "You know you might have to tune the gradient descent parameter,", "start": 2947.8, "duration": 3.46}, {"text": "uh, tune the learning rate sorry, tune the learning rate alpha.", "start": 2951.26, "duration": 2.565}, {"text": "And that's just another thing to fit in with.", "start": 2953.825, "duration": 1.665}, {"text": "We`ll try a few values and hope you didn't mess up how you set that value.", "start": 2955.49, "duration": 3.66}, {"text": "Um, support vector machine today has a very, uh,", "start": 2959.15, "duration": 5.035}, {"text": "robust, very mature software packages that you", "start": 2964.185, "duration": 3.345}, {"text": "can just download to train the support vector machine on- on any on,", "start": 2967.53, "duration": 3.405}, {"text": "you know, on a problem and you just run it and the algorithm will,", "start": 2970.935, "duration": 2.895}, {"text": "kind of, converge without you having to worry too much about the details.", "start": 2973.83, "duration": 3.6}, {"text": "Um, so I think in the grand scheme of things today I would say", "start": 2977.43, "duration": 3.51}, {"text": "support vector machines are not as effective as neural networks for many problems.", "start": 2980.94, "duration": 4.53}, {"text": "But, um, uh, but one great property of support vector machines is- is- is turn key.", "start": 2985.47, "duration": 5.27}, {"text": "You kind of just turn the key and it works and there isn't as", "start": 2990.74, "duration": 2.49}, {"text": "many parameters like the learning rate and other things that you had to fiddle with.", "start": 2993.23, "duration": 4.365}, {"text": "Okay, um so the road map is,", "start": 2997.595, "duration": 10.705}, {"text": "uh, we're going to develop the following set of ideas.", "start": 3008.3, "duration": 4.035}, {"text": "We talked about the optimal [NOISE] margin classifier today, and, uh,", "start": 3012.335, "duration": 8.61}, {"text": "we'll start with the separable case", "start": 3020.945, "duration": 5.415}, {"text": "and what that means is going to start off with datasets,", "start": 3026.36, "duration": 4.12}, {"text": "um, that we assume look like this and that are linearly separable.", "start": 3030.76, "duration": 5.084}, {"text": "Right, and so the optimal margin classifier is", "start": 3035.844, "duration": 3.026}, {"text": "the basic building block for the support vector machine,", "start": 3038.87, "duration": 2.445}, {"text": "and, uh, we'll first derive an algorithm, uh,", "start": 3041.315, "duration": 4.17}, {"text": "that' ll be- that will have some similarities to", "start": 3045.485, "duration": 2.145}, {"text": "logistic regression but that allows us to scale, uh,", "start": 3047.63, "duration": 3.975}, {"text": "in important ways that to find a linear classifier", "start": 3051.605, "duration": 3.495}, {"text": "for training sets like this that we assume for now can be linearly separated.", "start": 3055.1, "duration": 4.395}, {"text": "Um, so we'll do that today.", "start": 3059.495, "duration": 2.01}, {"text": "And then what you'll see on Wednesday is, um,", "start": 3061.505, "duration": 4.44}, {"text": "excuse me, next Monday,", "start": 3065.945, "duration": 1.65}, {"text": "which is next Monday is an idea called kernels.", "start": 3067.595, "duration": 4.065}, {"text": "And the kernel idea is one of the most powerful ideas in machine learning.", "start": 3071.66, "duration": 4.05}, {"text": "Is, um, how do you take a feature vector x, maybe this is R 2,", "start": 3075.71, "duration": 5.22}, {"text": "right, and map it to a much higher dimensional set of features.", "start": 3080.93, "duration": 6.63}, {"text": "In our example there that was R 5,", "start": 3087.56, "duration": 3.39}, {"text": "right, and then train an algorithm on this high dimensional set of features.", "start": 3090.95, "duration": 5.31}, {"text": "And- and the cool thing about kernels is that", "start": 3096.26, "duration": 2.34}, {"text": "this high dimensional set of features may not be R 5.", "start": 3098.6, "duration": 3.045}, {"text": "It might be R100,000 or it might even be R infinite.", "start": 3101.645, "duration": 6.33}, {"text": "Um, and so with the kernel formulation we're gonna take", "start": 3107.975, "duration": 3.84}, {"text": "our original set of features that you are given for the houses you're trying to sell.", "start": 3111.815, "duration": 4.485}, {"text": "For, uh, you know,", "start": 3116.3, "duration": 1.395}, {"text": "medical conditions you're trying to predict and map", "start": 3117.695, "duration": 2.175}, {"text": "this two-dimensional feature vector space", "start": 3119.87, "duration": 2.25}, {"text": "into maybe infinite dimensional set of features.", "start": 3122.12, "duration": 3.165}, {"text": "And, um, what this does is it relieves us", "start": 3125.285, "duration": 2.775}, {"text": "from a lot of the burden of manually picking features,", "start": 3128.06, "duration": 2.76}, {"text": "right, like do you want to have square root of X 1", "start": 3130.82, "duration": 2.7}, {"text": "or maybe X 1, X 2 to the power of two thirds.", "start": 3133.52, "duration": 2.91}, {"text": "So you just don't have to fiddle with these features too much", "start": 3136.43, "duration": 2.835}, {"text": "because the kernels will allow you to choose an infinitely large set of features.", "start": 3139.265, "duration": 5.25}, {"text": "Okay, um, and then finally,", "start": 3144.515, "duration": 3.075}, {"text": "uh, we'll talk about the inseparable case.", "start": 3147.59, "duration": 2.7}, {"text": "[NOISE] So we're gonna do this today and then", "start": 3150.29, "duration": 5.25}, {"text": "this next, uh, Monday okay.", "start": 3155.54, "duration": 6.52}, {"text": "So [NOISE] and by the way I,", "start": 3173.62, "duration": 3.595}, {"text": "you know, th-the machine learning world has become a little bit funny.", "start": 3177.215, "duration": 3.195}, {"text": "I think that if you read in the news", "start": 3180.41, "duration": 3.11}, {"text": "the media talks a lot about machine learning, the media just talks about,", "start": 3183.52, "duration": 3.54}, {"text": "you know, neural networks all the time, right?", "start": 3187.06, "duration": 1.845}, {"text": "And you'll hear about neural networks and deep learning a little bit later in this class.", "start": 3188.905, "duration": 3.105}, {"text": "But if you look at what actually happens in practice in machine learning.", "start": 3192.01, "duration": 4.305}, {"text": "Uh, the set of algorithms is actually used in practice,", "start": 3196.315, "duration": 3.355}, {"text": "is actually much wider than neural networks and deep learning.", "start": 3199.67, "duration": 2.7}, {"text": "So- so we do not live in a neural networks only world.", "start": 3202.37, "duration": 3.375}, {"text": "We actually use many, many tools in machine learning.", "start": 3205.745, "duration": 2.79}, {"text": "It's just that deep learning attracts the attention of the media in", "start": 3208.535, "duration": 4.455}, {"text": "some way there's quite disproportionate to what I find useful, you know,", "start": 3212.99, "duration": 5.76}, {"text": "I knew that's like- I loved that,", "start": 3218.75, "duration": 1.83}, {"text": "you know but- but they're not- they're not the only thing in the world,", "start": 3220.58, "duration": 3.39}, {"text": "uh, and so yeah and then late last night I was talking to an engineer, uh,", "start": 3223.97, "duration": 4.83}, {"text": "about factor analysis which we'll learn about later in CS229", "start": 3228.8, "duration": 2.76}, {"text": "right, there's an unsupervised learning algorithm and there's an application,", "start": 3231.56, "duration": 3.649}, {"text": "uh, that one of my teams is working on in manufacturing.", "start": 3235.209, "duration": 3.451}, {"text": "Where we're gonna use factor analysis or something very similar to it.", "start": 3238.66, "duration": 2.805}, {"text": "Which- which is totally not a neural network technique.", "start": 3241.465, "duration": 2.735}, {"text": "Right. But still there, there are all these other techniques that including", "start": 3244.2, "duration": 3.55}, {"text": "support vector machines and Naive Bayes I think do get used and are important.", "start": 3247.75, "duration": 5.65}, {"text": "All right so let's start developing the optimal margin classifier.", "start": 3254.52, "duration": 6.47}, {"text": "[NOISE]", "start": 3260.99, "duration": 5.895}, {"text": "So, um, first, let me define the functional margin,", "start": 3266.885, "duration": 5.13}, {"text": "which is, uh, informally,", "start": 3272.015, "duration": 2.25}, {"text": "the functional margin of the classifier is how well- how,", "start": 3274.265, "duration": 3.84}, {"text": "how confidently and accurately do you classify an example.", "start": 3278.105, "duration": 4.035}, {"text": "Um, so here's what I mean.", "start": 3282.14, "duration": 1.725}, {"text": "Uh, we're gonna go to binary classification,", "start": 3283.865, "duration": 3.39}, {"text": "and we're gonna use logistic regression, right?", "start": 3287.255, "duration": 3.165}, {"text": "So, so let's, let's start by motivating this with logistic regression [NOISE].", "start": 3290.42, "duration": 6.15}, {"text": "So this, this is a classifier H of theta", "start": 3296.57, "duration": 2.265}, {"text": "equals the logistic function of pi to theta transpose x.", "start": 3298.835, "duration": 2.94}, {"text": "And so, um, if you turn this into a binary classification, if, if,", "start": 3301.775, "duration": 5.7}, {"text": "if you have this algorithm predict not a probability but predict 0 or 1,", "start": 3307.475, "duration": 4.605}, {"text": "then what this classifier will do is, uh, predict 1.", "start": 3312.08, "duration": 4.09}, {"text": "If theta transpose x is greater than 0, right?", "start": 3317.47, "duration": 6.052}, {"text": "Um, and predict 0 otherwise.", "start": 3323.522, "duration": 6.378}, {"text": "Okay. Because theta transpose x greater than 0, this means that,", "start": 3329.9, "duration": 4.635}, {"text": "um, g of theta transpose x is greater than 0.5 [NOISE],", "start": 3334.535, "duration": 6.165}, {"text": "and you can have greater than or greater than equal to, it doesn't matter.", "start": 3340.7, "duration": 2.76}, {"text": "It is, it's exactly 0.5,", "start": 3343.46, "duration": 1.725}, {"text": "it doesn't really matter what you do.", "start": 3345.185, "duration": 1.74}, {"text": "Um, and so you predict 1 if theta transpose x is greater than equal to 0,", "start": 3346.925, "duration": 5.46}, {"text": "meaning that the upper probability- the estimated probability of", "start": 3352.385, "duration": 2.835}, {"text": "a class  being 1 is greater than 50/50, and so you predict 1.", "start": 3355.22, "duration": 3.705}, {"text": "And if theta transpose x is less than 0,", "start": 3358.925, "duration": 2.865}, {"text": "then you predict that this class is 0.", "start": 3361.79, "duration": 1.935}, {"text": "Okay. So this is what will happen if you have, um,", "start": 3363.725, "duration": 3.045}, {"text": "logistic regression output 1 or 0 rather than output a probability, right.", "start": 3366.77, "duration": 5.1}, {"text": "So in other words,", "start": 3371.87, "duration": 3.285}, {"text": "this means that if y_i is equal to 1, right?", "start": 3375.155, "duration": 6.465}, {"text": "Then hope or we want that", "start": 3381.62, "duration": 5.55}, {"text": "theta transpose x_i is much greater than 0.", "start": 3387.17, "duration": 6.81}, {"text": "Uh, this double greater than sign,", "start": 3393.98, "duration": 2.22}, {"text": "it means much greater, right?", "start": 3396.2, "duration": 1.75}, {"text": "Um, uh, because if the true label is 1,", "start": 3397.95, "duration": 4.205}, {"text": "then if the algorithm is doing well,", "start": 3402.155, "duration": 2.43}, {"text": "hopefully theta transpose x, right?", "start": 3404.585, "duration": 3.99}, {"text": "Will be faster there, right?", "start": 3408.575, "duration": 1.455}, {"text": "So the output probability is very,", "start": 3410.03, "duration": 1.62}, {"text": "very close to 1.", "start": 3411.65, "duration": 1.335}, {"text": "And if indeed theta transpose x is much greater than 0,", "start": 3412.985, "duration": 3.69}, {"text": "then g of theta transpose x will be very close to 1 which means that is,", "start": 3416.675, "duration": 5.175}, {"text": "it's giving a very good, very accurate prediction.", "start": 3421.85, "duration": 2.655}, {"text": "Very correct and confident prediction, right?", "start": 3424.505, "duration": 3.09}, {"text": "That, that equals 1.", "start": 3427.595, "duration": 1.755}, {"text": "Um, and if y_i is equal to 0,", "start": 3429.35, "duration": 4.47}, {"text": "then what we want or what we hope,", "start": 3433.82, "duration": 2.55}, {"text": "is that theta transpose xi is much less than 0, right?", "start": 3436.37, "duration": 5.145}, {"text": "Because, uh, if this is true,", "start": 3441.515, "duration": 1.395}, {"text": "then the algorithm is doing very well on this example.", "start": 3442.91, "duration": 2.91}, {"text": "Okay.", "start": 3445.82, "duration": 1.72}, {"text": "So, um.", "start": 3449.44, "duration": 8.83}, {"text": "So the functional margin which we'll define in a second,", "start": 3458.27, "duration": 3.405}, {"text": "uh, captures [NOISE] this idea that if a classifier has a large functional margin,", "start": 3461.675, "duration": 5.865}, {"text": "it means that these two statements are true, right?", "start": 3467.54, "duration": 4.5}, {"text": "Um, so looking ahead a little bit,", "start": 3472.04, "duration": 3.585}, {"text": "there's a different thing we'll define in a second", "start": 3475.625, "duration": 3.15}, {"text": "which is called the geometric margin and that's the following.", "start": 3478.775, "duration": 4.92}, {"text": "And for now, let's assume the data is,", "start": 3483.695, "duration": 1.875}, {"text": "is linearly separable.", "start": 3485.57, "duration": 3.01}, {"text": "Okay. Um, right.", "start": 3489.13, "duration": 3.32}, {"text": "So let's say that's the data set.", "start": 3495.82, "duration": 2.5}, {"text": "[NOISE]", "start": 3498.32, "duration": 1.29}, {"text": "Now,", "start": 3499.61, "duration": 1.23}, {"text": "[NOISE]", "start": 3500.84, "duration": 8.29}, {"text": "that seems like a pretty good decision boundary", "start": 3509.13, "duration": 2.135}, {"text": "for separating the positive [NOISE] and negative examples.", "start": 3511.265, "duration": 2.505}, {"text": "[NOISE] Um, that's another decision boundary in red,", "start": 3513.77, "duration": 6.12}, {"text": "that also separates the positive negative examples.", "start": 3519.89, "duration": 2.01}, {"text": "But somehow the green line looks much better than the red line, okay?", "start": 3521.9, "duration": 4.29}, {"text": "So, uh, why is that?", "start": 3526.19, "duration": 2.22}, {"text": "Well, the red line comes really close to a few of the training examples,", "start": 3528.41, "duration": 6.225}, {"text": "whereas the green line,", "start": 3534.635, "duration": 3.435}, {"text": "you know, has a much bigger separation, right?", "start": 3538.07, "duration": 4.64}, {"text": "Just has a much bigger distance from the positive and negative examples.", "start": 3542.71, "duration": 2.85}, {"text": "So even though the red line and the [NOISE] green line both, you know,", "start": 3545.56, "duration": 5.16}, {"text": "perfectly separate the positive and negative examples,", "start": 3550.72, "duration": 2.43}, {"text": "the green line has a much bigger separation,", "start": 3553.15, "duration": 5.26}, {"text": "uh, which is called the geometric margin.", "start": 3558.41, "duration": 1.68}, {"text": "But there's a much bigger geometric margin meaning a physical separation", "start": 3560.09, "duration": 3.15}, {"text": "from the trained examples even as it separates them.", "start": 3563.24, "duration": 3.915}, {"text": "Okay. Um, and so what I'd like to do in the,", "start": 3567.155, "duration": 5.595}, {"text": "uh, next several, I guess in the next, next,", "start": 3572.75, "duration": 3.33}, {"text": "next 20 minutes is formalize definite functional margin,", "start": 3576.08, "duration": 2.85}, {"text": "formalize definition geometric margin,", "start": 3578.93, "duration": 2.415}, {"text": "and it will pose the, the,", "start": 3581.345, "duration": 1.695}, {"text": "I guess the optimal margin classifier which based in", "start": 3583.04, "duration": 2.45}, {"text": "the algorithm that tries to maximize the geometric margin.", "start": 3585.49, "duration": 3.0}, {"text": "So what the rudimentary SVM does,", "start": 3588.49, "duration": 2.49}, {"text": "what the SVM and low-dimensional spaces will do,", "start": 3590.98, "duration": 2.28}, {"text": "also called the optimal margin classifier,", "start": 3593.26, "duration": 2.055}, {"text": "is pose an optimization problem to try to find", "start": 3595.315, "duration": 3.375}, {"text": "the green line to classify these examples, okay?", "start": 3598.69, "duration": 4.225}, {"text": "So, um, [NOISE] now,", "start": 3602.915, "duration": 7.065}, {"text": "um, in order to develop SVMs,", "start": 3609.98, "duration": 2.385}, {"text": "I'm going to change the notation a little bit again.", "start": 3612.365, "duration": 2.22}, {"text": "You know, because these algorithms have different properties, um,", "start": 3614.585, "duration": 3.69}, {"text": "using slightly different notation to describe them,", "start": 3618.275, "duration": 2.55}, {"text": "makes then the math a little bit easier.", "start": 3620.825, "duration": 2.07}, {"text": "So when developing SVMs,", "start": 3622.895, "duration": 2.58}, {"text": "we're going to use, um,", "start": 3625.475, "duration": 2.1}, {"text": "minus 1 and plus 1 to denote the class labels.", "start": 3627.575, "duration": 5.76}, {"text": "And, um, we're going to have a H output.", "start": 3633.335, "duration": 9.805}, {"text": "So rather than having a hypothesis output", "start": 3647.47, "duration": 3.085}, {"text": "a probability like you saw in logistic regression,", "start": 3650.555, "duration": 3.78}, {"text": "the support vector machine will output either minus 1 or plus 1.", "start": 3654.335, "duration": 4.05}, {"text": "And so, uh, g of z becomes minus 1 or 1, um, actually.", "start": 3658.385, "duration": 8.85}, {"text": "So output 1 if z is greater than equal to 0,", "start": 3667.235, "duration": 4.875}, {"text": "and minus 1 otherwise, okay.", "start": 3672.11, "duration": 4.74}, {"text": "So instead of a smooth transition from 0 to 1,", "start": 3676.85, "duration": 2.04}, {"text": "we have a hard transition,", "start": 3678.89, "duration": 1.23}, {"text": "an abrupt transition from negative 1 to, um, plus 1.", "start": 3680.12, "duration": 6.06}, {"text": "[NOISE]", "start": 3686.18, "duration": 10.92}, {"text": "And finally, where previously we had for logistic regression, right?", "start": 3697.1, "duration": 11.1}, {"text": "Where, uh, this was R N plus 1 with x_0 equals 1.", "start": 3708.2, "duration": 6.045}, {"text": "For the SVM, we will have h of, I'll just write this out.", "start": 3714.245, "duration": 6.745}, {"text": "Okay. Um, so for the SVM,", "start": 3734.83, "duration": 4.54}, {"text": "the parameters of the SVM will be the parameters w and b.", "start": 3739.37, "duration": 3.18}, {"text": "And hypothesis applied to x will be g of this,", "start": 3742.55, "duration": 4.605}, {"text": "and where dropping the x_0 equals 1 [NOISE] constraint.", "start": 3747.155, "duration": 3.54}, {"text": "So separate out w and b as follows.", "start": 3750.695, "duration": 3.015}, {"text": "So this is a standard notation used to develop support vector machines.", "start": 3753.71, "duration": 3.855}, {"text": "Um, and one way to think about this,", "start": 3757.565, "duration": 1.785}, {"text": "is if the parameters are,", "start": 3759.35, "duration": 1.335}, {"text": "you know, theta 0, theta 1,", "start": 3760.685, "duration": 1.965}, {"text": "theta 2, theta 3,", "start": 3762.65, "duration": 1.695}, {"text": "then this is a new b,", "start": 3764.345, "duration": 2.205}, {"text": "and this is a new w. Okay?", "start": 3766.55, "duration": 2.19}, {"text": "So you just separate out the, the, the, uh,", "start": 3768.74, "duration": 3.405}, {"text": "theta 0 which was previously multiplying to x_o, right?", "start": 3772.145, "duration": 6.945}, {"text": "And so um, uh, yeah, right.", "start": 3779.09, "duration": 3.105}, {"text": "And so this term here becomes sum from i equals 1 through N,", "start": 3782.195, "duration": 6.675}, {"text": "uh, w_i x_i plus b, right?", "start": 3788.87, "duration": 4.02}, {"text": "Since we've gotten rid of [NOISE] x_0.", "start": 3792.89, "duration": 7.23}, {"text": "[NOISE].", "start": 3800.12, "duration": 15.885}, {"text": "All right. So let me formalize the definition of a functional margin.", "start": 3816.005, "duration": 5.925}, {"text": "[NOISE] So um, ah,", "start": 3821.93, "duration": 7.54}, {"text": "so the parameters w and b are defined as linear classifier,", "start": 3829.72, "duration": 4.465}, {"text": "right, so you know,", "start": 3834.185, "duration": 1.23}, {"text": "wh- what- the formulas we just wrote down the parameters w and b,", "start": 3835.415, "duration": 3.315}, {"text": "defines the a- a, uh, uh,", "start": 3838.73, "duration": 1.71}, {"text": "re- really defines a hyperplane.", "start": 3840.44, "duration": 2.01}, {"text": "Ah, but defines a line,", "start": 3842.45, "duration": 1.8}, {"text": "or in high dimensions it'd be a plane or a hyperplane that defines a straight line,", "start": 3844.25, "duration": 4.035}, {"text": "ah, ah, separating out the positive and negative examples.", "start": 3848.285, "duration": 3.18}, {"text": "And so we're gonna say the functional margin of the,", "start": 3851.465, "duration": 4.14}, {"text": "actually my hyperplane [NOISE]", "start": 3855.605, "duration": 26.685}, {"text": "Okay, so the functional margin of", "start": 3882.29, "duration": 2.4}, {"text": "a hyperplane defined by this with respect to one training example.", "start": 3884.69, "duration": 5.445}, {"text": "We're going to write as this,", "start": 3890.135, "duration": 3.03}, {"text": "um, and hyperplane just means straight line,", "start": 3893.165, "duration": 2.37}, {"text": "right, but in high dimension.", "start": 3895.535, "duration": 1.215}, {"text": "So this is linear classifiers, so its just, you know,", "start": 3896.75, "duration": 1.92}, {"text": "functional margin of this classifier with respect to one training example,", "start": 3898.67, "duration": 4.29}, {"text": "we're going to define as this.", "start": 3902.96, "duration": 2.685}, {"text": "And so if you compare this with the equations we had up there,", "start": 3905.645, "duration": 4.125}, {"text": "um, you know, if y equals 1 we hope for that, if y equals 0, we hope for that.", "start": 3909.77, "duration": 3.915}, {"text": "So really what we hope for is for", "start": 3913.685, "duration": 2.505}, {"text": "our classifier to achieve a large functional margin, right?", "start": 3916.19, "duration": 3.615}, {"text": "And so, um, so if y_i equals 1 then what we want or what we hope for,", "start": 3919.805, "duration": 9.495}, {"text": "um, is that w transpose x_i plus b is greater than,", "start": 3929.3, "duration": 7.035}, {"text": "much greater than 0,", "start": 3936.335, "duration": 1.875}, {"text": "and that the label is equal to minus 1.", "start": 3938.21, "duration": 3.09}, {"text": "[NOISE] Then we want or we hope that [NOISE] this is much smaller than 0.", "start": 3941.3, "duration": 8.61}, {"text": "Um, and if you, kind of,", "start": 3949.91, "duration": 2.1}, {"text": "combine these two statements,", "start": 3952.01, "duration": 1.605}, {"text": "if you take y_i,", "start": 3953.615, "duration": 2.335}, {"text": "right, and multiply it with,", "start": 3957.07, "duration": 3.8}, {"text": "er, that, [NOISE] then, you know,", "start": 3961.24, "duration": 3.625}, {"text": "these two statements together is basically saying that you hope that", "start": 3964.865, "duration": 3.135}, {"text": "Gamma hat i is much greater than 0, right,", "start": 3968.0, "duration": 3.69}, {"text": "because y_i now is plus 1 or minus 1 and,", "start": 3971.69, "duration": 4.26}, {"text": "uh, uh, and so y is equal to 1 you want this to be very, very large.", "start": 3975.95, "duration": 4.005}, {"text": "If y_i is negative 1,", "start": 3979.955, "duration": 1.665}, {"text": "you want this to be a very,", "start": 3981.62, "duration": 1.2}, {"text": "very large negative number.", "start": 3982.82, "duration": 1.665}, {"text": "Um, and so either way it's just saying that you hope this would be very large, okay?", "start": 3984.485, "duration": 4.59}, {"text": "So we just hope that.", "start": 3989.075, "duration": 1.335}, {"text": "[NOISE] And- and as an aside,", "start": 3990.41, "duration": 10.17}, {"text": "ah, one property of this as well is that, um,", "start": 4000.58, "duration": 5.98}, {"text": "so long as Gamma hat i is greater than 0,", "start": 4008.85, "duration": 4.69}, {"text": "that means the algorithm,", "start": 4013.54, "duration": 2.445}, {"text": "um, right, is equal to y_i.", "start": 4015.985, "duration": 6.255}, {"text": "[NOISE] Ah, so- so- so long as the, um, functional margin,", "start": 4022.24, "duration": 5.31}, {"text": "so long as this Gamma hat i is greater than 0, it means that,", "start": 4027.55, "duration": 4.71}, {"text": "ah, either this is bigger than 0,", "start": 4032.26, "duration": 2.01}, {"text": "this is less than 0 depending on the sign of the label.", "start": 4034.27, "duration": 4.155}, {"text": "And it means that the algorithm gets", "start": 4038.425, "duration": 1.89}, {"text": "this one example correct at least, right?", "start": 4040.315, "duration": 3.39}, {"text": "And if- if much greater than 0 then it means, you know,", "start": 4043.705, "duration": 3.075}, {"text": "so if it is greater than 0 it means in- in the logistic regression case it means that,", "start": 4046.78, "duration": 3.21}, {"text": "the prediction is at least a little bit above 0.5,", "start": 4049.99, "duration": 2.58}, {"text": "a little bit below 0.5, probably 0 so that at least gets it, right?", "start": 4052.57, "duration": 2.895}, {"text": "And if it is much greater than 0 much less than 0,", "start": 4055.465, "duration": 2.625}, {"text": "then that means it, you know,", "start": 4058.09, "duration": 1.575}, {"text": "the probability of output in", "start": 4059.665, "duration": 1.695}, {"text": "the logistic regression case is either very close to 1 or very close to 0.", "start": 4061.36, "duration": 4.06}, {"text": "So one other definition,", "start": 4066.87, "duration": 3.4}, {"text": "[NOISE]", "start": 4070.27, "duration": 10.2}, {"text": "I'm gonna define", "start": 4080.47, "duration": 0.81}, {"text": "the functional margin with respect to the training set to be Gamma hat,", "start": 4081.28, "duration": 3.39}, {"text": "equals min over i of Gamma hat i,", "start": 4084.67, "duration": 4.83}, {"text": "where here i [NOISE] equals ranges over your training examples.", "start": 4089.5, "duration": 3.765}, {"text": "Okay. So, um, this is a worst-case notion,", "start": 4093.265, "duration": 2.955}, {"text": "but so this definition of a function margin,", "start": 4096.22, "duration": 2.955}, {"text": "on the left we defined functional margins with respect to a single training example,", "start": 4099.175, "duration": 4.02}, {"text": "which is how well are you doing on that one training example?", "start": 4103.195, "duration": 2.595}, {"text": "And we'll define the function margin with respect to the entire training set as,", "start": 4105.79, "duration": 3.975}, {"text": "how well are you doing on the worst example in your training set?", "start": 4109.765, "duration": 3.63}, {"text": "Okay, ah, this is a little bit of a plateau notion and we're for now,", "start": 4113.395, "duration": 4.275}, {"text": "for today, we're assuming that the training set is linearly separable.", "start": 4117.67, "duration": 3.57}, {"text": "So we're gonna assume that the training set,", "start": 4121.24, "duration": 1.65}, {"text": "you know, it looks like this.", "start": 4122.89, "duration": 1.32}, {"text": "[NOISE] And that you can separate", "start": 4124.21, "duration": 2.61}, {"text": "it on a straight line [NOISE] that will relax this later,", "start": 4126.82, "duration": 2.685}, {"text": "but because we're assuming, just for today,", "start": 4129.505, "duration": 2.055}, {"text": "that the training set is, um, linearly separable,", "start": 4131.56, "duration": 3.09}, {"text": "we'll use this kind of worst-case notion and define", "start": 4134.65, "duration": 2.7}, {"text": "the functional margin to be the functional margin of the worst training example.", "start": 4137.35, "duration": 4.38}, {"text": "Okay? [NOISE]. Now, one thing", "start": 4141.73, "duration": 9.51}, {"text": "about the definition of the functional margin is, it's actually", "start": 4151.24, "duration": 3.33}, {"text": "really easy to cheat and increase the functional margin, right?", "start": 4154.57, "duration": 3.915}, {"text": "And one thing you can do, um,", "start": 4158.485, "duration": 1.585}, {"text": "in regards to this formula is if you take w,", "start": 4160.07, "duration": 3.86}, {"text": "you know, and multiply it by 2 and take b and multiply it by 2.", "start": 4163.93, "duration": 5.16}, {"text": "[NOISE] then, um, everything", "start": 4169.09, "duration": 3.375}, {"text": "here just multiplies by two and you've doubled the functional margin,", "start": 4172.465, "duration": 3.795}, {"text": "right, but you haven't actually changed anything meaningful.", "start": 4176.26, "duration": 2.895}, {"text": "Okay, so- so one,", "start": 4179.155, "duration": 1.425}, {"text": "one way to cheat on the functional margin is just by scaling", "start": 4180.58, "duration": 2.79}, {"text": "the parameters by 2 or instead of 2 maybe you", "start": 4183.37, "duration": 3.21}, {"text": "can [NOISE] multiply all your parameters by 10 and then you've actually", "start": 4186.58, "duration": 3.0}, {"text": "increased the functional margin of your training examples as 10x,", "start": 4189.58, "duration": 3.15}, {"text": "but, ah, this doesn't actually change the decision boundary, right?", "start": 4192.73, "duration": 2.97}, {"text": "It doesn't actually change any classification,", "start": 4195.7, "duration": 1.59}, {"text": "just to multiply all of your parameters by a factor of 10.", "start": 4197.29, "duration": 3.63}, {"text": "Um, so one thing you could do is, ah,", "start": 4200.92, "duration": 6.015}, {"text": "replace, one thing you could do,", "start": 4206.935, "duration": 4.765}, {"text": "um, would be to normalize the length for your parameters.", "start": 4211.98, "duration": 4.72}, {"text": "So for example, hypothetically you could impose a constraint,", "start": 4216.7, "duration": 3.93}, {"text": "the normal w is equal to 1,", "start": 4220.63, "duration": 2.7}, {"text": "or another way to do that would be to take w and b and replace it with w", "start": 4223.33, "duration": 4.74}, {"text": "over normal b and replace b with, [NOISE] right,", "start": 4228.07, "duration": 5.13}, {"text": "just the value of parameters through by the magnitude,", "start": 4233.2, "duration": 2.759}, {"text": "by the- by the Euclidean length of the parameter vector w,", "start": 4235.959, "duration": 4.291}, {"text": "and this doesn't change any classification,", "start": 4240.25, "duration": 1.71}, {"text": "It's just rescaling the parameters.", "start": 4241.96, "duration": 1.5}, {"text": "Ah, but, ah, but,", "start": 4243.46, "duration": 1.545}, {"text": "but that it prevents, you know,", "start": 4245.005, "duration": 1.635}, {"text": "display of cheating on the functional margin.", "start": 4246.64, "duration": 4.185}, {"text": "Okay. Um, and in fact,", "start": 4250.825, "duration": 2.565}, {"text": "more generally you could actually scale w and b by", "start": 4253.39, "duration": 2.67}, {"text": "any other values you want and- and it doesn't- doesn't matter, right?", "start": 4256.06, "duration": 3.68}, {"text": "You can choose to replace this by w over 17 and b over 17 or any other number or any,", "start": 4259.74, "duration": 6.655}, {"text": "right, and the classification stays the same.", "start": 4266.395, "duration": 2.01}, {"text": "Okay. So we'll come back and use this property,", "start": 4268.405, "duration": 2.94}, {"text": "in a little bit. Okay. [NOISE]", "start": 4271.345, "duration": 13.65}, {"text": "All right. So to find the functional margin,", "start": 4284.995, "duration": 2.28}, {"text": "let's define the geometric margin.", "start": 4287.275, "duration": 1.755}, {"text": "An- and you'll see in a second how", "start": 4289.03, "duration": 1.26}, {"text": "the geometric and functional margin relate to each other.", "start": 4290.29, "duration": 3.6}, {"text": "Um, so les- let's,", "start": 4293.89, "duration": 3.045}, {"text": "let's define the, uh,", "start": 4296.935, "duration": 1.515}, {"text": "geometric margin with respect to a single example.", "start": 4298.45, "duration": 3.45}, {"text": "Which is, um- so let's see- let's say you have a classifier.", "start": 4301.9, "duration": 5.62}, {"text": "All right, so given parameters w and b that", "start": 4309.0, "duration": 3.61}, {"text": "defines a linear classifier and the equation,", "start": 4312.61, "duration": 3.495}, {"text": "wx plus b equals 0 defines the equation of a straight line.", "start": 4316.105, "duration": 4.2}, {"text": "Uh, so the axes here are x_1 and x_2,", "start": 4320.305, "duration": 2.88}, {"text": "and then half of this plane you know,", "start": 4323.185, "duration": 2.235}, {"text": "in this half of the plane,", "start": 4325.42, "duration": 1.125}, {"text": "you'll have w transpose x plus b is greater than 0.", "start": 4326.545, "duration": 3.9}, {"text": "And in this half, you'll have w transpose x plus b is less than 0.", "start": 4330.445, "duration": 4.635}, {"text": "And in between this- the straight line given by", "start": 4335.08, "duration": 3.3}, {"text": "this equation w transpose x plus b equals 0, right.", "start": 4338.38, "duration": 3.585}, {"text": "And so given parameters w and b, the upper right,", "start": 4341.965, "duration": 3.06}, {"text": "that's where your cost high will predict y equals 1 and the lower left", "start": 4345.025, "duration": 4.185}, {"text": "is where it'll predict y is equal to negative 1, okay.", "start": 4349.21, "duration": 4.725}, {"text": "Now, let's say you have one training example here, right?", "start": 4353.935, "duration": 4.635}, {"text": "So that's a training example, x_i, y_i.", "start": 4358.57, "duration": 5.235}, {"text": "And, uh, let's say it's a positive example, okay?", "start": 4363.805, "duration": 7.47}, {"text": "And so, um, your classifiers classify this example correctly, right?", "start": 4371.275, "duration": 6.945}, {"text": "Because in the upper right half- half plane-", "start": 4378.22, "duration": 3.42}, {"text": "here in this half plane w transpose x plus b is greater than 0.", "start": 4381.64, "duration": 4.02}, {"text": "And so in the- in this upper-right region, uh,", "start": 4385.66, "duration": 2.775}, {"text": "your classifier is predicting plus 1, right?", "start": 4388.435, "duration": 4.71}, {"text": "Whereas in this lower half region would be predicting h of x equals negative 1.", "start": 4393.145, "duration": 5.155}, {"text": "Right, and that's why this straight line where it switches", "start": 4399.69, "duration": 3.37}, {"text": "from predicting negative to positive is the decision boundary.", "start": 4403.06, "duration": 3.61}, {"text": "So what we're going to do is define this distance, um,", "start": 4406.83, "duration": 8.56}, {"text": "to be that geometric margin of this training example,", "start": 4415.39, "duration": 6.945}, {"text": "is the Euc- the Euclidean distance is what will define to be the geometric margin.", "start": 4422.335, "duration": 5.005}, {"text": "So let me just write down what that is.", "start": 4427.34, "duration": 2.48}, {"text": "[NOISE]", "start": 4429.82, "duration": 14.04}, {"text": "So the geometric margin of,", "start": 4443.86, "duration": 6.42}, {"text": "you know, the classifier of the hyperplane defined by w,", "start": 4450.28, "duration": 4.44}, {"text": "b with respect to one example x_i, y_i.", "start": 4454.72, "duration": 6.18}, {"text": "This is going to be gamma i equals", "start": 4460.9, "duration": 4.17}, {"text": "w transpose x plus b over [NOISE] the normal w. [NOISE]", "start": 4465.07, "duration": 7.56}, {"text": "Um, and let's see I'm not proving why this is the case,", "start": 4472.63, "duration": 3.69}, {"text": "the proof is given in the lecture notes but, uh,", "start": 4476.32, "duration": 2.085}, {"text": "the lecture notes shows why this is the right formula for", "start": 4478.405, "duration": 3.675}, {"text": "measuring the Euclidean distance that I just drew [NOISE] in the picture up there, okay.", "start": 4482.08, "duration": 3.765}, {"text": "Uh, but, and then, I'm not proving this", "start": 4485.845, "duration": 2.085}, {"text": "here but the proof is given in the lecture notes but this turns out", "start": 4487.93, "duration": 2.25}, {"text": "to be the way you compute the Euclidean distance between that example and uh,", "start": 4490.18, "duration": 4.32}, {"text": "and the decision boundary, okay?", "start": 4494.5, "duration": 2.805}, {"text": "Um, and uh, a- and this is [NOISE] for the positive example I guess.", "start": 4497.305, "duration": 6.975}, {"text": "Uh, more generally, um,", "start": 4504.28, "duration": 2.73}, {"text": "I'm going to define the geometric margin to be equal to this,", "start": 4507.01, "duration": 9.81}, {"text": "uh, and this definition applies to positive examples and the negative examples, okay?", "start": 4516.82, "duration": 6.58}, {"text": "And so the relationship between the geometric margin and the functional margin is", "start": 4523.44, "duration": 5.2}, {"text": "that the geometric margin is equal to the functional margin divided by", "start": 4528.64, "duration": 5.97}, {"text": "the norm of w. [NOISE]", "start": 4534.61, "duration": 18.0}, {"text": "Finally, um, the geometric margin with respect to the training set is, um,", "start": 4552.61, "duration": 18.315}, {"text": "where again uses worst-case notion of,", "start": 4570.925, "duration": 4.62}, {"text": "uh- look through all your training examples and pick the worst possible training example,", "start": 4575.545, "duration": 4.32}, {"text": "um, and that is your geometric margin on the training set.", "start": 4579.865, "duration": 5.37}, {"text": "Uh, an- and so I hope the- sorry,", "start": 4585.235, "duration": 1.665}, {"text": "I hope the notation is clear, right.", "start": 4586.9, "duration": 1.32}, {"text": "So gamma hat was the functional margin and", "start": 4588.22, "duration": 7.71}, {"text": "gamma is the geometric margin,", "start": 4595.93, "duration": 8.435}, {"text": "okay? And so, um,", "start": 4604.365, "duration": 4.675}, {"text": "what the optimal margin classifier", "start": 4626.22, "duration": 2.77}, {"text": "does is [NOISE] ,", "start": 4628.99, "duration": 8.49}, {"text": "um, choose the parameters w and", "start": 4637.48, "duration": 6.27}, {"text": "b to maximize the geometric margin, okay?", "start": 4643.75, "duration": 9.735}, {"text": "Um, so in other words,", "start": 4653.485, "duration": 1.59}, {"text": "thi- this- the optimal margin classifiers is the baby SVM, you know, it's like,", "start": 4655.075, "duration": 4.755}, {"text": "a SVM for linearly separable data, uh, at least for today.", "start": 4659.83, "duration": 6.33}, {"text": "[NOISE] And so the optimal margin classifier will choose that straight line,", "start": 4666.16, "duration": 3.825}, {"text": "because that straight line maximizes the distance or maximizes the geometric margin", "start": 4669.985, "duration": 5.595}, {"text": "to all of these examples, okay?", "start": 4675.58, "duration": 4.0}, {"text": "Now, uh, how you pose this mathematically,", "start": 4679.65, "duration": 4.99}, {"text": "there are a few steps of this derivations I don't want to do but I'll,", "start": 4684.64, "duration": 3.105}, {"text": "I'll just describe the beginning step and", "start": 4687.745, "duration": 2.655}, {"text": "the last step and leave the in bet- in between steps to the lecture notes.", "start": 4690.4, "duration": 3.99}, {"text": "But it turns out that, um,", "start": 4694.39, "duration": 1.695}, {"text": "one way to pose this problem is to maximize gamma w and b of gamma.", "start": 4696.085, "duration": 7.74}, {"text": "So you want to maximize the geometric margin subject to that.", "start": 4703.825, "duration": 5.185}, {"text": "Subject to the every training example, um, uh,", "start": 4722.43, "duration": 4.495}, {"text": "must have geometric margin,", "start": 4726.925, "duration": 2.565}, {"text": "uh, uh, greater than or equal to gamma, right?", "start": 4729.49, "duration": 4.2}, {"text": "So you want gamma to be as big as possible subject to", "start": 4733.69, "duration": 2.49}, {"text": "that every single training example must have at least that geometric margin.", "start": 4736.18, "duration": 3.03}, {"text": "[NOISE] This causes you to maximize the worst-case geometric margin.", "start": 4739.21, "duration": 4.59}, {"text": "And it turns out this is,", "start": 4743.8, "duration": 1.77}, {"text": "um- not in this form,", "start": 4745.57, "duration": 1.47}, {"text": "this is in a convex optimization problems.", "start": 4747.04, "duration": 2.1}, {"text": "So it's difficult to solve this without", "start": 4749.14, "duration": 1.56}, {"text": "a gradient descent and initially known local optima and so on.", "start": 4750.7, "duration": 2.7}, {"text": "But it turns out that via a few steps of V writing,", "start": 4753.4, "duration": 3.345}, {"text": "you can reformulate this problem as, um,", "start": 4756.745, "duration": 3.405}, {"text": "into the equivalent problem which is a minimizing norm of w subject", "start": 4760.15, "duration": 4.65}, {"text": "to the geometric margin, right.", "start": 4764.8, "duration": 7.935}, {"text": "Um, so it turns out- so I hope this problem makes sense, right?", "start": 4772.735, "duration": 3.435}, {"text": "So this problem is just you know solve for w and b to make sure that", "start": 4776.17, "duration": 4.05}, {"text": "every example has a geometric margin greater or", "start": 4780.22, "duration": 2.52}, {"text": "equal to gamma and you want gamma to be as big as possible.", "start": 4782.74, "duration": 2.655}, {"text": "So this is the way to formulate optimization problem that says,", "start": 4785.395, "duration": 3.3}, {"text": "''Maximize the geometric margin.''", "start": 4788.695, "duration": 2.145}, {"text": "And what we show in the lecture notes is that,", "start": 4790.84, "duration": 3.405}, {"text": "uh, through a few steps, uh,", "start": 4794.245, "duration": 1.92}, {"text": "you can rewrite this optimization problem into the following equivalent form", "start": 4796.165, "duration": 4.17}, {"text": "which is to try to minimize the norm of w, uh, subject to this.", "start": 4800.335, "duration": 4.365}, {"text": "And maybe one piece of intuition to take away is,", "start": 4804.7, "duration": 2.67}, {"text": "um, uh, the smaller w is the bigger, right?", "start": 4807.37, "duration": 4.47}, {"text": "Th- th- the, the less of a normalization division effect you have, right?", "start": 4811.84, "duration": 4.47}, {"text": "Uh, but the details I gave you in the lecture notes, okay?", "start": 4816.31, "duration": 3.015}, {"text": "Um, but this turns out to be a convex optimization problem and if you optimize this,", "start": 4819.325, "duration": 4.665}, {"text": "then you will have the optimal margin classifier and they're", "start": 4823.99, "duration": 3.3}, {"text": "very good numerical optimization packages to solve this optimization problem.", "start": 4827.29, "duration": 4.355}, {"text": "And if you give this a dataset then, you know,", "start": 4831.645, "duration": 2.34}, {"text": "assuming your data's separable [NOISE] and we'll fix that assumption,", "start": 4833.985, "duration": 2.535}, {"text": "uh, when we convene next week,", "start": 4836.52, "duration": 1.815}, {"text": "then you have the optimal management classifier", "start": 4838.335, "duration": 2.265}, {"text": "which is really a baby SVM and we add kernels to it,", "start": 4840.6, "duration": 2.64}, {"text": "then you have the full complexity of the SVM norm, okay?", "start": 4843.24, "duration": 4.0}, {"text": "All right, let's break for today,", "start": 4847.24, "duration": 1.635}, {"text": "uh, see, see you guys next Monday.", "start": 4848.875, "duration": 4.135}]