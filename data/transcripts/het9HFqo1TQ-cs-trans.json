[{"text": "What I'd like to do today is continue our discussion of supervised learning.", "start": 3.38, "duration": 7.21}, {"text": "So last Wednesday, you saw the linear regression algorithm, uh,", "start": 10.59, "duration": 4.815}, {"text": "including both gradient descent, how to formulate the problem,", "start": 15.405, "duration": 3.78}, {"text": "then gradient descent, and then the normal equations.", "start": 19.185, "duration": 3.075}, {"text": "What I'd like to do today is, um,", "start": 22.26, "duration": 2.82}, {"text": "talk about locally weighted regression which is a,", "start": 25.08, "duration": 3.11}, {"text": "a way to modify linear regressions and make it", "start": 28.19, "duration": 2.56}, {"text": "fit very non-linear functions so you aren't just fitting straight lines.", "start": 30.75, "duration": 3.39}, {"text": "And then I'll talk about", "start": 34.14, "duration": 1.41}, {"text": "a probabilistic interpretation of linear regression and that will", "start": 35.55, "duration": 3.56}, {"text": "lead us into the first classification algorithm", "start": 39.11, "duration": 2.73}, {"text": "you see in this class called logistic regression,", "start": 41.84, "duration": 2.46}, {"text": "and we'll talk about an algorithm called Newton's method for logistic regression.", "start": 44.3, "duration": 4.38}, {"text": "And so the dependency of ideas in this class is that,", "start": 48.68, "duration": 3.48}, {"text": "um, locally weighted regression will depend on what you learned in linear regression.", "start": 52.16, "duration": 5.04}, {"text": "And then, um, we're actually gonna just", "start": 57.2, "duration": 2.73}, {"text": "cover the key ideas of locally weighted regression,", "start": 59.93, "duration": 3.14}, {"text": "and let you play with some of the ideas yourself in the,", "start": 63.07, "duration": 2.65}, {"text": "um, problem set 1 which we'll release later this week.", "start": 65.72, "duration": 2.88}, {"text": "And then, um, I guess,", "start": 68.6, "duration": 2.19}, {"text": "give a probabilistic interpretation of linear regression,", "start": 70.79, "duration": 2.49}, {"text": "logistic regression depend on that, um,", "start": 73.28, "duration": 2.595}, {"text": "and Newton's method is for logistic regression, okay?", "start": 75.875, "duration": 4.035}, {"text": "To recap the notation you saw on Wednesday,", "start": 79.91, "duration": 3.765}, {"text": "we use this notation x_i, i- y_i to denote", "start": 83.675, "duration": 4.425}, {"text": "a single training example where x_i was n + 1 dimensional.", "start": 88.1, "duration": 5.085}, {"text": "So if you have two features,", "start": 93.185, "duration": 1.715}, {"text": "the size of the house and the number of bedrooms,", "start": 94.9, "duration": 2.66}, {"text": "then x_i would be 2 + 1, it would 3-dimensional because we have introduced a new,", "start": 97.56, "duration": 5.6}, {"text": "uh, sort of fake feature x_0 which was always set to the value of 1.", "start": 103.16, "duration": 4.53}, {"text": "Uh, and then yi,", "start": 107.69, "duration": 1.845}, {"text": "in the case of regression is always a real number and what's", "start": 109.535, "duration": 3.735}, {"text": "the number of training examples and what's the number of features", "start": 113.27, "duration": 2.73}, {"text": "and, uh, this was the hypothesis, right?", "start": 116.0, "duration": 3.51}, {"text": "It's a linear function of the features x, um,", "start": 119.51, "duration": 2.92}, {"text": "including this feature x_0 which is always set to 1, and, uh,", "start": 122.43, "duration": 4.5}, {"text": "j was the cost function you would minimize,", "start": 126.93, "duration": 2.874}, {"text": "you minimize this as function of j to find the parameters", "start": 129.804, "duration": 3.476}, {"text": "Theta for your straight line fit to the data, okay?", "start": 133.28, "duration": 3.91}, {"text": "So that's what you saw last Wednesday.", "start": 137.19, "duration": 2.97}, {"text": "Um, now if you have a dataset, that looks like that,", "start": 140.16, "duration": 10.75}, {"text": "where this is the size of a house and this is the price of a house.", "start": 150.91, "duration": 4.0}, {"text": "What you saw on Wednesday- last Wednesday,", "start": 154.91, "duration": 3.285}, {"text": "was an algorithm to fit a straight line,", "start": 158.195, "duration": 2.685}, {"text": "right to this data so the hypothesis was of the form", "start": 160.88, "duration": 4.23}, {"text": "Theta 0 + Theta 1 x_0- x_0- Theta 1 x_1 to be very specific.", "start": 165.11, "duration": 5.295}, {"text": "But with this dataset maybe it actually looks, you know,", "start": 170.405, "duration": 4.695}, {"text": "maybe the data looks a little bit like that and so", "start": 175.1, "duration": 2.28}, {"text": "one question that you have to address when, uh,", "start": 177.38, "duration": 3.09}, {"text": "fitting models to data is what are the features you want?", "start": 180.47, "duration": 3.28}, {"text": "Do you want to fit a straight line to this problem or do you", "start": 183.75, "duration": 2.99}, {"text": "want to fit a hypothesis of the form, um,", "start": 186.74, "duration": 3.685}, {"text": "Theta 1x + Theta 2x squared since this may be a quadratic function, right?", "start": 190.425, "duration": 8.595}, {"text": "Now the problem with quadratic functions is that quadratic functions eventually start,", "start": 199.02, "duration": 3.335}, {"text": "you know, curving back down so that will be a quadratic function.", "start": 202.355, "duration": 3.075}, {"text": "This arc starts curving back down.", "start": 205.43, "duration": 1.41}, {"text": "So maybe you don't want to fit a quadratic function.", "start": 206.84, "duration": 2.13}, {"text": "Uh, instead maybe you want, uh, to fit something like that.", "start": 208.97, "duration": 8.29}, {"text": "If- if housing prices sort of curved down a little bit but you don't want it", "start": 217.26, "duration": 3.98}, {"text": "to eventually curve back down the way a quadratic function would, right?", "start": 221.24, "duration": 4.815}, {"text": "Um, so- oh, and, and if you want to do this the way you would implement is", "start": 226.055, "duration": 5.025}, {"text": "is you define the first feature x_1 = x", "start": 231.08, "duration": 3.555}, {"text": "and the second feature x_2 = x squared or you", "start": 234.635, "duration": 2.985}, {"text": "define x_1 to be equal to x and x_2 = square root of x,", "start": 237.62, "duration": 4.415}, {"text": "right and by defining a new feature x_2 which would be the square root of x and square root of x.", "start": 242.035, "duration": 4.675}, {"text": "Then the machinery that you saw from Wednesday of", "start": 246.71, "duration": 2.535}, {"text": "linear regression applies to fit these types of,", "start": 249.245, "duration": 3.305}, {"text": "um, these types of functions of the data.", "start": 252.55, "duration": 2.94}, {"text": "So later this quarter you'll hear about feature selection algorithms,", "start": 255.49, "duration": 4.51}, {"text": "which is a type of algorithm for automatically deciding,", "start": 260.0, "duration": 3.485}, {"text": "do you want x squared as a feature or", "start": 263.485, "duration": 2.155}, {"text": "square root of x as a feature or maybe you want,", "start": 265.64, "duration": 2.79}, {"text": "um, log of x as a feature, right.", "start": 268.43, "duration": 2.85}, {"text": "But what set of features, um,", "start": 271.28, "duration": 1.74}, {"text": "does the best job fitting the data that you have", "start": 273.02, "duration": 3.255}, {"text": "if it's not fit well by a perfectly straight line.", "start": 276.275, "duration": 3.555}, {"text": "Um, what I would like to do today is- so,", "start": 279.83, "duration": 2.86}, {"text": "so you'll hear about feature selection later this quarter.", "start": 282.69, "duration": 2.84}, {"text": "What I want to share with you today is a different way", "start": 285.53, "duration": 2.25}, {"text": "of addressing this out- this problem of", "start": 287.78, "duration": 2.22}, {"text": "whether the data isn't just fit well by", "start": 290.0, "duration": 2.1}, {"text": "a straight line and in particular I wanna share with you an idea called,", "start": 292.1, "duration": 2.85}, {"text": "uh, locally weighted regression or locally weighted linear regression.", "start": 294.95, "duration": 3.96}, {"text": "So let me use a slightly different, um, example to illustrate this.", "start": 298.91, "duration": 5.18}, {"text": "Um, which is, uh, which is that, you know,", "start": 304.09, "duration": 3.185}, {"text": "if you have a dataset that looks like that.", "start": 307.275, "duration": 2.045}, {"text": "[NOISE] So it's pretty clear what the shape of this data is.", "start": 309.32, "duration": 11.79}, {"text": "Um, but how do you fit a curve that,", "start": 321.11, "duration": 2.16}, {"text": "you know, kind of looks like that, right?", "start": 323.27, "duration": 2.14}, {"text": "And it's, it's actually quite difficult to find features, is it square root of x,", "start": 325.41, "duration": 3.96}, {"text": "log of x, x cube, like third root of x, x the power of 2/3.", "start": 329.37, "duration": 3.99}, {"text": "But what is the set of features that lets you do this? So", "start": 333.36, "duration": 2.27}, {"text": "we'll sidestep all those problems with an algorithm called,", "start": 335.63, "duration": 2.805}, {"text": "uh, locally weighted regression.", "start": 338.435, "duration": 3.745}, {"text": "Um, okay,  and to introduce a bit more machine learning terminology.", "start": 353.3, "duration": 5.595}, {"text": "Um, in machine learning we sometimes distinguish between", "start": 358.895, "duration": 4.035}, {"text": "parametric learning algorithms and non-parametric learning algorithms.", "start": 362.93, "duration": 5.71}, {"text": "But in a parametric learning algorithm does, uh, uh,", "start": 370.03, "duration": 4.885}, {"text": "you fit some fixed set of parameters such as Theta", "start": 374.915, "duration": 8.305}, {"text": "i to data and so linear regression as you saw", "start": 383.22, "duration": 4.94}, {"text": "last Wednesday is a parametric learning algorithm", "start": 388.16, "duration": 3.24}, {"text": "because there's a fixed set of parameters, the Theta i's,", "start": 391.4, "duration": 2.13}, {"text": "so you fit to data and then you're done, right.", "start": 393.53, "duration": 2.885}, {"text": "Locally weighted regression will be our first exposure to", "start": 396.415, "duration": 4.675}, {"text": "a non-parametric learning algorithm.", "start": 401.09, "duration": 8.01}, {"text": "Um, and what that means is that the amount of data/parameters, uh,", "start": 409.1, "duration": 9.19}, {"text": "you need to keep grows and in", "start": 418.36, "duration": 9.31}, {"text": "this case it grows linearly with the size of the data,", "start": 427.67, "duration": 8.4}, {"text": "with size of training set, okay?", "start": 436.07, "duration": 2.065}, {"text": "So with the parametric learning algorithm,", "start": 438.135, "duration": 2.475}, {"text": "no matter how big your training, uh,", "start": 440.61, "duration": 2.67}, {"text": "your training set is, you fit the parameters Theta i.", "start": 443.28, "duration": 3.155}, {"text": "Then you could erase a training set from", "start": 446.435, "duration": 2.025}, {"text": "your computer memory and make predictions just using the parameters", "start": 448.46, "duration": 2.52}, {"text": "Theta i and in a non-parametric learning algorithm which we'll see in a second,", "start": 450.98, "duration": 4.469}, {"text": "the amount of stuff you need to keep around in", "start": 455.449, "duration": 1.981}, {"text": "computer memory or the amount of stuff you", "start": 457.43, "duration": 2.16}, {"text": "need to store around grows linearly as a function of the training set size.", "start": 459.59, "duration": 4.29}, {"text": "Uh, and so this type of algorithm is your- may,", "start": 463.88, "duration": 2.87}, {"text": "may, may not be great if you have a really,", "start": 466.75, "duration": 1.69}, {"text": "really massive dataset because you need to keep all of the data around", "start": 468.44, "duration": 3.155}, {"text": "your- in computer memory or on disk just to make predictions, okay?", "start": 471.595, "duration": 3.91}, {"text": "So- but we'll see an example of this", "start": 475.505, "duration": 2.07}, {"text": "and one of the effects of this is that with that, it'll,", "start": 477.575, "duration": 3.525}, {"text": "it'll be able to fit that data that I drew up there, uh,", "start": 481.1, "duration": 2.91}, {"text": "quite well without you needing to fiddle manually with features.", "start": 484.01, "duration": 4.815}, {"text": "Um, and again you get to practice implementing locally weighted regression in the homework.", "start": 488.825, "duration": 4.935}, {"text": "So I'm gonna go over the height of ideas relatively quickly and then let you,", "start": 493.76, "duration": 4.8}, {"text": "uh, uh, gain practice, uh, in the problem set. All right.", "start": 498.56, "duration": 4.23}, {"text": "So let me redraw that dataset, it'd be something like this.", "start": 502.79, "duration": 2.61}, {"text": "[NOISE] All right.", "start": 505.4, "duration": 5.005}, {"text": "So- so say you have a dataset like this.", "start": 510.405, "duration": 2.6}, {"text": "Um, now for linear regression if you want to evaluate", "start": 513.005, "duration": 6.885}, {"text": "h at a certain value of the input, right?", "start": 521.08, "duration": 11.32}, {"text": "So to make a prediction at a certain value of x what you- for", "start": 532.4, "duration": 4.105}, {"text": "linear regression what you do is you fit theta,", "start": 536.505, "duration": 5.565}, {"text": "you know, to minimize this cost function.", "start": 543.49, "duration": 3.16}, {"text": "[NOISE]", "start": 546.65, "duration": 10.065}, {"text": "And then you return Theta transpose x, right?", "start": 556.715, "duration": 3.92}, {"text": "So you fit a straight line and then, you know,", "start": 560.635, "duration": 2.745}, {"text": "if you want to make a prediction at this value x you then return say the transpose x.", "start": 563.38, "duration": 5.685}, {"text": "For locally weighted regression,", "start": 569.065, "duration": 3.805}, {"text": "um, you do something slightly different.", "start": 579.75, "duration": 4.66}, {"text": "Which is if this is the value of x", "start": 584.41, "duration": 2.25}, {"text": "and you want to make a prediction around that value of x.", "start": 586.66, "duration": 2.28}, {"text": "What you do is you look in a lo- local neighborhood", "start": 588.94, "duration": 3.509}, {"text": "at the training examples close to that point x where you want to make a prediction.", "start": 592.449, "duration": 4.441}, {"text": "And then, um, I'll describe this informally for now", "start": 596.89, "duration": 4.02}, {"text": "but we'll- we'll formalize this in math for the second.", "start": 600.91, "duration": 2.28}, {"text": "Um, but focusing mainly on these examples and,", "start": 603.19, "duration": 4.23}, {"text": "you know, looking a little bit at further all the examples.", "start": 607.42, "duration": 2.43}, {"text": "But really focusing mainly on these examples,", "start": 609.85, "duration": 2.25}, {"text": "you try to fit a straight line like that,", "start": 612.1, "duration": 3.525}, {"text": "focusing on the training examples that are close to where you want to make a prediction.", "start": 615.625, "duration": 4.41}, {"text": "And by close I mean the values are similar, uh, on the x axis.", "start": 620.035, "duration": 4.275}, {"text": "The x values are similar.", "start": 624.31, "duration": 2.04}, {"text": "And then to actually make a prediction, you will, uh,", "start": 626.35, "duration": 4.425}, {"text": "use this green line that you just fit to make a prediction at that value of x, okay?", "start": 630.775, "duration": 6.69}, {"text": "Now if you want to make a prediction at a different point.", "start": 637.465, "duration": 3.825}, {"text": "Um, let's say that, you know,", "start": 641.29, "duration": 1.92}, {"text": "the user now says, \"Hey, make a prediction for this point.\"", "start": 643.21, "duration": 2.88}, {"text": "Then what you would do is you focus on this local area,", "start": 646.09, "duration": 4.08}, {"text": "kinda look at those points.", "start": 650.17, "duration": 1.635}, {"text": "Um, and when I say focus say, you know,", "start": 651.805, "duration": 2.1}, {"text": "put most of the weights on these points but you", "start": 653.905, "duration": 1.755}, {"text": "kinda take a glance at the points further away,", "start": 655.66, "duration": 2.13}, {"text": "but mostly the attention is on these for the straight line to that,", "start": 657.79, "duration": 3.75}, {"text": "and then you use that straight line to make a prediction, okay.", "start": 661.54, "duration": 4.17}, {"text": "Um, and so to formalize this in locally weighted regression, um,", "start": 665.71, "duration": 9.435}, {"text": "you will fit Theta to minimize a modified cost function", "start": 675.145, "duration": 6.315}, {"text": "[NOISE]", "start": 681.46, "duration": 12.54}, {"text": "Where wi is a weight function.", "start": 694.0, "duration": 3.37}, {"text": "Um, and so a good- well the default choice,", "start": 703.44, "duration": 3.36}, {"text": "a common choice for wi will be this.", "start": 706.8, "duration": 2.28}, {"text": "[NOISE] Right, um, I'm gonna add something to this equation a little bit later.", "start": 709.08, "duration": 10.765}, {"text": "But, uh, wi is a weighting function where,", "start": 719.845, "duration": 4.2}, {"text": "notice that this, this formula has a defining property, right?", "start": 724.045, "duration": 4.98}, {"text": "If xi - x is small, then the weight will be close to 1.", "start": 729.025, "duration": 10.155}, {"text": "Because, uh, if xi x- so x is the location where you want to make", "start": 739.18, "duration": 4.95}, {"text": "a prediction and xi is the input x for your ith training example.", "start": 744.13, "duration": 6.045}, {"text": "So wi is a weighting function, um,", "start": 750.175, "duration": 3.585}, {"text": "that's a value between 0 and 1 that tells you how much should you pay attention to", "start": 753.76, "duration": 5.575}, {"text": "the values of xi, yi when fitting say this green line or that red line.", "start": 759.335, "duration": 6.53}, {"text": "And so if xi - x is small", "start": 765.865, "duration": 5.145}, {"text": "so that's a training example that is", "start": 771.01, "duration": 2.13}, {"text": "close to where you want to make the prediction for x.", "start": 773.14, "duration": 2.775}, {"text": "Then this is about e to the 0, right,", "start": 775.915, "duration": 3.09}, {"text": "e to the -0 if the- if the numerator here is", "start": 779.005, "duration": 2.925}, {"text": "small and e to the 0 is close to 1.", "start": 781.93, "duration": 3.495}, {"text": "Right, um, and conversely if xi - x is large,", "start": 785.425, "duration": 7.575}, {"text": "then wi is close to 0.", "start": 793.0, "duration": 4.605}, {"text": "And so if xi is very far away so let's see if it's fitting this green line.", "start": 797.605, "duration": 4.845}, {"text": "And this is your example, xi yi then it's saying, give this", "start": 802.45, "duration": 5.76}, {"text": "example all the way out there if you're fitting the green line where you look at", "start": 808.21, "duration": 3.27}, {"text": "this first x saying that example should have weight fairly close to 0, okay?", "start": 811.48, "duration": 6.93}, {"text": "Um, and so if you, um, look at the cost function,", "start": 818.41, "duration": 7.919}, {"text": "the main modification to the cost function we've made is that", "start": 826.329, "duration": 3.241}, {"text": "we've added this weighting term, right?", "start": 829.57, "duration": 4.605}, {"text": "And so what locally weighted regression does is the same.", "start": 834.175, "duration": 3.57}, {"text": "If an example xi is far from where you wanna make", "start": 837.745, "duration": 4.575}, {"text": "a prediction multiply that error term by 0 or by a constant very close to 0.", "start": 842.32, "duration": 6.575}, {"text": "Um, whereas if it's close to where you wanna make", "start": 848.895, "duration": 2.685}, {"text": "a prediction multiply that error term by 1.", "start": 851.58, "duration": 3.095}, {"text": "And so the net effect of this is that this is summing if, if,", "start": 854.675, "duration": 3.98}, {"text": "you know, the terms multiplied by 0 disappear, right?", "start": 858.655, "duration": 2.475}, {"text": "So the net effect of this is that the sums over essentially only the terms, uh,", "start": 861.13, "duration": 6.675}, {"text": "for the squared error for the examples that are close to", "start": 867.805, "duration": 3.675}, {"text": "the value, close to the value of x where you want to make a prediction, okay?", "start": 871.48, "duration": 5.445}, {"text": "Um, and that's why when you fit Theta to minimize this,", "start": 876.925, "duration": 8.265}, {"text": "you end up paying attention only to the points,", "start": 885.19, "duration": 3.48}, {"text": "only to the examples close to where you wanna make a prediction and", "start": 888.67, "duration": 3.06}, {"text": "fitting a line like a green line over there, okay?", "start": 891.73, "duration": 4.395}, {"text": "Um, so let me draw a couple more pictures to- to- to illustrate this.", "start": 896.125, "duration": 4.86}, {"text": "Um, so if- let me draw a slightly smaller data set just to make this easier to illustrate.", "start": 900.985, "duration": 7.95}, {"text": "Um, so that's your training set.", "start": 908.935, "duration": 1.695}, {"text": "So there's your examples x1, x2, x3, x4.", "start": 910.63, "duration": 2.7}, {"text": "And if you want to make a prediction here, right,", "start": 913.33, "duration": 2.715}, {"text": "at that point x, then, um,", "start": 916.045, "duration": 3.93}, {"text": "this curve here looks the- the- the shape of this curve is actually like this, right?", "start": 919.975, "duration": 7.425}, {"text": "Um, and this is the shape of a Gaussian bell curve.", "start": 927.4, "duration": 3.255}, {"text": "But this has nothing to do with a Gaussian density,", "start": 930.655, "duration": 2.655}, {"text": "right, so this thing does not integrate to 1.", "start": 933.31, "duration": 2.73}, {"text": "So- so it's just sometimes you ask well, is", "start": 936.04, "duration": 1.98}, {"text": "this- is this using a Gaussian density? The answer is no.", "start": 938.02, "duration": 2.55}, {"text": "Uh, this is just a function that, um,", "start": 940.57, "duration": 2.67}, {"text": "is shaped a lot like a Gaussian but, you know,", "start": 943.24, "duration": 2.61}, {"text": "Gaussian densities, probability density functions", "start": 945.85, "duration": 2.07}, {"text": "have to integrate to 1 and this does not.", "start": 947.92, "duration": 1.65}, {"text": "So there's nothing to do with a Gaussian probability density. Question?", "start": 949.57, "duration": 3.225}, {"text": "So how- how do you choose the width of the-", "start": 952.795, "duration": 2.85}, {"text": "Oh, so how do you choose the width, lemmi get back to that.", "start": 955.645, "duration": 2.4}, {"text": "Yeah. Um, and so for this example this height", "start": 958.045, "duration": 5.985}, {"text": "here says give this example a weight equal to the height of that thing.", "start": 964.03, "duration": 5.955}, {"text": "Give this example a weight to the height of this,", "start": 969.985, "duration": 2.94}, {"text": "height of this, height of that, right?", "start": 972.925, "duration": 2.25}, {"text": "Which is why if you actually- if you have an example this way out there,", "start": 975.175, "duration": 3.645}, {"text": "you know, is given a weight that's essentially 0.", "start": 978.82, "duration": 3.66}, {"text": "Which is why it's weighting only the nearby examples when trying to fit a straight line,", "start": 982.48, "duration": 4.56}, {"text": "right, uh, for the- for making predictions close to this, okay?", "start": 987.04, "duration": 6.345}, {"text": "Um, now so one last thing that I wanna mention which is,", "start": 993.385, "duration": 7.095}, {"text": "um, the- the- the question just now which is", "start": 1000.48, "duration": 2.82}, {"text": "how do you choose the width of this Gaussian density, right?", "start": 1003.3, "duration": 2.91}, {"text": "How fat it is or how thin should it be?", "start": 1006.21, "duration": 2.37}, {"text": "Um, and this decides how big a neighborhood should you look in", "start": 1008.58, "duration": 3.45}, {"text": "order to decide what's the neighborhood of points you should use to fit this,", "start": 1012.03, "duration": 3.555}, {"text": "you know, local straight line.", "start": 1015.585, "duration": 1.62}, {"text": "And so, um, for Gaussian function like this, uh,", "start": 1017.205, "duration": 4.245}, {"text": "this- I'm gonna call this the, um, bandwidth parameter tau, right?", "start": 1021.45, "duration": 10.515}, {"text": "And this is a parameter or a hyper-parameter of the algorithm.", "start": 1031.965, "duration": 5.04}, {"text": "And, uh, depending on the choice of tau, um,", "start": 1037.005, "duration": 3.735}, {"text": "uh, you can choose a fatter or a thinner bell-shaped curve,", "start": 1040.74, "duration": 4.11}, {"text": "which causes you to look in a bigger or a narrower window in order to decide,", "start": 1044.85, "duration": 6.09}, {"text": "um, you know, how many nearby examples to use in order to fit the straight line, okay?", "start": 1050.94, "duration": 5.52}, {"text": "And it turns out that, um, and I wanna leave-", "start": 1056.46, "duration": 2.79}, {"text": "I wanna leave you to discover this yourself in the problem set.", "start": 1059.25, "duration": 2.67}, {"text": "Um, if- if you've taken a little bit of machine learning elsewhere I've", "start": 1061.92, "duration": 3.0}, {"text": "heard of the terms [inaudible] Test. It's on?", "start": 1064.92, "duration": 7.23}, {"text": "Okay, good. It was on. Good.", "start": 1072.15, "duration": 1.665}, {"text": "It turns out that, um,", "start": 1073.815, "duration": 1.23}, {"text": "the choice of the bandwidth tau has an effect on,", "start": 1075.045, "duration": 3.45}, {"text": "uh, overfitting and underfitting.", "start": 1078.495, "duration": 1.47}, {"text": "If you don't know what those terms mean don't worry about it,", "start": 1079.965, "duration": 1.875}, {"text": "we'll define them later in this course.", "start": 1081.84, "duration": 1.56}, {"text": "But, uh, what you get to do in the problem set is, uh,", "start": 1083.4, "duration": 4.005}, {"text": "play with tau yourself and see why, um,", "start": 1087.405, "duration": 4.505}, {"text": "if tau is too broad, you end up fitting, um,", "start": 1091.91, "duration": 4.325}, {"text": "you end up over-smoothing the data and if tau is too", "start": 1096.235, "duration": 2.645}, {"text": "thin you end up fitting a very jagged fit to the data.", "start": 1098.88, "duration": 2.7}, {"text": "And if any of these things don't make sense", "start": 1101.58, "duration": 1.32}, {"text": "yet don't worry about it they'll make sense after you", "start": 1102.9, "duration": 2.04}, {"text": "play a bit in the- in the problem set, okay?", "start": 1104.94, "duration": 3.855}, {"text": "Um, so yeah, since- since you- you play with the varying tau in the problem", "start": 1108.795, "duration": 4.645}, {"text": "set and see for yourself the net impact of that, okay? Question?", "start": 1113.44, "duration": 5.545}, {"text": "Is tau raised to power there or is that just a- just a- [NOISE]", "start": 1118.985, "duration": 4.165}, {"text": "Thank you, uh, this is tau squared.", "start": 1123.15, "duration": 1.59}, {"text": "Yeah. Yeah.", "start": 1124.74, "duration": 9.02}, {"text": "So- so what happens if you need to invert, uh, the [inaudible]", "start": 1133.76, "duration": 1.54}, {"text": "What happens if you need to infer the value of h outside the scope of the dataset?", "start": 1135.3, "duration": 3.705}, {"text": "It turns out that you can still use this algorithm.", "start": 1139.005, "duration": 2.67}, {"text": "It's just that, um, its results may not be very good.", "start": 1141.675, "duration": 4.515}, {"text": "Yeah. It- it- it depends I guess.", "start": 1146.19, "duration": 3.015}, {"text": "Um, locally linear regression is usually not greater than extrapolation,", "start": 1149.205, "duration": 4.995}, {"text": "but then most- many learning algorithms are not good at extrapolation.", "start": 1154.2, "duration": 2.94}, {"text": "So all- all the formulas still work, you can still implement this.", "start": 1157.14, "duration": 2.985}, {"text": "But, um, yeah.", "start": 1160.125, "duration": 1.56}, {"text": "You can also try- you can also try a linear problem set and", "start": 1161.685, "duration": 2.205}, {"text": "see what happens. Yeah. One last question.", "start": 1163.89, "duration": 2.22}, {"text": "Is it possible to have like a vertical tau depending on whether some parts of your data have lots of- Yeah.", "start": 1166.11, "duration": 8.2}, {"text": "Yes, this is mostly for the variable tau depending-", "start": 1174.41, "duration": 3.1}, {"text": "Uh, uh, yes, it is, uh,", "start": 1177.51, "duration": 1.83}, {"text": "and there are quite complicated ways to choose", "start": 1179.34, "duration": 2.22}, {"text": "tau based on how many points there on the local region and so on.", "start": 1181.56, "duration": 2.565}, {"text": "Yes. There's a huge literature on different formulas actually for example", "start": 1184.125, "duration": 3.855}, {"text": "instead of this Gaussian bump thing, uh, there's,", "start": 1187.98, "duration": 2.43}, {"text": "uh, sometimes people use that triangle shape function.", "start": 1190.41, "duration": 2.85}, {"text": "So it actually goes to zero outside some small rings.", "start": 1193.26, "duration": 2.28}, {"text": "So there are, there are many versions of this algorithm.", "start": 1195.54, "duration": 3.18}, {"text": "Um, so I tend to use, uh,", "start": 1198.72, "duration": 2.46}, {"text": "a locally weighted linear regression when,", "start": 1201.18, "duration": 2.655}, {"text": "uh, you have a relatively low dimensional data set.", "start": 1203.835, "duration": 2.955}, {"text": "So when the number of features is not too big, right?", "start": 1206.79, "duration": 2.49}, {"text": "So when n is quite small like 2 or 3 or something and we have a lot of data.", "start": 1209.28, "duration": 4.725}, {"text": "And you don't wanna think about what features to use, right.", "start": 1214.005, "duration": 2.76}, {"text": "So- so that's the scenario.", "start": 1216.765, "duration": 1.815}, {"text": "So if, if you actually have a data set that looks like these up in drawing, you know,", "start": 1218.58, "duration": 3.72}, {"text": "locally weighted linear regression is,", "start": 1222.3, "duration": 2.34}, {"text": "is a, is a pretty good algorithm.", "start": 1224.64, "duration": 2.145}, {"text": "Um, just one last question. Then we're moving on.", "start": 1226.785, "duration": 2.505}, {"text": "When you have a lot of data like this, does it usually complicate the question, since you're [BACKGROUND]", "start": 1229.29, "duration": 6.51}, {"text": "Oh, sure. Yes, if you have a lot of data, that wants to be computationally expensive,", "start": 1235.8, "duration": 3.045}, {"text": "yes, it would be.", "start": 1238.845, "duration": 1.17}, {"text": "Uh, I guess a lot of data is relative.", "start": 1240.015, "duration": 2.145}, {"text": "Uh, yes we have, you know, 2, 3, 4 dimensional data", "start": 1242.16, "duration": 3.57}, {"text": "and hundreds of examples, I mean, thousands of examples.", "start": 1245.73, "duration": 2.61}, {"text": "Uh, it turns out the computation needed to fit the minimization is, uh,", "start": 1248.34, "duration": 4.065}, {"text": "similar to the normal equations,", "start": 1252.405, "duration": 2.13}, {"text": "and so you- it involves solving a linear system of", "start": 1254.535, "duration": 2.775}, {"text": "equations of dimension equal to the number of training examples you have.", "start": 1257.31, "duration": 3.39}, {"text": "So, if that's, you know, like a thousand or a few thousands, that's not too bad.", "start": 1260.7, "duration": 3.48}, {"text": "If you have millions of examples then,", "start": 1264.18, "duration": 2.115}, {"text": "then there are also multiple scaling algorithms like KD trees and", "start": 1266.295, "duration": 3.24}, {"text": "much more complicated algorithms to do this when you", "start": 1269.535, "duration": 1.905}, {"text": "have millions or hun- tens of millions of examples.", "start": 1271.44, "duration": 2.715}, {"text": "Yeah. Okay. So you get a better sense of this algorithm when you play with it,", "start": 1274.155, "duration": 7.385}, {"text": "um, in the problem set.", "start": 1281.54, "duration": 3.165}, {"text": "Now, the second topic-one of- so I'm gonna put aside locally weighted regression.", "start": 1284.705, "duration": 5.25}, {"text": "We won't talk about that set of ideas anymore, uh, today.", "start": 1289.955, "duration": 3.025}, {"text": "But, but what I wanna do today is, uh,", "start": 1292.98, "duration": 2.325}, {"text": "on last Wednesday I had said that- I had promised last Wednesday that", "start": 1295.305, "duration": 4.275}, {"text": "today I'll give a justification for why we use the squared error, right.", "start": 1299.58, "duration": 4.665}, {"text": "Why the squared error and why not to the fourth power or absolute value?", "start": 1304.245, "duration": 4.455}, {"text": "Um, and so, um, what I want to show you today- now is", "start": 1308.7, "duration": 4.65}, {"text": "a probabilistic interpretation of linear regression and", "start": 1313.35, "duration": 2.67}, {"text": "this probabilistic interpretation will put us in", "start": 1316.02, "duration": 2.295}, {"text": "good standing as we go on to logistic regression today,", "start": 1318.315, "duration": 3.315}, {"text": "uh, and then generalize linear models later this week.", "start": 1321.63, "duration": 3.645}, {"text": "We're going to keep up to-keep the notation there so we could continue to refer to it.", "start": 1325.275, "duration": 5.875}, {"text": "So why these squares? Why squared error?", "start": 1344.09, "duration": 3.28}, {"text": "Um, I'm gonna present a set of assumptions", "start": 1347.37, "duration": 2.745}, {"text": "under which these squares using squared error falls out very naturally.", "start": 1350.115, "duration": 4.935}, {"text": "Which is let's say for housing price prediction.", "start": 1355.05, "duration": 3.93}, {"text": "Let's assume that there's a true price of every house y i which is x transpose,", "start": 1358.98, "duration": 8.655}, {"text": "um, say there i, plus epsilon i.", "start": 1367.635, "duration": 5.265}, {"text": "Where epsilon i is an error term.", "start": 1372.9, "duration": 3.82}, {"text": "That includes, um, unmodeled effects,", "start": 1377.42, "duration": 4.49}, {"text": "you know, and just random noise.", "start": 1384.08, "duration": 3.77}, {"text": "Okay. So let's assume that the way, you know,", "start": 1389.6, "duration": 3.385}, {"text": "housing prices truly work is that every house's price", "start": 1392.985, "duration": 3.36}, {"text": "is a linear function of the size of the house and number of bedrooms,", "start": 1396.345, "duration": 3.645}, {"text": "plus an error term that captures unmodeled effects such as maybe one day", "start": 1399.99, "duration": 5.19}, {"text": "that seller is in an unusually good mood or", "start": 1405.18, "duration": 2.01}, {"text": "an unusually bad mood and so that makes the price go higher or lower.", "start": 1407.19, "duration": 3.105}, {"text": "We just don't model that, um, as well as random noise, right.", "start": 1410.295, "duration": 3.435}, {"text": "Or, or maybe the model will skew this street, you know,", "start": 1413.73, "duration": 3.75}, {"text": "preset to persistent capture, that's one of the features,", "start": 1417.48, "duration": 2.1}, {"text": "but other things have an impact on housing prices.", "start": 1419.58, "duration": 3.3}, {"text": "Um, and we're going to assume that, uh,", "start": 1422.88, "duration": 5.385}, {"text": "epsilon i is distributed Gaussian would mean 0 and co-variance sigma squared.", "start": 1428.265, "duration": 8.0}, {"text": "So I'm going to use this notation to mean- so the way you read", "start": 1436.265, "duration": 3.945}, {"text": "this notation is epsilon i this twiddle you pronounce as, it's distributed.", "start": 1440.21, "duration": 5.39}, {"text": "And then stripped n parens 0, sigma squared.", "start": 1445.6, "duration": 3.275}, {"text": "This is a normal distribution also called the Gaussian Distribution, same thing.", "start": 1448.875, "duration": 3.435}, {"text": "Normal distribution and Gaussian distribution mean the same thing.", "start": 1452.31, "duration": 2.16}, {"text": "The normal distribution would mean 0 and,", "start": 1454.47, "duration": 3.84}, {"text": "um, a variance sigma squared.", "start": 1458.31, "duration": 1.995}, {"text": "Okay. Um, and what this means is that the probability density of epsilon i is- this is", "start": 1460.305, "duration": 7.155}, {"text": "the Gaussian density, 1 over root 2 pi sigma e to", "start": 1467.46, "duration": 3.45}, {"text": "the negative epsilon i squared over 2 sigma squared.", "start": 1470.91, "duration": 5.49}, {"text": "Okay. And unlike the Bell state-the bell-shaped curve", "start": 1476.4, "duration": 3.18}, {"text": "I used earlier for locally weighted linear regression,", "start": 1479.58, "duration": 2.82}, {"text": "this thing does integrate to 1, right.", "start": 1482.4, "duration": 2.13}, {"text": "This-this function integrates to 1.", "start": 1484.53, "duration": 2.04}, {"text": "Uh, and so this is a Gaussian density,", "start": 1486.57, "duration": 2.85}, {"text": "this is a prob-prob-probability density function.", "start": 1489.42, "duration": 2.745}, {"text": "Um, and this is the familiar, you know,", "start": 1492.165, "duration": 5.1}, {"text": "Gaussian bell-shaped curve with mean 0 and co-variance- and variance,", "start": 1497.265, "duration": 6.435}, {"text": "uh, uh, sigma squared where sigma kinda controls the width of this Gaussian.", "start": 1503.7, "duration": 5.31}, {"text": "Okay? Uh, and if you haven't seen Gaussian's for a while we'll go over some of the, er,", "start": 1509.01, "duration": 4.155}, {"text": "probability, probability pre-reqs as well in the classes, Friday discussion sections.", "start": 1513.165, "duration": 7.345}, {"text": "So, in other words, um,", "start": 1523.28, "duration": 2.74}, {"text": "we assume that the way housing prices are determined is that,", "start": 1526.02, "duration": 2.94}, {"text": "first is a true price theta transpose x.", "start": 1528.96, "duration": 2.76}, {"text": "And then, you know, some random force of nature.", "start": 1531.72, "duration": 2.565}, {"text": "Right, the mood of the seller or, I-I-I don't know-I don't have other factors, right.", "start": 1534.285, "duration": 6.27}, {"text": "Perturbs it from this true value, theta transpose xi.", "start": 1540.555, "duration": 4.89}, {"text": "Um, and the huge assumption we're gonna make is that the epsilon I's", "start": 1545.445, "duration": 4.485}, {"text": "these error terms are IID. And IID", "start": 1549.93, "duration": 3.36}, {"text": "from statistics stands for Independently and Identically Distributed.", "start": 1553.29, "duration": 3.405}, {"text": "And what that means is that the error term for one house is independent,", "start": 1556.695, "duration": 4.125}, {"text": "uh, as the error term for a different house.", "start": 1560.82, "duration": 2.595}, {"text": "Which is actually not a true assumption.", "start": 1563.415, "duration": 1.995}, {"text": "Right. Because, you know, if,", "start": 1565.41, "duration": 1.185}, {"text": "if one house is priced on one street is", "start": 1566.595, "duration": 1.965}, {"text": "unusually high, probably a price on", "start": 1568.56, "duration": 1.98}, {"text": "a different house on the same street will also be unusually high.", "start": 1570.54, "duration": 2.805}, {"text": "And so- but, uh, this assumption that these epsilon I's are", "start": 1573.345, "duration": 3.81}, {"text": "IID since they're independently and identically distributed.", "start": 1577.155, "duration": 3.18}, {"text": "Um, is one of those assumptions that,", "start": 1580.335, "duration": 1.995}, {"text": "that, you know, is probably not absolutely true,", "start": 1582.33, "duration": 2.145}, {"text": "but may be good enough that if you make this assumption,", "start": 1584.475, "duration": 2.415}, {"text": "you get a pretty good model.", "start": 1586.89, "duration": 1.995}, {"text": "Okay. Um, and so let's see.", "start": 1588.885, "duration": 4.56}, {"text": "Under these set of assumptions this implies that [NOISE] the density or", "start": 1593.445, "duration": 11.025}, {"text": "the probability of y i given x i and theta this is going to be this.", "start": 1604.47, "duration": 9.16}, {"text": "Um, and I'll, I'll take this and write it in another way.", "start": 1623.51, "duration": 4.91}, {"text": "In other words, given x and theta,", "start": 1641.2, "duration": 4.8}, {"text": "what's the density- what's the probability of a particular house's price?", "start": 1646.0, "duration": 5.36}, {"text": "Well, it's going to be Gaussian with mean given", "start": 1651.36, "duration": 2.94}, {"text": "by theta transpose xi or theta transpose x,", "start": 1654.3, "duration": 3.285}, {"text": "and the variance is, um, given by sigma squared.", "start": 1657.585, "duration": 3.915}, {"text": "Okay. Um, and so, uh,", "start": 1661.5, "duration": 4.245}, {"text": "because the way that the price of a house is determined", "start": 1665.745, "duration": 2.985}, {"text": "is by taking theta transpose x with the, you know,", "start": 1668.73, "duration": 2.97}, {"text": "quote true price of the house and then adding", "start": 1671.7, "duration": 2.7}, {"text": "noise or adding error of variance sigma squared to it.", "start": 1674.4, "duration": 3.21}, {"text": "And so, um, the,", "start": 1677.61, "duration": 2.055}, {"text": "the assumptions on the left imply that given x and theta,", "start": 1679.665, "duration": 3.315}, {"text": "the density of y, you know, has this distribution.", "start": 1682.98, "duration": 3.39}, {"text": "Which is- really this is the random variable y,", "start": 1686.37, "duration": 2.715}, {"text": "and that's the mean, right,", "start": 1689.085, "duration": 3.3}, {"text": "and that's the variance of the Gaussian density.", "start": 1692.385, "duration": 4.14}, {"text": "Okay. Now, um, two pieces of notation.", "start": 1696.525, "duration": 4.92}, {"text": "Um, I want to, one that you should get familiar with.", "start": 1701.445, "duration": 4.41}, {"text": "Um, the reason I wrote the semicolon here is, uh,", "start": 1705.855, "duration": 4.56}, {"text": "that- the way you read this equation is the semicolon should be read as parameterized as.", "start": 1710.415, "duration": 6.475}, {"text": "Right, um, and so because, uh, uh, the,", "start": 1717.32, "duration": 6.115}, {"text": "the alternative way to write this would be to say P of xi given yi,", "start": 1723.435, "duration": 3.885}, {"text": "excuse me, P of y given xi comma theta.", "start": 1727.32, "duration": 3.78}, {"text": "But if you were to write this notation this way,", "start": 1731.1, "duration": 3.0}, {"text": "this would be conditioning on theta,", "start": 1734.1, "duration": 2.684}, {"text": "but theta is not a random variable.", "start": 1736.784, "duration": 1.816}, {"text": "So you shouldn't condition on theta,", "start": 1738.6, "duration": 1.725}, {"text": "which is why I'm gonna write a semicolon.", "start": 1740.325, "duration": 2.745}, {"text": "And so the way you read this is,", "start": 1743.07, "duration": 1.47}, {"text": "the probability of yi given xi and parameterize, oh,", "start": 1744.54, "duration": 3.42}, {"text": "excuse me, parameterized by theta is equal to that formula, okay?", "start": 1747.96, "duration": 5.76}, {"text": "Um, if, if, if you don't understand this distinction,", "start": 1753.72, "duration": 2.7}, {"text": "again, don't worry too much about it.", "start": 1756.42, "duration": 1.395}, {"text": "In, in statistics there are multiple schools of statistics called Bayesian statistics,", "start": 1757.815, "duration": 4.335}, {"text": "frequentist statistics, this is a frequentist interpretation.", "start": 1762.15, "duration": 3.33}, {"text": "Uh, for the purposes of machine learning, don't worry about it,", "start": 1765.48, "duration": 2.34}, {"text": "but I find that being more consistent with terminology", "start": 1767.82, "duration": 2.264}, {"text": "prevents some of our statistician friends from getting really upset, but, but,", "start": 1770.084, "duration": 3.481}, {"text": "but, you know, I'll try to follow statistics convention.", "start": 1773.565, "duration": 3.615}, {"text": "Uh, so- because just only unnecessary flack I guess,", "start": 1777.18, "duration": 3.99}, {"text": "um, but for the per- for practical purposes this is not that important.", "start": 1781.17, "duration": 3.225}, {"text": "If you forget this notation on your homework.", "start": 1784.395, "duration": 1.575}, {"text": "don't worry about it we won't penalize you,", "start": 1785.97, "duration": 1.59}, {"text": "but I'll try to be consistent.", "start": 1787.56, "duration": 1.635}, {"text": "Um, but this just means that theta in this view is not a random variable,", "start": 1789.195, "duration": 3.825}, {"text": "it's just theta is a set of parameters that parameterizes this probability distribution.", "start": 1793.02, "duration": 4.68}, {"text": "Okay? Um, and the way to read the second equation is, um,", "start": 1797.7, "duration": 6.255}, {"text": "when you write these equations usually don't write them with parentheses,", "start": 1803.955, "duration": 2.985}, {"text": "but the way to parse this equation is to say that this thing is a random variable.", "start": 1806.94, "duration": 4.83}, {"text": "The random variable y given x and parameterized by theta.", "start": 1811.77, "duration": 3.84}, {"text": "This thing that I just drew in", "start": 1815.61, "duration": 1.68}, {"text": "green parentheses is just a distributed Gaussian with that distribution, okay?", "start": 1817.29, "duration": 4.35}, {"text": "All right. Um, any questions about this?", "start": 1821.64, "duration": 8.23}, {"text": "Okay. So it turns out that", "start": 1831.14, "duration": 6.85}, {"text": "[NOISE] if you are willing to make those assumptions,", "start": 1837.99, "duration": 7.485}, {"text": "then linear regression, um,", "start": 1845.475, "duration": 4.71}, {"text": "falls out almost naturally of the assumptions we just made.", "start": 1850.185, "duration": 6.945}, {"text": "And in particular, under the assumptions we just made, um,", "start": 1857.13, "duration": 5.595}, {"text": "the likelihood of the parameters theta,", "start": 1862.725, "duration": 6.705}, {"text": "so this is pronounced the likelihood of the parameters theta,", "start": 1869.43, "duration": 8.04}, {"text": "uh, L of theta which is defined as the probability of the data.", "start": 1877.47, "duration": 6.55}, {"text": "Right? So this is probability of all the values of y of y1", "start": 1884.72, "duration": 3.925}, {"text": "up to ym given all the xs and given,", "start": 1888.645, "duration": 3.225}, {"text": "uh, the parameters theta parameterized by theta.", "start": 1891.87, "duration": 3.24}, {"text": "Um, this is equal to the product from I equals", "start": 1895.11, "duration": 7.65}, {"text": "1 through m of p of yi given xi parameterized by theta.", "start": 1902.76, "duration": 9.37}, {"text": "Um, because we assumed the examples were- because we assume the errors are IID, right,", "start": 1913.49, "duration": 5.92}, {"text": "that the error terms are independently and identically distributed to each other,", "start": 1919.41, "duration": 3.975}, {"text": "so the probability of all of the observations,", "start": 1923.385, "duration": 3.465}, {"text": "of all the values of y in your training set is equal to the product of the probabilities,", "start": 1926.85, "duration": 3.69}, {"text": "because of the independence assumption we made.", "start": 1930.54, "duration": 1.98}, {"text": "And so plugging in the definition of p of", "start": 1932.52, "duration": 2.91}, {"text": "y given x parameterized by theta that we had up there,", "start": 1935.43, "duration": 2.835}, {"text": "this is equal to product of that.", "start": 1938.265, "duration": 6.805}, {"text": "Okay? Now, um, again, one more piece of terminology.", "start": 1956.0, "duration": 8.44}, {"text": "Uh, you know, another question I've always been asked if you say, hey, Andrew,", "start": 1964.44, "duration": 3.3}, {"text": "what's the difference between likelihood and probability, right?", "start": 1967.74, "duration": 3.405}, {"text": "And so the likelihood of the parameters", "start": 1971.145, "duration": 2.52}, {"text": "is exactly the same thing as the probability of the data,", "start": 1973.665, "duration": 3.12}, {"text": "uh, but the reason we sometimes talk about likelihood,", "start": 1976.785, "duration": 2.61}, {"text": "and sometimes talk of probability is, um, we think of likelihood.", "start": 1979.395, "duration": 3.72}, {"text": "So this, this is some function, right?", "start": 1983.115, "duration": 1.875}, {"text": "This thing is a function of the data as well as a function of the parameters theta.", "start": 1984.99, "duration": 4.68}, {"text": "And if you view this number, whatever this number is,", "start": 1989.67, "duration": 2.895}, {"text": "if you view this thing as a function of the parameters holding the data fixed,", "start": 1992.565, "duration": 3.885}, {"text": "then we call that the likelihood.", "start": 1996.45, "duration": 1.605}, {"text": "So if you think of the training set the data as a fixed thing,", "start": 1998.055, "duration": 2.73}, {"text": "and then varying parameters theta,", "start": 2000.785, "duration": 2.415}, {"text": "then I'm going to use the term likelihood.", "start": 2003.2, "duration": 2.79}, {"text": "Whereas if you view the parameters theta as fixed and maybe varying the data,", "start": 2005.99, "duration": 4.08}, {"text": "I'm gonna say probability, right?", "start": 2010.07, "duration": 1.74}, {"text": "So, so you hear me use- well,", "start": 2011.81, "duration": 2.43}, {"text": "I'll, I'll try to be consistent.", "start": 2014.24, "duration": 1.485}, {"text": "I find I'm, I'm pretty good at being consistent but not perfect,", "start": 2015.725, "duration": 3.03}, {"text": "but I'm going to try to say likelihood of the parameters,", "start": 2018.755, "duration": 3.525}, {"text": "and probability of the data even though", "start": 2022.28, "duration": 2.64}, {"text": "those evaluate to the same thing as just, you know,", "start": 2024.92, "duration": 2.52}, {"text": "for this function, this function is a function of theta and", "start": 2027.44, "duration": 2.49}, {"text": "the parameters which one are you viewing as", "start": 2029.93, "duration": 1.74}, {"text": "fixed and which one are you viewing as, as variables.", "start": 2031.67, "duration": 2.025}, {"text": "So when you view this as a function of theta,", "start": 2033.695, "duration": 2.52}, {"text": "I'm gonna use this term likelihood.", "start": 2036.215, "duration": 1.935}, {"text": "Uh, but- so, so hopefully you hear me say likelihood of the parameters.", "start": 2038.15, "duration": 3.975}, {"text": "Hopefully you won't hear me say likelihood of the data, right?", "start": 2042.125, "duration": 3.96}, {"text": "And, and similarly, hopefully you hear me say probability of", "start": 2046.085, "duration": 2.565}, {"text": "the data and not the probability of the parameters, okay? Yeah.", "start": 2048.65, "duration": 4.74}, {"text": "[inaudible].", "start": 2053.39, "duration": 5.7}, {"text": "Like other parameters.", "start": 2059.09, "duration": 0.99}, {"text": "[inaudible].", "start": 2060.08, "duration": 0.39}, {"text": "Uh, okay.", "start": 2060.47, "duration": 4.65}, {"text": "So probability of the data.", "start": 2065.12, "duration": 2.4}, {"text": "No. Uh, uh, theta, I got it sorry, yes.", "start": 2067.52, "duration": 5.13}, {"text": "Likelihood of theta. Got it.", "start": 2072.65, "duration": 1.08}, {"text": "Yes. Sorry. Yes. Likelihood of theta.", "start": 2073.73, "duration": 1.965}, {"text": "That's right.", "start": 2075.695, "duration": 8.925}, {"text": "[inaudible]. Oh, uh, no. So- no.", "start": 2084.62, "duration": 1.785}, {"text": "Uh, uh, so theta is a set of parameters,", "start": 2086.405, "duration": 2.625}, {"text": "it's not a random variable.", "start": 2089.03, "duration": 1.545}, {"text": "So we- likelihood of theta doesn't mean theta is a random variable.", "start": 2090.575, "duration": 3.735}, {"text": "Right. Cool. Yeah. Thank you.", "start": 2094.31, "duration": 1.74}, {"text": "Um, by the way, the, the,", "start": 2096.05, "duration": 1.665}, {"text": "the stuff about what's a random variable and what's not,", "start": 2097.715, "duration": 1.965}, {"text": "the semicolon versus comma thing.", "start": 2099.68, "duration": 1.98}, {"text": "We explained this in more detail in the lecture notes.", "start": 2101.66, "duration": 2.61}, {"text": "To me this is part of, um, uh, you know,", "start": 2104.27, "duration": 3.645}, {"text": "a little bit paying homage to the- to the religion of Bayesian frequencies versus Bayesian,", "start": 2107.915, "duration": 6.27}, {"text": "uh, frequentist versus Bayesians in statistics.", "start": 2114.185, "duration": 2.775}, {"text": "From a- from a machine- from an applied machine learning", "start": 2116.96, "duration": 2.715}, {"text": "operational what you write code point of view,", "start": 2119.675, "duration": 2.685}, {"text": "it doesn't matter that much.", "start": 2122.36, "duration": 1.935}, {"text": "Uh, yeah. But theta is not a random variable,", "start": 2124.295, "duration": 2.49}, {"text": "we have likelihood of parameters which are not a variable. Yeah. Go ahead.", "start": 2126.785, "duration": 3.675}, {"text": "[inaudible].", "start": 2130.46, "duration": 1.2}, {"text": "Oh, what's the rationale for choosing,", "start": 2131.66, "duration": 5.925}, {"text": "uh, oh, sure, why is epsilon i Gaussian?", "start": 2137.585, "duration": 4.68}, {"text": "So, uh, uh, turns out because of central limit theorem,", "start": 2142.265, "duration": 3.405}, {"text": "uh, from statistics, uh, most error distributions are Gaussian, right?", "start": 2145.67, "duration": 3.69}, {"text": "If something is- if there's an era that's made up of", "start": 2149.36, "duration": 2.73}, {"text": "lots of little noise sources which are not too correlated,", "start": 2152.09, "duration": 2.91}, {"text": "then by central limit theorem it will be Gaussian.", "start": 2155.0, "duration": 2.52}, {"text": "So if you think that, most perturbations are,", "start": 2157.52, "duration": 2.28}, {"text": "the mood of the seller,", "start": 2159.8, "duration": 1.185}, {"text": "what's the school district, you know,", "start": 2160.985, "duration": 1.725}, {"text": "what's the weather like, or access to transportation,", "start": 2162.71, "duration": 2.4}, {"text": "and all of these sources are not too correlated,", "start": 2165.11, "duration": 2.37}, {"text": "and you add them up then the distribution will be Gaussian.", "start": 2167.48, "duration": 2.415}, {"text": "Um, and, and I think- well, yeah.", "start": 2169.895, "duration": 3.205}, {"text": "So you can use the central limit theorem,", "start": 2173.38, "duration": 2.47}, {"text": "I think the Gaussian has become a default noise distribution.", "start": 2175.85, "duration": 2.67}, {"text": "But for things where the true noise distribution is very far from Gaussian,", "start": 2178.52, "duration": 5.25}, {"text": "uh, this model does do that as well.", "start": 2183.77, "duration": 2.025}, {"text": "And in fact, for when you see generalized linear models on Wednesday,", "start": 2185.795, "duration": 3.825}, {"text": "you see when- how to generalize all of", "start": 2189.62, "duration": 2.25}, {"text": "these algorithms to very different distributions like Poisson, and so on.", "start": 2191.87, "duration": 4.57}, {"text": "All right. So, um, so we've seen the likelihood of the parameters theta.", "start": 2196.57, "duration": 8.785}, {"text": "Um, so I'm gonna use lower case l to denote the log-likelihood.", "start": 2205.355, "duration": 7.765}, {"text": "And the log-likelihood is just the log of the likelihood.", "start": 2215.17, "duration": 4.12}, {"text": "Um, and so- well, just- right.", "start": 2219.29, "duration": 10.935}, {"text": "And so, um, log of a product is equal to the sum of the logs.", "start": 2230.225, "duration": 5.385}, {"text": "Uh, and so this is equal to-", "start": 2235.61, "duration": 2.76}, {"text": "and so this is m log 1 over root.", "start": 2251.8, "duration": 4.67}, {"text": "Okay? Um. And so, um, one of the, uh, you know,", "start": 2269.5, "duration": 9.01}, {"text": "well-tested letters in statistics estimating parameters is to", "start": 2278.51, "duration": 4.56}, {"text": "use maximum likelihood estimation or MLE", "start": 2283.07, "duration": 6.16}, {"text": "which means you choose theta", "start": 2300.55, "duration": 3.68}, {"text": "to maximize the likelihood, right?", "start": 2306.88, "duration": 6.22}, {"text": "So given the data set,", "start": 2313.1, "duration": 3.195}, {"text": "how would you like to estimate theta?", "start": 2316.295, "duration": 2.37}, {"text": "Well, one natural way to choose theta is to choose", "start": 2318.665, "duration": 2.895}, {"text": "whatever value of theta has a highest likelihood.", "start": 2321.56, "duration": 2.85}, {"text": "Or in other words, choose a value of theta so that that value of", "start": 2324.41, "duration": 2.85}, {"text": "theta maximizes the probability of the data, right?", "start": 2327.26, "duration": 4.62}, {"text": "And so, um, for- to simplify the algebra rather than", "start": 2331.88, "duration": 5.31}, {"text": "maximizing the likelihood capital L is actually easier to maximize the log likelihood.", "start": 2337.19, "duration": 5.475}, {"text": "But the log is a strictly monotonically increasing function.", "start": 2342.665, "duration": 3.015}, {"text": "So the value of theta that maximizes", "start": 2345.68, "duration": 2.22}, {"text": "the log likelihood should be the same as", "start": 2347.9, "duration": 1.77}, {"text": "the value of theta that maximizes the likelihood.", "start": 2349.67, "duration": 2.28}, {"text": "And if you divide the log likelihood, um,", "start": 2351.95, "duration": 3.36}, {"text": "we conclude that if you're using maximum likelihood estimation,", "start": 2355.31, "duration": 3.48}, {"text": "what you'd like to do is choose a value of theta that maximizes this thing, right?", "start": 2358.79, "duration": 4.59}, {"text": "But, uh, this first term is just a constant,", "start": 2363.38, "duration": 3.285}, {"text": "theta doesn't even appear in this first term.", "start": 2366.665, "duration": 3.285}, {"text": "And so what you'd like to do is choose the value of", "start": 2369.95, "duration": 2.85}, {"text": "theta that maximizes this second term.", "start": 2372.8, "duration": 3.0}, {"text": "Ah, notice there's a minus sign there.", "start": 2375.8, "duration": 2.73}, {"text": "And so what you'd like to do is,", "start": 2378.53, "duration": 2.505}, {"text": "uh, uh, i.e, you know,", "start": 2381.035, "duration": 2.22}, {"text": "choose theta to minimize this term.", "start": 2383.255, "duration": 9.865}, {"text": "Right. Also, sigma squared is just a constant.", "start": 2401.68, "duration": 3.985}, {"text": "Right. No matter what sigma squared is,", "start": 2405.665, "duration": 1.95}, {"text": "you know, so, so, uh,", "start": 2407.615, "duration": 1.755}, {"text": "so if you want to minimize this term, excuse me,", "start": 2409.37, "duration": 2.7}, {"text": "if you want to maximize this term,", "start": 2412.07, "duration": 1.56}, {"text": "negative of this thing,", "start": 2413.63, "duration": 1.185}, {"text": "that's the same as minimizing this term.", "start": 2414.815, "duration": 2.865}, {"text": "Uh, but this is just J of theta.", "start": 2417.68, "duration": 4.775}, {"text": "The cost function you saw earlier for linear regression.", "start": 2422.455, "duration": 4.47}, {"text": "Okay? So this little proof shows that,", "start": 2426.925, "duration": 3.955}, {"text": "um, choosing the value of theta to minimize the least squares errors,", "start": 2430.88, "duration": 5.295}, {"text": "like you saw last Wednesday,", "start": 2436.175, "duration": 2.07}, {"text": "that's just finding the maximum likelihood estimate", "start": 2438.245, "duration": 3.33}, {"text": "for the parameters theta under this set of assumptions we made,", "start": 2441.575, "duration": 4.155}, {"text": "that the error terms are Gaussian and IID.", "start": 2445.73, "duration": 3.765}, {"text": "Okay, go ahead. Oh, thank you.", "start": 2449.495, "duration": 5.13}, {"text": "Yes. Great. Thanks. Go ahead.", "start": 2454.625, "duration": 3.975}, {"text": "[inaudible].", "start": 2458.6, "duration": 9.21}, {"text": "Oh, is there a situation where using this formula", "start": 2467.81, "duration": 1.62}, {"text": "instead of least squares cost function will be a good idea?", "start": 2469.43, "duration": 1.89}, {"text": "No. So this- I think this derivation shows that", "start": 2471.32, "duration": 2.82}, {"text": "this- this is completely equivalent to least squares.", "start": 2474.14, "duration": 3.21}, {"text": "Right. That if- if you want- if you're willing", "start": 2477.35, "duration": 2.58}, {"text": "to assume that the error terms are Gaussian and", "start": 2479.93, "duration": 2.31}, {"text": "IID and if you want to use", "start": 2482.24, "duration": 2.58}, {"text": "Maximum Likelihood Estimation which is a very natural procedure in statistics,", "start": 2484.82, "duration": 3.42}, {"text": "then, you know, then you should use least squares. Right. So yeah.", "start": 2488.24, "duration": 5.64}, {"text": "If you know for some reason that the errors are not IID, like, is there a better way to figure out a better cost function?", "start": 2493.88, "duration": 9.42}, {"text": "If you know for some reason errors are not IID, could you figure out a better cost function? Yes and no.", "start": 2503.3, "duration": 3.285}, {"text": "I think that, um, you know, when building learners algorithms,", "start": 2506.585, "duration": 4.005}, {"text": "ah, often we make model- we make assumptions about the world that we just know", "start": 2510.59, "duration": 4.365}, {"text": "are not 100% true because it leads to algorithms that are computationally efficient.", "start": 2514.955, "duration": 3.975}, {"text": "Um, and so if you knew that", "start": 2518.93, "duration": 2.625}, {"text": "your- if you knew that your training set was very very non IID,", "start": 2521.555, "duration": 3.375}, {"text": "there are- there're more sophisticated models you could build.", "start": 2524.93, "duration": 2.355}, {"text": "But, um, ah, ah, yeah.", "start": 2527.285, "duration": 3.33}, {"text": "But- but very often we wouldn't bother I think.", "start": 2530.615, "duration": 2.895}, {"text": "Yeah. More often than not we might not bother.", "start": 2533.51, "duration": 2.58}, {"text": "Ah, I can think of a few special cases where you", "start": 2536.09, "duration": 2.88}, {"text": "would bother there but only if you think the assumption is really really bad.", "start": 2538.97, "duration": 3.045}, {"text": "Ah, if you don't have enough data or something- something. Quite- quite rare.", "start": 2542.015, "duration": 3.795}, {"text": "All right. Um, lemme think why, all right.", "start": 2545.81, "duration": 4.665}, {"text": "I want to move on to make sure we get through the rest of things.", "start": 2550.475, "duration": 3.555}, {"text": "Any burning questions? Yeah, okay, cool.", "start": 2554.03, "duration": 2.49}, {"text": "All right. Um, so out of this machinery.", "start": 2556.52, "duration": 6.63}, {"text": "Right. So- so- so what did we do here?", "start": 2563.15, "duration": 2.55}, {"text": "Was we set up a set of probabilistic assumptions,", "start": 2565.7, "duration": 2.58}, {"text": "we made certain assumptions about P of Y given X,", "start": 2568.28, "duration": 3.425}, {"text": "where the key assumption was Gaussian errors in IID.", "start": 2571.705, "duration": 2.685}, {"text": "And then through maximum likelihood estimation,", "start": 2574.39, "duration": 2.55}, {"text": "we derived an algorithm which turns out to be exactly the least squares algorithm.", "start": 2576.94, "duration": 4.29}, {"text": "Right? Um, what I'd like to do is take this framework,", "start": 2581.23, "duration": 3.56}, {"text": "ah, and apply it to our first classification problem.", "start": 2584.79, "duration": 4.355}, {"text": "Right. And so the- the key steps are, you know, one,", "start": 2589.145, "duration": 3.63}, {"text": "make an assumption about P of Y given X,", "start": 2592.775, "duration": 2.31}, {"text": "P of Y given X parameters theta,", "start": 2595.085, "duration": 1.635}, {"text": "and then second is figure out maximum likelihood estimation.", "start": 2596.72, "duration": 2.445}, {"text": "So I'd like to take this framework and apply it to a different type of problem,", "start": 2599.165, "duration": 3.57}, {"text": "where the value of Y is now either 0 or 1.", "start": 2602.735, "duration": 4.035}, {"text": "So is a classification problem.", "start": 2606.77, "duration": 1.635}, {"text": "Okay? So, um, let's see.", "start": 2608.405, "duration": 6.505}, {"text": "So the classification problem.", "start": 2619.18, "duration": 2.92}, {"text": "In our first classification problem,", "start": 2622.1, "duration": 2.16}, {"text": "we're going to start with binary classification.", "start": 2624.26, "duration": 2.88}, {"text": "So the value of Y is either 0 or 1.", "start": 2627.14, "duration": 3.3}, {"text": "And sometimes we call this binary classification because there are two clauses.", "start": 2630.44, "duration": 5.085}, {"text": "Classification. Right. Um, and so right- so that's", "start": 2635.525, "duration": 13.125}, {"text": "a data set where I guess this is X and this is Y. Um, so", "start": 2648.65, "duration": 4.8}, {"text": "something that's not a good idea is to apply linear regression to this data set.", "start": 2653.45, "duration": 4.905}, {"text": "Some- sometimes you will do it and maybe you'll get away", "start": 2658.355, "duration": 2.055}, {"text": "with it but I wouldn't do it and here's.", "start": 2660.41, "duration": 2.415}, {"text": "Which is, um, is- is tempting to just fit a straight line to", "start": 2662.825, "duration": 3.195}, {"text": "this data and then take the straight line and threshold it at 0.5,", "start": 2666.02, "duration": 4.2}, {"text": "and then say, oh, if it's above 0.5 round off to 1,", "start": 2670.22, "duration": 2.85}, {"text": "if it's below 0.5 round it off to 0.", "start": 2673.07, "duration": 3.24}, {"text": "But it turns out that this, um, is not a good idea,", "start": 2676.31, "duration": 4.305}, {"text": "uh, for classification problems.", "start": 2680.615, "duration": 1.935}, {"text": "And- and here's why?", "start": 2682.55, "duration": 1.17}, {"text": "Which is- for this data set it's really obvious what the- what the pattern is.", "start": 2683.72, "duration": 3.78}, {"text": "Right? Everything to the left of this point predict 0.", "start": 2687.5, "duration": 2.235}, {"text": "Everything to the right of that point predict 1.", "start": 2689.735, "duration": 2.325}, {"text": "But let's say we now change the data set to just add one more example there.", "start": 2692.06, "duration": 5.655}, {"text": "Right. And the pattern is still really obvious.", "start": 2697.715, "duration": 2.175}, {"text": "It says everything to the left of this predict 0,", "start": 2699.89, "duration": 2.25}, {"text": "everything to the right of that predict 1.", "start": 2702.14, "duration": 1.845}, {"text": "But if you fit a straight line to this data set with this extra one point there,", "start": 2703.985, "duration": 3.765}, {"text": "and just not even the outlier it's really", "start": 2707.75, "duration": 1.77}, {"text": "obvious at this point way out there should be labeled one.", "start": 2709.52, "duration": 2.685}, {"text": "But with this extra example, um,", "start": 2712.205, "duration": 2.58}, {"text": "if we fit a straight line to the data,", "start": 2714.785, "duration": 2.16}, {"text": "you end up with maybe something like that.", "start": 2716.945, "duration": 2.865}, {"text": "Um, and somehow adding this one example,", "start": 2719.81, "duration": 3.555}, {"text": "it really didn't change anything, right?", "start": 2723.365, "duration": 1.815}, {"text": "But somehow the straight line fit moved from the green line to the, uh,", "start": 2725.18, "duration": 3.81}, {"text": "moved from the blue line to the green line.", "start": 2728.99, "duration": 1.74}, {"text": "And if you now threshold it at 0.5,", "start": 2730.73, "duration": 2.595}, {"text": "you end up with a very different decision boundary.", "start": 2733.325, "duration": 2.745}, {"text": "And so linear regression is just not a good algorithm for classification.", "start": 2736.07, "duration": 4.23}, {"text": "Some people use it and sometimes they get lucky and it's not too bad but", "start": 2740.3, "duration": 3.54}, {"text": "I- I- I personally never use linear regression for classification algorithms.", "start": 2743.84, "duration": 4.305}, {"text": "Right. Because you just don't know if you end up with", "start": 2748.145, "duration": 2.235}, {"text": "a really bad fit to the data like this, okay?", "start": 2750.38, "duration": 3.12}, {"text": "Um, so oh and- and- and the other unnatural thing", "start": 2753.5, "duration": 7.14}, {"text": "about using linear regression for a classification problem is that,", "start": 2760.64, "duration": 3.24}, {"text": "um, you know for a classification problem that the values are,", "start": 2763.88, "duration": 4.125}, {"text": "you know, 0 or 1.", "start": 2768.005, "duration": 1.545}, {"text": "Right. And so it outputs negative values or values even", "start": 2769.55, "duration": 3.69}, {"text": "greater than 1 seems- seems strange, um.", "start": 2773.24, "duration": 4.6}, {"text": "So what I'd like to share with you now is really,", "start": 2778.24, "duration": 5.29}, {"text": "probably by far the most commonly used", "start": 2783.53, "duration": 2.22}, {"text": "classification algorithm ah, called logistic regression.", "start": 2785.75, "duration": 3.3}, {"text": "Now let's say the two learning algorithms", "start": 2789.05, "duration": 6.45}, {"text": "I probably use the most often are linear regression and logistic regression.", "start": 2795.5, "duration": 3.825}, {"text": "Yeah, probably these two, actually. Um, and, uh, this is the algorithm.", "start": 2799.325, "duration": 7.05}, {"text": "So, um, as- as we designed a logistic regression algorithm,", "start": 2806.375, "duration": 4.185}, {"text": "one of the things we might naturally want is for", "start": 2810.56, "duration": 3.585}, {"text": "the hypothesis to output values between 0 and 1.", "start": 2814.145, "duration": 4.86}, {"text": "Right. And this is mathematical notation for the values for H of X or H prime,", "start": 2819.005, "duration": 5.7}, {"text": "H subscript theta of X, uh, lies in the set from 0 to 1.", "start": 2824.705, "duration": 4.59}, {"text": "Right? This 0 to 1 square bracket is the set of all real numbers from 0 to 1.", "start": 2829.295, "duration": 3.9}, {"text": "So this says, we want the hypothesis output values in you know", "start": 2833.195, "duration": 3.645}, {"text": "between 0 and 1, so that in the set of all numbers between z- from 0 to  1.", "start": 2836.84, "duration": 4.14}, {"text": "Um, and so we're going to choose the following form of the hypothesis.", "start": 2840.98, "duration": 5.655}, {"text": "Um, so. Okay. So we're gonna define a function,", "start": 2846.635, "duration": 16.57}, {"text": "g of z, that looks like this.", "start": 2863.205, "duration": 4.605}, {"text": "And this is called the sigmoid, uh,", "start": 2867.81, "duration": 4.73}, {"text": "or the logistic function.", "start": 2872.54, "duration": 6.01}, {"text": "Uh, these are synonyms,", "start": 2878.55, "duration": 1.29}, {"text": "they mean exactly the same thing.", "start": 2879.84, "duration": 1.32}, {"text": "So, uh, it can be called the sigmoid function,", "start": 2881.16, "duration": 2.52}, {"text": "or the logistic function,", "start": 2883.68, "duration": 1.08}, {"text": "it means exactly the same thing.", "start": 2884.76, "duration": 1.455}, {"text": "But we're gonna choose a function, g of z.", "start": 2886.215, "duration": 3.225}, {"text": "Uh, and this function is shaped as follows.", "start": 2889.44, "duration": 3.12}, {"text": "If you plot this function,", "start": 2892.56, "duration": 1.455}, {"text": "you find that it looks like this.", "start": 2894.015, "duration": 2.88}, {"text": "Um, where if the horizontal axis is z,", "start": 2896.895, "duration": 4.05}, {"text": "then this is g of z.", "start": 2900.945, "duration": 1.695}, {"text": "And so it crosses x intercept at 0,", "start": 2902.64, "duration": 3.81}, {"text": "um, and it, you know,", "start": 2906.45, "duration": 2.49}, {"text": "starts off, well, really,", "start": 2908.94, "duration": 1.47}, {"text": "really close to 0,", "start": 2910.41, "duration": 1.23}, {"text": "rises, and then asymptotes towards 1.", "start": 2911.64, "duration": 4.59}, {"text": "Okay? And so g of z output values are between 0 and 1.", "start": 2916.23, "duration": 4.89}, {"text": "And, um, what logistic regression does is instead of- let's see.", "start": 2921.12, "duration": 6.53}, {"text": "So previously, for linear regression,", "start": 2927.65, "duration": 2.1}, {"text": "we had chosen this form for the hypothesis, right?", "start": 2929.75, "duration": 3.18}, {"text": "We just made a choice that we'll say", "start": 2932.93, "duration": 1.53}, {"text": "the housing prices are a linear function of the features x.", "start": 2934.46, "duration": 3.39}, {"text": "And what logistic regression does is theta transpose x could be bigger than 1,", "start": 2937.85, "duration": 4.45}, {"text": "it can be less than 0, which is not very natural.", "start": 2942.3, "duration": 2.175}, {"text": "But instead, it's going to take theta transpose x and pass it", "start": 2944.475, "duration": 3.105}, {"text": "through this sigmoid function g. So this force,", "start": 2947.58, "duration": 3.6}, {"text": "the output values only between 0 and 1.", "start": 2951.18, "duration": 3.18}, {"text": "Okay? Um, so you know,", "start": 2954.36, "duration": 5.55}, {"text": "when designing a learning algorithm, uh,", "start": 2959.91, "duration": 2.145}, {"text": "sometimes you just have to choose the form of the hypothesis.", "start": 2962.055, "duration": 3.15}, {"text": "How are you gonna represent the function h,", "start": 2965.205, "duration": 2.475}, {"text": "or h of- h subscript theta.", "start": 2967.68, "duration": 1.515}, {"text": "And so we're making that choice here today.", "start": 2969.195, "duration": 2.715}, {"text": "And if you're wondering,", "start": 2971.91, "duration": 2.31}, {"text": "you know, there are lots of functions that we could have chosen, right?", "start": 2974.22, "duration": 3.03}, {"text": "There are lots of why, why not, why not this function?", "start": 2977.25, "duration": 3.81}, {"text": "Or why not, you know, there are lots of functions with vaguely this shape,", "start": 2981.06, "duration": 2.79}, {"text": "they go between 0 and 1.", "start": 2983.85, "duration": 1.245}, {"text": "So why are we choosing this specifically?", "start": 2985.095, "duration": 3.03}, {"text": "It turns out that there's a broader class of algorithms called generalized linear models.", "start": 2988.125, "duration": 4.125}, {"text": "You'll hear about on Wednesday, uh,", "start": 2992.25, "duration": 1.335}, {"text": "of which this is a special case.", "start": 2993.585, "duration": 1.98}, {"text": "So we've seen linear regression,", "start": 2995.565, "duration": 1.65}, {"text": "you'll see logistic regression in a second, and on Wednesday,", "start": 2997.215, "duration": 2.865}, {"text": "you'll see that both of these examples of a much bigger set", "start": 3000.08, "duration": 2.7}, {"text": "of algorithms derived using a broader set of principles.", "start": 3002.78, "duration": 2.79}, {"text": "So, so for now, just, you know,", "start": 3005.57, "duration": 1.71}, {"text": "take my word for it tha- that we want to use the logistic function.", "start": 3007.28, "duration": 3.36}, {"text": "Uh, uh, it'll turn out- you'll see on Wednesday that there's", "start": 3010.64, "duration": 2.4}, {"text": "a way to derive even this function from,", "start": 3013.04, "duration": 2.445}, {"text": "uh, from more basic principles,", "start": 3015.485, "duration": 2.205}, {"text": "rather than just putting all this, this all out.", "start": 3017.69, "duration": 1.83}, {"text": "But for now, let me just pull this out of a hat and say,", "start": 3019.52, "duration": 2.55}, {"text": "that's the one we want to use.", "start": 3022.07, "duration": 1.53}, {"text": "Okay.", "start": 3023.6, "duration": 0.39}, {"text": "[NOISE]", "start": 3023.99, "duration": 6.24}, {"text": "So, um, let's make some assumptions", "start": 3030.23, "duration": 17.37}, {"text": "about the distribution of y given x parameterized by theta.", "start": 3047.6, "duration": 5.085}, {"text": "So I'm going to assume that the data has the following distribution.", "start": 3052.685, "duration": 6.375}, {"text": "The probability of y being 1, uh, again,", "start": 3059.06, "duration": 2.805}, {"text": "from the breast cancer prediction that we had,", "start": 3061.865, "duration": 2.295}, {"text": "from, uh, the first lecture.", "start": 3064.16, "duration": 2.205}, {"text": "Right? It will be the chance of a tumor being cancerous,", "start": 3066.365, "duration": 3.194}, {"text": "or being, um, um, malignant.", "start": 3069.559, "duration": 2.026}, {"text": "Chance of y being 1, given the size of the tumor,", "start": 3071.585, "duration": 3.075}, {"text": "that's the feature x parameterized by theta.", "start": 3074.66, "duration": 3.465}, {"text": "That this is equal to the output of your hypothesis.", "start": 3078.125, "duration": 6.045}, {"text": "So in other words, we're gonna assume that, um,", "start": 3084.17, "duration": 2.4}, {"text": "what you want your learning algorithm to do is input", "start": 3086.57, "duration": 3.18}, {"text": "the features and tell me what's the chance that this tumor is malignant.", "start": 3089.75, "duration": 4.59}, {"text": "Right? What's the chance that y is equal to 1?", "start": 3094.34, "duration": 2.28}, {"text": "Um, and by logic, I guess,", "start": 3096.62, "duration": 4.5}, {"text": "because y can be only 1 or 0,", "start": 3101.12, "duration": 2.445}, {"text": "the chance of y being equal to 0,", "start": 3103.565, "duration": 2.595}, {"text": "this has got to be 1 minus that.", "start": 3106.16, "duration": 3.73}, {"text": "Right? Because if a tumor has a 10% chance of being malignant,", "start": 3110.71, "duration": 4.435}, {"text": "that means it has a 1 minus that.", "start": 3115.145, "duration": 2.415}, {"text": "It means it must have a 90% chance of being benign.", "start": 3117.56, "duration": 2.295}, {"text": "Right? Since these two probabilities must add up to 1. Okay? Yeah.", "start": 3119.855, "duration": 3.525}, {"text": "[inaudible]", "start": 3123.38, "duration": 10.92}, {"text": "Say that again.", "start": 3134.3, "duration": 0.57}, {"text": "[inaudible].", "start": 3134.87, "duration": 3.45}, {"text": "Oh, can we change the parameters here? Yes, you can,", "start": 3138.32, "duration": 2.13}, {"text": "but I'm not- yeah.", "start": 3140.45, "duration": 1.05}, {"text": "But I think just to stick with convention in logistic regression. You, you- yeah.", "start": 3141.5, "duration": 4.185}, {"text": "Sure. We can assume that p of y equals 1 was this,", "start": 3145.685, "duration": 2.355}, {"text": "and p of y equals 1 was that, but I think either way.", "start": 3148.04, "duration": 2.295}, {"text": "It's just one you call positive example,", "start": 3150.335, "duration": 1.425}, {"text": "one you call a negative example.", "start": 3151.76, "duration": 1.485}, {"text": "Right. So, so, uh, use this convention.", "start": 3153.245, "duration": 2.535}, {"text": "Okay. Um, and now,", "start": 3155.78, "duration": 3.285}, {"text": "bearing in mind that y, right?", "start": 3159.065, "duration": 3.75}, {"text": "By definition, because it is a binary classification problem.", "start": 3162.815, "duration": 3.375}, {"text": "But bear in mind that y can only take on two values, 0 or 1.", "start": 3166.19, "duration": 4.02}, {"text": "Um, there's a nifty,", "start": 3170.21, "duration": 2.699}, {"text": "sort of little algebra way to take these two equations and write them in one equation,", "start": 3172.909, "duration": 5.691}, {"text": "and this will make some of the math a little bit easier.", "start": 3178.6, "duration": 1.83}, {"text": "When I take these two equations,", "start": 3180.43, "duration": 1.5}, {"text": "take these two assumptions and take these two facts,", "start": 3181.93, "duration": 2.325}, {"text": "and compress it into one equation, which is this.", "start": 3184.255, "duration": 3.735}, {"text": "[NOISE] [BACKGROUND] Okay?", "start": 3187.99, "duration": 7.165}, {"text": "Oh, and I dropped the theta subscript just to simplify the notation of it.", "start": 3195.155, "duration": 3.795}, {"text": "But I'm, I'm gonna be a little bit sloppy sometimes.", "start": 3198.95, "duration": 2.25}, {"text": "Well, a little less formal,", "start": 3201.2, "duration": 1.26}, {"text": "whether I write the theta there or not.", "start": 3202.46, "duration": 1.605}, {"text": "Okay? Um, but these two definitions of p of y given x parameterized by theta,", "start": 3204.065, "duration": 6.015}, {"text": "bearing in mind that y is either 0 or 1,", "start": 3210.08, "duration": 2.175}, {"text": "can be compressed into one equation like this.", "start": 3212.255, "duration": 2.685}, {"text": "Uh, and, and let me just say why.", "start": 3214.94, "duration": 2.25}, {"text": "Right? It's because if y- use a different color.", "start": 3217.19, "duration": 8.05}, {"text": "Right. If y is equal to 1,", "start": 3225.76, "duration": 5.02}, {"text": "then this becomes h of x to the power of 1 times this thing to the power of 0.", "start": 3230.78, "duration": 6.675}, {"text": "Right? If y is equal to 1, then,", "start": 3237.455, "duration": 2.22}, {"text": "um, 1 - y is 0.", "start": 3239.675, "duration": 2.775}, {"text": "And, you know, anything to the power of 0 is just equal to 1.", "start": 3242.45, "duration": 6.15}, {"text": "[NOISE] And so if y is equal to 1,", "start": 3248.6, "duration": 3.45}, {"text": "you end up with p of y given x parameterized by theta equals h of x.", "start": 3252.05, "duration": 6.07}, {"text": "Right? Which is just what we had there.", "start": 3258.13, "duration": 5.54}, {"text": "And conversely, if y is equal to 0,", "start": 3263.98, "duration": 5.049}, {"text": "then, um, this thing will be 0, and this thing will be 1.", "start": 3269.029, "duration": 5.431}, {"text": "And so you end up with p of y given x parameterized theta is equal to 1 minus h of x,", "start": 3274.46, "duration": 6.48}, {"text": "which is just equal to that second equation.", "start": 3280.94, "duration": 4.215}, {"text": "Okay? Right. Um, and so this is a nifty way", "start": 3285.155, "duration": 5.505}, {"text": "to take these two equations and compress them into one line,", "start": 3290.66, "duration": 3.225}, {"text": "because depending on whether y is 0 or 1,", "start": 3293.885, "duration": 2.79}, {"text": "one of these two terms switches off,", "start": 3296.675, "duration": 2.19}, {"text": "because it's exponentiated to the power of 0.", "start": 3298.865, "duration": 2.895}, {"text": "Um, and anything to the power of 0 is just equal to 1.", "start": 3301.76, "duration": 3.42}, {"text": "Right? So one of these terms is just, you know, 1.", "start": 3305.18, "duration": 3.165}, {"text": "Just leaving the other term, and just selecting the,", "start": 3308.345, "duration": 2.67}, {"text": "the appropriate equation, depending on whether y is 0 or 1.", "start": 3311.015, "duration": 2.91}, {"text": "Okay? So with that, um, uh, so with this little, uh,", "start": 3313.925, "duration": 5.19}, {"text": "on a notational trick,", "start": 3319.115, "duration": 1.77}, {"text": "it will make the data derivations simpler.", "start": 3320.885, "duration": 4.395}, {"text": "Okay? Um, yeah. So let me use a new board.", "start": 3325.28, "duration": 17.64}, {"text": "[NOISE] I want that.", "start": 3342.92, "duration": 2.77}, {"text": "All right. Actually we can reuse along with this.", "start": 3353.26, "duration": 3.92}, {"text": "All right. So, uh,", "start": 3358.99, "duration": 2.305}, {"text": "we're gonna use maximum likelihood estimation again.", "start": 3361.295, "duration": 2.31}, {"text": "So let's write down the likelihood of the parameters.", "start": 3363.605, "duration": 5.085}, {"text": "Um, so well, it's actually p of all the y's given all the", "start": 3368.69, "duration": 3.96}, {"text": "x's parameterized by theta was equal to this, uh,", "start": 3372.65, "duration": 4.56}, {"text": "which is now equal to product from i equals 1 through m,", "start": 3377.21, "duration": 3.765}, {"text": "h of x_i to the power of y_i,", "start": 3380.975, "duration": 4.62}, {"text": "times 1 minus h of x_i to the power of 1 minus y_i.", "start": 3385.595, "duration": 6.825}, {"text": "Okay. Where all I did was take this definition of p of", "start": 3392.42, "duration": 3.96}, {"text": "y given x parameterized by theta, uh, you know, from that,", "start": 3396.38, "duration": 3.275}, {"text": "after we did that little exponentiation trick and wrote it in here.", "start": 3399.655, "duration": 4.38}, {"text": "Okay. Um. [NOISE]", "start": 3404.035, "duration": 6.57}, {"text": "And then, uh, with maximum likelihood estimation", "start": 3410.605, "duration": 4.26}, {"text": "we'll want to find the value of theta that maximizes the likelihood,", "start": 3414.865, "duration": 4.095}, {"text": "maximizes the likelihood of the parameters.", "start": 3418.96, "duration": 2.37}, {"text": "And so, um, same as what we did for linear regression to make the algebra,", "start": 3421.33, "duration": 5.925}, {"text": "you have to, to, to make the algebra a bit more simple,", "start": 3427.255, "duration": 2.61}, {"text": "we're going to take the log of the likelihood and so compute the log likelihood.", "start": 3429.865, "duration": 4.185}, {"text": "And so that's equal to, um, [NOISE] let's see, right.", "start": 3434.05, "duration": 7.125}, {"text": "And so if you take the log of that,", "start": 3441.175, "duration": 1.845}, {"text": "um, you end up with- you end up with that.", "start": 3443.02, "duration": 20.05}, {"text": "Okay? And, um, it- so, so, in other words, uh,", "start": 3465.42, "duration": 8.095}, {"text": "the last thing you want to do is,", "start": 3473.515, "duration": 2.055}, {"text": "try to choose the value of theta to try to", "start": 3475.57, "duration": 7.68}, {"text": "maximize L of theta.", "start": 3483.25, "duration": 4.77}, {"text": "Okay. Now, so, so just,", "start": 3488.02, "duration": 3.84}, {"text": "just to summarize where we are, right.", "start": 3491.86, "duration": 1.86}, {"text": "Uh, if you're trying to predict,", "start": 3493.72, "duration": 1.62}, {"text": "your malignancy and benign, uh,", "start": 3495.34, "duration": 2.535}, {"text": "tumors, you'd have a training set with XI YI.", "start": 3497.875, "duration": 3.855}, {"text": "You define the likelihood, define the log-likelihood.", "start": 3501.73, "duration": 3.825}, {"text": "And then what you need to do is have an algorithm", "start": 3505.555, "duration": 2.355}, {"text": "such as gradient descent, or gradient descent, talk about that in", "start": 3507.91, "duration": 2.34}, {"text": "a sec to try to find the value of theta that maximizes the log-likelihood.", "start": 3510.25, "duration": 4.815}, {"text": "And then having chosen the value of theta when a new patient", "start": 3515.065, "duration": 4.125}, {"text": "walks into the doctor's office you would take the features of the new tumor", "start": 3519.19, "duration": 4.41}, {"text": "and then use H of theta to estimate the chance of this new tumor in the new patient", "start": 3523.6, "duration": 5.4}, {"text": "that walks in tomorrow to estimate the chance that", "start": 3529.0, "duration": 2.13}, {"text": "this new thing is ah is- is malignant or benign.", "start": 3531.13, "duration": 3.66}, {"text": "Okay? So the algorithm we're going to use to", "start": 3534.79, "duration": 7.05}, {"text": "choose theta to try to maximize the log-likelihood is", "start": 3541.84, "duration": 2.82}, {"text": "a gradient ascent or batch gradient ascent.", "start": 3544.66, "duration": 4.635}, {"text": "And what that means is we will update the parameters theta J according to theta J", "start": 3549.295, "duration": 7.635}, {"text": "plus the partial derivative with respect to the log-likelihood.", "start": 3556.93, "duration": 8.16}, {"text": "Okay? Um, and the differences from what you saw", "start": 3565.09, "duration": 3.495}, {"text": "that linear regression from last time is the following.", "start": 3568.585, "duration": 4.74}, {"text": "Just two differences I guess.", "start": 3573.325, "duration": 2.04}, {"text": "For linear regression. Last week,", "start": 3575.365, "duration": 2.955}, {"text": "I have written this down,", "start": 3578.32, "duration": 1.35}, {"text": "theta J gets updated as theta J minus", "start": 3579.67, "duration": 3.195}, {"text": "partial with respect to theta J of J of theta, right?", "start": 3582.865, "duration": 3.585}, {"text": "So you saw this on Wednesday.", "start": 3586.45, "duration": 1.455}, {"text": "So the two differences between that is well,", "start": 3587.905, "duration": 3.465}, {"text": "first instead of J of theta you're now", "start": 3591.37, "duration": 3.63}, {"text": "trying to optimize the log-likelihood instead of this squared cost function.", "start": 3595.0, "duration": 4.185}, {"text": "And the second change is, previously you were trying to minimize the squared error.", "start": 3599.185, "duration": 4.2}, {"text": "That's why we had the minus.", "start": 3603.385, "duration": 1.665}, {"text": "And today you're trying to maximize the log-likelihood which is why there's a plus sign.", "start": 3605.05, "duration": 5.715}, {"text": "Okay? And so, um, so gradient descent you know,", "start": 3610.765, "duration": 5.625}, {"text": "is trying to climb down this hill whereas gradient ascent has a,", "start": 3616.39, "duration": 6.045}, {"text": "um, uh, has a- has a concave function like this.", "start": 3622.435, "duration": 3.645}, {"text": "And it's trying to, like,", "start": 3626.08, "duration": 2.145}, {"text": "climb up the hill rather than climb down the hill.", "start": 3628.225, "duration": 3.015}, {"text": "So that's why there's a plus symbol here instead of", "start": 3631.24, "duration": 3.27}, {"text": "a minus symbol because we are trying to maximize", "start": 3634.51, "duration": 2.16}, {"text": "the function rather than minimize the function.", "start": 3636.67, "duration": 3.7}, {"text": "So the last thing to really flesh out this algorithm which is done in the lecture notes,", "start": 3640.8, "duration": 6.94}, {"text": "but I don't want to do it here today is to plug in", "start": 3647.74, "duration": 4.02}, {"text": "the definition of H of theta into this equation and then take this thing.", "start": 3651.76, "duration": 5.355}, {"text": "So that's the log-likelihood of theta and then through", "start": 3657.115, "duration": 4.485}, {"text": "calculus and algebra you can take derivatives of this whole thing with respect to theta.", "start": 3661.6, "duration": 5.58}, {"text": "This is done in detail in the lecture notes.", "start": 3667.18, "duration": 1.77}, {"text": "I don't want to use this in class,", "start": 3668.95, "duration": 1.26}, {"text": "but go ahead and take derivatives of this big formula with respect to", "start": 3670.21, "duration": 4.215}, {"text": "the parameters theta in order to figure out what is that thing, right?", "start": 3674.425, "duration": 4.725}, {"text": "What is this thing that I just circled?", "start": 3679.15, "duration": 1.905}, {"text": "And it turns out that if you do", "start": 3681.055, "duration": 2.445}, {"text": "so you will find that batch gradient ascent is the following.", "start": 3683.5, "duration": 8.25}, {"text": "You update theta J according to- oh,", "start": 3691.75, "duration": 9.6}, {"text": "actually I'm sorry, I forgot the learning rate.", "start": 3701.35, "duration": 1.89}, {"text": "Yeah, it's your learning rate Alpha.", "start": 3703.24, "duration": 1.885}, {"text": "Okay. Learning rate Alpha times this.", "start": 3705.125, "duration": 4.625}, {"text": "Okay? Because this term here is", "start": 3713.24, "duration": 4.125}, {"text": "the partial derivative respect to Theta J after log-likelihood.", "start": 3717.365, "duration": 5.985}, {"text": "Okay? And the full calculus and so on derivations given the lecture notes.", "start": 3726.9, "duration": 6.145}, {"text": "Okay? Um, yeah.", "start": 3733.045, "duration": 2.175}, {"text": "[inaudible].", "start": 3735.22, "duration": 3.72}, {"text": "Is there a chance of local maximum in this case?", "start": 3738.94, "duration": 1.635}, {"text": "No. There isn't. It turns out that this function that the log-likelihood", "start": 3740.575, "duration": 4.425}, {"text": "function L of Theta full logistic regression it always looks like that.", "start": 3745.0, "duration": 4.29}, {"text": "Uh, so this is a concave function.", "start": 3749.29, "duration": 2.265}, {"text": "So there are no local op.", "start": 3751.555, "duration": 1.605}, {"text": "The only maximum is a global maxima.", "start": 3753.16, "duration": 2.19}, {"text": "There's actually another reason why we chose the logistic function because if you", "start": 3755.35, "duration": 3.06}, {"text": "choose a logistic function rather than some other function that will give you 0 to 1,", "start": 3758.41, "duration": 3.555}, {"text": "you're guaranteed that the likelihood function has only one global maximum.", "start": 3761.965, "duration": 4.71}, {"text": "And this, there's actually a big class about, actually what you'll see on Wednesday,", "start": 3766.675, "duration": 4.635}, {"text": "this is a big class of algorithms of which linear regression is one example,", "start": 3771.31, "duration": 4.14}, {"text": "logistic regression is another example and for all of the algorithms in", "start": 3775.45, "duration": 3.18}, {"text": "this class there are no local optima problems when you- when you derive them this way.", "start": 3778.63, "duration": 3.81}, {"text": "So you see that on Wednesday when we talk about generalized linear models.", "start": 3782.44, "duration": 3.855}, {"text": "Okay? Um, so actually, but now that I think about,", "start": 3786.295, "duration": 3.525}, {"text": "there's just one question for you to think about.", "start": 3789.82, "duration": 2.475}, {"text": "This looks exactly the same as what we've figured out for linear regression, right?", "start": 3792.295, "duration": 4.74}, {"text": "That when actually the difference for linear regression was I had", "start": 3797.035, "duration": 2.955}, {"text": "a minus sign here and I reversed these two terms.", "start": 3799.99, "duration": 2.94}, {"text": "I think I had H theta of XI minus YI.", "start": 3802.93, "duration": 3.465}, {"text": "If you put the minus sign there and reverse these two terms,", "start": 3806.395, "duration": 2.805}, {"text": "so take the minus minus,", "start": 3809.2, "duration": 1.605}, {"text": "this is actually exactly the same as what we had come up with for linear regression.", "start": 3810.805, "duration": 3.765}, {"text": "So why, why, why is this different, right?", "start": 3814.57, "duration": 2.145}, {"text": "I started off saying, don't use linear regression for classification problems", "start": 3816.715, "duration": 3.285}, {"text": "because of ah because of that problem that a single example could", "start": 3820.0, "duration": 3.66}, {"text": "really you know- I started off with an example assuming that linear regression is", "start": 3823.66, "duration": 4.17}, {"text": "really bad for classification and we did", "start": 3827.83, "duration": 2.07}, {"text": "all this work and I came back to the same algorithm.", "start": 3829.9, "duration": 2.16}, {"text": "So what happened? Just, yeah go ahead.", "start": 3832.06, "duration": 3.9}, {"text": "[BACKGROUND].", "start": 3835.96, "duration": 3.96}, {"text": "Yeah. All right, cool. Awesome. Right? So what happened is", "start": 3839.92, "duration": 2.07}, {"text": "the definition of H of theta is now different than", "start": 3841.99, "duration": 2.55}, {"text": "before but the surface level of the equation turns out to be the same.", "start": 3844.54, "duration": 5.145}, {"text": "Okay? And again it turns out that for every algorithm in", "start": 3849.685, "duration": 3.195}, {"text": "this class of algorithms you'll see you on Wednesday you end up with the same thing.", "start": 3852.88, "duration": 3.33}, {"text": "Actually this is a general property of a much bigger class of algorithms", "start": 3856.21, "duration": 3.63}, {"text": "called generalized linear models.", "start": 3859.84, "duration": 2.37}, {"text": "Although, yeah, i- i- interesting historical diverge, because of the confusion", "start": 3862.21, "duration": 6.78}, {"text": "between these two algorithms in the early history of machine", "start": 3868.99, "duration": 2.67}, {"text": "learning there was some debate about you know between academics saying,", "start": 3871.66, "duration": 3.36}, {"text": "no, I invented that, no, I invented that.", "start": 3875.02, "duration": 2.235}, {"text": "And then he goes, no, it's actually different algorithms.", "start": 3877.255, "duration": 2.985}, {"text": "[LAUGHTER] Alright, any questions? Oh go ahead.", "start": 3880.24, "duration": 7.14}, {"text": "[BACKGROUND].", "start": 3887.38, "duration": 5.97}, {"text": "Oh, great question. Is there a equivalent of normal equations to logistic regression?", "start": 3893.35, "duration": 4.695}, {"text": "Um, short answer is no.", "start": 3898.045, "duration": 2.25}, {"text": "So for linear regression the normal equations", "start": 3900.295, "duration": 2.985}, {"text": "gives you like a one shot way to just find the best value of theta.", "start": 3903.28, "duration": 3.075}, {"text": "There is no known way to just have a close form equation", "start": 3906.355, "duration": 3.42}, {"text": "unless you find the best value of theta which is why you always have to use an algorithm,", "start": 3909.775, "duration": 4.155}, {"text": "an iterative optimization algorithm such as", "start": 3913.93, "duration": 2.07}, {"text": "gradient ascent or ah and we'll see in a second Newton's method.", "start": 3916.0, "duration": 4.72}, {"text": "All right, cool. So, um, there's a great lead in to, um,", "start": 3921.12, "duration": 9.1}, {"text": "the last topic for today which is Newton's method.", "start": 3930.22, "duration": 6.3}, {"text": "[NOISE]", "start": 3936.52, "duration": 20.6}, {"text": "Um, you know, gradient ascent right is a good algorithm.", "start": 3957.12, "duration": 3.67}, {"text": "I use gradient ascent all the time but it takes a baby step, takes a baby step,", "start": 3960.79, "duration": 3.075}, {"text": "take a baby step, it takes a lot of iterations for gradient assent to converge.", "start": 3963.865, "duration": 4.56}, {"text": "Um, there's another algorithm called Newton's method", "start": 3968.425, "duration": 3.03}, {"text": "which allows you to take much bigger jumps so that's theta,", "start": 3971.455, "duration": 3.21}, {"text": "you know, so- so, uh,", "start": 3974.665, "duration": 1.65}, {"text": "there are problems where you might need you know,say", "start": 3976.315, "duration": 2.445}, {"text": "100 iterations or 1000 iterations of gradient ascent.", "start": 3978.76, "duration": 3.195}, {"text": "That if you run this algorithm called Newton's method you might need", "start": 3981.955, "duration": 3.585}, {"text": "only 10 iterations to get a very good value of theta.", "start": 3985.54, "duration": 4.23}, {"text": "But each iteration will be more expensive.", "start": 3989.77, "duration": 2.13}, {"text": "We'll talk about pros and cons in a second.", "start": 3991.9, "duration": 1.59}, {"text": "But, um, let's see how- let's- let's describe this algorithm which is sometimes much", "start": 3993.49, "duration": 5.34}, {"text": "faster for gradient than gradient ascent for optimizing the value of theta.", "start": 3998.83, "duration": 5.73}, {"text": "Okay? So what we'd like to do is, uh, all right,", "start": 4004.56, "duration": 5.97}, {"text": "so let me- let me use", "start": 4010.53, "duration": 1.2}, {"text": "this simplified one-dimensional problem to describe Newton's method.", "start": 4011.73, "duration": 5.29}, {"text": "So I'm going to solve a slightly different problem with Newton's method which", "start": 4022.7, "duration": 4.42}, {"text": "is say you have some function f, right,", "start": 4027.12, "duration": 4.17}, {"text": "and you want to find a theta such that f of theta is equal to 0.", "start": 4031.29, "duration": 11.025}, {"text": "Okay? So this is a problem that Newton's method solves.", "start": 4042.315, "duration": 3.225}, {"text": "And the way we're going to use this later is what you", "start": 4045.54, "duration": 3.63}, {"text": "really want is to maximize L of theta,", "start": 4049.17, "duration": 4.51}, {"text": "right, and well at the maximum the first derivative must be 0.", "start": 4057.71, "duration": 5.815}, {"text": "So i.e. you want to value where the derivative L prime of theta is equal to 0, right?", "start": 4063.525, "duration": 8.58}, {"text": "And L prime is the derivative of theta because this", "start": 4072.105, "duration": 3.465}, {"text": "is where L prime is another notation for the first derivative of theta.", "start": 4075.57, "duration": 4.05}, {"text": "So you want to maximize a function or minimize a function.", "start": 4079.62, "duration": 2.79}, {"text": "What that really means is you want to find a point where the derivative is equal to 0.", "start": 4082.41, "duration": 4.395}, {"text": "So the way we're going to use Newton's method is we're going to set F of theta equal to", "start": 4086.805, "duration": 4.365}, {"text": "the derivative and then try to find the point where the derivative is equal to 0.", "start": 4091.17, "duration": 4.53}, {"text": "Okay? But to explain Newton's method I'm gonna, you", "start": 4095.7, "duration": 3.63}, {"text": "know, work on this other problem where you have a function F and you just", "start": 4099.33, "duration": 3.54}, {"text": "want to find the value of theta where F of", "start": 4102.87, "duration": 2.04}, {"text": "theta is equal to 0 and then- and we'll set F", "start": 4104.91, "duration": 2.34}, {"text": "equal to L prime theta and that's how we'll we'll apply this to um, logistic regression.", "start": 4107.25, "duration": 5.805}, {"text": "So, let me draw in pictures how this algorithm works.", "start": 4113.055, "duration": 5.955}, {"text": "Uh. [NOISE] [BACKGROUND] All right.", "start": 4119.01, "duration": 14.61}, {"text": "So let's say that's the function f, and, you know,", "start": 4133.62, "duration": 3.66}, {"text": "to make this drawable on a whiteboard,", "start": 4137.28, "duration": 2.265}, {"text": "I'm gonna assume theta is just a real number for now.", "start": 4139.545, "duration": 2.46}, {"text": "So theta is just a single,", "start": 4142.005, "duration": 1.634}, {"text": "you know, like a scalar, a real number.", "start": 4143.639, "duration": 2.356}, {"text": "Um, so this is how Newton's method works.", "start": 4145.995, "duration": 5.265}, {"text": "Um, oh, and the goal is to find this point.", "start": 4151.26, "duration": 3.225}, {"text": "Right? The goal is to find the value of theta where f of theta is equal to 0.", "start": 4154.485, "duration": 5.46}, {"text": "Okay? So let's say you start off, um, right.", "start": 4159.945, "duration": 5.33}, {"text": "Let's say you start off at this point.", "start": 4165.275, "duration": 1.75}, {"text": "Right? At the first iteration,", "start": 4167.025, "duration": 1.559}, {"text": "you have randomly initialized data,", "start": 4168.584, "duration": 1.576}, {"text": "and actually theta is zero or something.", "start": 4170.16, "duration": 1.395}, {"text": "But let's say you start off at that point.", "start": 4171.555, "duration": 1.965}, {"text": "This is how one iteration of Newton's method will work,", "start": 4173.52, "duration": 4.65}, {"text": "which is- let me use a different color.", "start": 4178.17, "duration": 5.295}, {"text": "Right. Start off with theta 0,", "start": 4183.465, "duration": 1.755}, {"text": "that's just a first value consideration.", "start": 4185.22, "duration": 2.775}, {"text": "What we're going to do is look at the function f,", "start": 4187.995, "duration": 2.145}, {"text": "and then find a line that is just tangent to f. So take the derivative of f and", "start": 4190.14, "duration": 5.43}, {"text": "find a line that is just tangent to f. So take that red line.", "start": 4195.57, "duration": 6.3}, {"text": "It just touches the function f. And we're gonna use, if you will,", "start": 4201.87, "duration": 3.36}, {"text": "use a straight line approximation to f,", "start": 4205.23, "duration": 1.935}, {"text": "and solve for where f touches the horizontal axis.", "start": 4207.165, "duration": 4.005}, {"text": "So we're gonna solve for the point where this straight line touches the horizontal axis.", "start": 4211.17, "duration": 5.025}, {"text": "Okay? And then we're going to set this,", "start": 4216.195, "duration": 3.315}, {"text": "and that's one iteration of Newton's method.", "start": 4219.51, "duration": 2.4}, {"text": "So we're gonna move from this value to this value, right?", "start": 4221.91, "duration": 4.08}, {"text": "And then in the second iteration of Newton's method,", "start": 4225.99, "duration": 3.255}, {"text": "we're gonna look at this point.", "start": 4229.245, "duration": 2.115}, {"text": "And again, you know,", "start": 4231.36, "duration": 1.65}, {"text": "take a line that is just tangent to it,", "start": 4233.01, "duration": 2.34}, {"text": "and then solve for where this touches the horizontal axis,", "start": 4235.35, "duration": 4.65}, {"text": "and then that's after two iterations of Newton's method.", "start": 4240.0, "duration": 4.68}, {"text": "Right. And then you repeat.", "start": 4244.68, "duration": 1.815}, {"text": "Take this, sometimes you can overshoot a little bit, but that's okay.", "start": 4246.495, "duration": 3.705}, {"text": "Right? And then that's,", "start": 4250.2, "duration": 1.38}, {"text": "um, there's a cycle back to red.", "start": 4251.58, "duration": 3.015}, {"text": "Let's take the three,", "start": 4254.595, "duration": 1.665}, {"text": "then you take this, let's take the four.", "start": 4256.26, "duration": 2.31}, {"text": "[NOISE] Excuse me.", "start": 4258.57, "duration": 10.33}, {"text": "So you can tell that Newton's method is actually a pretty fast algorithm.", "start": 4271.13, "duration": 5.98}, {"text": "Right? When in just one,", "start": 4277.11, "duration": 2.19}, {"text": "two, three, four iterations,", "start": 4279.3, "duration": 2.31}, {"text": "we've gotten really really close to the point where f of theta is equal to 0.", "start": 4281.61, "duration": 6.635}, {"text": "So let's write out the math for how you do this.", "start": 4288.245, "duration": 5.07}, {"text": "So um, let's see.", "start": 4293.315, "duration": 4.03}, {"text": "I'm going to- so let me just write out the,", "start": 4297.345, "duration": 2.175}, {"text": "the derive, um, you know,", "start": 4299.52, "duration": 2.025}, {"text": "how you go from theta 0 to theta 1.", "start": 4301.545, "duration": 2.4}, {"text": "So I'm going to use this horizontal distance.", "start": 4303.945, "duration": 2.82}, {"text": "I'm gonna denote this as, uh, delta.", "start": 4306.765, "duration": 3.405}, {"text": "This triangle is uppercase Greek alphabet delta.", "start": 4310.17, "duration": 3.705}, {"text": "Right? This is lowercase delta, that's uppercase delta.", "start": 4313.875, "duration": 3.03}, {"text": "Right? Uh, and then the height here,", "start": 4316.905, "duration": 2.85}, {"text": "well that's just f of theta 0.", "start": 4319.755, "duration": 2.7}, {"text": "Right? This is the height of- it's just f of theta 0.", "start": 4322.455, "duration": 3.36}, {"text": "And so, um, let's see.", "start": 4325.815, "duration": 7.125}, {"text": "Right.", "start": 4332.94, "duration": 2.49}, {"text": "So, uh, what we'd like to do is solve for the value of delta,", "start": 4335.43, "duration": 4.65}, {"text": "because one iteration of Newton's method is a set, you know,", "start": 4340.08, "duration": 4.08}, {"text": "of theta 1 is set to theta 0 minus delta.", "start": 4344.16, "duration": 5.64}, {"text": "Right? So how do you solve for delta?", "start": 4349.8, "duration": 2.355}, {"text": "Well, from, uh, calculus we know that", "start": 4352.155, "duration": 3.405}, {"text": "the slope of the function f is the height over the run.", "start": 4355.56, "duration": 3.63}, {"text": "Well, height over the width.", "start": 4359.19, "duration": 1.38}, {"text": "And so we know that the derivative of del- f prime,", "start": 4360.57, "duration": 4.29}, {"text": "that's the derivative of f at the point theta 0,", "start": 4364.86, "duration": 3.45}, {"text": "that's equal to the height, that's f of theta,", "start": 4368.31, "duration": 3.39}, {"text": "divided by the horizontal. Right? So the derivative,", "start": 4371.7, "duration": 5.055}, {"text": "meaning the slope of the red line is by definition the derivative is", "start": 4376.755, "duration": 3.225}, {"text": "this ratio between this height over this width.", "start": 4379.98, "duration": 3.69}, {"text": "Um, and so delta is equal to f of theta 0 over f prime of theta 0.", "start": 4383.67, "duration": 9.69}, {"text": "And if you plug that in,", "start": 4393.36, "duration": 1.98}, {"text": "then you find that a single iteration of Newton's method is", "start": 4395.34, "duration": 4.05}, {"text": "the following rule of theta t plus 1 gets updated as", "start": 4399.39, "duration": 5.625}, {"text": "theta t minus f of theta t over f prime of theta t. Okay.", "start": 4405.015, "duration": 11.625}, {"text": "Where instead of 0 and 1 I replaced them with t and t plus 1.", "start": 4416.64, "duration": 5.1}, {"text": "Right? Um, and finally to, to- you know,", "start": 4421.74, "duration": 5.16}, {"text": "the very first thing we did was let's let f of theta be equal to say L prime of theta.", "start": 4426.9, "duration": 8.355}, {"text": "Right? Because we wanna find the place where the first derivative of L is 0.", "start": 4435.255, "duration": 5.205}, {"text": "Then this becomes theta t plus 1,", "start": 4440.46, "duration": 3.525}, {"text": "gets updated as theta t minus L prime of", "start": 4443.985, "duration": 4.725}, {"text": "theta t over L double prime of theta t. So it's really,", "start": 4448.71, "duration": 7.46}, {"text": "uh, the first derivative divided by the second derivative.", "start": 4456.17, "duration": 3.675}, {"text": "Okay?", "start": 4459.845, "duration": 1.525}, {"text": "So Newton's method", "start": 4462.64, "duration": 17.48}, {"text": "is a very fast algorithm,", "start": 4480.12, "duration": 1.62}, {"text": "and, uh, it has, um,", "start": 4481.74, "duration": 3.48}, {"text": "Newton's method enjoys a property called quadratic convergence.", "start": 4485.22, "duration": 5.77}, {"text": "Not a great name. Don't worry- don't worry too much about what it means.", "start": 4491.36, "duration": 3.985}, {"text": "But informally, what it means is that, um,", "start": 4495.345, "duration": 2.625}, {"text": "if on one iteration Newton's method has 0.01 error,", "start": 4497.97, "duration": 5.61}, {"text": "so on the X axis,", "start": 4503.58, "duration": 1.23}, {"text": "you're 0.01 away from the,", "start": 4504.81, "duration": 2.055}, {"text": "from the value, from the true minimum,", "start": 4506.865, "duration": 2.115}, {"text": "or the true value of f is equal to 0.", "start": 4508.98, "duration": 2.07}, {"text": "Um, after one iteration,", "start": 4511.05, "duration": 1.92}, {"text": "the error could go to 0.0001 error,", "start": 4512.97, "duration": 3.855}, {"text": "and after two iterations it goes 0.00000001.", "start": 4516.825, "duration": 3.595}, {"text": "But roughly Newton's method,", "start": 4524.09, "duration": 2.83}, {"text": "um, under certain assumptions, uh, uh,", "start": 4526.92, "duration": 2.625}, {"text": "that functions move not too far from quadratic,", "start": 4529.545, "duration": 2.76}, {"text": "the number of significant digits that you have", "start": 4532.305, "duration": 2.835}, {"text": "converged, the minimum doubles on a single iteration.", "start": 4535.14, "duration": 3.03}, {"text": "So this is called quadratic convergence.", "start": 4538.17, "duration": 1.725}, {"text": "Um, and so when you get near the minimum,", "start": 4539.895, "duration": 2.115}, {"text": "Newton's method converges extremely rapidly.", "start": 4542.01, "duration": 2.775}, {"text": "Right? So, so after a single iteration, it becomes much more accurate,", "start": 4544.785, "duration": 2.715}, {"text": "after another iteration it becomes way, way, way more accurate,", "start": 4547.5, "duration": 2.46}, {"text": "which is why Newton's method requires relatively few iterations.", "start": 4549.96, "duration": 4.98}, {"text": "Um, and, uh, let's see.", "start": 4554.94, "duration": 3.39}, {"text": "I have written out Newton's method for when theta is a real number.", "start": 4558.33, "duration": 5.28}, {"text": "Um, when theta is a vector, right?", "start": 4563.61, "duration": 6.22}, {"text": "Then the generalization of the rule I wrote above is the following,", "start": 4572.72, "duration": 4.569}, {"text": "theta t plus 1 gets updated as theta t plus H that,", "start": 4577.289, "duration": 7.816}, {"text": "where H is the Hessian matrix.", "start": 4585.105, "duration": 4.975}, {"text": "So these details are written in the lecture notes.", "start": 4595.04, "duration": 3.16}, {"text": "Um, but to give you a sense,", "start": 4598.2, "duration": 2.115}, {"text": "it- when theta is a vector,", "start": 4600.315, "duration": 2.039}, {"text": "this is the vector of derivatives.", "start": 4602.354, "duration": 3.626}, {"text": "All right, so  I guess this R_n plus 1 dimensional.", "start": 4606.56, "duration": 3.46}, {"text": "If theta is in R_n plus 1,", "start": 4610.02, "duration": 4.575}, {"text": "then this derivative respect to theta", "start": 4614.595, "duration": 3.135}, {"text": "of the log-likelihood becomes a vector of derivatives,", "start": 4617.73, "duration": 3.18}, {"text": "and the Hessian matrix,", "start": 4620.91, "duration": 1.815}, {"text": "this becomes a matrix as R_n plus 1 by n plus 1.", "start": 4622.725, "duration": 5.595}, {"text": "So it becomes a squared matrix with the dimension equal to the parameter vector theta.", "start": 4628.32, "duration": 4.8}, {"text": "And the Hessian matrix is defined as the matrix of partial derivatives.", "start": 4633.12, "duration": 5.71}, {"text": "Right? So um, [NOISE] and so", "start": 4642.53, "duration": 3.67}, {"text": "the disadvantage of Newton's method is that in high-dimensional problems,", "start": 4646.2, "duration": 5.474}, {"text": "if theta is a vector,", "start": 4651.674, "duration": 1.606}, {"text": "then each step of Newton's method is much more expensive,", "start": 4653.28, "duration": 3.449}, {"text": "because, um, you're, you're either solving a linear system equations,", "start": 4656.729, "duration": 3.391}, {"text": "or having to invert a pretty big matrix.", "start": 4660.12, "duration": 2.22}, {"text": "So if theta is ten-dimensional,", "start": 4662.34, "duration": 2.85}, {"text": "you know, this involves inverting a 10 by 10 matrix, which is fine.", "start": 4665.19, "duration": 3.6}, {"text": "But if theta was 10,000 or 100,000,", "start": 4668.79, "duration": 2.91}, {"text": "then each iteration requires computing like a", "start": 4671.7, "duration": 3.15}, {"text": "100,000 by a 100,000 matrix and inverting that, which is very hard.", "start": 4674.85, "duration": 3.345}, {"text": "Right? It's actually very difficult to do that in very high-dimensional problems.", "start": 4678.195, "duration": 3.81}, {"text": "Um, so, you know, some rules of thumb,", "start": 4682.005, "duration": 4.515}, {"text": "um, if the number of parameters you have", "start": 4686.52, "duration": 2.67}, {"text": "for- if the number of parameters in your iteration is not too big,", "start": 4689.19, "duration": 2.745}, {"text": "if you have 10 parameters, or 50 parameters,", "start": 4691.935, "duration": 3.015}, {"text": "I would almost certainly- I would very likely use Newton's method,", "start": 4694.95, "duration": 4.815}, {"text": "uh, because then you probably get convergence in maybe 10 iterations,", "start": 4699.765, "duration": 4.695}, {"text": "or, you know, 15 iterations, or even less than 10 iterations.", "start": 4704.46, "duration": 3.735}, {"text": "But if you have a very large number of parameters,", "start": 4708.195, "duration": 2.085}, {"text": "if you have, you know, 10,000 parameters,", "start": 4710.28, "duration": 1.98}, {"text": "then rather than dealing with a 10,000 by 10,000 matrix, or even bigger,", "start": 4712.26, "duration": 4.68}, {"text": "the 50 by 1000 by 50,000 matrix,", "start": 4716.94, "duration": 2.28}, {"text": "and you have 50,000 parameters,", "start": 4719.22, "duration": 1.365}, {"text": "I will use, uh, gradient descent then.", "start": 4720.585, "duration": 2.415}, {"text": "Okay? But if the number of parameters is not too big,", "start": 4723.0, "duration": 3.15}, {"text": "so that the computational cost per iteration is manageable,", "start": 4726.15, "duration": 3.51}, {"text": "then Newton's method converges in a very small number of iterations,", "start": 4729.66, "duration": 3.69}, {"text": "and, and could be much faster algorithm than gradient descent.", "start": 4733.35, "duration": 3.225}, {"text": "All right. So, um, that's it for, uh, Newton's method.", "start": 4736.575, "duration": 6.195}, {"text": "Um, on Wednesday, I guess we are running out of time.", "start": 4742.77, "duration": 2.775}, {"text": "On Wednesday, you'll hear about generalized linear models.", "start": 4745.545, "duration": 2.28}, {"text": "Um, I think unfortunately I- I promised to be in Washington DC,", "start": 4747.825, "duration": 4.14}, {"text": "uh, uh, tonight, I guess through Wednesday.", "start": 4751.965, "duration": 2.1}, {"text": "So, uh, you'll hear from some- I think Anand will give the lecture on Wednesday,", "start": 4754.065, "duration": 4.905}, {"text": "uh, but I will be back next week.", "start": 4758.97, "duration": 2.445}, {"text": "So un- unfortunately was trying to do this,", "start": 4761.415, "duration": 2.145}, {"text": "but because of his health things, he can't lecture.", "start": 4763.56, "duration": 2.295}, {"text": "So Anand will do this Wednesday.", "start": 4765.855, "duration": 1.86}, {"text": "Thanks everyone. See you on Wednesday.", "start": 4767.715, "duration": 1.815}]