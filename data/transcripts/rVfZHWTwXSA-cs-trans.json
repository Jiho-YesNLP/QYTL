[{"text": "All right. [NOISE] Um, let's get started.", "start": 3.53, "duration": 4.27}, {"text": "So, um, let's see, logistical reminder,", "start": 7.8, "duration": 3.9}, {"text": "uh the class midterm, um,", "start": 11.7, "duration": 2.64}, {"text": "is this Wednesday and it's 48-hour take-home midterm.", "start": 14.34, "duration": 3.375}, {"text": "Um, and the logistical details you can find,", "start": 17.715, "duration": 3.105}, {"text": "uh, at this Piazza post, okay?", "start": 20.82, "duration": 2.22}, {"text": "So the midterm will start Wednesday evening.", "start": 23.04, "duration": 1.92}, {"text": "You have 48 hours to do it and then submit it online through Gradescope, uh,", "start": 24.96, "duration": 4.05}, {"text": "and because of the midterm,", "start": 29.01, "duration": 2.16}, {"text": "there won't be a section,", "start": 31.17, "duration": 1.32}, {"text": "uh, this Friday, okay?", "start": 32.49, "duration": 1.545}, {"text": "Oh and the midterm will cover everything up to and including EM,", "start": 34.035, "duration": 4.385}, {"text": "uh, which we'll spend most of today talking about, okay?", "start": 38.42, "duration": 3.485}, {"text": "Certainly don't look so stressed. It'll be fun.", "start": 41.905, "duration": 1.715}, {"text": "[LAUGHTER].", "start": 43.62, "duration": 2.13}, {"text": "Maybe. All right.", "start": 45.75, "duration": 1.845}, {"text": "Um, so what I'd like to do today is start our foray into, uh, unsupervised learning.", "start": 47.595, "duration": 6.545}, {"text": "Uh, so far I've spent a lot of time on", "start": 54.14, "duration": 2.715}, {"text": "supervised learning algorithms including advice on", "start": 56.855, "duration": 2.835}, {"text": "how to apply supervised learning algorithms.", "start": 59.69, "duration": 2.85}, {"text": "These pens are great.", "start": 62.54, "duration": 1.27}, {"text": "In which you'd have, you know,", "start": 63.81, "duration": 1.685}, {"text": "positive examples and negative examples and you run", "start": 65.495, "duration": 4.19}, {"text": "logistic regression or something or SVM or", "start": 69.685, "duration": 2.515}, {"text": "something to find the line- find the decision boundary between them.", "start": 72.2, "duration": 3.375}, {"text": "Um, in unsupervised learning,", "start": 75.575, "duration": 2.475}, {"text": "you're given unlabeled data.", "start": 78.05, "duration": 2.235}, {"text": "So rather than given data with x and y,", "start": 80.285, "duration": 4.055}, {"text": "you're given only x.", "start": 84.34, "duration": 1.975}, {"text": "And so your training set now looks like X1,", "start": 86.315, "duration": 2.7}, {"text": "X2, up to Xm.", "start": 89.015, "duration": 5.085}, {"text": "And you're asked to find something interesting about the data.", "start": 94.1, "duration": 3.635}, {"text": "Uh, so the first unsupervised learning algorithm we'll talk about", "start": 97.735, "duration": 2.815}, {"text": "is clustering in which given a dataset like this,", "start": 100.55, "duration": 3.105}, {"text": "hopefully, we can have an algorithm that can figure", "start": 103.655, "duration": 3.405}, {"text": "out that this dataset has two separate clusters.", "start": 107.06, "duration": 3.78}, {"text": "Um, and so one of the most common uses of clustering is, uh, market segmentation.", "start": 110.84, "duration": 5.85}, {"text": "If you have a website,", "start": 116.69, "duration": 1.08}, {"text": "you know, selling things online,", "start": 117.77, "duration": 1.245}, {"text": "you have a huge database of many different users and run clustering", "start": 119.015, "duration": 3.105}, {"text": "to decide what are the different market segments, right?", "start": 122.12, "duration": 3.01}, {"text": "So there may be, you know,", "start": 125.13, "duration": 1.29}, {"text": "people of a certain age range, of a certain gender,", "start": 126.42, "duration": 2.25}, {"text": "people of a different age range,", "start": 128.67, "duration": 1.11}, {"text": "different level of education,", "start": 129.78, "duration": 1.2}, {"text": "people that live in the East Coast versus West Coast versus other parts of the country.", "start": 130.98, "duration": 3.08}, {"text": "But by clustering you can group people into,", "start": 134.06, "duration": 3.27}, {"text": "uh, different groups, right?", "start": 137.33, "duration": 2.11}, {"text": "So, um, I want to show you an animation of, um,", "start": 139.44, "duration": 5.03}, {"text": "really the most commonly used er,", "start": 144.47, "duration": 2.1}, {"text": "er, clustering algorithm called k-means clustering.", "start": 146.57, "duration": 3.63}, {"text": "And let me show you an animation of what k-means does and", "start": 150.2, "duration": 3.18}, {"text": "then we'll write- write out the math an- an- and tell you how you can implement it.", "start": 153.38, "duration": 3.705}, {"text": "So, um, let's say you're given data like this.", "start": 157.085, "duration": 3.015}, {"text": "So all these are unlabeled examples.", "start": 160.1, "duration": 2.01}, {"text": "Uh, so just x plotted here.", "start": 162.11, "duration": 2.07}, {"text": "And we want an algorithm to try to find maybe the two clusters here.", "start": 164.18, "duration": 3.81}, {"text": "Uh, the first step of k-means is to pick two points", "start": 167.99, "duration": 3.93}, {"text": "denoted by the two crop- two crosses called cluster centroids and,", "start": 171.92, "duration": 4.51}, {"text": "uh, the cluster centroids are your best guess for", "start": 176.43, "duration": 1.97}, {"text": "where the centers of the two clusters you're trying to find.", "start": 178.4, "duration": 3.18}, {"text": "And then k-means is an iterative algorithm and repeatedly you do two things.", "start": 181.58, "duration": 4.53}, {"text": "So first thing is, go through each of your training examples.", "start": 186.11, "duration": 3.105}, {"text": "Oh I'm sorry. Oh okay. Thank you.", "start": 189.215, "duration": 2.17}, {"text": "All right. Let me know if that happens again.", "start": 191.385, "duration": 2.295}, {"text": "Okay. Right. So, uh, you guys saw that, right?", "start": 193.68, "duration": 2.865}, {"text": "So. Right near two cluster centroids.", "start": 196.545, "duration": 1.97}, {"text": "So the first thing you do is go through each of your training examples,", "start": 198.515, "duration": 2.685}, {"text": "the green dots and for each of them you color them either red", "start": 201.2, "duration": 3.66}, {"text": "or blue depending on which is the closer cluster centroid.", "start": 204.86, "duration": 3.72}, {"text": "So here we've taken every dot and colored it in red or blue", "start": 208.58, "duration": 3.315}, {"text": "depending on which side it is- which cluster centroid it's closer to.", "start": 211.895, "duration": 4.06}, {"text": "And then, uh, the second thing you do, uh, is,", "start": 215.955, "duration": 3.675}, {"text": "uh, look at all the blue dots and compute the average, right?", "start": 219.63, "duration": 3.96}, {"text": "Just find the mean of all the blue dots,", "start": 223.59, "duration": 1.86}, {"text": "um, and move the blue cluster centroid there.", "start": 225.45, "duration": 2.885}, {"text": "And similarly, look at all the red dots- and look at only the red dots", "start": 228.335, "duration": 3.69}, {"text": "and find a mean- finding the- oh now what's wrong with this?", "start": 232.025, "duration": 3.95}, {"text": "Let's say- oh this thing though is very strange.", "start": 235.975, "duration": 2.48}, {"text": "Right. Apparently, if I keep moving my mouse,", "start": 238.455, "duration": 2.025}, {"text": "it doesn't do that. All right. Thank you.", "start": 240.48, "duration": 2.31}, {"text": "Uh, and then find the mean of all the red dots and move your,", "start": 242.79, "duration": 3.98}, {"text": "uh, red cluster centroid there.", "start": 246.77, "duration": 1.74}, {"text": "So let me do that, right?", "start": 248.51, "duration": 1.93}, {"text": "So the cluster centroids move as follows, um,", "start": 250.44, "duration": 4.07}, {"text": "to the mean of the red and the blue dots and this is", "start": 254.51, "duration": 1.98}, {"text": "just a standard arithmetic average, right?", "start": 256.49, "duration": 2.24}, {"text": "Uh, and then you repeat again where you, er,", "start": 258.73, "duration": 4.04}, {"text": "look at each of the dots and color it either red or", "start": 262.77, "duration": 2.69}, {"text": "blue depending on which cluster centroid is closer.", "start": 265.46, "duration": 2.985}, {"text": "So when I recolor every point based on,", "start": 268.445, "duration": 2.445}, {"text": "you know, what's closer,", "start": 270.89, "duration": 1.68}, {"text": "so that's the new set of colors, right?", "start": 272.57, "duration": 2.685}, {"text": "Um, and then the second part of the algorithm was", "start": 275.255, "duration": 3.315}, {"text": "again look at the blue dots, find the mean,", "start": 278.57, "duration": 2.79}, {"text": "look at the red dots, find the mean,", "start": 281.36, "duration": 1.83}, {"text": "and then move the cluster centroids over.", "start": 283.19, "duration": 2.76}, {"text": "[NOISE] Excuse me, uh,", "start": 285.95, "duration": 1.935}, {"text": "to that mean, okay?", "start": 287.885, "duration": 2.205}, {"text": "Um, and so, er,", "start": 290.09, "duration": 1.775}, {"text": "and it turns out if you keep running the algorithm, nothing changes.", "start": 291.865, "duration": 3.445}, {"text": "So the algorithm has converged.", "start": 295.31, "duration": 1.11}, {"text": "So if you look at this picture and you repeatedly color each point", "start": 296.42, "duration": 3.885}, {"text": "red or blue depending on which cluster centroid is closer, nothing changes.", "start": 300.305, "duration": 4.05}, {"text": "And you repeatedly look at each of the two clusters of color dots", "start": 304.355, "duration": 3.465}, {"text": "and compute a mean and move the clu- clu- clusters there, nothing changes.", "start": 307.82, "duration": 3.51}, {"text": "So this algorithm has converged even if you keep on running these two steps, okay?", "start": 311.33, "duration": 4.335}, {"text": "So um, let's see.", "start": 315.665, "duration": 4.395}, {"text": "Let's write down in math what we just did.", "start": 320.06, "duration": 4.14}, {"text": "[NOISE] All right. So this is,", "start": 324.2, "duration": 7.785}, {"text": "um, a clustering algorithm and specifically this is a k-means clustering algorithm.", "start": 331.985, "duration": 8.875}, {"text": "So your dataset now does not come with any labels.", "start": 342.31, "duration": 6.22}, {"text": "Um, and so in, uh,", "start": 348.53, "duration": 2.19}, {"text": "k-means, step one is, uh,", "start": 350.72, "duration": 2.34}, {"text": "initialize the cluster centroids, right?", "start": 353.06, "duration": 9.865}, {"text": "I'm gonna call them Mu_1 up to Mu_k, uh, randomly, okay?", "start": 362.925, "duration": 9.01}, {"text": "So this was a step where you plop down the red cross and the blue cross.", "start": 371.935, "duration": 4.73}, {"text": "Uh, and when they did it on the PowerPoints, you know,", "start": 376.665, "duration": 2.665}, {"text": "I did it as if we're just choosing these as random vectors.", "start": 379.33, "duration": 4.265}, {"text": "In practice a good way of the- actually the most common way", "start": 383.595, "duration": 2.975}, {"text": "to select a random initial cluster centroid isn't quite what I showed,", "start": 386.57, "duration": 3.315}, {"text": "is to actually pick k examples out of your training set and just", "start": 389.885, "duration": 4.065}, {"text": "set the cluster centroids to be equal to k randomly chosen the examples, right?", "start": 393.95, "duration": 4.15}, {"text": "So in a low-dimensional space like a 2D plot,", "start": 398.1, "duration": 2.43}, {"text": "you know, you can do on the diagram,", "start": 400.53, "duration": 1.28}, {"text": "it doesn't really matter but when you work with very high dimensional datasets,", "start": 401.81, "duration": 3.78}, {"text": "the more common way to initialize these is just pick, you know,", "start": 405.59, "duration": 3.24}, {"text": "k training examples and set the cluster centroids to be", "start": 408.83, "duration": 3.15}, {"text": "at exactly the location of those examples.", "start": 411.98, "duration": 3.555}, {"text": "But then low dimensionless spaces it- it- you know,", "start": 415.535, "duration": 2.955}, {"text": "it doesn't make a big difference.", "start": 418.49, "duration": 1.89}, {"text": "Um, and then next you repeat until convergence.", "start": 420.38, "duration": 4.35}, {"text": "Um, step one is-", "start": 424.73, "duration": 8.26}, {"text": "right? So this is a- well", "start": 448.75, "duration": 9.4}, {"text": "now I'll just write this down,", "start": 458.15, "duration": 3.22}, {"text": "okay? Um, so does that make sense?", "start": 494.68, "duration": 5.48}, {"text": "So the two steps you would alternate between the first one", "start": 500.16, "duration": 3.01}, {"text": "is set Ci for every value of i.", "start": 503.17, "duration": 3.57}, {"text": "So for every example,", "start": 506.74, "duration": 1.585}, {"text": "set Ci equal to, you know,", "start": 508.325, "duration": 2.265}, {"text": "either 1 or 2 depending on whether, er,", "start": 510.59, "duration": 3.155}, {"text": "that example Xi is closer to cluster centroid one or cluster centroid two, right?", "start": 513.745, "duration": 4.815}, {"text": "So ju- just take each point and color either red or blue.", "start": 518.56, "duration": 2.16}, {"text": "Uh, or and represent that by setting Ci equals 1 or 2,", "start": 520.72, "duration": 5.19}, {"text": "er, if you have two clusters.", "start": 525.91, "duration": 1.35}, {"text": "If k is equal to 2, right? Oh yeah.", "start": 527.26, "duration": 2.01}, {"text": "[inaudible]", "start": 529.27, "duration": 6.84}, {"text": "Oh. Er, the notes say L1 norm squared?", "start": 536.11, "duration": 4.72}, {"text": "From this morning?", "start": 542.13, "duration": 3.02}, {"text": "Uh, what notes were sent out this morning?", "start": 545.27, "duration": 3.67}, {"text": "[inaudible].", "start": 548.94, "duration": 2.61}, {"text": "Oh that's red. It shouldn't be L1 norm.", "start": 551.55, "duration": 2.595}, {"text": "Uh, if it says L1 norm,", "start": 554.145, "duration": 1.875}, {"text": "that's a mistake. Sorry about that.", "start": 556.02, "duration": 2.22}, {"text": "Er, but usually- an- and it turns out whether you use", "start": 558.24, "duration": 3.76}, {"text": "L2 norm and L2 norm squared that gives you", "start": 562.0, "duration": 1.758}, {"text": "the same answer because the algorithm is the same either way.", "start": 563.758, "duration": 2.532}, {"text": "But it is usually- do we have a typo on the notes?", "start": 566.29, "duration": 2.55}, {"text": "[inaudible].", "start": 568.84, "duration": 2.75}, {"text": "Oh I see. Oh got it. Oh- oh- oh okay.", "start": 571.59, "duration": 2.535}, {"text": "Let's say in notes we wrote that.", "start": 574.125, "duration": 1.65}, {"text": "Okay. Cool. But by default,", "start": 575.775, "duration": 1.935}, {"text": "when we write that norm we actually use- we mean L2 norm.", "start": 577.71, "duration": 3.18}, {"text": "Yeah, right? But by- by default this is the L2 norm of x if is unspecified.", "start": 580.89, "duration": 5.97}, {"text": "Er, if it's L1 norm, we usually write this.", "start": 586.86, "duration": 2.0}, {"text": "So L2 norm is more common and with or without the square it you get the same result.", "start": 588.86, "duration": 4.6}, {"text": "Okay. Cool. Thank you. All right.", "start": 593.46, "duration": 3.42}, {"text": "So let's color the dots.", "start": 596.88, "duration": 1.395}, {"text": "Um, paint each dot either red or blue.", "start": 598.275, "duration": 3.315}, {"text": "Uh, and then, um, uh,", "start": 601.59, "duration": 2.205}, {"text": "for this, um, this is,", "start": 603.795, "duration": 3.54}, {"text": "you know, some key examples and take", "start": 607.335, "duration": 2.585}, {"text": "all the examples assigned to a certain cluster, right?", "start": 609.92, "duration": 2.85}, {"text": "Assigned to cluster j and set Mu_j to", "start": 612.77, "duration": 2.34}, {"text": "be average of all the points assigned to that cluster J. Yeah.", "start": 615.11, "duration": 2.58}, {"text": "[inaudible]", "start": 617.69, "duration": 2.45}, {"text": "Oh sure. Er, no that does not work.", "start": 620.14, "duration": 7.71}, {"text": "Uh, you know, I don't think -I don't know if I have- all right.", "start": 627.85, "duration": 6.41}, {"text": "Now, that the black markers are working,", "start": 634.26, "duration": 2.43}, {"text": "um, this is better?", "start": 636.69, "duration": 2.085}, {"text": "All right let me try to use this?", "start": 638.775, "duration": 1.29}, {"text": "Is there a part of this that's unclear?", "start": 640.065, "duration": 1.465}, {"text": "If this part you can't see, I'll write it out more clearly.", "start": 641.53, "duration": 3.57}, {"text": "Yes. Go ahead. Let's go ahead.", "start": 645.1, "duration": 1.8}, {"text": "[inaudible].", "start": 646.9, "duration": 2.67}, {"text": "Oh sure. How I do it, lights in front?", "start": 649.57, "duration": 1.89}, {"text": "[inaudible].", "start": 651.46, "duration": 10.1}, {"text": "Got it. Let there be light.", "start": 661.56, "duration": 4.41}, {"text": "All right. Awesome great.", "start": 665.97, "duration": 1.74}, {"text": "That was an easy request to satisfy, great, you know, okay.", "start": 667.71, "duration": 4.8}, {"text": "I guess we'll actually look at it for another minute.", "start": 672.51, "duration": 2.01}, {"text": "All right. Is that okay? Thank you.", "start": 674.52, "duration": 3.13}, {"text": "Okay. This wasn't part of it.", "start": 686.18, "duration": 3.32}, {"text": "Okay. All right. Now, I can move it up.", "start": 692.75, "duration": 2.59}, {"text": "[NOISE].", "start": 695.34, "duration": 10.14}, {"text": "All right. Um, so it turns out that,", "start": 705.48, "duration": 2.715}, {"text": "uh, um, this algorithm can be proven to converge.", "start": 708.195, "duration": 4.005}, {"text": "Um, the, exactly why it is written out in the lecture notes.", "start": 712.2, "duration": 3.96}, {"text": "But it turns out, if you write this as", "start": 716.16, "duration": 1.71}, {"text": "a cost function, right?", "start": 717.87, "duration": 11.07}, {"text": "Um, so the cost function for a certain set of assignments of, uh,", "start": 728.94, "duration": 5.175}, {"text": "points of examples to cluster", "start": 734.115, "duration": 1.905}, {"text": "centroids and for a certain set of positions of the cluster centroids.", "start": 736.02, "duration": 3.15}, {"text": "So, so c, these are the assignments and these are the centroids, right?", "start": 739.17, "duration": 10.005}, {"text": "So, so this cost here is sum of your training", "start": 749.175, "duration": 3.015}, {"text": "set of what's the square distance between each point,", "start": 752.19, "duration": 3.48}, {"text": "and the cluster centroid it is assigned to, right?", "start": 755.67, "duration": 3.325}, {"text": "So it turns out, um,", "start": 758.995, "duration": 1.39}, {"text": "I want to prove this, uh,", "start": 760.385, "duration": 1.275}, {"text": "little bit more detail in lecture notes but I'm going to prove this.", "start": 761.66, "duration": 2.07}, {"text": "It turns out that on every iteration,", "start": 763.73, "duration": 2.19}, {"text": "k-means would drive this cost function down,", "start": 765.92, "duration": 3.245}, {"text": "um, and so, you know,", "start": 769.165, "duration": 1.415}, {"text": "beyond a certain point this cost function,", "start": 770.58, "duration": 1.62}, {"text": "it can't go even,", "start": 772.2, "duration": 1.23}, {"text": "it can't go, uh, uh, any lower.", "start": 773.43, "duration": 2.025}, {"text": "Well, this, this can't go below 0, right?", "start": 775.455, "duration": 2.34}, {"text": "And so this shows that k-means must converge,", "start": 777.795, "duration": 2.595}, {"text": "or at least this function must converge because it's, uh, a", "start": 780.39, "duration": 2.55}, {"text": "strictly non-negative function that's going down on every iteration.", "start": 782.94, "duration": 3.36}, {"text": "So at some point, it has to stop going down,", "start": 786.3, "duration": 1.8}, {"text": "and then you could declare k-means are converged.", "start": 788.1, "duration": 2.73}, {"text": "Um, in practice, if you run k-means in a very,", "start": 790.83, "duration": 3.15}, {"text": "very large data set,", "start": 793.98, "duration": 1.2}, {"text": "then as you plot the number of iterations, uh,", "start": 795.18, "duration": 2.82}, {"text": "j may go down,", "start": 798.0, "duration": 1.409}, {"text": "and you know, and,", "start": 799.409, "duration": 1.156}, {"text": "and just because of,", "start": 800.565, "duration": 1.155}, {"text": "a lack of compute or lack of patience,", "start": 801.72, "duration": 2.46}, {"text": "you might just stop this running after a while.", "start": 804.18, "duration": 1.71}, {"text": "It is going down too slowly.", "start": 805.89, "duration": 1.275}, {"text": "So that's sort of k-means in practice where maybe", "start": 807.165, "duration": 2.625}, {"text": "it hasn't totally converged, we just cut it off and call it good enough.", "start": 809.79, "duration": 3.21}, {"text": "Um, now, uh, uh,", "start": 813.0, "duration": 3.435}, {"text": "the most frequently asked question I get for k-means is how do you choose k?", "start": 816.435, "duration": 4.23}, {"text": "It turns out that, um,", "start": 820.665, "duration": 1.77}, {"text": "when I use k-means,", "start": 822.435, "duration": 1.455}, {"text": "I still usually choose k by hand.", "start": 823.89, "duration": 2.205}, {"text": "And so, and, and this is why.", "start": 826.095, "duration": 1.98}, {"text": "Which is in unsupervised learning,", "start": 828.075, "duration": 2.1}, {"text": "um, sometimes it's just ambiguous, right?", "start": 830.175, "duration": 4.605}, {"text": "How many clusters there are [NOISE].", "start": 834.78, "duration": 6.66}, {"text": "Right? Um, with this dataset,", "start": 841.44, "duration": 2.4}, {"text": "some of you will see two clusters,", "start": 843.84, "duration": 1.815}, {"text": "and some of you will see four clusters,", "start": 845.655, "duration": 3.96}, {"text": "and it's just inherently ambiguous what is the right number of clusters.", "start": 849.615, "duration": 4.245}, {"text": "So there are some formulas you can find online,", "start": 853.86, "duration": 2.61}, {"text": "the criteria like AIC and BIC for automatically choosing the number of clusters.", "start": 856.47, "duration": 3.735}, {"text": "In practice, I tend not to use them because, uh, um,", "start": 860.205, "duration": 3.39}, {"text": "I usually look at the downstream application of what you actually want", "start": 863.595, "duration": 3.795}, {"text": "to use k-means for in order to make a decision on the number of clusters.", "start": 867.39, "duration": 3.51}, {"text": "So for example, if you're doing a market segmentation,", "start": 870.9, "duration": 3.165}, {"text": "um, you know, because your marketers want to design different marketing campaigns, right?", "start": 874.065, "duration": 3.945}, {"text": "For different groups of users,", "start": 878.01, "duration": 1.575}, {"text": "then your marketers might have the bandwidth to design four separate marketing campaigns,", "start": 879.585, "duration": 4.695}, {"text": "but not 100 marketing campaigns.", "start": 884.28, "duration": 1.68}, {"text": "So that would be a good reason to choose four clusters rather than 100 clusters.", "start": 885.96, "duration": 3.27}, {"text": "So it's often, uh, uh,", "start": 889.23, "duration": 1.65}, {"text": "if you look at the purpose of what you're doing this for.", "start": 890.88, "duration": 2.175}, {"text": "Um, I think in the previous exercise,", "start": 893.055, "duration": 1.77}, {"text": "uh, in the homework, you see a, um,", "start": 894.825, "duration": 1.875}, {"text": "image compression, uh, exercise where you want to cluster,", "start": 896.7, "duration": 3.705}, {"text": "uh, colors into smaller number of clusters.", "start": 900.405, "duration": 2.835}, {"text": "You implement this. This is actually one of the most fun exercises I think.", "start": 903.24, "duration": 2.94}, {"text": "Um, uh, uh, that, uh,", "start": 906.18, "duration": 2.4}, {"text": "uh, but so there you'd, you know, be saying, well,", "start": 908.58, "duration": 2.58}, {"text": "how much do you want to compress the image to decide how many clusters to,", "start": 911.16, "duration": 3.645}, {"text": "to try to use, okay?", "start": 914.805, "duration": 1.14}, {"text": "So I usually, um, pick the number of clusters, you know,", "start": 915.945, "duration": 3.255}, {"text": "either manually or looking at what you want to use k-means cluster for.", "start": 919.2, "duration": 4.275}, {"text": "Um, when we're trying to cluster news articles, uh,", "start": 923.475, "duration": 1.815}, {"text": "the Google News example,", "start": 925.29, "duration": 1.41}, {"text": "I think I showed in the first lecture.", "start": 926.7, "duration": 1.53}, {"text": "You say, well, how many clusters is going to make sense for,", "start": 928.23, "duration": 2.7}, {"text": "for, for news articles, okay?", "start": 930.93, "duration": 2.145}, {"text": "All right. So good. So, uh, yeah?", "start": 933.075, "duration": 2.715}, {"text": "[inaudible].", "start": 935.79, "duration": 5.49}, {"text": "Oh sure. Well, k-means get stuck on local minima.", "start": 941.28, "duration": 2.64}, {"text": "Yes, k-means gets stuck on sort of local minima sometimes.", "start": 943.92, "duration": 3.12}, {"text": "And so, if you're worried about local minima,", "start": 947.04, "duration": 2.445}, {"text": "the thing you can do is, uh,", "start": 949.485, "duration": 2.25}, {"text": "run k-means, say, 10 times, or 100 times,", "start": 951.735, "duration": 2.505}, {"text": "or 1000 times from different random initializations of the cluster centroids.", "start": 954.24, "duration": 4.08}, {"text": "And then run it, you know, say 100 times, uh,", "start": 958.32, "duration": 2.55}, {"text": "and then pick whichever run results in the lowest value for this cost function, okay?", "start": 960.87, "duration": 6.94}, {"text": "All right. Um, so you'll play with this more in, um,", "start": 969.77, "duration": 3.97}, {"text": "uh, in the programming exercise.", "start": 973.74, "duration": 4.98}, {"text": "Now, um, there's a,", "start": 978.72, "duration": 2.685}, {"text": "there's a problem that seems closely related.", "start": 981.405, "duration": 3.135}, {"text": "Um, but, but it's actually", "start": 984.54, "duration": 5.22}, {"text": "quite different ways to write the algorithms which is density estimation.", "start": 989.76, "duration": 4.89}, {"text": "So, so let me motivate this.", "start": 994.65, "duration": 1.65}, {"text": "Um, I actually have a- well, right,", "start": 996.3, "duration": 2.505}, {"text": "sometime back had some friends working on a problem which I'll simplify a little bit,", "start": 998.805, "duration": 5.205}, {"text": "um, of, uh, uh, you know,", "start": 1004.01, "duration": 2.01}, {"text": "you have aircraft engines coming off the assembly line.", "start": 1006.02, "duration": 2.745}, {"text": "All right. And every time an aircraft engine comes off the assembly line,", "start": 1008.765, "duration": 3.075}, {"text": "you measure some features of these engines.", "start": 1011.84, "duration": 1.95}, {"text": "You measure some features about the vibration,", "start": 1013.79, "duration": 2.505}, {"text": "and you measure some features about the heat that the aircraft engine is producing.", "start": 1016.295, "duration": 5.595}, {"text": "And, um, let's say that you get a dataset,", "start": 1021.89, "duration": 3.7}, {"text": "right, that looks like this, okay?", "start": 1026.29, "duration": 4.6}, {"text": "And, um, the anomaly detection problem", "start": 1030.89, "duration": 4.389}, {"text": "is if you get a new aircraft engine that comes off the assembly line,", "start": 1039.34, "duration": 5.695}, {"text": "and if the vibration feature takes on this value,", "start": 1045.035, "duration": 2.685}, {"text": "and the heat feature takes on this value,", "start": 1047.72, "duration": 2.07}, {"text": "is that aircraft engine an anomalous one,", "start": 1049.79, "duration": 2.925}, {"text": "is it an unusual one, right?", "start": 1052.715, "duration": 1.86}, {"text": "And so the application of this is,", "start": 1054.575, "duration": 2.085}, {"text": "um, that as your aircraft engine comes off the assembly line,", "start": 1056.66, "duration": 3.285}, {"text": "if you see a very unusual signature in terms of", "start": 1059.945, "duration": 2.385}, {"text": "the vibrations and the heat the aircraft engine is generating,", "start": 1062.33, "duration": 3.03}, {"text": "then probably something is wrong with this aircraft engine,", "start": 1065.36, "duration": 2.28}, {"text": "and you have your people, have your,", "start": 1067.64, "duration": 1.575}, {"text": "have your team inspect it further or test it further,", "start": 1069.215, "duration": 2.88}, {"text": "uh, before you ship the airplane,", "start": 1072.095, "duration": 1.47}, {"text": "before you ship the engine to a,", "start": 1073.565, "duration": 1.695}, {"text": "to a airplane maker and then something goes wrong in the air, and there's a,", "start": 1075.26, "duration": 2.715}, {"text": "there's a major accident,", "start": 1077.975, "duration": 1.455}, {"text": "or major disaster, right?", "start": 1079.43, "duration": 1.695}, {"text": "And so anomaly detection, uh, uh,", "start": 1081.125, "duration": 2.67}, {"text": "is most commonly done,", "start": 1083.795, "duration": 2.175}, {"text": "or one of the common ways to, um,", "start": 1085.97, "duration": 3.255}, {"text": "implement anomaly detection is the model p of", "start": 1089.225, "duration": 2.745}, {"text": "x which is given all of these blue examples,", "start": 1091.97, "duration": 3.3}, {"text": "given all of these dots,", "start": 1095.27, "duration": 1.23}, {"text": "can you model what is the density from which x was drawn?", "start": 1096.5, "duration": 4.65}, {"text": "So then if p of x is very small,", "start": 1101.15, "duration": 5.249}, {"text": "then you flag an anomaly, right?", "start": 1106.399, "duration": 3.691}, {"text": "Meaning that, Gee, I think something's funny here, uh,", "start": 1110.09, "duration": 2.535}, {"text": "and maybe someone should inspect this aircraft engine a little bit further.", "start": 1112.625, "duration": 5.67}, {"text": "Um, so anomaly detection is used for,", "start": 1118.295, "duration": 2.385}, {"text": "a task like this,", "start": 1120.68, "duration": 1.515}, {"text": "for an inspection task like this.", "start": 1122.195, "duration": 1.59}, {"text": "Um, it's used for,", "start": 1123.785, "duration": 1.485}, {"text": "um, uh, many years ago,", "start": 1125.27, "duration": 1.53}, {"text": "I was actually working with some telecoms providers, you know, uh, uh,", "start": 1126.8, "duration": 3.27}, {"text": "helping out telecoms company on, um,", "start": 1130.07, "duration": 2.64}, {"text": "anomaly detection to figure out if something's", "start": 1132.71, "duration": 2.61}, {"text": "gone wrong with a cell tower network, right?", "start": 1135.32, "duration": 2.73}, {"text": "So if one day one of the cell towers start throwing off", "start": 1138.05, "duration": 2.49}, {"text": "network patterns that seem very unusual,", "start": 1140.54, "duration": 2.49}, {"text": "then maybe something's wrong with that cell tower,", "start": 1143.03, "duration": 1.83}, {"text": "like something's gone wrong.", "start": 1144.86, "duration": 1.2}, {"text": "We sent out the technicians to fix it.", "start": 1146.06, "duration": 2.61}, {"text": "Uh, it is also used for computer security.", "start": 1148.67, "duration": 2.16}, {"text": "If a computer, say if a computer at Stanford starts sending out very strange,", "start": 1150.83, "duration": 4.245}, {"text": "you know, um, uh, uh, network traffic,", "start": 1155.075, "duration": 2.325}, {"text": "that's very unusual relative to everything it's done before,", "start": 1157.4, "duration": 2.43}, {"text": "relative what this is,", "start": 1159.83, "duration": 1.05}, {"text": "is a very anomalous network traffic,", "start": 1160.88, "duration": 1.815}, {"text": "then maybe IT staff should have a look to", "start": 1162.695, "duration": 2.325}, {"text": "see if that particular computer has been hacked.", "start": 1165.02, "duration": 2.265}, {"text": "So these are some of the applications of anomaly detection.", "start": 1167.285, "duration": 3.015}, {"text": "And the good way to do this is,", "start": 1170.3, "duration": 1.8}, {"text": "given an unlabeled data set, model p of x.", "start": 1172.1, "duration": 2.565}, {"text": "And then if you have very low probability samples,", "start": 1174.665, "duration": 2.235}, {"text": "you flag that as a possible anomaly for further study.", "start": 1176.9, "duration": 4.275}, {"text": "Now, given this dataset,", "start": 1181.175, "duration": 3.255}, {"text": "um, uh, how do you model this?", "start": 1184.43, "duration": 2.1}, {"text": "One interesting thing about this green dot is that", "start": 1186.53, "duration": 2.655}, {"text": "neither the vibration nor the heat signature is actually out of range, right?", "start": 1189.185, "duration": 3.915}, {"text": "You know, like there are a lot of aircraft engines with vibrations in that range.", "start": 1193.1, "duration": 3.96}, {"text": "There are a lot of aircraft engines with heat in that range.", "start": 1197.06, "duration": 2.31}, {"text": "So neither feature by itself is actually that unusual.", "start": 1199.37, "duration": 3.03}, {"text": "It's actually the combination of the two that is unusual.", "start": 1202.4, "duration": 2.85}, {"text": "Um, and so thus,", "start": 1205.25, "duration": 1.245}, {"text": "thus, what I want to do is,", "start": 1206.495, "duration": 1.215}, {"text": "uh, come up with an algorithm to model this.", "start": 1207.71, "duration": 2.97}, {"text": "And in fact, we'll come up with an algorithm that can model, you know,", "start": 1210.68, "duration": 3.705}, {"text": "maybe, maybe your data density looks like this,", "start": 1214.385, "duration": 2.235}, {"text": "maybe more of an L shape like that.", "start": 1216.62, "duration": 1.485}, {"text": "But how do you model p of x with the data coming from an L shape?", "start": 1218.105, "duration": 5.23}, {"text": "Um, and it turns out that there is no textbook distribution, right?", "start": 1223.335, "duration": 4.18}, {"text": "You know, there isn't, you know, if you look at a simple exponential family of model,", "start": 1227.515, "duration": 3.81}, {"text": "the types of distributions,", "start": 1231.325, "duration": 1.335}, {"text": "there is no distribution for modeling very,", "start": 1232.66, "duration": 3.045}, {"text": "very complex distributions like this.", "start": 1235.705, "duration": 2.57}, {"text": "So what we're going to talk about is, um,", "start": 1238.275, "duration": 2.48}, {"text": "the mixture of Gaussians model which we look at data like this,", "start": 1240.755, "duration": 3.315}, {"text": "and say, it looks like this data actually comes from two Gaussian.", "start": 1244.07, "duration": 3.195}, {"text": "There's one Gaussian, maybe there's", "start": 1247.265, "duration": 1.665}, {"text": "one type of aircraft engine that, that, that, you know,", "start": 1248.93, "duration": 2.175}, {"text": "is drawn from a Gaussian like the one below,", "start": 1251.105, "duration": 2.685}, {"text": "and a separate aircraft- type of", "start": 1253.79, "duration": 1.98}, {"text": "aircraft engine that's drawn from a Gaussian like that above.", "start": 1255.77, "duration": 3.93}, {"text": "And this is why there's a lot of probability [NOISE] mass in the L-shaped region,", "start": 1259.7, "duration": 3.87}, {"text": "but very low probability outside that L-shaped region, right?", "start": 1263.57, "duration": 3.83}, {"text": "And, and, and these ellipses I'm drawing are the contours of these two Gaussians, right?", "start": 1267.4, "duration": 4.77}, {"text": "And so, um, what I'd like to do next is,", "start": 1272.17, "duration": 4.18}, {"text": "uh, develop the mixture of Gaussians model, um,", "start": 1276.35, "duration": 4.26}, {"text": "which is useful for anomaly detection,", "start": 1280.61, "duration": 1.844}, {"text": "and, and, uh, uh, and,", "start": 1282.454, "duration": 2.436}, {"text": "and then this will lead us to our second unsupervised programming algorithm, okay?", "start": 1284.89, "duration": 6.015}, {"text": "So, um, in order to make the mixture of Gaussians model a bit easier to develop,", "start": 1290.905, "duration": 8.65}, {"text": "let me just use a one-dimensional example where x is in R, okay?", "start": 1299.555, "duration": 6.375}, {"text": "So, um, let's see.", "start": 1305.93, "duration": 2.88}, {"text": "So let's say that, uh,", "start": 1308.81, "duration": 1.62}, {"text": "we gather a data set that looks like this.", "start": 1310.43, "duration": 2.04}, {"text": "[NOISE]", "start": 1312.47, "duration": 10.075}, {"text": "Right. So it's just one row number.", "start": 1322.545, "duration": 2.775}, {"text": "So it's just on num- number line I plotted a few dots.", "start": 1325.32, "duration": 2.64}, {"text": "Um, so it looks like this data maybe comes from two Gaussians.", "start": 1327.96, "duration": 3.99}, {"text": "Right? It looks like, you know, there's some data from this Gaussian.", "start": 1331.95, "duration": 2.985}, {"text": "And there's some data from that Gaussian on the right.", "start": 1334.935, "duration": 4.95}, {"text": "Um, and is- and if only we knew.", "start": 1339.885, "duration": 4.23}, {"text": "Right? Which example had come from which Gaussian, if,", "start": 1344.115, "duration": 4.38}, {"text": "if we knew that these examples had come from Gaussian 1,", "start": 1348.495, "duration": 4.274}, {"text": "which I want to denote with crosses.", "start": 1352.769, "duration": 1.816}, {"text": "And if only we knew- no, that was here.", "start": 1354.585, "duration": 2.85}, {"text": "What- but actually this is fine. I'll leave that one there.", "start": 1357.435, "duration": 2.055}, {"text": "If only we knew that", "start": 1359.49, "duration": 1.41}, {"text": "these examples had come from Gaussian 2 which I'm going to draw with Os,", "start": 1360.9, "duration": 5.325}, {"text": "then we just fit Gaussian 1 to the crosses,", "start": 1366.225, "duration": 2.355}, {"text": "fit Gaussian 2 to the Os and then we'd be pretty much done.", "start": 1368.58, "duration": 2.58}, {"text": "Right? Um, oh, and, and, and sorry.", "start": 1371.16, "duration": 2.31}, {"text": "And so these are the two Gaussians.", "start": 1373.47, "duration": 1.5}, {"text": "And so the overall density would be something like this.", "start": 1374.97, "duration": 3.15}, {"text": "Right? Tha- that's the probability.", "start": 1378.12, "duration": 2.46}, {"text": "A lot of probability mass on left.", "start": 1380.58, "duration": 1.17}, {"text": "A lot of probability mass on the right, low, less probability mass on the,", "start": 1381.75, "duration": 2.91}, {"text": "uh, in sort of in, in the middle.", "start": 1384.66, "duration": 2.58}, {"text": "Okay? So the overall density I'll just draw again, would be,", "start": 1387.24, "duration": 3.45}, {"text": "low high, low high something like that.", "start": 1390.69, "duration": 2.565}, {"text": "Right? Um, but the reason- and,", "start": 1393.255, "duration": 5.325}, {"text": "and, and if you actually had these labels.", "start": 1398.58, "duration": 2.04}, {"text": "If you knew that these examples came from Gaussian 1,", "start": 1400.62, "duration": 2.625}, {"text": "those examples come from Gaussian 2,", "start": 1403.245, "duration": 2.04}, {"text": "then you can actually use an algorithm very similar to GDA,", "start": 1405.285, "duration": 3.06}, {"text": "Gaussian discriminant analysis to fit this model.", "start": 1408.345, "duration": 3.0}, {"text": "Uh, that the problem with this density estimation problem is,", "start": 1411.345, "duration": 3.315}, {"text": "you just see this data and maybe the data came from two different Gaussians.", "start": 1414.66, "duration": 4.68}, {"text": "But you don't know which example actually came from which Gaussian.", "start": 1419.34, "duration": 2.7}, {"text": "Okay? So the EM algorithm or the expectation-maximization algorithm will allow us to, uh,", "start": 1422.04, "duration": 6.465}, {"text": "fit a model despite not knowing which Gaussian each example that come from.", "start": 1428.505, "duration": 8.515}, {"text": "So let me first write down the,", "start": 1447.5, "duration": 2.71}, {"text": "um, mixture of Gaussians model.", "start": 1450.21, "duration": 2.89}, {"text": "Uh, and then we'll describe the EM algorithm for this.", "start": 1456.41, "duration": 7.045}, {"text": "So let's imagine- let's suppose that as a,", "start": 1463.455, "duration": 6.505}, {"text": "um, so the term we sometimes use is latent,", "start": 1472.43, "duration": 4.42}, {"text": "but latent just means hidden or unobserved.", "start": 1476.85, "duration": 5.71}, {"text": "Um, random variables z.", "start": 1483.26, "duration": 3.655}, {"text": "Right?", "start": 1486.915, "duration": 7.095}, {"text": "And x_i, z_i,", "start": 1494.01, "duration": 23.58}, {"text": "um.", "start": 1517.59, "duration": 15.85}, {"text": "Okay? So- this part here.", "start": 1537.35, "duration": 4.87}, {"text": "So let's imagine that, um,", "start": 1542.22, "duration": 2.13}, {"text": "there's some hidden random variable z and,", "start": 1544.35, "duration": 2.34}, {"text": "and the term latent just means hidden or unobserved.", "start": 1546.69, "duration": 2.52}, {"text": "Right? It means that it exists but you don't get to see the value directly.", "start": 1549.21, "duration": 3.075}, {"text": "So when I say latent,", "start": 1552.285, "duration": 1.41}, {"text": "it just means hidden or unobserved.", "start": 1553.695, "duration": 1.98}, {"text": "So let's imagine that there's a hidden or latent random variable z and,", "start": 1555.675, "duration": 4.2}, {"text": "uh, x_i and z_i have this joint distribution.", "start": 1559.875, "duration": 3.165}, {"text": "And this, this, this is very,", "start": 1563.04, "duration": 1.26}, {"text": "very similar to the model you saw in Gaussian discriminant analysis.", "start": 1564.3, "duration": 3.6}, {"text": "But z_i is multinomial with some set of parameters Phi.", "start": 1567.9, "duration": 4.38}, {"text": "For a mixture of two Gaussians,", "start": 1572.28, "duration": 1.8}, {"text": "this would just be Bernoulli with two values.", "start": 1574.08, "duration": 2.16}, {"text": "But if it were a mixture of k Gaussians then z, you know,", "start": 1576.24, "duration": 3.48}, {"text": "can take on values from 1 through k. [NOISE] Right?", "start": 1579.72, "duration": 4.185}, {"text": "Um, and it was two Gaussians it'll just be Bernoulli.", "start": 1583.905, "duration": 3.045}, {"text": "And then once you know that one example comes from, uh.", "start": 1586.95, "duration": 3.975}, {"text": "Gaussian number j, then x condition that z_i is equal to j.", "start": 1590.925, "duration": 5.28}, {"text": "That is drawn from a Gaussian distribution with some mean and some covariance Sigma.", "start": 1596.205, "duration": 5.88}, {"text": "Okay? So the two unimportant ways.", "start": 1602.085, "duration": 3.525}, {"text": "This is different than GDA.", "start": 1605.61, "duration": 1.815}, {"text": "Um, one, well, I've set z to be 1 of k values instead of one of two values.", "start": 1607.425, "duration": 5.325}, {"text": "And GDA, Gaussian discriminant analysis.", "start": 1612.75, "duration": 2.025}, {"text": "We had z, you know, uh,", "start": 1614.775, "duration": 2.265}, {"text": "why the labels y took on one of two values.", "start": 1617.04, "duration": 2.82}, {"text": "Uh, and then second is,", "start": 1619.86, "duration": 1.665}, {"text": "I have Sigma j instead of Sigma.", "start": 1621.525, "duration": 2.175}, {"text": "So by, by convention when we fit mixture of Gaussians models,", "start": 1623.7, "duration": 3.195}, {"text": "we let each Gaussian have his own covariance matrix Sigma.", "start": 1626.895, "duration": 3.0}, {"text": "But you can actually force it to be the same way you want.", "start": 1629.895, "duration": 1.71}, {"text": "So- but these are the trivial differences.", "start": 1631.605, "duration": 2.265}, {"text": "Uh, the most significant difference is that,", "start": 1633.87, "duration": 2.46}, {"text": "in Gaussian discriminant analysis,", "start": 1636.33, "duration": 1.934}, {"text": "we had labeled examples x_i, y_i.", "start": 1638.264, "duration": 3.781}, {"text": "Where z- y was observed.", "start": 1642.045, "duration": 2.565}, {"text": "Right? And then the main difference between this and Gaussian discriminant analysis is,", "start": 1644.61, "duration": 4.5}, {"text": "now we have replaced that with", "start": 1649.11, "duration": 1.77}, {"text": "this latent or hidden random variable z_i that you do not get to see in the training set.", "start": 1650.88, "duration": 4.335}, {"text": "Okay?", "start": 1655.215, "duration": 0.585}, {"text": "So now, uh,", "start": 1655.8, "duration": 15.18}, {"text": "actually you guys are right.", "start": 1670.98, "duration": 0.75}, {"text": "These pens are terrible.", "start": 1671.73, "duration": 2.32}, {"text": "All right. Oh, that was better.", "start": 1674.36, "duration": 2.395}, {"text": "Cool. All right.", "start": 1676.755, "duration": 8.505}, {"text": "So if we need the z_i's.", "start": 1685.26, "duration": 7.12}, {"text": "Right? Then we can use,", "start": 1692.84, "duration": 2.455}, {"text": "um, maximum likelihood estimation.", "start": 1695.295, "duration": 3.885}, {"text": "All right? So if only we knew the value of the z_i's, which we don't.", "start": 1699.18, "duration": 3.135}, {"text": "But if only we did, then we could use", "start": 1702.315, "duration": 2.025}, {"text": "maximum likelihood estimation or MLE to estimate everything.", "start": 1704.34, "duration": 3.39}, {"text": "You know. So we would write", "start": 1707.73, "duration": 1.23}, {"text": "the log likelihood of the parameters.", "start": 1708.96, "duration": 3.105}, {"text": "Right? Equals sum, um,", "start": 1712.065, "duration": 4.38}, {"text": "log p of x_i,", "start": 1716.445, "duration": 2.295}, {"text": "z_i, you know, given the parameters.", "start": 1718.74, "duration": 4.42}, {"text": "Right? And then you take the derivative,", "start": 1723.23, "duration": 3.01}, {"text": "set the derivatives equal to 0 and then you guys did this in problem set 1.", "start": 1726.24, "duration": 3.09}, {"text": "Right? And, and then you would find that Phi j is equal to", "start": 1729.33, "duration": 3.855}, {"text": "1 over m. Right?", "start": 1733.185, "duration": 8.785}, {"text": "Okay. So if only you knew the values of the z_i's, uh,", "start": 1762.71, "duration": 4.63}, {"text": "then you could use maximum likelihood estimates,", "start": 1767.34, "duration": 2.46}, {"text": "um, will- and, and this is what you get.", "start": 1769.8, "duration": 2.07}, {"text": "And this is pretty much the formulas.", "start": 1771.87, "duration": 1.665}, {"text": "Actually the- the- these two are exactly the formulas,", "start": 1773.535, "duration": 3.375}, {"text": "uh, we had for, uh, Gaussian discriminant analysis.", "start": 1776.91, "duration": 2.865}, {"text": "Except with replace y with z.", "start": 1779.775, "duration": 2.88}, {"text": "Right? And then there's some other formula for Sigma that's written in the lecture notes.", "start": 1782.655, "duration": 3.825}, {"text": "But I won't, but I won't write down here.", "start": 1786.48, "duration": 1.665}, {"text": "Okay? Um, but the reason we can't use this,", "start": 1788.145, "duration": 5.595}, {"text": "use these formulas is we don't actually know what are the values of z.", "start": 1793.74, "duration": 3.405}, {"text": "So what we will do in", "start": 1797.145, "duration": 3.525}, {"text": "the EM algorithm", "start": 1800.67, "duration": 2.26}, {"text": "is two steps.", "start": 1808.01, "duration": 3.7}, {"text": "Um, in the first step,", "start": 1811.71, "duration": 3.3}, {"text": "we will, uh, guess the value of the z's.", "start": 1815.01, "duration": 5.085}, {"text": "And in the second step we will use", "start": 1820.095, "duration": 2.97}, {"text": "these equations using the values of this z's we just guessed.", "start": 1823.065, "duration": 3.525}, {"text": "So let me- so, so sometimes in, um,", "start": 1826.59, "duration": 2.73}, {"text": "the machine learning is something that's called- there's a bootstrap procedure", "start": 1829.32, "duration": 2.475}, {"text": "where you get something that runs an algorithm.", "start": 1831.795, "duration": 3.03}, {"text": "You're using your guesses and then you", "start": 1834.825, "duration": 1.905}, {"text": "update your guesses and then run the algorithm again.", "start": 1836.73, "duration": 1.935}, {"text": "Let me, let me make that concrete by writing this down.", "start": 1838.665, "duration": 3.895}, {"text": "So the EM algorithm has two steps.", "start": 1854.18, "duration": 3.94}, {"text": "The E-step, um,", "start": 1858.12, "duration": 11.73}, {"text": "also called the expectation step is set to w i j.", "start": 1869.85, "duration": 6.96}, {"text": "So w i j, um,", "start": 1886.13, "duration": 3.04}, {"text": "is going to be the probability that z_i is equal to j.", "start": 1889.17, "duration": 6.66}, {"text": "Okay? Um, given all the parameters.", "start": 1895.83, "duration": 2.34}, {"text": "And, and much as we did with, um,", "start": 1898.17, "duration": 2.97}, {"text": "generative learning algorithms, right,", "start": 1901.14, "duration": 2.535}, {"text": "with generative learning algorithms,", "start": 1903.675, "duration": 1.305}, {"text": "we used Bayes' rule to estimate the probability of y given x,", "start": 1904.98, "duration": 5.565}, {"text": "and so to compute this,", "start": 1910.545, "duration": 1.545}, {"text": "you use a similar Bayes' rule type of calculation.", "start": 1912.09, "duration": 4.23}, {"text": "And so this would be [NOISE].", "start": 1916.32, "duration": 9.84}, {"text": "Oops, right, um, where,", "start": 1926.16, "duration": 16.5}, {"text": "for example this term here P of x_i given z_i equals j.", "start": 1942.66, "duration": 7.05}, {"text": "This would be a Gaussian density, right?", "start": 1949.71, "duration": 2.31}, {"text": "This comes from a Gaussian density with mean Mu j and covariance Sigma j, right?", "start": 1952.02, "duration": 6.945}, {"text": "And so this term here would be 1 over 2 Pi,", "start": 1958.965, "duration": 4.995}, {"text": "to the N over 2 Sigma j,", "start": 1963.96, "duration": 2.715}, {"text": "so one-half e to the negative one-half.", "start": 1966.675, "duration": 4.915}, {"text": "All right. And then this term here,", "start": 1979.64, "duration": 3.895}, {"text": "I guess this would be Phi j,", "start": 1983.535, "duration": 1.635}, {"text": "that's just a Bernoulli probability,", "start": 1985.17, "duration": 1.965}, {"text": "remember z is multinomial.", "start": 1987.135, "duration": 2.265}, {"text": "Right, so z is multinomial with parameters Phi.", "start": 1989.4, "duration": 6.48}, {"text": "So I guess the parameters Phi for multinomial distributions", "start": 1995.88, "duration": 2.82}, {"text": "tell you, what's the chance of z being 1, 2, 3, 4,", "start": 1998.7, "duration": 3.495}, {"text": "and so on up to k, and so the chance of z_i being equal to k is", "start": 2002.195, "duration": 3.825}, {"text": "just- chance of z_i being equal to j is just Phi j right?", "start": 2006.02, "duration": 4.14}, {"text": "It's just v to the off one of the parameters in your multinomial probability for,", "start": 2010.16, "duration": 5.085}, {"text": "um, for the odds of z being different values.", "start": 2015.245, "duration": 2.67}, {"text": "okay? And so, um,", "start": 2017.915, "duration": 1.965}, {"text": "and similarly the terms in the denominator.", "start": 2019.88, "duration": 2.235}, {"text": "This term here is from Gaussian and that second term is from the,", "start": 2022.115, "duration": 4.92}, {"text": "um, multinomial probability that you have for z.", "start": 2027.035, "duration": 3.465}, {"text": "And so that's how you plug in all of these numbers and use Bayes rule and use", "start": 2030.5, "duration": 4.26}, {"text": "this equation to compute given- all given the position of all these Gaussians,", "start": 2034.76, "duration": 4.35}, {"text": "what is the chance of w i j taking on a certain value, okay.", "start": 2039.11, "duration": 5.29}, {"text": "And, and so to make this really concrete,", "start": 2046.87, "duration": 4.03}, {"text": "you remember how I guess 1 or 0s, or the other way, um,", "start": 2050.9, "duration": 4.38}, {"text": "If you were to look at these,", "start": 2055.28, "duration": 1.905}, {"text": "uh, if you were to scan from right to left,", "start": 2057.185, "duration": 1.98}, {"text": "remember how, you know,", "start": 2059.165, "duration": 1.515}, {"text": "you get a sigmoid function,", "start": 2060.68, "duration": 1.65}, {"text": "or the sigmoid can be this way or this way or it depends on the sign.", "start": 2062.33, "duration": 2.865}, {"text": "I guess if these are positive samples these are negatives.", "start": 2065.195, "duration": 2.37}, {"text": "You have a sigmoid function like this.", "start": 2067.565, "duration": 2.01}, {"text": "And so w i j is just the height of this Sigma,", "start": 2069.575, "duration": 4.92}, {"text": "it's just a chance of, you know,", "start": 2074.495, "duration": 3.465}, {"text": "each of these examples being,", "start": 2077.96, "duration": 1.605}, {"text": "coming from either the z equals 1 or z equals 0", "start": 2079.565, "duration": 4.095}, {"text": "and then you store all of these numbers in the variables w i j.", "start": 2083.66, "duration": 4.47}, {"text": "Okay. So w i j is just to compute the posterior choice of this,", "start": 2088.13, "duration": 4.275}, {"text": "this example coming from the left Gaussian versus the right Gaussian.", "start": 2092.405, "duration": 2.985}, {"text": "You just saw that in the variable w i j.", "start": 2095.39, "duration": 3.78}, {"text": "So that's the E-step,", "start": 2099.17, "duration": 11.504}, {"text": "um, and you compute the w i j for every single training example i.", "start": 2110.674, "duration": 5.446}, {"text": "Right? I think it's the M-step is, um, yeah.", "start": 2116.12, "duration": 7.2}, {"text": "[BACKGROUND]", "start": 2123.32, "duration": 6.66}, {"text": "Sorry, is this what?", "start": 2129.98, "duration": 1.17}, {"text": "[BACKGROUND]", "start": 2131.15, "duration": 5.49}, {"text": "Oh this one.", "start": 2136.64, "duration": 0.51}, {"text": "Yes.", "start": 2137.15, "duration": 0.135}, {"text": "Sorry, yes. Thank you, there we go, thank you.", "start": 2137.285, "duration": 2.775}, {"text": "Yes, there's a following Gaussian.", "start": 2140.06, "duration": 2.86}, {"text": "Okay. So in the- so the E-step tells us, you know,", "start": 2143.56, "duration": 5.58}, {"text": "we're trying to guess the values of the z's right,", "start": 2149.14, "duration": 2.1}, {"text": "when we figure out what's the probability of z", "start": 2151.24, "duration": 1.68}, {"text": "being 1, 2, 3, 4 up to k was stored here.", "start": 2152.92, "duration": 2.895}, {"text": "And then in the M-step,", "start": 2155.815, "duration": 1.56}, {"text": "what we're going to do is use the formulas we have for maximum likelihood estimation,", "start": 2157.375, "duration": 7.48}, {"text": "and I want you to compare these with the equations I had above, right.", "start": 2164.855, "duration": 6.385}, {"text": "Okay. Well, I hope you see.", "start": 2186.76, "duration": 3.34}, {"text": "So these equations are a lot like the equations above,", "start": 2190.1, "duration": 3.72}, {"text": "except that instead of indicator z_i equals j,", "start": 2193.82, "duration": 4.245}, {"text": "we replaced it with w i j, right?", "start": 2198.065, "duration": 4.665}, {"text": "Which by the way is the expected value of this indicator function.", "start": 2202.73, "duration": 5.8}, {"text": "Right, because the expected value of an indicator function is", "start": 2210.52, "duration": 4.465}, {"text": "just equal to the probability of that thing in the middle being true.", "start": 2214.985, "duration": 4.095}, {"text": "Okay? Um, and then,", "start": 2219.08, "duration": 5.295}, {"text": "and then there's a formula for Sigma j as well that you can get from the lecture notes,", "start": 2224.375, "duration": 4.545}, {"text": "but i won't, I won't write down here.", "start": 2228.92, "duration": 1.68}, {"text": "Okay. So, um, one intuition of,", "start": 2230.6, "duration": 6.87}, {"text": "um, this mixture of Gaussians algorithm is", "start": 2237.47, "duration": 3.45}, {"text": "that it's a little bit like k-means but with soft assignment.", "start": 2240.92, "duration": 3.36}, {"text": "So in k-means, in", "start": 2244.28, "duration": 2.55}, {"text": "the first step we would take each point and just assign it to one of the k,", "start": 2246.83, "duration": 3.615}, {"text": "k cluster centroids, right?", "start": 2250.445, "duration": 2.16}, {"text": "And if it was a little bit closer to", "start": 2252.605, "duration": 2.145}, {"text": "the red cluster centroid than the blue cluster centroid,", "start": 2254.75, "duration": 2.61}, {"text": "we would just assign it to the red cluster centroid.", "start": 2257.36, "duration": 2.445}, {"text": "So even if it was just a little bit closer to one cluster centroid than another,", "start": 2259.805, "duration": 2.88}, {"text": "k means we just make what's called a hard assignment meaning, you know,", "start": 2262.685, "duration": 3.555}, {"text": "whatever cluster centroid it's closest to,", "start": 2266.24, "duration": 1.56}, {"text": "we just assigned it 100 percent of that ce- cluster centroid.", "start": 2267.8, "duration": 3.255}, {"text": "So yeah, EM is,", "start": 2271.055, "duration": 3.48}, {"text": "uh, you can think, uh,", "start": 2274.535, "duration": 1.98}, {"text": "EM implements a softer way of assigning points to", "start": 2276.515, "duration": 5.1}, {"text": "the different cluster centroids because instead of just", "start": 2281.615, "duration": 3.225}, {"text": "picking the one closest Gaussian center and assigning it there,", "start": 2284.84, "duration": 3.36}, {"text": "it uses these probabilities and gives it a waiting,", "start": 2288.2, "duration": 2.865}, {"text": "in terms of how much is assigned to Gaussian 1 versus Gaussian 2.", "start": 2291.065, "duration": 3.99}, {"text": "Um, and the second updates,", "start": 2295.055, "duration": 1.8}, {"text": "you know, the means accordingly, right?", "start": 2296.855, "duration": 1.845}, {"text": "Sum over all the x_i's to the extent they're assigned to", "start": 2298.7, "duration": 3.06}, {"text": "that cluster centroid divided by", "start": 2301.76, "duration": 1.86}, {"text": "the number of examples assigned to that cluster centroid.", "start": 2303.62, "duration": 2.745}, {"text": "Okay? So, so, so that's one intuition be- between EM and k-means,", "start": 2306.365, "duration": 6.734}, {"text": "um, and in a second, uh, uh, but,", "start": 2313.099, "duration": 2.461}, {"text": "but when you run this algorithm,", "start": 2315.56, "duration": 3.465}, {"text": "it turns out that this algorithm will converge with some caveats I'll get to later,", "start": 2319.025, "duration": 6.36}, {"text": "and this will find a pretty decent estimate, um,", "start": 2325.385, "duration": 4.035}, {"text": "of the parameters, you know,", "start": 2329.42, "duration": 3.345}, {"text": "of say fitting a mixture of two Gaussians model.", "start": 2332.765, "duration": 3.645}, {"text": "Okay? So this is, um,", "start": 2336.41, "duration": 4.8}, {"text": "the- a- and so if you are given a dataset of say airplane engines,", "start": 2341.21, "duration": 4.545}, {"text": "you can run this algorithm for the mixture of two Gaussians.", "start": 2345.755, "duration": 2.79}, {"text": "And then when a new airplane engine rolls off the assembly line, um, um, so,", "start": 2348.545, "duration": 5.4}, {"text": "so after you're fitting the k-means algorithm,", "start": 2353.945, "duration": 2.55}, {"text": "you now have a- after fitting the EM algorithm,", "start": 2356.495, "duration": 2.715}, {"text": "you now have a joint density of a P of x comma z.", "start": 2359.21, "duration": 3.09}, {"text": "And so the density for x is just sum of all the values of z of P of x comma z.", "start": 2362.3, "duration": 7.36}, {"text": "And so, and so", "start": 2370.42, "duration": 10.09}, {"text": "a mixture of Gaussians can fit distributions that look like this,", "start": 2380.51, "duration": 3.75}, {"text": "it can fit distributions that look like this, right?", "start": 2384.26, "duration": 2.58}, {"text": "These are, these are both mixtures of two Gaussians.", "start": 2386.84, "duration": 2.34}, {"text": "So this gives you a very rich family of models to fit very complicated distributions.", "start": 2389.18, "duration": 4.98}, {"text": "And now that, um, right,", "start": 2394.16, "duration": 2.625}, {"text": "and you can also fit, I don't know, something like this.", "start": 2396.785, "duration": 3.315}, {"text": "So this is a mixture of two Gaussians,", "start": 2400.1, "duration": 1.86}, {"text": "I guess one thin narrow Gaussian here and one much wider fatter Gaussian.", "start": 2401.96, "duration": 4.335}, {"text": "So a mixture of two Gaussians can actually fit a model of different things, um, uh,", "start": 2406.295, "duration": 3.75}, {"text": "can fit a lot- and a mixture of more than two Gaussians can fit even richer models.", "start": 2410.045, "duration": 5.19}, {"text": "And so by doing this,", "start": 2415.235, "duration": 2.215}, {"text": "you can now model P of x for many complicated densities,", "start": 2417.45, "duration": 4.465}, {"text": "or including this one, right,", "start": 2421.915, "duration": 1.695}, {"text": "this example I had just now.", "start": 2423.61, "duration": 2.145}, {"text": "This will allow you to fit", "start": 2425.755, "duration": 1.335}, {"text": "a probability density function that puts a lot of probability models on,", "start": 2427.09, "duration": 3.105}, {"text": "on a region that looks like this.", "start": 2430.195, "duration": 1.715}, {"text": "And so when you have a new example you can evaluate P of x,", "start": 2431.91, "duration": 3.305}, {"text": "and if P of x is large,", "start": 2435.215, "duration": 2.175}, {"text": "then you can say nope this looks okay and the P of x is less than Epsilon.", "start": 2437.39, "duration": 5.59}, {"text": "You can flag an anomaly and say take a look- take another look at this here.", "start": 2443.32, "duration": 5.095}, {"text": "Okay? So, um, I kind just wrote down this algorithm,", "start": 2448.415, "duration": 6.945}, {"text": "with a little bit of a hand-wavy explanation for how it's derived, right?", "start": 2455.36, "duration": 3.69}, {"text": "So like I said, if only you knew the values of C and just use maximum likelihood estimation,", "start": 2459.05, "duration": 4.635}, {"text": "so let's guess the values of z and plug that", "start": 2463.685, "duration": 2.415}, {"text": "into the formulas of maximum likely estimation.", "start": 2466.1, "duration": 2.49}, {"text": "It turns out that hand-wavy explanation works,", "start": 2468.59, "duration": 2.655}, {"text": "in the particular case of, um,", "start": 2471.245, "duration": 1.605}, {"text": "the EM mixtures of Gaussians but that there is a more formal way of", "start": 2472.85, "duration": 4.95}, {"text": "deriving the EM algorithm that shows that this is a maximum likelihood estimation algorithm,", "start": 2477.8, "duration": 6.285}, {"text": "and that they converge at at least a local optimum.", "start": 2484.085, "duration": 2.985}, {"text": "Um, and in particular,", "start": 2487.07, "duration": 1.545}, {"text": "there- what we'll do is show that if your goal is, um,", "start": 2488.615, "duration": 5.175}, {"text": "uh given a model P of x,", "start": 2493.79, "duration": 3.405}, {"text": "z parameterized by Theta,", "start": 2497.195, "duration": 2.1}, {"text": "if your goal is to maximize P of x,", "start": 2499.295, "duration": 4.665}, {"text": "right? Oh, excuse me.", "start": 2503.96, "duration": 2.29}, {"text": "So this is what maximum likelihood is supposed to do.", "start": 2511.7, "duration": 3.025}, {"text": "That EM is exactly trying to do that, okay.", "start": 2514.725, "duration": 2.955}, {"text": "So, um, I'll go on in a minute to present this more general derivation,", "start": 2517.68, "duration": 5.79}, {"text": "the - the form of general derivation of the EM algorithm tha- that doesn't rely on", "start": 2523.47, "duration": 4.02}, {"text": "this hand-wavy argument of", "start": 2527.49, "duration": 1.41}, {"text": "I guess it's easier use maximum likelihood with the guess values.", "start": 2528.9, "duration": 3.0}, {"text": "So I'll do the the rigorous derivation of EM in a minute.", "start": 2531.9, "duration": 4.125}, {"text": "But before I do that, let me just pause and check if there any questions.", "start": 2536.025, "duration": 4.485}, {"text": "Yeah.", "start": 2540.51, "duration": 8.83}, {"text": "[inaudible].", "start": 2549.34, "duration": 0.77}, {"text": "Um, yeah,", "start": 2550.11, "duration": 8.64}, {"text": "uh, maybe- let's see.", "start": 2558.75, "duration": 1.965}, {"text": "Maybe I'll help to not think of them as weights.", "start": 2560.715, "duration": 3.0}, {"text": "Um, yeah, I think thi- this is actually the weighting you assigned to a certain Gaussian,", "start": 2563.715, "duration": 5.685}, {"text": "so there's one intuition, uh,", "start": 2569.4, "duration": 1.75}, {"text": "hen - hence the weights, but, um,", "start": 2571.15, "duration": 1.85}, {"text": "um, let me think,", "start": 2573.0, "duration": 4.095}, {"text": "what's going to explain this?", "start": 2577.095, "duration": 1.65}, {"text": "So one way to think of this as wij is", "start": 2578.745, "duration": 5.04}, {"text": "how much xi is assigned to,", "start": 2583.785, "duration": 10.125}, {"text": "you know, to- to- to the um, \u00b5j Gaussian.", "start": 2593.91, "duration": 9.3}, {"text": "So, um, wij is the strength of how", "start": 2603.21, "duration": 6.15}, {"text": "strongly you want to assign that training example", "start": 2609.36, "duration": 2.835}, {"text": "xi to that cluster or to that- to that particular Gaussian.", "start": 2612.195, "duration": 4.065}, {"text": "Um, and so this is the number of 2,", "start": 2616.26, "duration": 1.68}, {"text": "0, and 1 right?", "start": 2617.94, "duration": 1.68}, {"text": "And, uh, the strength of all the assignments,", "start": 2619.62, "duration": 2.25}, {"text": "and every point is a sign with a total strength equal to 1,", "start": 2621.87, "duration": 3.675}, {"text": "because all these properties must sum up to 1.", "start": 2625.545, "duration": 2.16}, {"text": "And so, when I take this point and assign it, you know,", "start": 2627.705, "duration": 3.12}, {"text": "0.8 to a more close Gaussian and 0.2 to a more distant Gaussian.", "start": 2630.825, "duration": 4.71}, {"text": "And this is our guess for, you know,", "start": 2635.535, "duration": 2.115}, {"text": "well there's an 80% chance that it came with that Gaussian and a 20% chance", "start": 2637.65, "duration": 2.76}, {"text": "it came with the second Gaussian. That makes sense?", "start": 2640.41, "duration": 2.91}, {"text": "[inaudible].", "start": 2643.32, "duration": 7.02}, {"text": "Oh I see. So let's see.", "start": 2650.34, "duration": 1.59}, {"text": "Um, so when you're running the EM algorithm,", "start": 2651.93, "duration": 3.12}, {"text": "you never know what are the true values of z, right?", "start": 2655.05, "duration": 2.715}, {"text": "You're- you're given a data set,", "start": 2657.765, "duration": 1.275}, {"text": "so you're only told the x's,", "start": 2659.04, "duration": 1.71}, {"text": "and as far as we know, uh,", "start": 2660.75, "duration": 2.55}, {"text": "these airplane engines were generated off,", "start": 2663.3, "duration": 2.715}, {"text": "you know, two different Gaussians.", "start": 2666.015, "duration": 1.47}, {"text": "Maybe there are two separate assembly processes.", "start": 2667.485, "duration": 2.325}, {"text": "You know, one from the, uh,", "start": 2669.81, "duration": 1.53}, {"text": "uh, one from plant number one,", "start": 2671.34, "duration": 2.19}, {"text": "one from plant number two,", "start": 2673.53, "duration": 1.38}, {"text": "and maybe they actually operate a little bit differently,", "start": 2674.91, "duration": 1.95}, {"text": "but by the time they merge onto one, um,", "start": 2676.86, "duration": 3.075}, {"text": "uh, you know, by- by the time", "start": 2679.935, "duration": 2.445}, {"text": "the two suppliers of aircraft engines get to you, they've been mixed together,", "start": 2682.38, "duration": 3.39}, {"text": "and so you can't tell anymore which aircraft engine came", "start": 2685.77, "duration": 3.12}, {"text": "from proce- plant one and which pla- aircraft engine came from plant two.", "start": 2688.89, "duration": 3.645}, {"text": "Um, and they you know there are two plants,", "start": 2692.535, "duration": 1.905}, {"text": "where you just see the stream of aircraft engines,", "start": 2694.44, "duration": 1.8}, {"text": "you're hypothesizing that there are two types.", "start": 2696.24, "duration": 2.175}, {"text": "And so in every iteration of EM,", "start": 2698.415, "duration": 2.565}, {"text": "you're taking each, uh,", "start": 2700.98, "duration": 2.205}, {"text": "aircraft engine and guessing, you know, for this one,", "start": 2703.185, "duration": 3.015}, {"text": "I think there's 80% chance that it came from process one,", "start": 2706.2, "duration": 2.1}, {"text": "and a 20% chance it came from process two,", "start": 2708.3, "duration": 1.935}, {"text": "so that's the E-step.", "start": 2710.235, "duration": 1.635}, {"text": "And then in the M-step,", "start": 2711.87, "duration": 1.739}, {"text": "you look at all the engines that you're kind of guessing were generated by process one,", "start": 2713.609, "duration": 5.161}, {"text": "and you update your Gaussian to be a better model for all of the things that", "start": 2718.77, "duration": 3.45}, {"text": "were- that you kind of think were generated by process one.", "start": 2722.22, "duration": 3.615}, {"text": "And if there's something that you're absolutely sure came from process one,", "start": 2725.835, "duration": 3.345}, {"text": "then it has a weight of one close to one in this.", "start": 2729.18, "duration": 2.67}, {"text": "Do you think that was something that, you know,", "start": 2731.85, "duration": 1.62}, {"text": "there's a 10% chance it comes from process one,", "start": 2733.47, "duration": 2.34}, {"text": "then that example is given a lower weight and now you", "start": 2735.81, "duration": 2.955}, {"text": "update the mean for that Gaussian. That make sense?", "start": 2738.765, "duration": 3.045}, {"text": "Cool. All right.", "start": 2741.81, "duration": 5.35}, {"text": "So, [NOISE] 33 minutes.", "start": 2750.81, "duration": 31.48}, {"text": "Yeah.", "start": 2784.31, "duration": 4.72}, {"text": "Okay cool. All right.", "start": 2789.03, "duration": 2.805}, {"text": "Well I still remember when, um,", "start": 2791.835, "duration": 2.55}, {"text": "I was an undergrad doing a summer internship at AT&T Bell Labs.", "start": 2794.385, "duration": 3.465}, {"text": "Um, and then someone a few offices down had", "start": 2797.85, "duration": 2.49}, {"text": "learned about EM for the mixture of Gaussians for the first time,", "start": 2800.34, "duration": 2.31}, {"text": "and he was running it on his computer,", "start": 2802.65, "duration": 1.5}, {"text": "and he's going around to every single office.", "start": 2804.15, "duration": 2.415}, {"text": "Saying, \"Oh my God you've got to check this out,", "start": 2806.565, "duration": 1.845}, {"text": "this is unbelievable look at what this algorithm can do for three mixes of Gaussian. \"", "start": 2808.41, "duration": 3.3}, {"text": "So tha- that shows you,", "start": 2811.71, "duration": 1.8}, {"text": "those are the type of people I hang out with", "start": 2813.51, "duration": 1.47}, {"text": "[LAUGHTER].", "start": 2814.98, "duration": 10.62}, {"text": "All right. Um, so in order to derive yo - you know,", "start": 2825.6, "duration": 3.32}, {"text": "so -so this is a slightly hand-wavy argument.", "start": 2828.92, "duration": 1.71}, {"text": "As I uh, let's get- let's guess the values of the z's.", "start": 2830.63, "duration": 2.61}, {"text": "Let's just have these weights and plug them into maximum likelihood.", "start": 2833.24, "duration": 2.64}, {"text": "Um, what I would like to do is give", "start": 2835.88, "duration": 2.14}, {"text": "a more rigorous de- derivation for why EM Algorithm is a reasonable algorithm,", "start": 2838.02, "duration": 4.935}, {"text": "and why it's a maximum likelihood estimation algorithm", "start": 2842.955, "duration": 2.595}, {"text": "and why we can expect it to converge.", "start": 2845.55, "duration": 2.55}, {"text": "And it turns out that rather than just proving,", "start": 2848.1, "duration": 2.535}, {"text": "you know, that this is a sound algorithm,", "start": 2850.635, "duration": 1.755}, {"text": "what we'll see on Wednesday is that this view of EM, uh,", "start": 2852.39, "duration": 4.07}, {"text": "allows us to derive EM in a- in", "start": 2856.46, "duration": 2.26}, {"text": "a more correct way for other models as well, the mixtures of Gaussian.", "start": 2858.72, "duration": 3.81}, {"text": "On, on Wednesday, we'll talk about,", "start": 2862.53, "duration": 1.8}, {"text": "uh, uh a model called factor analysis,", "start": 2864.33, "duration": 2.82}, {"text": "it lets you model Gaussians in extremely high dimensional spaces,", "start": 2867.15, "duration": 3.24}, {"text": "where if you have 1,000 dimensional data,", "start": 2870.39, "duration": 1.605}, {"text": "but only 30 examples,", "start": 2871.995, "duration": 1.44}, {"text": "how do you fit a Gaussian into that?", "start": 2873.435, "duration": 1.335}, {"text": "So we'll talk about that on Wednesday.", "start": 2874.77, "duration": 1.365}, {"text": "And it turns out this derivation of EM we're", "start": 2876.135, "duration": 2.325}, {"text": "going to go about- through now is crucial for,", "start": 2878.46, "duration": 2.745}, {"text": "um, applying EM accurately in- in- in problems like that.", "start": 2881.205, "duration": 4.59}, {"text": "Okay, so.", "start": 2885.795, "duration": 2.1}, {"text": "Uh, in order that live up to that derivation,", "start": 2887.895, "duration": 3.99}, {"text": "let me describe, um, Jensen's inequality.", "start": 2891.885, "duration": 3.9}, {"text": "So let f be a, a convex function.", "start": 2895.785, "duration": 8.425}, {"text": "Um, to do EM,", "start": 2904.64, "duration": 2.215}, {"text": "we're actually going to need a concave function,", "start": 2906.855, "duration": 1.725}, {"text": "so it'll be all minus of everything,", "start": 2908.58, "duration": 1.77}, {"text": "but we'll get to that in a second.", "start": 2910.35, "duration": 1.995}, {"text": "But so, a convex function means the second derivative is greater than 0,", "start": 2912.345, "duration": 10.095}, {"text": "or in other words, it looks like that, right?", "start": 2922.44, "duration": 1.68}, {"text": "So that's a convex function.", "start": 2924.12, "duration": 2.35}, {"text": "Uh, let x be a random variable.", "start": 2927.05, "duration": 3.56}, {"text": "Then f of the expected value of x is less than or equal to the expected value of x.", "start": 2937.37, "duration": 10.705}, {"text": "Okay.", "start": 2948.075, "duration": 0.54}, {"text": "Now, [NOISE] um, [NOISE]", "start": 2948.615, "duration": 17.354}, {"text": "maybe, um, here's an example. All right.", "start": 2965.969, "duration": 5.836}, {"text": "So here's the, um, let's see,", "start": 2971.805, "duration": 5.4}, {"text": "there's the function f of x,", "start": 2977.205, "duration": 1.755}, {"text": "and let's say that these are the values 1, 2, 3, 4, 5.", "start": 2978.96, "duration": 6.255}, {"text": "And suppose that X is equal to 1 with", "start": 2985.215, "duration": 5.115}, {"text": "probability one-half and is equal to 5 with probability one-half, right?", "start": 2990.33, "duration": 8.1}, {"text": "Just for the illustration.", "start": 2998.43, "duration": 1.455}, {"text": "Then here is f of 1.", "start": 2999.885, "duration": 6.595}, {"text": "Here is f of 5.", "start": 3009.58, "duration": 2.86}, {"text": "Um, here is f of 3.", "start": 3012.44, "duration": 4.08}, {"text": "And f of 3 is f of the expected value of X,", "start": 3016.52, "duration": 4.41}, {"text": "right, because so the expected value of X.", "start": 3020.93, "duration": 3.555}, {"text": "And sometimes I write this without the square brackets, right.", "start": 3024.485, "duration": 3.255}, {"text": "It's the average of X is equal to 3.", "start": 3027.74, "duration": 2.685}, {"text": "Um, and so the expected value,", "start": 3030.425, "duration": 3.45}, {"text": "excuse me, f of the expected value of X is equal to this value,", "start": 3033.875, "duration": 4.245}, {"text": "whereas the expected value of f of", "start": 3038.12, "duration": 4.86}, {"text": "x is the mean of f of 1 and f of 5.", "start": 3042.98, "duration": 7.72}, {"text": "All right. So the expected value of f of x.", "start": 3052.15, "duration": 3.1}, {"text": "F of x is a 50% chance of being f of 1,", "start": 3055.25, "duration": 2.52}, {"text": "and a 50% chance of being f of 5.", "start": 3057.77, "duration": 1.965}, {"text": "And so the expected value of f of x is equal to this value in the middle.", "start": 3059.735, "duration": 5.1}, {"text": "It's really take these two, right.", "start": 3064.835, "duration": 2.28}, {"text": "Take this value and this value and take their mean.", "start": 3067.115, "duration": 2.175}, {"text": "So is this value up here, and,", "start": 3069.29, "duration": 2.325}, {"text": "and this value is the expected value of f of x.", "start": 3071.615, "duration": 6.24}, {"text": "Okay. And so in this example the expected value", "start": 3077.855, "duration": 3.585}, {"text": "of f of x is greater than f of the expected value of X,", "start": 3081.44, "duration": 3.645}, {"text": "right, as, as predicted by Jensen's inequality.", "start": 3085.085, "duration": 2.43}, {"text": "Um, I'm gonna just draw one illustration that may or may not help,", "start": 3087.515, "duration": 3.885}, {"text": "and some of my friends like it,", "start": 3091.4, "duration": 1.26}, {"text": "I sometimes use it but if it's confusing then don't worry about it.", "start": 3092.66, "duration": 2.58}, {"text": "But it turns out that if you draw a line that connects these two,", "start": 3095.24, "duration": 4.365}, {"text": "then the midpoint of this line, um,", "start": 3099.605, "duration": 2.82}, {"text": "is the height of f of expected value of x,", "start": 3102.425, "duration": 3.135}, {"text": "right, so the height of this.", "start": 3105.56, "duration": 1.89}, {"text": "You know, so, so given these two points,", "start": 3107.45, "duration": 1.89}, {"text": "this point and this point, if you draw this line,", "start": 3109.34, "duration": 1.98}, {"text": "it's called a chord, um,", "start": 3111.32, "duration": 1.905}, {"text": "then the height of this point is the expected value of f of x.", "start": 3113.225, "duration": 6.485}, {"text": "And this point is,", "start": 3119.71, "duration": 8.7}, {"text": "um, f of the expected value of x.", "start": 3128.41, "duration": 3.775}, {"text": "Right. And in any convex function,", "start": 3132.185, "duration": 4.165}, {"text": "you know, really take any convex function.", "start": 3138.22, "duration": 3.115}, {"text": "That's also convex function.", "start": 3141.335, "duration": 1.68}, {"text": "If you draw any chord,", "start": 3143.015, "duration": 1.83}, {"text": "that green point is always higher,", "start": 3144.845, "duration": 4.95}, {"text": "right, than that green point which is why- which is", "start": 3149.795, "duration": 4.395}, {"text": "another way of seeing why Jensen's inequality holds true.", "start": 3154.19, "duration": 4.32}, {"text": "Okay. If this visualization doesn't help don't worry", "start": 3158.51, "duration": 2.34}, {"text": "about it but it's just a- actually what a lot of", "start": 3160.85, "duration": 2.37}, {"text": "my friends do is we keep on forgetting which direction Jensen's inequality goes.", "start": 3163.22, "duration": 6.09}, {"text": "[LAUGHTER] Why are we not using Jensen [LAUGHTER] that's not great.", "start": 3169.31, "duration": 1.8}, {"text": "So a lot of my friends don't remember, we draw this picture and draw that chord,", "start": 3171.11, "duration": 4.35}, {"text": "and we quickly figure out which way the inequality goes.", "start": 3175.46, "duration": 2.295}, {"text": "Um, all right.", "start": 3177.755, "duration": 4.77}, {"text": "So one addendum further.", "start": 3182.525, "duration": 16.545}, {"text": "If f is strictly greater than 0.", "start": 3199.07, "duration": 4.455}, {"text": "And so if this is the case,", "start": 3203.525, "duration": 2.025}, {"text": "we say f is strictly convex.", "start": 3205.55, "duration": 2.83}, {"text": "Then-", "start": 3216.82, "duration": 11.2}, {"text": "Okay.", "start": 3228.02, "duration": 0.99}, {"text": "So, um, let's see, a straight line is also a convex function, right.", "start": 3229.01, "duration": 5.13}, {"text": "So this is a convex function,", "start": 3234.14, "duration": 1.5}, {"text": "this is a convex function and this is a convex function.", "start": 3235.64, "duration": 1.62}, {"text": "It turns out a straight line, that's also a convex function.", "start": 3237.26, "duration": 3.615}, {"text": "But so in this addendum is saying that if f is", "start": 3240.875, "duration": 2.685}, {"text": "a strictly convex function meaning basically it's now a straight line, right.", "start": 3243.56, "duration": 4.095}, {"text": "And a bit modern, it's not a straight line.", "start": 3247.655, "duration": 2.46}, {"text": "But if the curvature if it's always bending up, uh,", "start": 3250.115, "duration": 4.47}, {"text": "then the only way for the left and right-hand sides to be equal is if x is a constant,", "start": 3254.585, "duration": 5.7}, {"text": "meaning it's a random variable that always takes on the same value.", "start": 3260.285, "duration": 3.975}, {"text": "Okay. So Jensen's inequality says that,", "start": 3264.26, "duration": 3.405}, {"text": "you know, um, left-hand side is going to be the same as right hand side.", "start": 3267.665, "duration": 4.44}, {"text": "Sorry, I think I reversed the order of these two for that equation that doesn't have it.", "start": 3272.105, "duration": 3.375}, {"text": "But so Jensen equality says", "start": 3275.48, "duration": 1.74}, {"text": "left-hand side is always less than or equal to the right-hand side,", "start": 3277.22, "duration": 2.505}, {"text": "and the only way it's equal as if X, you know,", "start": 3279.725, "duration": 3.345}, {"text": "is a random variable that always takes on the same value.", "start": 3283.07, "duration": 3.09}, {"text": "Okay, yeah.", "start": 3286.16, "duration": 1.12}, {"text": "What if- What if the value of f of 1 was equal to the value of f of 3. Wouldn't that?", "start": 3287.28, "duration": 9.35}, {"text": "Yeah. So it turns out what if value f of 1 is equal to the value of f of 3.", "start": 3296.63, "duration": 4.02}, {"text": "It turns out, it does.", "start": 3300.65, "duration": 1.545}, {"text": "Vary. So let's see.", "start": 3302.195, "duration": 1.665}, {"text": "So one way that [NOISE] could happen would be if the function were like that.", "start": 3303.86, "duration": 4.905}, {"text": "And then if you take the- draw the chord,", "start": 3308.765, "duration": 2.145}, {"text": "take the mean it's still higher.", "start": 3310.91, "duration": 1.56}, {"text": "[inaudible] The point of f of 1. [inaudible]", "start": 3312.47, "duration": 7.98}, {"text": "Then it's important. If, if you kind of flat that part here,", "start": 3320.45, "duration": 3.69}, {"text": "then the function is not strictly convex.", "start": 3324.14, "duration": 2.04}, {"text": "And so it's still less than equal to but it's not,", "start": 3326.18, "duration": 3.03}, {"text": "but they can't be equal to if x is random.", "start": 3329.21, "duration": 2.37}, {"text": "Okay. So and we'll use this in a little bit.", "start": 3331.58, "duration": 6.42}, {"text": "We'll actually end up using this.", "start": 3338.0, "duration": 1.77}, {"text": "Um, and again for the strict probabilistic, you know,", "start": 3339.77, "duration": 3.81}, {"text": "if those of you that, I don't know, take classes in advanced probability,", "start": 3343.58, "duration": 3.629}, {"text": "the technical way of saying x is a constant is x is equal to EX with probability 1.", "start": 3347.209, "duration": 8.771}, {"text": "You know, I, I think that for all practical human purposes", "start": 3359.89, "duration": 3.46}, {"text": "you do not need to worry about this.", "start": 3363.35, "duration": 1.38}, {"text": "But I think if you [LAUGHTER] take a class in measure theory.", "start": 3364.73, "duration": 2.805}, {"text": "The Professor in measure theory will be happy if you say this and you", "start": 3367.535, "duration": 3.015}, {"text": "say x is a constant but maybe, maybe none of you.", "start": 3370.55, "duration": 2.835}, {"text": "Okay. Just don't worry about it.", "start": 3373.385, "duration": 1.53}, {"text": "Um, oh yes.", "start": 3374.915, "duration": 4.05}, {"text": "Okay. Now, um,", "start": 3378.965, "duration": 1.395}, {"text": "just one more addendum,", "start": 3380.36, "duration": 2.13}, {"text": "um, to this is that", "start": 3382.49, "duration": 3.36}, {"text": "the form of Jensen's inequality we are going to use", "start": 3385.85, "duration": 2.67}, {"text": "is actually a form for a concave function.", "start": 3388.52, "duration": 2.535}, {"text": "So instead of convex,", "start": 3391.055, "duration": 1.905}, {"text": "um, I'm gonna say concave.", "start": 3392.96, "duration": 2.28}, {"text": "And so, you know,", "start": 3395.24, "duration": 1.62}, {"text": "a concave function is just a negative of a convex function, right.", "start": 3396.86, "duration": 3.48}, {"text": "If you take a convex function and take the negative of that, it becomes concave.", "start": 3400.34, "duration": 3.9}, {"text": "And so the whole thing works with the- with everything flipped around the other way.", "start": 3404.24, "duration": 8.595}, {"text": "Okay. And yep, so this is strictly concave.", "start": 3412.835, "duration": 7.615}, {"text": "Okay. So the form of Jensen's inequality we are gonna use is actually the, um,", "start": 3421.03, "duration": 6.22}, {"text": "concave form of Jensen's inequality,", "start": 3427.25, "duration": 2.805}, {"text": "and we're actually going to apply it to the log function.", "start": 3430.055, "duration": 3.15}, {"text": "So the log function, right.", "start": 3433.205, "duration": 1.305}, {"text": "Log x looks like this.", "start": 3434.51, "duration": 1.08}, {"text": "And so that's a concave function.", "start": 3435.59, "duration": 1.77}, {"text": "And so the inequality we'll use would be", "start": 3437.36, "duration": 2.01}, {"text": "in this direction that I have in orange.", "start": 3439.37, "duration": 2.86}, {"text": "All right.", "start": 3443.41, "duration": 2.48}, {"text": "So here's the density estimation problem.", "start": 3450.97, "duration": 29.035}, {"text": "Meaning, density estimation means you want to estimate P of x. All right.", "start": 3480.005, "duration": 3.825}, {"text": "So we have a model", "start": 3483.83, "duration": 2.23}, {"text": "for P of x, z, with parameters theta.", "start": 3488.68, "duration": 6.16}, {"text": "And so, you know, instead of writing out mu,", "start": 3494.84, "duration": 3.69}, {"text": "sigma- mu, sigma, and phi,", "start": 3498.53, "duration": 3.165}, {"text": "like we did for the mixture of Gaussians.", "start": 3501.695, "duration": 1.605}, {"text": "I'm just gonna capture all the parameters you have.", "start": 3503.3, "duration": 1.83}, {"text": "Whatever your parameters are, I'm just gonna capture them in one variable theta.", "start": 3505.13, "duration": 4.35}, {"text": "And you only observe x.", "start": 3509.48, "duration": 6.555}, {"text": "So your training set looks like that.", "start": 3516.035, "duration": 6.015}, {"text": "So the, um, log likelihood of the parameters theta is equal", "start": 3522.05, "duration": 8.85}, {"text": "to some of your training examples log P of x_i, parameterized by theta.", "start": 3530.9, "duration": 8.865}, {"text": "Um, and this in turn is log of sum over z,", "start": 3539.765, "duration": 8.415}, {"text": "P of x_i, z_i parameterized by theta, right.", "start": 3548.18, "duration": 9.4}, {"text": "Because P of x, you know,", "start": 3557.8, "duration": 3.835}, {"text": "is just taking the joint distribution and summing out, marginalizing out z_i.", "start": 3561.635, "duration": 5.295}, {"text": "Okay. [NOISE] And so what we want is maximum likelihood estimation", "start": 3566.93, "duration": 8.49}, {"text": "which is define the value of theta that maximizes this log-likelihood.", "start": 3575.42, "duration": 8.925}, {"text": "And what we would like to do is derive an EM,", "start": 3584.345, "duration": 3.87}, {"text": "derive an algorithm which will turn out to be an EM algorithm as", "start": 3588.215, "duration": 3.975}, {"text": "an iterative algorithm for finding", "start": 3592.19, "duration": 2.895}, {"text": "the maximum likelihood estimates of the parameters theta.", "start": 3595.085, "duration": 3.405}, {"text": "[NOISE]", "start": 3598.49, "duration": 6.51}, {"text": "So, um, let me draw a picture of that,", "start": 3605.0, "duration": 6.93}, {"text": "I'd like you to keep in mind as we go through the math, which is,", "start": 3611.93, "duration": 3.735}, {"text": "you know, the, the horizontal axis", "start": 3615.665, "duration": 2.985}, {"text": "is the space of possible values of the parameters Theta.", "start": 3618.65, "duration": 3.0}, {"text": "And so there's some function O of Theta that you try to maximize.", "start": 3621.65, "duration": 8.65}, {"text": "This right. And so what EM does is, um,", "start": 3633.07, "duration": 5.53}, {"text": "let's say you initialize Theta as some value, you know,", "start": 3638.6, "duration": 4.47}, {"text": "maybe randomly initialize, um,", "start": 3643.07, "duration": 2.895}, {"text": "sim- similar to the k-means cluster centroids.", "start": 3645.965, "duration": 2.475}, {"text": "Where just randomly initialize your mu's with a mixture of Gaussian's.", "start": 3648.44, "duration": 3.9}, {"text": "What the EM algorithm does is in the E-step,", "start": 3652.34, "duration": 3.225}, {"text": "we're going to construct a lower bound shown in green here for the log-likelihood.", "start": 3655.565, "duration": 8.025}, {"text": "And this lower bound,", "start": 3663.59, "duration": 1.53}, {"text": "this green curve has two properties.", "start": 3665.12, "duration": 1.905}, {"text": "One is that it is a lower bound.", "start": 3667.025, "duration": 1.845}, {"text": "So everywhere you look, you know,", "start": 3668.87, "duration": 1.515}, {"text": "over all values of Theta,", "start": 3670.385, "duration": 1.695}, {"text": "the green curve lies below the blue curve.", "start": 3672.08, "duration": 2.43}, {"text": "So this is a lower bound.", "start": 3674.51, "duration": 1.395}, {"text": "And the second property that the green curve has is that it is", "start": 3675.905, "duration": 4.425}, {"text": "equal to the blue curve at the current value of Theta.", "start": 3680.33, "duration": 4.785}, {"text": "Okay. So what the E-step does,", "start": 3685.115, "duration": 3.435}, {"text": "uh, which you'll see later on,", "start": 3688.55, "duration": 1.44}, {"text": "and just keep this picture in mind as we go through the E-step and the M-step is,", "start": 3689.99, "duration": 3.39}, {"text": "um, construct the lower bound that looks like this, right.", "start": 3693.38, "duration": 3.78}, {"text": "Oh, and, and also, uh, to, uh,", "start": 3697.16, "duration": 2.28}, {"text": "to foreshadow probably the derivation.", "start": 3699.44, "duration": 2.535}, {"text": "Right? There- there was an addendum to Jensen's inequality where we said,", "start": 3701.975, "duration": 3.465}, {"text": "well, under these conditions it holds with equality.", "start": 3705.44, "duration": 2.865}, {"text": "Right. E of f of x equals f of e of x.", "start": 3708.305, "duration": 2.01}, {"text": "We said, \"Well, the two things are equal with under certain conditions.\"", "start": 3710.315, "duration": 3.165}, {"text": "Um, we want things to be equal.", "start": 3713.48, "duration": 2.07}, {"text": "We want the green curve to be equal to the blue curve at the old value of Theta.", "start": 3715.55, "duration": 3.975}, {"text": "So we- we'll use that addendum to Jensen's inequality when we drive that.", "start": 3719.525, "duration": 3.315}, {"text": "Um, so this E-step is draw the green curve.", "start": 3722.84, "duration": 5.19}, {"text": "And then what the M-step does is it takes a green curve,", "start": 3728.03, "duration": 4.485}, {"text": "and then it finds the maximum.", "start": 3732.515, "duration": 2.695}, {"text": "Actually, certainly stroke [inaudible] so I'll draw in green.", "start": 3736.12, "duration": 3.28}, {"text": "What the M-step does is it takes the green curve,", "start": 3739.4, "duration": 2.715}, {"text": "and it finds the maximum.", "start": 3742.115, "duration": 2.67}, {"text": "And one step of EM will then move Theta from this green value to this red value.", "start": 3744.785, "duration": 8.315}, {"text": "Okay. So the E-step constructs the green curve,", "start": 3753.1, "duration": 4.05}, {"text": "and the M-step, uh,", "start": 3757.15, "duration": 1.86}, {"text": "finds the maximum of the green curve.", "start": 3759.01, "duration": 2.76}, {"text": "And this is one iteration of EM.", "start": 3761.77, "duration": 2.525}, {"text": "The second iteration of EM,", "start": 3764.295, "duration": 2.165}, {"text": "now that you're at this red thing is will construct a new lower bound,", "start": 3766.46, "duration": 4.455}, {"text": "and then again, you use a different lower bound.", "start": 3770.915, "duration": 1.905}, {"text": "Everywhere the red curve is below the blue curve,", "start": 3772.82, "duration": 2.535}, {"text": "and the values are equal at this new value.", "start": 3775.355, "duration": 3.585}, {"text": "That's the E-step, and then M-step will maximize this red curve,", "start": 3778.94, "duration": 7.05}, {"text": "um, and so on. Now you're here.", "start": 3785.99, "duration": 3.915}, {"text": "Construct another thing, do that.", "start": 3789.905, "duration": 4.065}, {"text": "Right. And you can kinda tell that as you keep running EM,", "start": 3793.97, "duration": 3.165}, {"text": "this is constantly trying to increase L of Theta.", "start": 3797.135, "duration": 3.915}, {"text": "Trying to increase the log-likelihood,", "start": 3801.05, "duration": 1.575}, {"text": "until it converges to a local optimum.", "start": 3802.625, "duration": 2.07}, {"text": "Okay. Um, the EM algorithm does converge only to local optimum.", "start": 3804.695, "duration": 3.915}, {"text": "So if, you know, there was another even bigger thing there that it may never", "start": 3808.61, "duration": 3.69}, {"text": "find its way over to that other- that, uh, better optimum.", "start": 3812.3, "duration": 3.81}, {"text": "But the EM algorithm by repeatedly doing this,", "start": 3816.11, "duration": 2.7}, {"text": "will hopefully converge to a pretty good local optimum.", "start": 3818.81, "duration": 3.33}, {"text": "Okay. All right.", "start": 3822.14, "duration": 2.31}, {"text": "So let's write on how we do that.", "start": 3824.45, "duration": 6.21}, {"text": "Um, let me think.", "start": 3830.66, "duration": 6.04}, {"text": "Actually, let me use the other board.", "start": 3837.79, "duration": 3.35}, {"text": "No, I think this is okay. All right.", "start": 3841.66, "duration": 8.875}, {"text": "So I've already said that our goal is to find the parameters theta that maximize this.", "start": 3850.535, "duration": 5.985}, {"text": "[NOISE] All right.", "start": 3856.52, "duration": 4.53}, {"text": "Uh, and so that equation we said are just now is sum over i log,", "start": 3861.05, "duration": 5.685}, {"text": "sum over zi, p of", "start": 3866.735, "duration": 3.6}, {"text": "xi comma zi given Theta.", "start": 3870.335, "duration": 7.065}, {"text": "Okay. So this is just what we had written down,", "start": 3877.4, "duration": 2.415}, {"text": "I guess, uh, on the left.", "start": 3879.815, "duration": 2.335}, {"text": "What I'm going to do next is,", "start": 3882.88, "duration": 3.065}, {"text": "um, divide by- [NOISE]", "start": 3885.945, "duration": 9.545}, {"text": "multiply and divide by this.", "start": 3895.49, "duration": 2.8}, {"text": "Okay. Um, where Qi of zi is a probability distribution,", "start": 3907.7, "duration": 11.5}, {"text": "i.e., the sum over zi,", "start": 3926.77, "duration": 3.73}, {"text": "Qi of zi equals 1.", "start": 3930.5, "duration": 5.595}, {"text": "Okay. So I'm going to multiply and divide by some probability distribution,", "start": 3936.095, "duration": 4.08}, {"text": "and we'll, we'll decide later", "start": 3940.175, "duration": 2.085}, {"text": "how to come up with this probability distribution Qi, right.", "start": 3942.26, "duration": 2.61}, {"text": "But, you know, I'm allowed to construct", "start": 3944.87, "duration": 2.07}, {"text": "a probability distribution and multiply and divide by the same thing.", "start": 3946.94, "duration": 2.67}, {"text": "Right. Now, if you look at this,", "start": 3949.61, "duration": 5.205}, {"text": "all right, let's put square brackets here.", "start": 3954.815, "duration": 2.19}, {"text": "If this Qi, that is the probability distribution meaning that sum over zi Qi,", "start": 3957.005, "duration": 4.485}, {"text": "zi sums over- sums to 1.", "start": 3961.49, "duration": 2.205}, {"text": "Then this thing inside is, um,", "start": 3963.695, "duration": 3.42}, {"text": "equal to sum over i log of an expected value of zi", "start": 3967.115, "duration": 7.98}, {"text": "drawn from the Qi distribution of [NOISE] right, actually, if I,", "start": 3975.095, "duration": 8.565}, {"text": "let me use colors to make this clearer.", "start": 3983.66, "duration": 12.46}, {"text": "Right. So the way you compute the expected value of z-, you know,", "start": 4002.04, "duration": 5.17}, {"text": "some function of zi is you sum over all the possible values of", "start": 4007.21, "duration": 3.96}, {"text": "zi of the probability of zi times whatever that function is.", "start": 4011.17, "duration": 4.005}, {"text": "So this equation is just the expected value with respect to zi", "start": 4015.175, "duration": 4.215}, {"text": "drawn from that Qi distribution of that thing in the square brackets,", "start": 4019.39, "duration": 4.44}, {"text": "in the purple square brackets.", "start": 4023.83, "duration": 2.32}, {"text": "Now, using the, um,", "start": 4028.5, "duration": 8.695}, {"text": "concave form of Jensen's inequality,", "start": 4037.195, "duration": 3.505}, {"text": "we have that this is greater than", "start": 4041.76, "duration": 3.52}, {"text": "or equal to [NOISE].", "start": 4045.28, "duration": 29.31}, {"text": "So this is a form of Jensen's inequality where,", "start": 4074.59, "duration": 3.27}, {"text": "um, f of E, x is greater than or equal to E of f of x,", "start": 4077.86, "duration": 9.21}, {"text": "where here, um, this is the logarithmic function.", "start": 4087.07, "duration": 6.3}, {"text": "Right. So the log function is a concave function. It looks like that.", "start": 4093.37, "duration": 3.48}, {"text": "And so, um, using the,", "start": 4096.85, "duration": 3.36}, {"text": "I guess here using,", "start": 4100.21, "duration": 1.305}, {"text": "using the form Jensen's inequality with the signs reversed, um.", "start": 4101.515, "duration": 4.41}, {"text": "Right, f of Ex is greater than equals E of fx.", "start": 4105.925, "duration": 3.075}, {"text": "So you get log of expectation is greater than equal to expectation of the log, all right.", "start": 4109.0, "duration": 5.26}, {"text": "And then finally, let me just take this expectation and unpack it one more time.", "start": 4116.25, "duration": 6.025}, {"text": "So this is now sum over i, sum over zi.", "start": 4122.275, "duration": 3.585}, {"text": "[NOISE].", "start": 4125.86, "duration": 14.76}, {"text": "Okay. So I just took this expected value and", "start": 4140.62, "duration": 4.41}, {"text": "turned it back into the sum of the random variable probability, times that thing.", "start": 4145.03, "duration": 5.035}, {"text": "Okay. So, um, if you remember this picture from the middle,", "start": 4150.065, "duration": 8.045}, {"text": "what we wanted to do was to construct a function,", "start": 4158.11, "duration": 3.764}, {"text": "construct this green curve.", "start": 4161.874, "duration": 1.681}, {"text": "There's a lower bound for the blue curve.", "start": 4163.555, "duration": 2.955}, {"text": "And if you view this formula here as a function of Theta right,", "start": 4166.51, "duration": 6.51}, {"text": "so your x, um, x is just your data,", "start": 4173.02, "duration": 3.39}, {"text": "and z is a variable you sum over.", "start": 4176.41, "duration": 2.205}, {"text": "So this whole thing is the function of Theta, right?", "start": 4178.615, "duration": 2.055}, {"text": "Because x's are fixed,", "start": 4180.67, "duration": 1.38}, {"text": "z is just something you f- sum over.", "start": 4182.05, "duration": 1.77}, {"text": "So this whole formula here,", "start": 4183.82, "duration": 1.65}, {"text": "this is a function of the parameters Theta.", "start": 4185.47, "duration": 3.075}, {"text": "And what we've shown is that this thing, you know,", "start": 4188.545, "duration": 4.305}, {"text": "this formula here, this is a lower bound for the log-likelihood,", "start": 4192.85, "duration": 5.26}, {"text": "uh, for- for, for, for this thing.", "start": 4198.11, "duration": 2.41}, {"text": "I guess this is L of Theta.", "start": 4200.52, "duration": 2.71}, {"text": "So- go ahead.", "start": 4203.69, "duration": 2.26}, {"text": "[inaudible].", "start": 4205.95, "duration": 6.55}, {"text": "Oh, how I got to this equation?", "start": 4212.5, "duration": 1.905}, {"text": "Uh, sure. Um, let me think.", "start": 4214.405, "duration": 3.24}, {"text": "So let's see. What's a good way to do this?", "start": 4217.645, "duration": 9.495}, {"text": "Um, uh, yeah.", "start": 4227.14, "duration": 3.72}, {"text": "Let's say that z takes on values from 1 through 5, right.", "start": 4230.86, "duration": 3.555}, {"text": "Let's say z takes on values from 1 through 10.", "start": 4234.415, "duration": 1.785}, {"text": "So you roll a 10 sided dice.", "start": 4236.2, "duration": 1.575}, {"text": "And I want to compute, um, you know,", "start": 4237.775, "duration": 3.33}, {"text": "the expected value of, uh,", "start": 4241.105, "duration": 2.37}, {"text": "some function of, of some function g, g of z.", "start": 4243.475, "duration": 4.35}, {"text": "Right. Then the expected value of g of z is sum of", "start": 4247.825, "duration": 3.405}, {"text": "all the possible values of z of the probability that you get that z,", "start": 4251.23, "duration": 4.575}, {"text": "times g of z.", "start": 4255.805, "duration": 2.88}, {"text": "Right. So that's, that's what's the expected value is of a function of a random variable.", "start": 4258.685, "duration": 4.92}, {"text": "And, and this is- and the expected value of z is sum over z,", "start": 4263.605, "duration": 4.83}, {"text": "P of z times z.", "start": 4268.435, "duration": 2.475}, {"text": "That's the average of random variable.", "start": 4270.91, "duration": 1.86}, {"text": "And so, um, in the notation that we have,", "start": 4272.77, "duration": 4.395}, {"text": "the probability of z taking on different values is denoted by Qi of z,", "start": 4277.165, "duration": 5.85}, {"text": "which is why we wind up with that formula.", "start": 4283.015, "duration": 3.09}, {"text": "Does that makes sense? Does it?", "start": 4286.105, "duration": 3.09}, {"text": "Okay. Is that okay? Does that make sense? Yeah. All right.", "start": 4289.195, "duration": 4.935}, {"text": "If, if one of these steps doesn't make sense, let me know.", "start": 4294.13, "duration": 2.085}, {"text": "Th- other questions?", "start": 4296.215, "duration": 2.335}, {"text": "Okay. All right.", "start": 4304.98, "duration": 9.67}, {"text": "Hope that makes sense. [NOISE]. Um. [NOISE]", "start": 4314.65, "duration": 9.54}, {"text": "Now, one of the things we want when constructing", "start": 4324.19, "duration": 5.91}, {"text": "this green lower bound is we want that green lower", "start": 4330.1, "duration": 2.55}, {"text": "bound to be equal to the blue function at this point, right?", "start": 4332.65, "duration": 4.03}, {"text": "And this is actually how you guarantee that when you optimize the green function.", "start": 4336.68, "duration": 4.175}, {"text": "By improving on the green function,", "start": 4340.855, "duration": 1.635}, {"text": "you're improving on the blue function.", "start": 4342.49, "duration": 1.38}, {"text": "So we want this lower bound to be tight.", "start": 4343.87, "duration": 2.025}, {"text": "Right, the, the two functions be equal, tangent to each other.", "start": 4345.895, "duration": 3.42}, {"text": "So in other words we want this inequality to hold with equality.", "start": 4349.315, "duration": 4.185}, {"text": "So we want, um, yeah,", "start": 4353.5, "duration": 2.625}, {"text": "so we want the left hand side and the right hand side to be equal", "start": 4356.125, "duration": 2.985}, {"text": "for the current value of Theta, right?", "start": 4359.11, "duration": 5.18}, {"text": "[NOISE]", "start": 4364.29, "duration": 19.66}, {"text": "So on a given iteration of EM where", "start": 4383.95, "duration": 8.82}, {"text": "the current parameters are equal to Theta, we want,", "start": 4392.77, "duration": 10.23}, {"text": "we want- I know this was a lot of math but, you know,", "start": 4431.43, "duration": 3.76}, {"text": "we want the left and right hand sides to be equal to each other.", "start": 4435.19, "duration": 3.795}, {"text": "Right. Because that's what it means for,", "start": 4438.985, "duration": 1.995}, {"text": "uh, for the lower bound to be tight,", "start": 4440.98, "duration": 5.805}, {"text": "for the green color to be exactly touching", "start": 4446.785, "duration": 1.815}, {"text": "the blue curve as we construct that lower bound.", "start": 4448.6, "duration": 3.585}, {"text": "And so for this to be true,", "start": 4452.185, "duration": 4.194}, {"text": "we need the random variable inside to be a constant.", "start": 4461.88, "duration": 3.91}, {"text": "So we need P of x_i, z_i,", "start": 4465.79, "duration": 3.15}, {"text": "divided by Qi of z_i to be equal to const- to, to a constant.", "start": 4468.94, "duration": 8.29}, {"text": "Meaning that no matter what value of z_i you plug in,", "start": 4477.54, "duration": 4.51}, {"text": "this should evaluate to the same value.", "start": 4482.05, "duration": 3.615}, {"text": "In other words, the ratio between the numerator and denominator must be the same.", "start": 4485.665, "duration": 4.83}, {"text": "Um, unfortunately so far,", "start": 4490.495, "duration": 2.49}, {"text": "we have not yet specified,", "start": 4492.985, "duration": 1.5}, {"text": "how we choose this distribution for z_i, right.", "start": 4494.485, "duration": 3.075}, {"text": "So, so far the only constraint we have is that", "start": 4497.56, "duration": 2.34}, {"text": "Qi has to be a probability density- has to be a probability distribution over z_i,", "start": 4499.9, "duration": 4.14}, {"text": "but you could choose one of the distributions you want for z_i.", "start": 4504.04, "duration": 2.52}, {"text": "And it turns out that, um, uh,", "start": 4506.56, "duration": 5.01}, {"text": "we can set Qi of z_i to be proportional to p of x_i,", "start": 4511.57, "duration": 9.31}, {"text": "z_i parameterized by Theta.", "start": 4522.21, "duration": 2.965}, {"text": "And this means that for any value of z,", "start": 4525.175, "duration": 2.46}, {"text": "you know, so z_indicates as it could from Gaussian one and Gaussian two.", "start": 4527.635, "duration": 4.02}, {"text": "Right. So this means that the chance of Gaussian one is", "start": 4531.655, "duration": 2.775}, {"text": "proportional to the chance of Gaussian one versus Gaussian two.", "start": 4534.43, "duration": 2.88}, {"text": "Whether z_i takes on one or two is proportional to this.", "start": 4537.31, "duration": 4.755}, {"text": "And I don't want to prove it but one way to ensure this,", "start": 4542.065, "duration": 4.215}, {"text": "and this is proven in the lecture notes.", "start": 4546.28, "duration": 2.055}, {"text": "But it turns out that one way to ensure.", "start": 4548.335, "duration": 2.355}, {"text": "Um, well so the Qis need to sum to 1.", "start": 4550.69, "duration": 3.075}, {"text": "So one way to ensure that this is proportional to", "start": 4553.765, "duration": 2.295}, {"text": "the right-hand side is to just take the right-hand side.", "start": 4556.06, "duration": 4.24}, {"text": "Sorry. Let me move here.", "start": 4561.87, "duration": 3.32}, {"text": "So one- so let's see.", "start": 4568.41, "duration": 3.32}, {"text": "Right. So the Qis have to sum to 1.", "start": 4575.82, "duration": 3.175}, {"text": "And so one way to ensure the proportionality is to just take the right-hand side,", "start": 4578.995, "duration": 5.305}, {"text": "and normalize it to sum to 1.", "start": 4586.38, "duration": 5.66}, {"text": "Um, and after, after a couple of", "start": 4597.12, "duration": 2.74}, {"text": "steps that are in the lecture notes but I don't want to do here,", "start": 4599.86, "duration": 3.4}, {"text": "you can show that this results in sending Qi of z_i to be equal to that,", "start": 4604.83, "duration": 5.005}, {"text": "that posterior probability, okay?", "start": 4609.835, "duration": 4.095}, {"text": "And so, um, sorry I skipped a couple steps here.", "start": 4613.93, "duration": 4.865}, {"text": "You can get from the lecture notes,", "start": 4618.795, "duration": 1.575}, {"text": "but it turns out that if you want this to be", "start": 4620.37, "duration": 2.58}, {"text": "a constant meaning whether you plugged in z_i equals 1 or z_i equals 2 or whatever,", "start": 4622.95, "duration": 3.93}, {"text": "these evaluate to the same constant.", "start": 4626.88, "duration": 1.89}, {"text": "The only way to do that is make sure", "start": 4628.77, "duration": 1.875}, {"text": "the numerator and denominator are proportional to each other.", "start": 4630.645, "duration": 3.475}, {"text": "And because Qi of z_i is a density that must sum to 1.", "start": 4634.12, "duration": 4.47}, {"text": "One way to make sure they're proportional is to just", "start": 4638.59, "duration": 2.94}, {"text": "set this to be with the right-hand side but normalize the sum to 1.", "start": 4641.53, "duration": 2.88}, {"text": "Okay. And we derived this a little bit more carefully in the lecture notes.", "start": 4644.41, "duration": 3.81}, {"text": "So just to summarize,", "start": 4648.22, "duration": 10.52}, {"text": "this gives us the EM algorithm.", "start": 4658.74, "duration": 3.82}, {"text": "Let's take all of this- everything we just did and wrap in the EM algorithm.", "start": 4662.56, "duration": 3.375}, {"text": "In the E-step, we're going to set Qi of", "start": 4665.935, "duration": 4.695}, {"text": "z_i equal to that.", "start": 4670.63, "duration": 11.94}, {"text": "And previously this was the w_i_js.", "start": 4682.57, "duration": 2.73}, {"text": "Right. So instead of- so previously,", "start": 4685.3, "duration": 1.83}, {"text": "we're restoring these probabilities in the variables you call w_i_js.", "start": 4687.13, "duration": 4.69}, {"text": "And then in the M-step,", "start": 4693.15, "duration": 3.77}, {"text": "we're going to take that lower bound that we constructed,", "start": 4704.01, "duration": 3.325}, {"text": "which is this function,", "start": 4707.335, "duration": 2.214}, {"text": "and maximize it with respect to Theta.", "start": 4718.2, "duration": 3.49}, {"text": "Okay. Um, and so remember in the M-step we", "start": 4721.69, "duration": 4.95}, {"text": "constructed this thing on the right-hand side as a lower bound for the log-likelihood.", "start": 4726.64, "duration": 4.905}, {"text": "And so for the fixed value of Q,", "start": 4731.545, "duration": 2.415}, {"text": "you can maximize this with respect to Theta and that updates the Theta,", "start": 4733.96, "duration": 4.11}, {"text": "you know, maximizing the green lower boundary,", "start": 4738.07, "duration": 1.89}, {"text": "that's what the M-step does.", "start": 4739.96, "duration": 2.13}, {"text": "And if you iterate these two steps,", "start": 4742.09, "duration": 1.86}, {"text": "then you find that this should converge to a local optima.", "start": 4743.95, "duration": 4.47}, {"text": "Okay. Oh and just maybe that's the obvious question.", "start": 4748.42, "duration": 3.855}, {"text": "Um, why don't we try to maximize right Theta,", "start": 4752.275, "duration": 4.365}, {"text": "uh, why are we trying to maximize the log-likelihood directly?", "start": 4756.64, "duration": 3.885}, {"text": "It turns out that if you take the mixture of Gaussians model,", "start": 4760.525, "duration": 3.36}, {"text": "try to take derivatives of this and set derivatives equal to 0,", "start": 4763.885, "duration": 3.135}, {"text": "there's no known way to solve for the value of Theta that maximizes the log-likelihood.", "start": 4767.02, "duration": 3.915}, {"text": "But you find that for the mixture of Gaussians model and for", "start": 4770.935, "duration": 2.745}, {"text": "many models including factor analysis that we talked about on Wednesday,", "start": 4773.68, "duration": 3.285}, {"text": "if you actually plug in the Gaussian density- uh,", "start": 4776.965, "duration": 2.88}, {"text": "if you actually plug in that mixture of Gaussians model for P,", "start": 4779.845, "duration": 3.54}, {"text": "um, and take, you know,", "start": 4783.385, "duration": 1.605}, {"text": "take, take derivatives, set derivatives equal to 0 and solve,", "start": 4784.99, "duration": 2.4}, {"text": "you will be able to find an analytic solution to maximize this M step,", "start": 4787.39, "duration": 3.765}, {"text": "and that'll be exactly what we had worked out in the early derivation of the EM algorithm.", "start": 4791.155, "duration": 4.47}, {"text": "Okay. But so this derivation shows that,", "start": 4795.625, "duration": 2.985}, {"text": "uh, the EM algorithm, you know,", "start": 4798.61, "duration": 3.015}, {"text": "is a maximum likelihood estimation algorithm with", "start": 4801.625, "duration": 3.255}, {"text": "optimization solved by constructing lower bounds and optimizing lower bounds, okay?", "start": 4804.88, "duration": 5.295}, {"text": "All right. Um, that's it for today,", "start": 4810.175, "duration": 3.284}, {"text": "and only it's stuff up to here,", "start": 4813.459, "duration": 2.206}, {"text": "right, and so this stuff will be up", "start": 4815.665, "duration": 2.625}, {"text": "to the midterm but we'll talk about factor analysis", "start": 4818.29, "duration": 2.85}, {"text": "a lot on Wednesday, but it will not be on the midterm.", "start": 4821.14, "duration": 2.145}, {"text": "Okay. So let's break for today, and I'll see you guys on Wednesday.", "start": 4823.285, "duration": 4.195}]