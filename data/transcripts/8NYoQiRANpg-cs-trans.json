[{"text": "All right. Good morning.", "start": 3.5, "duration": 2.155}, {"text": "Um, let's get started.", "start": 5.655, "duration": 2.1}, {"text": "So, ah, today you'll see the Support Vector Machine Algorithm.", "start": 7.755, "duration": 5.52}, {"text": "Um, and this is one of my favorite algorithms because it's very turnkey, right?", "start": 13.275, "duration": 4.275}, {"text": "If you have a classification problem, um,", "start": 17.55, "duration": 2.325}, {"text": "you just, kind of, run it and it more or less works.", "start": 19.875, "duration": 3.365}, {"text": "So in particular, I'll talk a bit more about", "start": 23.24, "duration": 3.48}, {"text": "the optimization problem that you have to solve for the support vector machine,", "start": 26.72, "duration": 4.86}, {"text": "then talk about something called the representer theorem,", "start": 31.58, "duration": 3.66}, {"text": "and this will be a key idea to how we'll work in potentially very high-dimensional,", "start": 35.24, "duration": 5.785}, {"text": "like 100,000 dimensional,", "start": 41.025, "duration": 1.365}, {"text": "or a million dimensional,", "start": 42.39, "duration": 1.2}, {"text": "or 100 billion dimensional,", "start": 43.59, "duration": 1.55}, {"text": "or even infinite-dimensional feature spaces.", "start": 45.14, "duration": 2.295}, {"text": "And just to teach you how to represent", "start": 47.435, "duration": 2.4}, {"text": "feature vectors and how to represent parameters that may be,", "start": 49.835, "duration": 3.46}, {"text": "you know, 100 billion dimensional,", "start": 53.295, "duration": 1.575}, {"text": "or 100 trillion dimensional, or infinite dimensional.", "start": 54.87, "duration": 2.94}, {"text": "Um, and based on this we derived kernels which is", "start": 57.81, "duration": 3.29}, {"text": "the mechanism for work on these incredibly high dimensional fea- feature spaces,", "start": 61.1, "duration": 4.265}, {"text": "and then hopefully, time permitting wrap up with", "start": 65.365, "duration": 3.505}, {"text": "a few examples of concrete implementations of these ideas.", "start": 68.87, "duration": 4.62}, {"text": "So to recap, on last Wednesday,", "start": 73.49, "duration": 3.48}, {"text": "we had started to talk about the optimal margin classifier, which said that,", "start": 76.97, "duration": 5.37}, {"text": "if you have a dataset that looks like this,", "start": 82.34, "duration": 2.67}, {"text": "then you want to find", "start": 85.01, "duration": 2.73}, {"text": "the decision boundary with the greatest possible geometric margin, right?", "start": 87.74, "duration": 4.795}, {"text": "So the geometric margin, um,", "start": 92.535, "duration": 2.1}, {"text": "can be calculated by this formula,", "start": 94.635, "duration": 1.95}, {"text": "and this is just the- the- the derivations in the lecture notes.", "start": 96.585, "duration": 2.745}, {"text": "It's just, you know, measuring the distance,", "start": 99.33, "duration": 1.86}, {"text": "uh, to the nearest point, right?", "start": 101.19, "duration": 3.24}, {"text": "Um, and for now let's assume the data can be separated by a straight line.", "start": 104.43, "duration": 4.445}, {"text": "Um, and so Gamma i is- this is sort of geometry,", "start": 108.875, "duration": 4.125}, {"text": "I guess, derivation in the lecture notes.", "start": 113.0, "duration": 2.26}, {"text": "This is the formula for co- computing the distance from the example x_i, y_i,", "start": 115.26, "duration": 5.04}, {"text": "to the decision boundary governed by the parameters w and b.", "start": 120.3, "duration": 3.38}, {"text": "Um, and Gamma is the worst case geometric margin, right?", "start": 123.68, "duration": 5.98}, {"text": "You will make- so- right.", "start": 129.66, "duration": 3.945}, {"text": "Of all of your M training examples,", "start": 133.605, "duration": 2.265}, {"text": "which one has the least or has the worst possible geometric margin?", "start": 135.87, "duration": 3.2}, {"text": "And, the support vector, the optimal margin classifier,", "start": 139.07, "duration": 3.164}, {"text": "we tried to make this as big as possible.", "start": 142.234, "duration": 2.866}, {"text": "And by the way, what we'll- what you see later on", "start": 145.1, "duration": 2.61}, {"text": "is that the optimal margin classifier is basically this algorithm.", "start": 147.71, "duration": 2.895}, {"text": "And optimal margin classifier plus kernels meaning basically take", "start": 150.605, "duration": 4.365}, {"text": "this idea of pi in a 100 billion dimensional feature space", "start": 154.97, "duration": 3.21}, {"text": "that's a support vector machine, okay?", "start": 158.18, "duration": 2.825}, {"text": "So I saw- one thing I didn't have time to talk about, uh,", "start": 161.005, "duration": 5.515}, {"text": "on Wednesday was the derivation of", "start": 166.52, "duration": 3.03}, {"text": "this classification problem, so  where does this optimization objective come from?", "start": 169.55, "duration": 4.48}, {"text": "So let me- let me just go over that very briefly.", "start": 174.03, "duration": 3.395}, {"text": "Um, so, the way I motivated these definitions we said that given a training set,", "start": 177.425, "duration": 5.085}, {"text": "you want to find the decision boundary parameterized by w and b,", "start": 182.51, "duration": 3.88}, {"text": "um, that maximizes the geometric margin, right?", "start": 186.39, "duration": 3.945}, {"text": "And so again, as recap, your classifier will", "start": 190.335, "duration": 2.625}, {"text": "output g equals w transpose x plus b.", "start": 192.96, "duration": 4.44}, {"text": "Um, and so you want to find premises w and b.", "start": 197.4, "duration": 3.985}, {"text": "They'll define the decision boundary where", "start": 201.385, "duration": 2.455}, {"text": "your classifications switch from positive to negative,", "start": 203.84, "duration": 2.46}, {"text": "that maximizes the geometric module.", "start": 206.3, "duration": 3.39}, {"text": "And so one way to pose this as an optimization problem is- um, let's see,", "start": 209.69, "duration": 6.225}, {"text": "is to try to find the biggest possible value of Gamma", "start": 215.915, "duration": 5.715}, {"text": "subject to that- subject to that the,", "start": 221.63, "duration": 12.7}, {"text": "um, geometric margin must be greater than or equal to Gamma, right?", "start": 234.33, "duration": 3.45}, {"text": "So, um, so, in this optimization problem,", "start": 237.78, "duration": 3.34}, {"text": "the parameters you get to fiddle with are,", "start": 241.12, "duration": 2.665}, {"text": "Gamma, w and b.", "start": 243.785, "duration": 1.92}, {"text": "And if you solve this optimization problem,", "start": 245.705, "duration": 2.535}, {"text": "then you are finding the values of w and b that defines a straight line,", "start": 248.24, "duration": 4.6}, {"text": "that defines a decision boundary, um,", "start": 252.84, "duration": 2.58}, {"text": "so that- so, so this constraint says that every example, right?", "start": 255.42, "duration": 5.835}, {"text": "So this constraint says every example has geometric margin greater than or equal to Gamma.", "start": 261.255, "duration": 8.18}, {"text": "This is- this is what they are saying.", "start": 269.435, "duration": 2.28}, {"text": "And you wanna set Gamma as big as possible,", "start": 271.715, "duration": 2.94}, {"text": "which means that you're maximizing the worst-case geometric margin.", "start": 274.655, "duration": 4.205}, {"text": "This makes sense, right?", "start": 278.86, "duration": 1.66}, {"text": "So- so if- if I- so the only way to make Gamma say 17,", "start": 280.52, "duration": 5.7}, {"text": "or 20, or whatever,", "start": 286.22, "duration": 2.16}, {"text": "is if every training example has geometric margin bigger than 17, right?", "start": 288.38, "duration": 4.95}, {"text": "And so this optimization problem was trying to find w and b to drive up", "start": 293.33, "duration": 4.86}, {"text": "Gamma as big as possible and have every example", "start": 298.19, "duration": 2.52}, {"text": "have geometric margin even bigger than Gamma.", "start": 300.71, "duration": 2.76}, {"text": "So this optimization problem maximizes the Geom- causes, um,", "start": 303.47, "duration": 5.75}, {"text": "causes you to find w and b with as big a geometric margin as", "start": 309.22, "duration": 4.88}, {"text": "poss- so as big as the worst-case geometric margin as possible, okay?", "start": 314.1, "duration": 4.725}, {"text": "Um, and so, does this make sense actually, right?", "start": 318.825, "duration": 5.07}, {"text": "Okay. Actually rai- raise your hand if this makes sense.", "start": 323.895, "duration": 1.995}, {"text": "Uh, oh, good.", "start": 325.89, "duration": 1.86}, {"text": "Okay. Well, many of you. All right.", "start": 327.75, "duration": 1.575}, {"text": "Let me see if I can explain this in a slightly different way.", "start": 329.325, "duration": 2.505}, {"text": "So let's say you have a few training examples, you know,", "start": 331.83, "duration": 2.73}, {"text": "the training examples geometric margins are,", "start": 334.56, "duration": 2.61}, {"text": "17, 2, and 5, right?", "start": 337.17, "duration": 5.04}, {"text": "Then the geometric margin in this case is a worst-case value 2, right?", "start": 342.21, "duration": 4.45}, {"text": "And so if you are solving an optimization problem", "start": 346.66, "duration": 3.04}, {"text": "where I want every example- where I want the- the- the, uh, uh,", "start": 349.7, "duration": 4.605}, {"text": "where I want the min of i- of Gamma i to be as big as possible,", "start": 354.305, "duration": 8.004}, {"text": "one way to enforce this is to say that Gamma i must be bigger than or equal to Gamma,", "start": 362.309, "duration": 4.851}, {"text": "for every possible value of i.", "start": 367.16, "duration": 2.19}, {"text": "And then I'm going to lift Gamma up as much as possible, right?", "start": 369.35, "duration": 2.88}, {"text": "Because the only way to lift Gamma up", "start": 372.23, "duration": 3.045}, {"text": "subject to this is if every va- value of Gamma i is bigger than that.", "start": 375.275, "duration": 3.96}, {"text": "And so, lifting Gamma up,", "start": 379.235, "duration": 2.37}, {"text": "maximizing Gamma has effective maximizing the worst-case examples geometric margin,", "start": 381.605, "duration": 5.775}, {"text": "which is, which is, which is how we define this optimization problem, okay?", "start": 387.38, "duration": 5.745}, {"text": "Um, and then the last one step to turn this problem into this one on the left,", "start": 393.125, "duration": 7.355}, {"text": "is this interesting observation that, um,", "start": 400.48, "duration": 3.945}, {"text": "you might remember when we talked about the functional margin,", "start": 404.425, "duration": 3.9}, {"text": "which is the numerator here, that,", "start": 408.325, "duration": 2.9}, {"text": "you know, the functional margin you can scale w and b", "start": 411.225, "duration": 3.535}, {"text": "by any number and the decision boundary stays the same, right?", "start": 414.76, "duration": 3.885}, {"text": "And so, you know,", "start": 418.645, "duration": 1.2}, {"text": "if- if your classifier is y,", "start": 419.845, "duration": 2.355}, {"text": "so this is g of w transpose x plus b, right?", "start": 422.2, "duration": 3.545}, {"text": "So if- let's see the example I want to use, uh, 2, 1.", "start": 425.745, "duration": 5.76}, {"text": "If w was the vector 2, 1- [NOISE]", "start": 431.505, "duration": 9.11}, {"text": "Let's say that's the classifier, right?", "start": 440.615, "duration": 3.29}, {"text": "Then you can take W and B,", "start": 443.905, "duration": 2.04}, {"text": "and multiply it by any number you want.", "start": 445.945, "duration": 2.985}, {"text": "I can multiply this by 10,", "start": 448.93, "duration": 1.83}, {"text": "[NOISE] and this defines the same straight line, right?", "start": 450.76, "duration": 7.83}, {"text": "Um, so in particular, I think,", "start": 458.59, "duration": 3.48}, {"text": "uh, let's see with this 2 1x.", "start": 462.07, "duration": 3.63}, {"text": "[NOISE] This actually defines the decision boundary that looks like that.", "start": 465.7, "duration": 5.4}, {"text": "Uh, if this is X1 and this is X2,", "start": 471.1, "duration": 3.075}, {"text": "then this is the equation of the straight line where W transpose X plus", "start": 474.175, "duration": 4.545}, {"text": "B equals 0, right?", "start": 478.72, "duration": 5.82}, {"text": "Uh, that's uh, one, and two.", "start": 484.54, "duration": 2.19}, {"text": "Uh, you can- you can verify it for yourself.", "start": 486.73, "duration": 2.25}, {"text": "You plug in this point,", "start": 488.98, "duration": 1.86}, {"text": "then W transpose X plus B equals 0.", "start": 490.84, "duration": 1.8}, {"text": "We plug in this point, W transpose X equals 0,", "start": 492.64, "duration": 2.82}, {"text": "um and so that's the decision boundary where the,", "start": 495.46, "duration": 2.76}, {"text": "uh- as yet we'll predict positive [NOISE]", "start": 498.22, "duration": 3.12}, {"text": "everywhere here and we'll predict [NOISE] negative everywhere to the lower left,", "start": 501.34, "duration": 3.78}, {"text": "and this straight line, you know,", "start": 505.12, "duration": 2.79}, {"text": "stays the same even when you multiply these parameters by any constant, okay?", "start": 507.91, "duration": 4.47}, {"text": "Um, and so, um,", "start": 512.38, "duration": 4.54}, {"text": "to simplify this, uh,", "start": 516.92, "duration": 3.275}, {"text": "notice that you could choose anything you want for the normal W, right?", "start": 520.195, "duration": 4.125}, {"text": "Just by scaling this by a factor of 10,", "start": 524.32, "duration": 1.935}, {"text": "you can increase it, or scaling it by a factor of 1 over 10, you can decrease it.", "start": 526.255, "duration": 3.735}, {"text": "But you have the flexibility to scale the parameters W and B, you know,", "start": 529.99, "duration": 4.755}, {"text": "up or down by any fixed constant without changing the decision boundary,", "start": 534.745, "duration": 5.7}, {"text": "and so the trick to simplify this equation into that one is if you choose", "start": 540.445, "duration": 5.475}, {"text": "[NOISE] to scale the normal W to be equal to 1 over gamma.", "start": 545.92, "duration": 6.75}, {"text": "Um, uh because if you do that,", "start": 552.67, "duration": 4.02}, {"text": "then this optimization objective", "start": 556.69, "duration": 3.3}, {"text": "[NOISE] becomes- [NOISE] Um,", "start": 559.99, "duration": 9.46}, {"text": "maximize 1 over norm of W subject to-", "start": 569.78, "duration": 5.16}, {"text": "[NOISE]", "start": 574.94, "duration": 10.62}, {"text": "right?", "start": 585.56, "duration": 0.435}, {"text": "Uh, so it substitutes norm of W equals 1 of gamma,", "start": 585.995, "duration": 3.875}, {"text": "and so that cancels out,", "start": 589.87, "duration": 3.185}, {"text": "and so you end up with this optimization problem instead of maximizing 1 over norm W,", "start": 593.055, "duration": 5.395}, {"text": "you can minimize one half the norm of W squared subject to this.", "start": 598.45, "duration": 4.23}, {"text": "[NOISE] Right?", "start": 602.68, "duration": 6.25}, {"text": "Okay, and so that's a rough- I know I did this relatively quickly.", "start": 611.43, "duration": 5.38}, {"text": "Again- as usual the full derivation is written on", "start": 616.81, "duration": 2.4}, {"text": "your lecture notes but hopefully this gives you a flavor for why.", "start": 619.21, "duration": 3.3}, {"text": "If you solve this optimization problem and you're minimizing over W and B", "start": 622.51, "duration": 5.235}, {"text": "that you are solving for the parameters W and B that", "start": 627.745, "duration": 2.835}, {"text": "give you the optimal margin classifier. Okay.", "start": 630.58, "duration": 3.82}, {"text": "Now, delta margin classifier,", "start": 634.59, "duration": 5.83}, {"text": "we've been deriving this algorithm as if you know the features X I um,", "start": 640.42, "duration": 6.27}, {"text": "let's see, we've been deriving this algorithm as if the features X I are", "start": 646.69, "duration": 4.35}, {"text": "some reasonable dimensional feature X equals R2,", "start": 651.04, "duration": 3.96}, {"text": "X equals 100 or something.", "start": 655.0, "duration": 1.665}, {"text": "Um, what we will talk about", "start": 656.665, "duration": 3.615}, {"text": "later is a case where the features X I become you know, 100 trillion dimensional right?", "start": 660.28, "duration": 5.58}, {"text": "Or infinite dimensional.", "start": 665.86, "duration": 1.53}, {"text": "And um, what's- uh,", "start": 667.39, "duration": 4.57}, {"text": "what we will assume is that W,", "start": 672.78, "duration": 6.099}, {"text": "can be represented [NOISE] as a sum- as a linear combination of the training examples.", "start": 678.879, "duration": 8.926}, {"text": "Okay? So um, in order to derive the support vector machine,", "start": 687.805, "duration": 3.915}, {"text": "we're gonna make an additional restriction that the parameters W", "start": 691.72, "duration": 4.275}, {"text": "can be expressed as a linear combination of the training examples.", "start": 695.995, "duration": 4.95}, {"text": "Right? So um, and it turns out that when X I is you know, 100 trillion dimensional,", "start": 700.945, "duration": 6.165}, {"text": "doing this will let us derive algorithms that work", "start": 707.11, "duration": 2.865}, {"text": "even in these  100 trillion or these infinite-dimensional feature spaces.", "start": 709.975, "duration": 3.78}, {"text": "Now, I'm just deriving this uh,", "start": 713.755, "duration": 2.52}, {"text": "just as an assumption.", "start": 716.275, "duration": 1.35}, {"text": "It turns out that there's a theorem called the representer theorem that", "start": 717.625, "duration": 3.525}, {"text": "shows that you can make this assumption without losing any performance.", "start": 721.15, "duration": 3.825}, {"text": "Uh, the proof that represents the theorem is quite complicated.", "start": 724.975, "duration": 2.58}, {"text": "I don't wanna do this in this class, uh,", "start": 727.555, "duration": 1.845}, {"text": "it is actually written out, the proof for why you can", "start": 729.4, "duration": 1.89}, {"text": "make this assumption is also written in the lecture notes,", "start": 731.29, "duration": 2.04}, {"text": "it's a pretty long and involved proof involving primal dual optimization.", "start": 733.33, "duration": 3.54}, {"text": "Um, I don't wanna present the whole proof here but let me give you", "start": 736.87, "duration": 2.64}, {"text": "a flavor for why this is a reasonable assumption to make.", "start": 739.51, "duration": 3.485}, {"text": "Okay? And when- just to- just to make things complicated later on uh,", "start": 742.995, "duration": 4.575}, {"text": "we actually do this.", "start": 747.57, "duration": 1.32}, {"text": "Right? So Y I is always plus minus 1.", "start": 748.89, "duration": 2.235}, {"text": "So- so we're actually by- by convention,", "start": 751.125, "duration": 2.295}, {"text": "we're actually going to assume that W I can be written right?", "start": 753.42, "duration": 3.58}, {"text": "So in- in this example this is plus minus 1 right?", "start": 757.0, "duration": 3.075}, {"text": "So um, this makes some of the math", "start": 760.075, "duration": 2.685}, {"text": "a little bit downstream, come out easier but it is- but it's still", "start": 762.76, "duration": 2.73}, {"text": "saying that W is- can be represented as a linear combination of the training examples.", "start": 765.49, "duration": 6.495}, {"text": "Okay? So um [NOISE] let me just", "start": 771.985, "duration": 3.315}, {"text": "describe less formally why this is a reasonable assumption,", "start": 775.3, "duration": 3.165}, {"text": "but it's actually not an assumption.", "start": 778.465, "duration": 1.335}, {"text": "The representer theorem proves that you know,", "start": 779.8, "duration": 1.71}, {"text": "this is just true at the optimal value of W. But let me convey a couple ways why um,", "start": 781.51, "duration": 6.51}, {"text": "this is a reasonable thing to do, or assume I guess.", "start": 788.02, "duration": 3.795}, {"text": "So um, maybe here's intuition number one.", "start": 791.815, "duration": 7.005}, {"text": "And I'm going to refer to logistic regression.", "start": 798.82, "duration": 3.33}, {"text": "[NOISE] Right? Where uh,", "start": 802.15, "duration": 6.915}, {"text": "suppose that you run logistic regression with uh, gradient descent,", "start": 809.065, "duration": 4.575}, {"text": "say stochastic gradient descent, then you initialize the parameters to be equal to 0 at first.", "start": 813.64, "duration": 5.37}, {"text": "And then for each iteration of stochastic gradient descent,", "start": 819.01, "duration": 4.96}, {"text": "right [NOISE] you update theta it gets updated as theta minus", "start": 825.39, "duration": 4.6}, {"text": "the learning rate times [NOISE] you know,", "start": 829.99, "duration": 4.35}, {"text": "[NOISE] times X and Y okay?", "start": 834.34, "duration": 5.76}, {"text": "And so- sorry here alpha is the learning rate, uh,", "start": 840.1, "duration": 3.765}, {"text": "nothing, this is overloaded notation,", "start": 843.865, "duration": 1.635}, {"text": "this alpha has nothing to do with that alpha.", "start": 845.5, "duration": 1.92}, {"text": "But so this is saying that on every iteration,", "start": 847.42, "duration": 2.52}, {"text": "you're updating the parameters theta as- uh,", "start": 849.94, "duration": 2.985}, {"text": "by- by adding or subtracting some constant times some training example.", "start": 852.925, "duration": 5.085}, {"text": "And so kind of proof by induction,", "start": 858.01, "duration": 2.295}, {"text": "right if theta starts out at 0,", "start": 860.305, "duration": 2.025}, {"text": "and if- if on every iteration of", "start": 862.33, "duration": 2.49}, {"text": "gradient descent you're adding a multiple of some training example,", "start": 864.82, "duration": 3.54}, {"text": "then no matter how many iterations you run gradient descent,", "start": 868.36, "duration": 3.975}, {"text": "theta is still a linear combination of your training examples.", "start": 872.335, "duration": 4.11}, {"text": "Okay. And- and again I did this with theta- the- the- it was really", "start": 876.445, "duration": 3.705}, {"text": "theta 0 theta 1 up to theta n. Right?", "start": 880.15, "duration": 4.155}, {"text": "Whereas here we have uh,", "start": 884.305, "duration": 1.56}, {"text": "B and then W1 down to WN.", "start": 885.865, "duration": 3.675}, {"text": "Wow, this pen is really bad.", "start": 889.54, "duration": 1.35}, {"text": "[NOISE] I feel like- alright um,", "start": 890.89, "duration": 3.45}, {"text": "I feel like we should throw these away so they don't keep haunting us in the future.", "start": 894.34, "duration": 4.695}, {"text": "Okay. Right, so but- but um,", "start": 899.035, "duration": 2.895}, {"text": "if you- but uh, uh,", "start": 901.93, "duration": 2.64}, {"text": "so I did this a theta rather than W, but it turns out if you work through", "start": 904.57, "duration": 3.67}, {"text": "the algebra this is the proof by induction that, you know,", "start": 908.24, "duration": 3.7}, {"text": "as you run a logistic regression after every iteration the parameters theta or", "start": 911.94, "duration": 4.79}, {"text": "the parameters W are always a linear combination of the training examples.", "start": 916.73, "duration": 5.31}, {"text": "Um, and this is also true if you use batch gradient descent.", "start": 922.04, "duration": 3.15}, {"text": "[NOISE] If you use batch gradient descent [NOISE] then the update rule is this.", "start": 925.19, "duration": 4.26}, {"text": "Um, Yeah, right, [NOISE] okay, alright.", "start": 929.45, "duration": 9.88}, {"text": "And so it turns out you can derive", "start": 939.33, "duration": 1.61}, {"text": "gradient descent for the support vector machine learning algorithm as well.", "start": 940.94, "duration": 3.675}, {"text": "You can derive gradient descent optimized W subject to", "start": 944.615, "duration": 2.325}, {"text": "this and you can have a proof by induction.", "start": 946.94, "duration": 2.58}, {"text": "You know that no matter how many iterations you run during descent,", "start": 949.52, "duration": 2.64}, {"text": "it will always be a linear combination of the training examples.", "start": 952.16, "duration": 4.15}, {"text": "So that's one intuition for how", "start": 956.31, "duration": 4.57}, {"text": "[NOISE] you might see that assuming W is a linear combination of the training examples,", "start": 960.88, "duration": 6.675}, {"text": "you know is a- is a reasonable assumption.", "start": 967.555, "duration": 1.945}, {"text": "[NOISE] I wanna present a second set of", "start": 969.5, "duration": 5.85}, {"text": "intuitions and this one will be easier if you're", "start": 975.35, "duration": 2.865}, {"text": "good at visualizing high dimensional spaces I guess.", "start": 978.215, "duration": 3.195}, {"text": "But uh, let me just give intuition number two which is um let's see.", "start": 981.41, "duration": 7.215}, {"text": "So um, so first of all let's take our example just now right?", "start": 988.625, "duration": 8.635}, {"text": "Let's say that the classifier uses this,", "start": 997.26, "duration": 4.395}, {"text": "2, 1 [NOISE] X minus 2, right?", "start": 1001.655, "duration": 3.78}, {"text": "So this is W and this is B.", "start": 1005.435, "duration": 4.69}, {"text": "Then it turns out that the decision boundary is this where this is 1 and this is uh,", "start": 1010.125, "duration": 7.825}, {"text": "2 and it turns out that", "start": 1017.95, "duration": 2.49}, {"text": "the vector W is always at 90 degrees to the decision boundary right?", "start": 1020.44, "duration": 5.78}, {"text": "This is a factor of I guess geometry or something or linear algebra, right?", "start": 1026.22, "duration": 5.38}, {"text": "Where as the vector W 2, 1.", "start": 1031.6, "duration": 3.18}, {"text": "So the vector W, you know,", "start": 1034.78, "duration": 1.62}, {"text": "is sort of 2 to the right side and 1 up is always at- well, alright.", "start": 1036.4, "duration": 5.065}, {"text": "The vector w is always at 90 degrees um,", "start": 1041.465, "duration": 3.56}, {"text": "to the decision boundary and the decision boundary separates where", "start": 1045.025, "duration": 3.255}, {"text": "you predict positive from where you predict negative.", "start": 1048.28, "duration": 3.45}, {"text": "Okay? And so it it turns out that uh,", "start": 1051.73, "duration": 7.01}, {"text": "if you have uh,", "start": 1058.74, "duration": 2.07}, {"text": "to take a simple example,", "start": 1060.81, "duration": 1.32}, {"text": "let's say you have um,", "start": 1062.13, "duration": 2.055}, {"text": "two training examples, a positive example and a negative example.", "start": 1064.185, "duration": 6.33}, {"text": "Right? Then by illus- X2 right?", "start": 1070.515, "duration": 4.31}, {"text": "The linear algebra way of saying this is that", "start": 1074.825, "duration": 2.615}, {"text": "the vector W lies in the span of the training examples.", "start": 1077.44, "duration": 3.345}, {"text": "Okay? Oh and- and- and um,", "start": 1080.785, "duration": 2.525}, {"text": "the way to picture this is that W sets the direction of", "start": 1083.31, "duration": 3.61}, {"text": "the decision boundary and as you vary B then the position so you- the relative position,", "start": 1086.92, "duration": 5.28}, {"text": "you know setting different values of B", "start": 1092.2, "duration": 2.125}, {"text": "will move that decision boundary back and forth like this.", "start": 1094.325, "duration": 2.405}, {"text": "And W uh, pins the direction of the decision boundary.", "start": 1096.73, "duration": 4.815}, {"text": "Okay? Um, and just one last example for- for why this might be true um,", "start": 1101.545, "duration": 7.725}, {"text": "is uh- so we're going to be working in very very high dimensional feature spaces.", "start": 1112.76, "duration": 7.01}, {"text": "For this example, let's say you have uh,", "start": 1119.77, "duration": 2.46}, {"text": "[NOISE] three features X1, X2, X3 right?", "start": 1122.23, "duration": 2.34}, {"text": "And_ and later we'll get to where this is like 100 trillion right?", "start": 1124.57, "duration": 2.97}, {"text": "Um, and let's say for the sake of illustration", "start": 1127.54, "duration": 2.909}, {"text": "that all of your examples lie in the plane of X1 and X2.", "start": 1130.449, "duration": 3.541}, {"text": "So let's say X3 is equal to 0.", "start": 1133.99, "duration": 2.77}, {"text": "Okay, so let's say if all your training examples x equals 0, um,", "start": 1140.14, "duration": 5.56}, {"text": "then the decision boundary,", "start": 1145.7, "duration": 3.24}, {"text": "you know, will be- will be some sort of vertical plane that looks like this, right?", "start": 1148.94, "duration": 4.74}, {"text": "So this is going to be the plane specifying,", "start": 1153.68, "duration": 4.62}, {"text": "um, w transpose x plus b equals 0 when now w and x are three-dimensional.", "start": 1158.3, "duration": 5.505}, {"text": "Um, and so the vector w, uh,", "start": 1163.805, "duration": 4.68}, {"text": "will have a- should have W_3 equals 0 right.", "start": 1168.485, "duration": 6.99}, {"text": "If- if one of the features is always 0,", "start": 1175.475, "duration": 2.19}, {"text": "is always fixed then you know,", "start": 1177.665, "duration": 2.28}, {"text": "W_3 should be equal to 0 and that's another way of saying that the vector w,", "start": 1179.945, "duration": 4.83}, {"text": "you know, should be, um,", "start": 1184.775, "duration": 1.905}, {"text": "represented as a- as a- in- in the span of just the features x1,", "start": 1186.68, "duration": 3.375}, {"text": "x2, as a span of the training examples [NOISE] okay.", "start": 1190.055, "duration": 4.735}, {"text": "All right, I'm not sure if- if either intuition 1 or intuition 2 convinces you,", "start": 1196.15, "duration": 5.26}, {"text": "I think hopefully that's good enough.", "start": 1201.41, "duration": 1.38}, {"text": "But this- the second intuition would be easier if you're used to", "start": 1202.79, "duration": 3.09}, {"text": "thinking about vectors in high-dimensional feature spaces.", "start": 1205.88, "duration": 5.13}, {"text": "Um, and again the formal proof of this result which is called", "start": 1211.01, "duration": 4.65}, {"text": "the representation theorem is given in the lecture notes, but it's a very bizarre", "start": 1215.66, "duration": 5.595}, {"text": "I don't know, it's actually- it's actually one of the most complicated- it's one- it's", "start": 1221.255, "duration": 3.645}, {"text": "definitely the high end in terms of complexity of the- of the full derivation,", "start": 1224.9, "duration": 3.795}, {"text": "of the formal derivation of this result.", "start": 1228.695, "duration": 2.575}, {"text": "Um, so.", "start": 1231.55, "duration": 2.74}, {"text": "[NOISE] All right,", "start": 1234.29, "duration": 7.665}, {"text": "so let's assume that W can be written as follows.", "start": 1241.955, "duration": 4.56}, {"text": "Um, so optimization problem was this,", "start": 1246.515, "duration": 5.055}, {"text": "you wanna solve for w and b so that the norm of w squared is as small as", "start": 1251.57, "duration": 6.12}, {"text": "possible and so that the a-this is bigger than the other one, right?", "start": 1257.69, "duration": 8.31}, {"text": "Um, for every value of i.", "start": 1266.0, "duration": 2.58}, {"text": "[NOISE] So let's see,", "start": 1268.58, "duration": 5.475}, {"text": "norm of w squared.", "start": 1274.055, "duration": 1.245}, {"text": "This is just equal to w transpose w,", "start": 1275.3, "duration": 4.74}, {"text": "um, and so if you plug in this definition of W,", "start": 1280.04, "duration": 6.015}, {"text": "you know, into these equations you have as", "start": 1286.055, "duration": 3.405}, {"text": "the optimization objective min of one half, um,", "start": 1289.46, "duration": 5.01}, {"text": "sum from i equals 1 through m. [NOISE]", "start": 1294.47, "duration": 5.42}, {"text": "So this is w transpose W,", "start": 1299.89, "duration": 11.4}, {"text": "um, which is equal to I guess sum of i's sum over j,", "start": 1311.29, "duration": 9.94}, {"text": "alpha i, alpha j, y_i y_j.", "start": 1321.23, "duration": 4.305}, {"text": "And then, um, X_i transpose X_j right?", "start": 1325.535, "duration": 7.98}, {"text": "And, um, I'm going to take this.", "start": 1333.515, "duration": 3.135}, {"text": "So this is an inner product between X_I and X_J.", "start": 1336.65, "duration": 3.045}, {"text": "And I'm gonna use- I'm just gonna write it as this.", "start": 1339.695, "duration": 3.565}, {"text": "Right, x_i this notation so x comma z, uh,", "start": 1344.14, "duration": 6.37}, {"text": "equals x transpose z, uh,", "start": 1350.51, "duration": 2.235}, {"text": "is the inner product between two vectors.", "start": 1352.745, "duration": 5.445}, {"text": "This is maybe another alternative notation for writing", "start": 1358.19, "duration": 2.85}, {"text": "inner products and when we derive kernels you see that, uh,", "start": 1361.04, "duration": 3.795}, {"text": "expressing your algorithm in terms of inner products between features", "start": 1364.835, "duration": 2.835}, {"text": "X is-is the key mathematical step needed to derive kernels and we'll", "start": 1367.67, "duration": 3.78}, {"text": "use this slightly different sort of open-angle brackets", "start": 1371.45, "duration": 3.42}, {"text": "close-angle brackets notation to denote", "start": 1374.87, "duration": 2.415}, {"text": "the-the inner product between two different feature vectors.", "start": 1377.285, "duration": 4.14}, {"text": "So that is the optimization objective,", "start": 1381.425, "duration": 4.275}, {"text": "um, oh, and then this constraint it becomes something else i guess, this becomes, uh, uh,", "start": 1385.7, "duration": 5.685}, {"text": "what is it, um, y_i times W which is,", "start": 1391.385, "duration": 9.075}, {"text": "um, transpose x plus b is greater than 1.", "start": 1400.46, "duration": 10.605}, {"text": "And again this simplifies or if you just multiply this out.", "start": 1411.065, "duration": 3.645}, {"text": "[NOISE].", "start": 1414.71, "duration": 14.88}, {"text": "So just to make sure that mapping is clear,", "start": 1429.59, "duration": 2.19}, {"text": "um- uh, all these pens are dying.", "start": 1431.78, "duration": 4.11}, {"text": "All right I'll not [NOISE]. All right.", "start": 1435.89, "duration": 8.73}, {"text": "So that becomes this and this becomes that, okay.", "start": 1444.62, "duration": 9.375}, {"text": "Um, and the key property we're going to use is that,", "start": 1453.995, "duration": 5.145}, {"text": "if you look at these two equations in terms of how we pose the optimization problem,", "start": 1459.14, "duration": 3.72}, {"text": "the only place that the feature vectors appears is in this inner product.", "start": 1462.86, "duration": 6.55}, {"text": "Right, um, and it turns", "start": 1470.2, "duration": 3.91}, {"text": "out when we talked about the Kernel Trick and we talked with the application of kernels,", "start": 1474.11, "duration": 3.87}, {"text": "it turns out that, um,", "start": 1477.98, "duration": 1.77}, {"text": "if you can compute this very efficiently,", "start": 1479.75, "duration": 2.58}, {"text": "that's when you can get away with manipulating even infinite dimensional feature vectors.", "start": 1482.33, "duration": 4.875}, {"text": "We- we'll get to this in a second.", "start": 1487.205, "duration": 1.695}, {"text": "But the reason we want to write the whole algorithm in terms of inner products is, uh,", "start": 1488.9, "duration": 3.945}, {"text": "there'll be important cases where the feature vectors", "start": 1492.845, "duration": 2.895}, {"text": "are 100 trillion dimensional but you", "start": 1495.74, "duration": 3.24}, {"text": "can compute the- or even infinite dimensional but you can", "start": 1498.98, "duration": 2.91}, {"text": "compute the inner product very efficiently without needing to loop over,", "start": 1501.89, "duration": 3.345}, {"text": "you know, the other 100 trillion elements in an array, right?", "start": 1505.235, "duration": 2.775}, {"text": "And- and we'll see exactly how to do that,", "start": 1508.01, "duration": 1.95}, {"text": "um, later in- in- very shortly.", "start": 1509.96, "duration": 3.675}, {"text": "Okay?", "start": 1513.635, "duration": 0.795}, {"text": "[NOISE]", "start": 1514.43, "duration": 10.68}, {"text": "So. All right,", "start": 1525.11, "duration": 4.785}, {"text": "um, now it turns out that,", "start": 1529.895, "duration": 2.49}, {"text": "uh, we've now expressed the whole,", "start": 1532.385, "duration": 2.31}, {"text": "um, optimization algorithm in terms of these parameters Alpha, right?", "start": 1534.695, "duration": 5.565}, {"text": "Defined here, uh, and b.", "start": 1540.26, "duration": 1.905}, {"text": "So now the parameters Theta,", "start": 1542.165, "duration": 1.59}, {"text": "now- now the parameter z is optimized for our Alpha, um,", "start": 1543.755, "duration": 3.84}, {"text": "it turns out that by convention", "start": 1547.595, "duration": 2.625}, {"text": "in the way that you see support vector machines referred to,", "start": 1550.22, "duration": 2.85}, {"text": "you know, in research papers or in textbooks.", "start": 1553.07, "duration": 2.58}, {"text": "It turns out there's a further simplification of", "start": 1555.65, "duration": 2.28}, {"text": "that optimization problem which is that you can simplify to this,", "start": 1557.93, "duration": 3.96}, {"text": "[NOISE] um, and the derivation to go from that to this is again relatively complicated.", "start": 1561.89, "duration": 8.58}, {"text": "[NOISE] But it turns out you can", "start": 1570.47, "duration": 4.515}, {"text": "further simplify the optimization problem I wrote there to this.", "start": 1574.985, "duration": 5.865}, {"text": "Okay? And again, uh,", "start": 1580.85, "duration": 2.745}, {"text": "you- you can copy this down if you want but this is also written in the lecture notes.", "start": 1583.595, "duration": 4.17}, {"text": "And by convention this slightly simplified version optimization problem", "start": 1587.765, "duration": 4.155}, {"text": "is called the dual optimization problem.", "start": 1591.92, "duration": 3.13}, {"text": "Um, the way to simplify that optimization problem to this one that's actually done by,", "start": 1596.98, "duration": 7.81}, {"text": "um, using convex optimization theory, uh,", "start": 1604.79, "duration": 2.91}, {"text": "and- and- and again the derivation", "start": 1607.7, "duration": 2.325}, {"text": "is written in the lecture notes but I don't want to do that here.", "start": 1610.025, "duration": 2.61}, {"text": "If- if you want think of it as doing", "start": 1612.635, "duration": 1.425}, {"text": "a bunch more algebra to simplify that problem to this one", "start": 1614.06, "duration": 3.09}, {"text": "and consequently, you cancel out B along", "start": 1617.15, "duration": 1.68}, {"text": "the way, it's a little more complicated than that but-but right,", "start": 1618.83, "duration": 3.03}, {"text": "the full derivation is given in the lecture notes.", "start": 1621.86, "duration": 3.96}, {"text": "Um, and so, um,", "start": 1625.82, "duration": 3.78}, {"text": "finally, you know, the way you train for-the way you make a prediction, right,", "start": 1629.6, "duration": 4.98}, {"text": "as you saw for the alpha i's and maybe for b, right, since you solve", "start": 1634.58, "duration": 6.06}, {"text": "this optimization problem or that optimization problem for", "start": 1640.64, "duration": 3.27}, {"text": "the Alpha i's and then to make a prediction,", "start": 1643.91, "duration": 4.51}, {"text": "um, you need to compute", "start": 1651.13, "duration": 7.66}, {"text": "h of W b of x for a new test example which is g of w transpose x plus b.", "start": 1658.79, "duration": 7.695}, {"text": "Right. But because of the definition of w- w this is equal to g of,", "start": 1666.485, "duration": 8.995}, {"text": "um, that's W transpose X plus b because this is", "start": 1679.9, "duration": 7.63}, {"text": "w and so that's equal to g of sum over", "start": 1687.53, "duration": 6.12}, {"text": "i Alpha_i y_i inner product between X_i and X plus b.", "start": 1693.65, "duration": 6.42}, {"text": "And so once again, you know,", "start": 1700.07, "duration": 1.98}, {"text": "once you have stored the Alphas in your computer memory,", "start": 1702.05, "duration": 3.27}, {"text": "um, you can make predictions using just inner products again, right?", "start": 1705.32, "duration": 3.54}, {"text": "And so the entire algorithm both", "start": 1708.86, "duration": 1.68}, {"text": "the optimization objective you need to deal with during training.", "start": 1710.54, "duration": 2.58}, {"text": "As well as how you make predictions is, um-uh,", "start": 1713.12, "duration": 3.21}, {"text": "is expressed only in terms of inner products, okay?", "start": 1716.33, "duration": 3.85}, {"text": "So we're now ready to", "start": 1722.38, "duration": 6.91}, {"text": "apply kernels and sometimes in", "start": 1729.29, "duration": 6.45}, {"text": "machine learning people sometimes we call this a kernel trick and let me", "start": 1735.74, "duration": 2.91}, {"text": "just the other recipe for what this means,", "start": 1738.65, "duration": 3.435}, {"text": "uh, step 1 is write your whole algorithm, [NOISE] um.", "start": 1742.085, "duration": 6.675}, {"text": "[NOISE]", "start": 1748.76, "duration": 2.335}, {"text": "In terms of X_i,", "start": 1751.095, "duration": 2.355}, {"text": "X_j, in terms of inner products.", "start": 1753.45, "duration": 3.045}, {"text": "Uh, and instead of carrying the superscript, you know X_i,", "start": 1756.495, "duration": 3.66}, {"text": "X_j, I'm sometimes gonna write inner product between X and Z, right?", "start": 1760.155, "duration": 4.305}, {"text": "Where X and Z are supposed to be proxies for", "start": 1764.46, "duration": 3.045}, {"text": "two different training examples X_i and X_j but it", "start": 1767.505, "duration": 2.355}, {"text": "simplifies the notation, uh, right a little bit.", "start": 1769.86, "duration": 4.005}, {"text": "Two, um, let there be some mapping,", "start": 1773.865, "duration": 8.905}, {"text": "um, from your original input features", "start": 1785.96, "duration": 5.35}, {"text": "X to some high dimensional set of features Phi.", "start": 1791.31, "duration": 5.76}, {"text": "Um, and so one example would be,", "start": 1797.07, "duration": 3.27}, {"text": "let's say you try to predict the housing prices", "start": 1800.34, "duration": 2.28}, {"text": "or predicting a house will be sold in the next month.", "start": 1802.62, "duration": 2.355}, {"text": "So maybe X in this case is the size of the house,", "start": 1804.975, "duration": 3.465}, {"text": "uh, or maybe is, uh,", "start": 1808.44, "duration": 1.74}, {"text": "size and yeah, let write.", "start": 1810.18, "duration": 2.325}, {"text": "Maybe X is the size of a house,", "start": 1812.505, "duration": 2.325}, {"text": "and so you could, um,", "start": 1814.83, "duration": 3.165}, {"text": "take this 1D feature and expand it to a high dimensional feature vector with X,", "start": 1817.995, "duration": 6.06}, {"text": "X squared, X cubed,", "start": 1824.055, "duration": 1.99}, {"text": "X to the 4th, right?", "start": 1826.045, "duration": 1.735}, {"text": "So this would be one way of defining a high dimensional feature mapping.", "start": 1827.78, "duration": 3.27}, {"text": "Or another one could be, if you have two features X_1 and X_2,", "start": 1831.05, "duration": 3.705}, {"text": "uh, corresponding to the size of the house and number of bedrooms,", "start": 1834.755, "duration": 3.435}, {"text": "now you can map this to different Phi X,", "start": 1838.19, "duration": 2.825}, {"text": "which may be X_1, X_2,", "start": 1841.015, "duration": 2.51}, {"text": "X_1 times X_2, X_1 squared X_2,", "start": 1843.525, "duration": 2.715}, {"text": "uh, X_1 X_2 squared, and so on.", "start": 1846.24, "duration": 3.3}, {"text": "They are kind of polynomials, set of features,", "start": 1849.54, "duration": 1.83}, {"text": "or maybe another set of features as well, okay?", "start": 1851.37, "duration": 3.06}, {"text": "And what we'll be able to do is,", "start": 1854.43, "duration": 2.955}, {"text": "work with, um, feature mappings, Phi of X,", "start": 1857.385, "duration": 3.72}, {"text": "where the original input X may be 1D or 2D or, or whatever,", "start": 1861.105, "duration": 4.515}, {"text": "and Phi of X could be,", "start": 1865.62, "duration": 2.31}, {"text": "you know, 100,000 dimensional or infinite dimensional.", "start": 1867.93, "duration": 4.27}, {"text": "That we'll be able to do this very efficiently right.", "start": 1872.48, "duration": 3.565}, {"text": "Or even infinite dimensional, okay?", "start": 1876.045, "duration": 4.575}, {"text": "So I guess we will get some concrete examples of this later,", "start": 1880.62, "duration": 2.91}, {"text": "but I want to give you the overall recipe.", "start": 1883.53, "duration": 2.74}, {"text": "And then, what we're going to do is to find a way to compute K of X comma Z,", "start": 1886.52, "duration": 11.395}, {"text": "equals Phi of X transpose Phi of Z.", "start": 1897.915, "duration": 6.96}, {"text": "So this is called the kernel function.", "start": 1904.875, "duration": 2.385}, {"text": "And what we're gonna do is,", "start": 1907.26, "duration": 1.83}, {"text": "we'll see that there are clever tricks so", "start": 1909.09, "duration": 2.43}, {"text": "that you can compute the inner product between X", "start": 1911.52, "duration": 2.67}, {"text": "and Z even when Phi of X and Phi of Z are incredibly high dimensional, right?", "start": 1914.19, "duration": 5.1}, {"text": "We'll see an example of this in a- in- in very very soon.", "start": 1919.29, "duration": 3.555}, {"text": "And step four is, um,", "start": 1922.845, "duration": 3.39}, {"text": "replace X, Z in algorithm", "start": 1926.235, "duration": 5.064}, {"text": "with K of X, Z, okay?", "start": 1932.39, "duration": 7.375}, {"text": "Um, because if you could do this then what you're doing is,", "start": 1939.765, "duration": 4.77}, {"text": "you're running the whole learning algorithm on this high dimensional set of features,", "start": 1944.535, "duration": 5.685}, {"text": "um, and the problem with swapping out X for Phi of X,", "start": 1950.22, "duration": 5.88}, {"text": "right, is that, it can be very computationally expensive if you're", "start": 1956.1, "duration": 2.82}, {"text": "working with 100,000 dimensional feature vectors, right.", "start": 1958.92, "duration": 2.865}, {"text": "I,I- even by to this standards, you know, 100,000, it's.", "start": 1961.785, "duration": 2.91}, {"text": "it's not the biggest I've seen, I've seen, actually,", "start": 1964.695, "duration": 2.205}, {"text": "biggest I've seen that you have a billion features, uh,", "start": 1966.9, "duration": 2.49}, {"text": "but even by today's standards,", "start": 1969.39, "duration": 1.11}, {"text": "100,000 features is actually quite a lot.", "start": 1970.5, "duration": 2.1}, {"text": "Um, uh, and- and if you're launching I said,", "start": 1972.6, "duration": 4.26}, {"text": "just 100,000 is, is- this is a lot- lot of large number of features, I guess.", "start": 1976.86, "duration": 3.9}, {"text": "Um, and the problem of using this is it's quite computationally expensive,", "start": 1980.76, "duration": 4.68}, {"text": "to carry around these 100,000 or million", "start": 1985.44, "duration": 2.25}, {"text": "dimensional or 100 million dimensional feature vectors or whatever.", "start": 1987.69, "duration": 3.82}, {"text": "Um, but that's what you would do if you were to swap in Phi of X,", "start": 1992.18, "duration": 4.48}, {"text": "you know in the naive straightforward way for X, but what we'll see is that,", "start": 1996.66, "duration": 4.08}, {"text": "if you can compute K of X, Z then you could,", "start": 2000.74, "duration": 2.655}, {"text": "because you've written your whole algorithm just in terms of inner products,", "start": 2003.395, "duration": 3.555}, {"text": "then you don't ever need to explicitly compute Phi of X,", "start": 2006.95, "duration": 3.36}, {"text": "you can always just compute these kernels. Yeah.", "start": 2010.31, "duration": 3.6}, {"text": "[inaudible]", "start": 2013.91, "duration": 7.68}, {"text": "Let me get to that later,", "start": 2021.59, "duration": 8.94}, {"text": "you know, I will go for some kernels and I will talk about uh,", "start": 2030.53, "duration": 2.55}, {"text": "bias-variance probably on Wednesday.", "start": 2033.08, "duration": 2.385}, {"text": "Yeah. I think the no free lunch theorem is", "start": 2035.465, "duration": 2.685}, {"text": "a fascinating theoretical concept but I think that it has been,", "start": 2038.15, "duration": 4.05}, {"text": "I don't know, it's been less useful actually because I think we", "start": 2042.2, "duration": 2.64}, {"text": "have inductive biases that turn out to be useful.", "start": 2044.84, "duration": 3.37}, {"text": "There's a famous theorem in learning theory called no free lunch.", "start": 2048.22, "duration": 3.64}, {"text": "It was like 20 years ago.", "start": 2051.86, "duration": 1.455}, {"text": "That basically says that, in the worst case,", "start": 2053.315, "duration": 2.475}, {"text": "learning algorithms do not work [NOISE].", "start": 2055.79, "duration": 2.88}, {"text": "For any learning algorithm,", "start": 2058.67, "duration": 1.02}, {"text": "I can come up with some data distribution so that your learning algorithm sucks.", "start": 2059.69, "duration": 2.685}, {"text": "That, that's roughly the no free lunch theorem,", "start": 2062.375, "duration": 1.755}, {"text": "proved about like 20 years ago.", "start": 2064.13, "duration": 1.575}, {"text": "But it turns out most of the world- most of the time,", "start": 2065.705, "duration": 1.905}, {"text": "the universe is not that hostile toward us.", "start": 2067.61, "duration": 2.01}, {"text": "So- so, yeah, so as the learning algorithms turned out okay [LAUGHTER].", "start": 2069.62, "duration": 7.2}, {"text": "Um, all right, let's go through one example of kernels.", "start": 2076.82, "duration": 6.12}, {"text": "Um, so for this example,", "start": 2082.94, "duration": 2.4}, {"text": "let's say that your offer is not input features was three-dimensional X_1, X_2, X_3.", "start": 2085.34, "duration": 5.31}, {"text": "And let's say I'm gonna choose the feature mapping,", "start": 2090.65, "duration": 2.895}, {"text": "Phi of X to be,", "start": 2093.545, "duration": 1.605}, {"text": "um, o- so pair-wise, um, monomial terms.", "start": 2095.15, "duration": 4.275}, {"text": "So I'm gonna choose X_1 times X_1, X_1 X_2,", "start": 2099.425, "duration": 4.23}, {"text": "X_1 X_3, X_2 X_1, all.", "start": 2103.655, "duration": 7.35}, {"text": "Okay. And there are a couple of duplicates so", "start": 2114.04, "duration": 2.815}, {"text": "X_1 X_3 is equal to X_3 X_1 but I'll just write it out this way.", "start": 2116.855, "duration": 4.285}, {"text": "And so notice that, er,", "start": 2121.51, "duration": 2.575}, {"text": "if you have- if X is in R_n, right?", "start": 2124.085, "duration": 3.705}, {"text": "Then Phi of X is in R_n squared, right.", "start": 2127.79, "duration": 4.68}, {"text": "So got the three-dimensional features to nine dimensional.", "start": 2132.47, "duration": 3.36}, {"text": "And I'm using small numbers for illustration.", "start": 2135.83, "duration": 2.52}, {"text": "In practice, think of X as 1,000 dimensional and so this is now a million.", "start": 2138.35, "duration": 4.32}, {"text": "Or think of this as maybe 10,000 and this is now like 100 million, okay.", "start": 2142.67, "duration": 4.04}, {"text": "So n squared features is much bigger [NOISE].", "start": 2146.71, "duration": 2.92}, {"text": "Um, and then similarly, Phi of Z is going to be Z_1 Z_1, Z_1 Z_2,", "start": 2149.63, "duration": 7.239}, {"text": "okay? So we've gone from n features like 10,000 features,", "start": 2167.41, "duration": 6.94}, {"text": "to n squared features which,", "start": 2174.35, "duration": 1.605}, {"text": "in this case, 100 million features.", "start": 2175.955, "duration": 2.355}, {"text": "Um, so because there are n squared elements, right?", "start": 2178.31, "duration": 9.75}, {"text": "You will need order n squared time to compute Phi of", "start": 2188.06, "duration": 9.24}, {"text": "X or to compute", "start": 2197.3, "duration": 3.21}, {"text": "phi X transpose Phi of Z explicitly, right?", "start": 2200.51, "duration": 7.545}, {"text": "So if you wanna compute the inner product between Phi of X and Phi of", "start": 2208.055, "duration": 2.235}, {"text": "Z and they do it explicitly, in the obvious way,", "start": 2210.29, "duration": 2.46}, {"text": "it'll take n squared time to just compute all of", "start": 2212.75, "duration": 2.22}, {"text": "these inner products and then do the- and,", "start": 2214.97, "duration": 2.52}, {"text": "and then they'll compute this,", "start": 2217.49, "duration": 1.155}, {"text": "er, com- compute this, right.", "start": 2218.645, "duration": 1.665}, {"text": "And it- it's actually n squared over 2,", "start": 2220.31, "duration": 1.8}, {"text": "because a lot of these things are duplicated but that's the order n-squared.", "start": 2222.11, "duration": 4.36}, {"text": "But let's see if we can find a better way to do that.", "start": 2235.27, "duration": 3.61}, {"text": "So what we want is to write out the kernel of x, z.", "start": 2238.88, "duration": 4.96}, {"text": "So this phi of x transpose phi of z, right?", "start": 2243.85, "duration": 5.725}, {"text": "And, uh, what I'm gonna prove is that this can be computed", "start": 2249.575, "duration": 4.034}, {"text": "as x transpose z squared, right?", "start": 2253.609, "duration": 4.771}, {"text": "And the cool thing is that remember x is n-dimensional, z is n-dimensional.", "start": 2258.38, "duration": 8.025}, {"text": "So x transpose z squared,", "start": 2266.405, "duration": 2.145}, {"text": "this is an order n time computation, right?", "start": 2268.55, "duration": 3.975}, {"text": "Because taking x transpose z, you know,", "start": 2272.525, "duration": 2.13}, {"text": "that's just in a product of two n-dimensional vectors and then you take that number,", "start": 2274.655, "duration": 5.04}, {"text": "x transpose z is a real number,", "start": 2279.695, "duration": 1.575}, {"text": "and you just square that number.", "start": 2281.27, "duration": 1.95}, {"text": "So that's the order n time computation.", "start": 2283.22, "duration": 3.585}, {"text": "Um, and so let me just prove that x transpose z is equal to,", "start": 2286.805, "duration": 4.26}, {"text": "well, le- le- le- let me, let me,", "start": 2291.065, "duration": 1.32}, {"text": "let me prove this step, right?", "start": 2292.385, "duration": 2.295}, {"text": "Um, and so x transpose z squared that's equal to,", "start": 2294.68, "duration": 4.86}, {"text": "um, right.", "start": 2299.54, "duration": 7.98}, {"text": "So this is x transpose z, right?", "start": 2307.52, "duration": 5.565}, {"text": "And then times this is also x transpose z.", "start": 2313.085, "duration": 7.47}, {"text": "So this formula is z transpose z squared,", "start": 2320.555, "duration": 2.31}, {"text": "it's x transpose z times itself.", "start": 2322.865, "duration": 1.575}, {"text": "Um, and then if I rearranged sums,", "start": 2324.44, "duration": 3.87}, {"text": "this is equal to sum from i equals 1 through n,", "start": 2328.31, "duration": 3.435}, {"text": "sum from j equals 1 through n, um,", "start": 2331.745, "duration": 3.585}, {"text": "x_i z_i, x_j z_j.", "start": 2335.33, "duration": 4.485}, {"text": "Um, and this in turn is,", "start": 2339.815, "duration": 3.81}, {"text": "you know, sum over i, sum over j,", "start": 2343.625, "duration": 4.335}, {"text": "of x_i x_j times z_i z_j, right?.", "start": 2347.96, "duration": 10.59}, {"text": "And so what this is doing,", "start": 2358.55, "duration": 1.875}, {"text": "is it's marching through all possible pairs of i and j and multiplying x_i x_j,", "start": 2360.425, "duration": 9.24}, {"text": "with the corresponding z_i z_j and adding that up.", "start": 2369.665, "duration": 5.565}, {"text": "But of course; if you were to compute phi of x transpose phi of z,", "start": 2375.23, "duration": 5.085}, {"text": "what you do is you take this and multiply with that and then add it to the sum,", "start": 2380.315, "duration": 5.495}, {"text": "then take this and multiply with that and add it to the sum,", "start": 2385.81, "duration": 3.0}, {"text": "and so on until you end up taking this and multiplying", "start": 2388.81, "duration": 2.655}, {"text": "that and adding it to your sum, right?", "start": 2391.465, "duration": 3.03}, {"text": "So that's why, um- so that's why this formula is just,", "start": 2394.495, "duration": 7.63}, {"text": "you know, marching down these two lists this, and multiplying, multiplying,", "start": 2402.125, "duration": 3.885}, {"text": "multiplying and add it up,", "start": 2406.01, "duration": 1.125}, {"text": "which is exactly, um, phi transpose.", "start": 2407.135, "duration": 4.915}, {"text": "Which is exactly phi of x transpose phi of z.", "start": 2412.66, "duration": 6.49}, {"text": "Okay? So this proves that, um,", "start": 2419.15, "duration": 3.645}, {"text": "you've turned what was previously an order n square time calculation,", "start": 2422.795, "duration": 4.05}, {"text": "into an order n time calculation.", "start": 2426.845, "duration": 3.42}, {"text": "Which means that, um,", "start": 2430.265, "duration": 2.22}, {"text": "if n was 10,000,", "start": 2432.485, "duration": 2.715}, {"text": "instead of needing to manipulate 100,000", "start": 2435.2, "duration": 3.42}, {"text": "dimensional [NOISE] vectors to come up with these.", "start": 2438.62, "duration": 3.57}, {"text": "Sorry. That's my phone buzzing. This is really loud.", "start": 2442.19, "duration": 2.61}, {"text": "Okay. Instead of needing to manipulate sort of 100,000 dimensional vectors,", "start": 2444.8, "duration": 5.58}, {"text": "you could do so manipulating only 10,000 dimensional vectors, okay?.", "start": 2450.38, "duration": 5.28}, {"text": "Now, um, a few other examples of kernels.", "start": 2455.66, "duration": 5.38}, {"text": "It turns out that,", "start": 2471.79, "duration": 2.365}, {"text": "um, if you choose this kernel- so let's see.", "start": 2474.155, "duration": 3.255}, {"text": "We had k of x comma z equals x transpose z squared, um,", "start": 2477.41, "duration": 7.17}, {"text": "if we now add a plus c there where c is a constant, um,", "start": 2484.58, "duration": 4.785}, {"text": "so c is just some fixed real number,", "start": 2489.365, "duration": 2.25}, {"text": "that corresponds to modifying your features as follows.", "start": 2491.615, "duration": 4.56}, {"text": "Um, where instead of just this- you know,", "start": 2496.175, "duration": 2.66}, {"text": "binomial terms of pairs of these things,", "start": 2498.835, "duration": 2.91}, {"text": "if we add plus c there,", "start": 2501.745, "duration": 1.845}, {"text": "it corresponds to adding x_1,", "start": 2503.59, "duration": 2.715}, {"text": "x_2, x_3, uh, to this- to your set of features.", "start": 2506.305, "duration": 4.145}, {"text": "Ah, technically, there's actually weighting on this.", "start": 2510.45, "duration": 2.33}, {"text": "There's your root 2c, root 2c,", "start": 2512.78, "duration": 2.595}, {"text": "root 2c and then as a constant c there as well.", "start": 2515.375, "duration": 3.315}, {"text": "And you can prove this yourself,", "start": 2518.69, "duration": 1.2}, {"text": "and it turns out that if this is your new definition for phi of x,", "start": 2519.89, "duration": 3.465}, {"text": "and make the same change to phi of z.", "start": 2523.355, "duration": 1.905}, {"text": "You know, so root 2c z_1 and so on.", "start": 2525.26, "duration": 2.43}, {"text": "Then if you can take the inner product of these,", "start": 2527.69, "duration": 1.935}, {"text": "then it can be computed as this.", "start": 2529.625, "duration": 2.895}, {"text": "Right? And so that's- and, and,", "start": 2532.52, "duration": 1.5}, {"text": "and so the role of the, um,", "start": 2534.02, "duration": 2.235}, {"text": "constant c it trades off the relative weighting between the binomial terms the- you know,", "start": 2536.255, "duration": 5.58}, {"text": "x_i x_j, compared to the,", "start": 2541.835, "duration": 1.89}, {"text": "to the single- to the first-degree terms the x_1 or, x_2 x_3.", "start": 2543.725, "duration": 5.095}, {"text": "Um, other examples, uh,", "start": 2549.97, "duration": 3.25}, {"text": "if you choose this to the power of d, right?", "start": 2553.22, "duration": 10.1}, {"text": "Um, notice that this still is an order n time computation, right?", "start": 2563.32, "duration": 7.12}, {"text": "X transpose z takes order n time,", "start": 2570.44, "duration": 1.74}, {"text": "you add a number to it and you take this the power of", "start": 2572.18, "duration": 1.86}, {"text": "d. So you can compute this in order n time.", "start": 2574.04, "duration": 2.52}, {"text": "But this corresponds to now phi of x has all- um,", "start": 2576.56, "duration": 8.16}, {"text": "the number of terms turns out to be n plus d choose d but it doesn't matter.", "start": 2584.72, "duration": 4.125}, {"text": "Uh, it turns out this contains all features of, uh,", "start": 2588.845, "duration": 3.735}, {"text": "monomials up to, uh,", "start": 2592.58, "duration": 6.54}, {"text": "order d. So by which I mean, um,", "start": 2599.12, "duration": 2.64}, {"text": "i- i- if, let's say d is equal to 5, right?", "start": 2601.76, "duration": 4.23}, {"text": "Then this contains- then phi of x contains all the features of the form", "start": 2605.99, "duration": 4.035}, {"text": "x_1 x_2 x_5 x_17 x_29, right?", "start": 2610.025, "duration": 4.665}, {"text": "This is a fifth degree thing, uh, or x,", "start": 2614.69, "duration": 3.15}, {"text": "or x_1 x_2 squared x_3 x, you know, 18.", "start": 2617.84, "duration": 4.29}, {"text": "This is also a fifth order polynomial-", "start": 2622.13, "duration": 2.715}, {"text": "a fifth order monomial it's called and so if you, um,", "start": 2624.845, "duration": 4.095}, {"text": "choose this as your kernel,", "start": 2628.94, "duration": 1.125}, {"text": "this corresponds to constructing phi of x to", "start": 2630.065, "duration": 2.835}, {"text": "contain all of these features and there are not exponentially many of them, right?", "start": 2632.9, "duration": 3.54}, {"text": "There a lot of these features.", "start": 2636.44, "duration": 1.17}, {"text": "Any or all the, um,", "start": 2637.61, "duration": 2.1}, {"text": "all, all the- these are called monomials.", "start": 2639.71, "duration": 1.41}, {"text": "Basically all the polynomial terms, all the monomial terms,", "start": 2641.12, "duration": 2.355}, {"text": "up to a fifth degree polynomial,", "start": 2643.475, "duration": 2.219}, {"text": "up to a fifth order monomial term.", "start": 2645.694, "duration": 2.071}, {"text": "So- and there are- it turns out there are n plus z choose ds which is,", "start": 2647.765, "duration": 3.405}, {"text": "uh, roughly n plus d to the power of d very roughly.", "start": 2651.17, "duration": 3.18}, {"text": "So this is a very, very large number of features, um,", "start": 2654.35, "duration": 3.42}, {"text": "but your computation doesn't blow up exponentially even as d increases.", "start": 2657.77, "duration": 5.67}, {"text": "Okay? So, um, what a support vector machine is, is, um,", "start": 2663.44, "duration": 8.1}, {"text": "taking the optimal margin classifier that we derived earlier,", "start": 2671.54, "duration": 4.56}, {"text": "and applying the kernel trick to it,", "start": 2676.1, "duration": 3.03}, {"text": "uh, in which we already had the- so well.", "start": 2679.13, "duration": 3.585}, {"text": "So optimal margin classifier plus the kernel trick,", "start": 2682.715, "duration": 10.435}, {"text": "right, that is the support vector machine.", "start": 2693.37, "duration": 3.31}, {"text": "Okay? And so if you choose some of these kernels for example,", "start": 2696.68, "duration": 5.37}, {"text": "then you could run an SVM in these very,", "start": 2702.05, "duration": 2.79}, {"text": "very high-dimensional feature spaces, uh,", "start": 2704.84, "duration": 2.07}, {"text": "in these, you know, 100 trillion dimensional feature spaces.", "start": 2706.91, "duration": 3.645}, {"text": "But your computational time,", "start": 2710.555, "duration": 1.995}, {"text": "scales only linearly, um, as order n,", "start": 2712.55, "duration": 3.405}, {"text": "as the numb- as a dimension of your input features x rather", "start": 2715.955, "duration": 2.835}, {"text": "than as a function of this 100 trillion dimensional feature space,", "start": 2718.79, "duration": 3.12}, {"text": "you're actually building a linear classifier.", "start": 2721.91, "duration": 2.775}, {"text": "Okay? So, um, why is this a good idea?", "start": 2724.685, "duration": 4.56}, {"text": "Let me just, sheesh.", "start": 2729.245, "duration": 3.955}, {"text": "Let's show a quick video to give you intuition for what this is doing.", "start": 2733.21, "duration": 5.845}, {"text": "Um, let's see.", "start": 2739.055, "duration": 4.29}, {"text": "Okay. I think the projector takes a while to warm up, does it?", "start": 2743.345, "duration": 2.385}, {"text": "[NOISE] All right. Any questions while we're- Yeah?", "start": 2745.73, "duration": 5.82}, {"text": "[inaudible]", "start": 2751.55, "duration": 8.595}, {"text": "Uh, yes. So, uh,", "start": 2760.145, "duration": 1.8}, {"text": "this kernel function appears- applies only to this visual mapping.", "start": 2761.945, "duration": 3.12}, {"text": "So each kernel function of, um, uh,", "start": 2765.065, "duration": 2.745}, {"text": "uh, yes, after trivial differences, right?", "start": 2767.81, "duration": 4.175}, {"text": "If you have a feature mapping where the features that", "start": 2771.985, "duration": 1.545}, {"text": "could- are permuted or something,", "start": 2773.53, "duration": 1.605}, {"text": "then the Kernel function stays the same.", "start": 2775.135, "duration": 1.995}, {"text": "Uh, uh, so there are trivial chunk function- transformations like that but, uh,", "start": 2777.13, "duration": 4.02}, {"text": "if we have a totally different feature mapping,", "start": 2781.15, "duration": 1.83}, {"text": "you would expect to need a totally different kernel function.", "start": 2782.98, "duration": 2.42}, {"text": "Cool.", "start": 2785.4, "duration": 8.53}, {"text": "So I wanted to- let's see.", "start": 2793.93, "duration": 5.775}, {"text": "Ah, cool, awesome. Uh, I want to give you a visual picture [NOISE]", "start": 2799.705, "duration": 13.335}, {"text": "of what this um,", "start": 2813.04, "duration": 1.32}, {"text": "[NOISE].", "start": 2814.36, "duration": 15.45}, {"text": "All right, um, this is a YouTube video that, uh,", "start": 2829.81, "duration": 3.025}, {"text": "Kian Katanforoosh who teaches CS230 found and suggested I use.", "start": 2832.835, "duration": 3.705}, {"text": "So I don't- I don't know who Udi Aharoni is but", "start": 2836.54, "duration": 3.06}, {"text": "this is a nice visualization of what a support vector machine is doing.", "start": 2839.6, "duration": 3.36}, {"text": "So um, let's see how the uh,", "start": 2842.96, "duration": 4.275}, {"text": "uh, learning algorithm where you're trying to separate the blue dots from the red dots.", "start": 2847.235, "duration": 4.395}, {"text": "Right? So um, the blue and the red dots can't be separated by a straight line,", "start": 2851.63, "duration": 4.845}, {"text": "but you put them on the plane and you use a feature mapping", "start": 2856.475, "duration": 2.895}, {"text": "phi to throw these points into much higher-dimensional space.", "start": 2859.37, "duration": 3.42}, {"text": "So there's now three of these points in the three-dimensional space.", "start": 2862.79, "duration": 2.76}, {"text": "In the three-dimensional space,", "start": 2865.55, "duration": 1.89}, {"text": "you can then find w. So w is now three-dimensional because it applied", "start": 2867.44, "duration": 3.84}, {"text": "the optimal margin classifier in", "start": 2871.28, "duration": 1.41}, {"text": "this three-dimensional space that separates the blue dots and the red dots.", "start": 2872.69, "duration": 4.47}, {"text": "Uh, and if you now you know examine what this is doing back in the original space,", "start": 2877.16, "duration": 6.015}, {"text": "then your linear classifier actually defines that elliptical decision boundary.", "start": 2883.175, "duration": 5.535}, {"text": "That makes sense right? So you're taking the data- all right um,", "start": 2888.71, "duration": 5.82}, {"text": "so you're taking the data, uh,", "start": 2894.53, "duration": 2.925}, {"text": "mapping it to a much higher dimensional feature space,", "start": 2897.455, "duration": 2.925}, {"text": "three-dimensional visualization that in practice can be", "start": 2900.38, "duration": 2.745}, {"text": "100 trillion dimensions and then finding", "start": 2903.125, "duration": 2.595}, {"text": "a linear decision boundary in that 100 trillion-dimensional space uh,", "start": 2905.72, "duration": 3.68}, {"text": "which is going to be a hyperplane like a- like a straight, you know,", "start": 2909.4, "duration": 2.97}, {"text": "like a plane or a straight line or a plane and then when you look at what you just did in", "start": 2912.37, "duration": 3.99}, {"text": "the original feature space you found a very non-linear decision boundary, okay?", "start": 2916.36, "duration": 5.59}, {"text": "Um, so this is why uh- and again here you can only", "start": 2922.5, "duration": 6.17}, {"text": "visualize relatively low dimensional feature spaces even, even on a display like that.", "start": 2928.67, "duration": 5.535}, {"text": "But you find that if you use an", "start": 2934.205, "duration": 2.25}, {"text": "SVM kernel you know, um, right,", "start": 2936.455, "duration": 10.875}, {"text": "you could learn very non-linear decision boundaries like that.", "start": 2947.33, "duration": 3.645}, {"text": "But that is a linear decision boundary in a very high-dimensional space.", "start": 2950.975, "duration": 3.555}, {"text": "But when you project it back down to you know,", "start": 2954.53, "duration": 1.845}, {"text": "2D you end up with a very non-linear decision boundary like that okay? All right.", "start": 2956.375, "duration": 9.055}, {"text": "So.", "start": 2965.59, "duration": 1.705}, {"text": "Yeah.", "start": 2967.295, "duration": 0.495}, {"text": "[inaudible] digital words [inaudible]", "start": 2967.79, "duration": 7.2}, {"text": "Oh sure, yes. So uh, in this high dimensional space", "start": 2974.99, "duration": 2.895}, {"text": "represented by the feature mapping phi of X", "start": 2977.885, "duration": 1.905}, {"text": "does the data always have to be linearly separable?", "start": 2979.79, "duration": 2.31}, {"text": "So far we're pretending that it does,", "start": 2982.1, "duration": 1.77}, {"text": "I'll come here back and fix that assumption later today.", "start": 2983.87, "duration": 2.34}, {"text": "Yeah.", "start": 2986.21, "duration": 1.42}, {"text": "Okay, so um now,", "start": 2993.28, "duration": 3.85}, {"text": "how do you make kernels?", "start": 2997.13, "duration": 3.64}, {"text": "Right? Um, so here's here's some- so here's some intuition you might have about kernels.", "start": 3002.85, "duration": 9.16}, {"text": "Um if X and Z are", "start": 3012.01, "duration": 6.09}, {"text": "similar you know if two if two- and for", "start": 3018.1, "duration": 5.22}, {"text": "the examples X and Z are close to each other or similar to each other then K of x z,", "start": 3023.32, "duration": 6.15}, {"text": "which is the inner product between X and Z, right?", "start": 3029.47, "duration": 5.495}, {"text": "Presumably this should be large.", "start": 3034.965, "duration": 2.58}, {"text": "Um and conversely if X and Z are dissimilar then K of x z,", "start": 3037.545, "duration": 10.075}, {"text": "you know this maybe should be small, right?", "start": 3047.62, "duration": 4.005}, {"text": "Because uh the inner product of", "start": 3051.625, "duration": 1.695}, {"text": "two very similar vectors that are pointing the same direction", "start": 3053.32, "duration": 2.79}, {"text": "should be large and the inner product of two dissimilar vectors should be small.", "start": 3056.11, "duration": 4.98}, {"text": "Right? So this is one uh guiding principle behind,", "start": 3061.09, "duration": 3.585}, {"text": "you know, what you see in a lot of kernels.", "start": 3064.675, "duration": 1.695}, {"text": "Just if- if this is phi of x and this is phi of z,", "start": 3066.37, "duration": 3.81}, {"text": "the inner product is large but then they kinda point off in random directions,", "start": 3070.18, "duration": 4.575}, {"text": "the inner product will be small right?", "start": 3074.755, "duration": 2.325}, {"text": "That's how vector inner product works.", "start": 3077.08, "duration": 2.46}, {"text": "Um and so- well what if we just pull", "start": 3079.54, "duration": 3.27}, {"text": "a function out of these three here, out of the air um, which is K", "start": 3082.81, "duration": 3.9}, {"text": "of xz equals e to the negative x minus z squared over 2 sigma squared.", "start": 3086.71, "duration": 8.56}, {"text": "Right? So this is one example of a similarity", "start": 3095.79, "duration": 3.76}, {"text": "sim sim sim sim- if you think of kernels as a similarity measure of a function,", "start": 3099.55, "duration": 4.755}, {"text": "this you know let's just make up", "start": 3104.305, "duration": 2.085}, {"text": "another similarity measure of a function and this does have the property that if", "start": 3106.39, "duration": 4.14}, {"text": "X and Z are very close to each other then this would be e to the 0 which is about 1.", "start": 3110.53, "duration": 6.375}, {"text": "But if X and Z are very far apart then this would be small, right?", "start": 3116.905, "duration": 3.135}, {"text": "So this function it- it actually satisfies this criteria.", "start": 3120.04, "duration": 4.395}, {"text": "It satisfies those criteria and the question is uh,", "start": 3124.435, "duration": 3.915}, {"text": "is it okay to use this as a kernel function?", "start": 3128.35, "duration": 4.8}, {"text": "Right? So it turns out that um a function like that K of x z,", "start": 3133.15, "duration": 6.885}, {"text": "you can use it as a kernel function.", "start": 3140.035, "duration": 3.615}, {"text": "Only if there exists", "start": 3143.65, "duration": 3.79}, {"text": "some phi such that K of x z equals phi of X transpose phi Z right?", "start": 3150.12, "duration": 10.915}, {"text": "So we derived the whole algorithm assuming this to be true and it", "start": 3161.035, "duration": 3.375}, {"text": "turns out if you plug in the kernel function for which this isn't true,", "start": 3164.41, "duration": 3.57}, {"text": "then all of the derivation we wrote down breaks down and the optimization", "start": 3167.98, "duration": 4.05}, {"text": "problem you know um, uh, can have very strange solutions, right?", "start": 3172.03, "duration": 5.175}, {"text": "That don't correspond to good classification though a good classifier at all.", "start": 3177.205, "duration": 4.17}, {"text": "Um and so this puts some constraints on", "start": 3181.375, "duration": 1.875}, {"text": "what kernel functions we could  or for example,", "start": 3183.25, "duration": 3.21}, {"text": "one thing it must satisfy is K of X X which is phi X transpose phi of Z.", "start": 3186.46, "duration": 6.63}, {"text": "This would better be greater than equal to 0, right?", "start": 3193.09, "duration": 2.79}, {"text": "Sorry right?", "start": 3195.88, "duration": 2.325}, {"text": "Because inner product of a vector with itself had better be non-negative.", "start": 3198.205, "duration": 3.015}, {"text": "So K of X X is ever 0 or less than 0,", "start": 3201.22, "duration": 3.15}, {"text": "then this is not a valid kernel function, okay?", "start": 3204.37, "duration": 3.015}, {"text": "Um, more generally, there's", "start": 3207.385, "duration": 3.99}, {"text": "a theorem that uh proves when is something a valid kernel.", "start": 3211.375, "duration": 5.43}, {"text": "Um, somebody just outlined that that proof very briefly which is uh,", "start": 3216.805, "duration": 5.385}, {"text": "less than X_1 up to X_d you know be any d points, right?", "start": 3222.19, "duration": 9.24}, {"text": "And let's let K- sorry about overloading of", "start": 3231.43, "duration": 7.47}, {"text": "notation um this is a- so K represents", "start": 3238.9, "duration": 3.75}, {"text": "a kernel function and I'm gonna use K to represent the kernel matrix as well.", "start": 3242.65, "duration": 5.65}, {"text": "Sometimes it's also called the gram matrix uh but it's called the kernel matrix.", "start": 3249.54, "duration": 5.44}, {"text": "So that K_ ij is equal to the kernel function", "start": 3254.98, "duration": 4.305}, {"text": "applied to two of those points um X_i and X_j, right?", "start": 3259.285, "duration": 5.205}, {"text": "So you have d points.", "start": 3264.49, "duration": 1.065}, {"text": "So just apply the Kernel function to every pair of", "start": 3265.555, "duration": 2.595}, {"text": "those points and put them in a matrix, in a big d by d matrix like that.", "start": 3268.15, "duration": 4.48}, {"text": "So it turns out that uh,", "start": 3276.15, "duration": 4.66}, {"text": "given any vector Z- I think you've seen something similar to this in problem set one,", "start": 3280.81, "duration": 8.055}, {"text": "but given any vector z,", "start": 3288.865, "duration": 2.07}, {"text": "z transpose K z which is sum over i sum over", "start": 3290.935, "duration": 6.075}, {"text": "j z i k i j z j, right?", "start": 3297.01, "duration": 6.39}, {"text": "Um if K is a valid kernel function so if there is some feature mapping phi,", "start": 3303.4, "duration": 6.6}, {"text": "then this should equal to sum of i sum of j Z_i phi of X_i transpose phi", "start": 3310.0, "duration": 9.735}, {"text": "of z X_j times Z_j and by a couple other steps.", "start": 3319.735, "duration": 9.835}, {"text": "Um let's see.", "start": 3329.57, "duration": 2.945}, {"text": "This phi of X_i transpose phi of X_j.", "start": 3332.515, "duration": 2.715}, {"text": "I'm gonna to expand out that inner product.", "start": 3335.23, "duration": 1.845}, {"text": "So sum over k, phi of X_ i,", "start": 3337.075, "duration": 4.065}, {"text": "element k times phi of X_j element k times Z_ j,", "start": 3341.14, "duration": 6.51}, {"text": "um and then we are arranging", "start": 3347.65, "duration": 5.97}, {"text": "sums is sum- sum over", "start": 3353.62, "duration": 1.59}, {"text": "K oh sorry I'm running out of whiteboard let me just do it on the next board.", "start": 3355.21, "duration": 6.13}, {"text": "So we arrange sums, sum of k,", "start": 3374.16, "duration": 3.7}, {"text": "sum of i, sum of j,", "start": 3377.86, "duration": 4.815}, {"text": "z i phi of x i subscript k,", "start": 3382.675, "duration": 6.195}, {"text": "times phi of x [NOISE] j subscript k times z j.", "start": 3388.87, "duration": 9.73}, {"text": "Which is sum of the k [NOISE]", "start": 3399.27, "duration": 9.43}, {"text": "squared and therefore this must be greater than or equal to 0.", "start": 3408.7, "duration": 3.66}, {"text": "Right. And so this proves that the matrix K,", "start": 3412.36, "duration": 4.65}, {"text": "ah, the kernel matrix k is positive semi-definite.", "start": 3417.01, "duration": 4.11}, {"text": "Okay. Um, and so more generally,", "start": 3421.12, "duration": 5.37}, {"text": "it turns out that this is also a sufficient condition, um,", "start": 3426.49, "duration": 4.32}, {"text": "for a kernel function to- for our function k to be a valid kernel function.", "start": 3430.81, "duration": 7.005}, {"text": "So let me just write this out.", "start": 3437.815, "duration": 3.265}, {"text": "This is called a Mercer's Theorem, M-E-R-C-E-R.", "start": 3441.12, "duration": 7.26}, {"text": "Wait, um, so K is a valid kernel.", "start": 3448.38, "duration": 7.69}, {"text": "[NOISE] So K is", "start": 3456.07, "duration": 6.18}, {"text": "a valid kernel function i.e there exists phi such that K of x z,", "start": 3462.25, "duration": 7.89}, {"text": "equals phi of x,", "start": 3470.14, "duration": 1.71}, {"text": "transpose phi of z if and only", "start": 3471.85, "duration": 4.59}, {"text": "if for any d points,", "start": 3476.44, "duration": 6.819}, {"text": "you know, x one up to x z,", "start": 3484.95, "duration": 3.73}, {"text": "on the corresponding kernel matrix", "start": 3488.68, "duration": 3.96}, {"text": "[NOISE] is a positive semi-definite.", "start": 3492.64, "duration": 9.48}, {"text": "So if you write this K greater equals 0.", "start": 3502.12, "duration": 2.085}, {"text": "Okay. Um, and I proved just one-dimens- one- one direction of this implication.", "start": 3504.205, "duration": 5.87}, {"text": "Right. This proof outline here shows that if it is a valid kernel function,", "start": 3510.075, "duration": 4.38}, {"text": "ah, then this is positive semi-definite.", "start": 3514.455, "duration": 2.355}, {"text": "Um, this outline didn't prove the opposite direction.", "start": 3516.81, "duration": 2.905}, {"text": "You see if and only if.", "start": 3519.715, "duration": 1.23}, {"text": "Right. Shows both directions.", "start": 3520.945, "duration": 1.815}, {"text": "So this, ah, algebra we did just now", "start": 3522.76, "duration": 2.745}, {"text": "proves that dimension of the proof I didn't prove the reverse dimension.", "start": 3525.505, "duration": 3.495}, {"text": "But this turns out to be", "start": 3529.0, "duration": 1.22}, {"text": "an if and only if condition.", "start": 3530.22, "duration": 1.275}, {"text": "And so this gives maybe one test for,", "start": 3531.495, "duration": 3.12}, {"text": "um, whether or not something is a valid kernel function.", "start": 3534.615, "duration": 3.6}, {"text": "Okay. Um, and it turns out that- the kernel I wrote up there, um,", "start": 3538.215, "duration": 6.475}, {"text": "that one, K of x z, uh.", "start": 3544.69, "duration": 8.85}, {"text": "Right. And it turns out this is a valid kernel.", "start": 3553.54, "duration": 2.505}, {"text": "This is called the Gaussian kernel.", "start": 3556.045, "duration": 2.695}, {"text": "This is, uh, probably the most widely used kernel.", "start": 3560.07, "duration": 4.345}, {"text": "Um, well a- actually well,", "start": 3564.415, "duration": 2.955}, {"text": "uh, let me [NOISE].", "start": 3567.37, "duration": 9.24}, {"text": "Well, but the actually the most widely used kernel is-is maybe the linear kernel, um,", "start": 3576.61, "duration": 7.74}, {"text": "which just uses K of x z equals x transpose z,", "start": 3584.35, "duration": 5.85}, {"text": "ah, and so this is using you know phi of x equals x.", "start": 3590.2, "duration": 3.855}, {"text": "Right. So no- no- no high dimensional features.", "start": 3594.055, "duration": 2.46}, {"text": "So sometimes you call it the linear kernel.", "start": 3596.515, "duration": 1.725}, {"text": "It just means you're not using a high dimensional feature mapping", "start": 3598.24, "duration": 2.85}, {"text": "or the feature mapping is just equal to the original features.", "start": 3601.09, "duration": 2.97}, {"text": "Ah, this is this is actually a pretty commonly used kernel function,", "start": 3604.06, "duration": 3.87}, {"text": "ah, you- you're not taking advantage of kernels in other words.", "start": 3607.93, "duration": 3.15}, {"text": "Ah,but after the linear kernel", "start": 3611.08, "duration": 1.8}, {"text": "the Gaussian kernel is probably the most widely used kernel, uh,", "start": 3612.88, "duration": 3.795}, {"text": "the one I wrote up there and this corresponds to", "start": 3616.675, "duration": 3.855}, {"text": "a feature dimensional space that is um, infinite dimensional.", "start": 3620.53, "duration": 5.61}, {"text": "Right. And, ah, this is actually- this particular kernel function,", "start": 3626.14, "duration": 5.04}, {"text": "corresponds to using all monomial features.", "start": 3631.18, "duration": 2.58}, {"text": "So if you have, ah, you know,", "start": 3633.76, "duration": 1.395}, {"text": "X one and also X 1,", "start": 3635.155, "duration": 1.905}, {"text": "X 2 and X 1 squared X 2 and X 1 squared X 5 to the", "start": 3637.06, "duration": 4.71}, {"text": "10 and so on up to X 1 to the 10,000 and X 2 to the 17.", "start": 3641.77, "duration": 5.76}, {"text": "Right. Whatever. Um, ah,", "start": 3647.53, "duration": 3.6}, {"text": "so this particular kernel corresponds to using", "start": 3651.13, "duration": 2.475}, {"text": "all these polynomial features without end going to arbitrarily high dimensional um,", "start": 3653.605, "duration": 5.64}, {"text": "by giving a smaller weighting to the very very high dimensional ones.", "start": 3659.245, "duration": 3.045}, {"text": "Which is why it's wide.", "start": 3662.29, "duration": 1.845}, {"text": "Yeah.", "start": 3664.135, "duration": 0.96}, {"text": "Okay.", "start": 3665.095, "duration": 0.975}, {"text": "Um, great.", "start": 3666.07, "duration": 9.81}, {"text": "So the, ah, kernel to end- toward the end,", "start": 3675.88, "duration": 2.82}, {"text": "I'll give some other examples of kernels.", "start": 3678.7, "duration": 2.01}, {"text": "Um, so it turns out that the kernel trick", "start": 3680.71, "duration": 2.655}, {"text": "is more general than the support vector machine.", "start": 3683.365, "duration": 3.405}, {"text": "Um, it was really popularized by", "start": 3686.77, "duration": 2.685}, {"text": "the support vector machine where you know researchers, ah,", "start": 3689.455, "duration": 3.255}, {"text": "because Vladimir Vapnik and", "start": 3692.71, "duration": 1.62}, {"text": "Corinna Cortes found that applying these kernel tricks to a support vector machine,", "start": 3694.33, "duration": 4.95}, {"text": "makes for a very effective learning algorithm.", "start": 3699.28, "duration": 2.445}, {"text": "But the kernel trick is actually more general and if you have", "start": 3701.725, "duration": 2.475}, {"text": "any learning algorithm that you can write in terms of inner products like this,", "start": 3704.2, "duration": 4.845}, {"text": "then you can apply the kernel trick to it.", "start": 3709.045, "duration": 2.655}, {"text": "Ah and so you- you play with this for a different learning algorithm in the ah,", "start": 3711.7, "duration": 3.81}, {"text": "in the programming assignments as well.", "start": 3715.51, "duration": 1.98}, {"text": "And the way to apply the kernel trick is,", "start": 3717.49, "duration": 2.19}, {"text": "take a learning algorithm write the whole thing in terms of", "start": 3719.68, "duration": 2.295}, {"text": "inner products and then replace it", "start": 3721.975, "duration": 2.475}, {"text": "with K of x z for some appropriately chosen kernel function K of x z.", "start": 3724.45, "duration": 5.76}, {"text": "And all of the discriminative learning algorithms we've learned so far,", "start": 3730.21, "duration": 4.485}, {"text": "um, ah, can be written in this way so that you can apply the kernel trick.", "start": 3734.695, "duration": 4.365}, {"text": "So linear regression, logistic regression,", "start": 3739.06, "duration": 2.16}, {"text": "ah, everything of the generalized linear model family,", "start": 3741.22, "duration": 2.55}, {"text": "the perceptron algorithm, all of the- all of those algorithms,", "start": 3743.77, "duration": 3.315}, {"text": "um, you can actually apply the kernel trick to.", "start": 3747.085, "duration": 2.97}, {"text": "Which means that you could um,", "start": 3750.055, "duration": 1.935}, {"text": "apply linear regression in an infinite dimensional feature space if you wish.", "start": 3751.99, "duration": 3.96}, {"text": "Right. Um, and later in this class we'll talk about principal components analysis,", "start": 3755.95, "duration": 5.01}, {"text": "which you've heard of but when we talk about principal components analysis,", "start": 3760.96, "duration": 2.715}, {"text": "turns out that's yet another algorithm that can be written only in terms of", "start": 3763.675, "duration": 3.285}, {"text": "linear products and so there's an algorithm called kernel PCA,", "start": 3766.96, "duration": 3.15}, {"text": "kernel principal component analysis.", "start": 3770.11, "duration": 1.59}, {"text": "If you don't know what PCA is, don't worry about it we'll get to it later.", "start": 3771.7, "duration": 2.16}, {"text": "But a lot of algorithms can be,", "start": 3773.86, "duration": 1.68}, {"text": "ah, married with the kernel trick.", "start": 3775.54, "duration": 2.46}, {"text": "So implicitly apply the algorithm even in an infinite dimensional feature space,", "start": 3778.0, "duration": 4.455}, {"text": "but without needing your computer to have", "start": 3782.455, "duration": 1.935}, {"text": "an infinite amount of memory or using infinite amounts of computation.", "start": 3784.39, "duration": 2.88}, {"text": "Ah, for this- actually", "start": 3787.27, "duration": 2.82}, {"text": "the single place this is most powerfully applied is the- is the support vector machine.", "start": 3790.09, "duration": 3.93}, {"text": "In practice I don't- in practice the kernel trick is", "start": 3794.02, "duration": 2.91}, {"text": "applied all the time for support vector machines and less often in other algorithms.", "start": 3796.93, "duration": 3.84}, {"text": "[NOISE] All right.", "start": 3800.77, "duration": 9.52}, {"text": "Um, [NOISE] any questions, before", "start": 3817.53, "duration": 2.84}, {"text": "we move on. No. Okay. [NOISE]", "start": 3820.37, "duration": 13.97}, {"text": "All right. So last two things I wanna do today, Um,", "start": 3834.34, "duration": 4.335}, {"text": "one is fix the assumption that we had made that the data is linearly separable, right?", "start": 3838.675, "duration": 8.955}, {"text": "Um, so, you know, uh,", "start": 3847.63, "duration": 4.695}, {"text": "sometimes you don't want your learning algorithm to have, uh,", "start": 3852.325, "duration": 2.925}, {"text": "um, zero errors on the training set, right?", "start": 3855.25, "duration": 3.27}, {"text": "So when- when you take this", "start": 3858.52, "duration": 1.35}, {"text": "low dimensional data and map it to a very high dimensional feature space,", "start": 3859.87, "duration": 3.24}, {"text": "the data does become much more separable.", "start": 3863.11, "duration": 2.25}, {"text": "Uh, but it turns out that if your data set is a little bit noisy, [NOISE]", "start": 3865.36, "duration": 18.24}, {"text": "right, if your data looks like this,", "start": 3883.6, "duration": 2.28}, {"text": "you've, maybe you wanted to find a decision boundary like that,", "start": 3885.88, "duration": 5.775}, {"text": "uh, and you don't want it to try so hard to separate every little example,", "start": 3891.655, "duration": 5.025}, {"text": "right, that's defined a really complicated decision boundary like that, right?", "start": 3896.68, "duration": 3.87}, {"text": "So sometimes either the low-dimensional space or in the high dimensional space Phi, um,", "start": 3900.55, "duration": 4.77}, {"text": "you don't actually want the algorithms to separate out your data", "start": 3905.32, "duration": 2.46}, {"text": "perfectly and- and then sometimes even in high dimensional feature space,", "start": 3907.78, "duration": 3.195}, {"text": "your data may not be linearly separable.", "start": 3910.975, "duration": 2.115}, {"text": "You don't want the algorithm to, you know,", "start": 3913.09, "duration": 2.16}, {"text": "have zero error on the training set.", "start": 3915.25, "duration": 2.625}, {"text": "And so, um, there's an algorithm called the L_1", "start": 3917.875, "duration": 4.275}, {"text": "norm [NOISE] soft margin SVM,", "start": 3922.15, "duration": 6.705}, {"text": "which is a, um,", "start": 3928.855, "duration": 1.53}, {"text": "modification to the basic algorithm.", "start": 3930.385, "duration": 4.245}, {"text": "So the basic algorithm was min over this, right,", "start": 3934.63, "duration": 5.745}, {"text": "subject to, [NOISE] okay.", "start": 3940.375, "duration": 8.595}, {"text": "Um, [NOISE] and so what the L_1 norm sub margin does is the following;", "start": 3948.97, "duration": 9.63}, {"text": "It says, um, you know,", "start": 3958.6, "duration": 2.4}, {"text": "previously this is saying that remember this is the geometric margin.", "start": 3961.0, "duration": 3.75}, {"text": "[NOISE] Right.", "start": 3964.75, "duration": 4.05}, {"text": "If you normalize this by the norm of w becomes- excuse me,", "start": 3968.8, "duration": 2.7}, {"text": "this is the functional margin.", "start": 3971.5, "duration": 1.365}, {"text": "Um, if you divide this by the norm of w it becomes the geometric margin.", "start": 3972.865, "duration": 4.515}, {"text": "Um, so this optimization problem was", "start": 3977.38, "duration": 3.41}, {"text": "saying let's make sure each example has functional margin greater or equal to 1.", "start": 3980.79, "duration": 4.59}, {"text": "And in the L_1 soft margin SVM we're going to relax this.", "start": 3985.38, "duration": 3.81}, {"text": "We're gonna say that this needs to be bigger than 1 minus", "start": 3989.19, "duration": 2.7}, {"text": "c. There's a Greek alphabet C. Um,", "start": 3991.89, "duration": 4.705}, {"text": "and then we're gonna modify the cost function as follows.", "start": 3996.595, "duration": 3.405}, {"text": "[NOISE] Where these c I's are greater than or equal to 0.", "start": 4000.0, "duration": 9.57}, {"text": "Okay. So remember, um,", "start": 4009.57, "duration": 2.22}, {"text": "if the function margin is greater or equal to 0,", "start": 4011.79, "duration": 2.445}, {"text": "it means the algorithm is classifying that example correctly, right?", "start": 4014.235, "duration": 3.63}, {"text": "So long as this thing is getting 0, then, you know,", "start": 4017.865, "duration": 3.495}, {"text": "y and this thing will have the same sign either both positive or both negative.", "start": 4021.36, "duration": 4.755}, {"text": "Uh, that's what it means for a product of two things to be greater than zero,", "start": 4026.115, "duration": 3.735}, {"text": "both things have to have the same sign, right,", "start": 4029.85, "duration": 2.625}, {"text": "and so if this is if- if, um,", "start": 4032.475, "duration": 2.85}, {"text": "so as long as this is bigger than 0,", "start": 4035.325, "duration": 2.205}, {"text": "it means it's classifying that example correctly.", "start": 4037.53, "duration": 2.325}, {"text": "Um, and the SVM is asking for it to not just classify correctly,", "start": 4039.855, "duration": 4.935}, {"text": "but classify correctly with the- with the functional margin of the- at least 1.", "start": 4044.79, "duration": 4.89}, {"text": "Um, and if you allow CI to be positive,", "start": 4049.68, "duration": 5.085}, {"text": "then that's relaxing that constraint.", "start": 4054.765, "duration": 3.225}, {"text": "Okay. Um, but you don't want the CIs to be too big which is why", "start": 4057.99, "duration": 3.99}, {"text": "you add to the optimization cost function,", "start": 4061.98, "duration": 4.079}, {"text": "a cost for making CI too big.", "start": 4066.059, "duration": 3.331}, {"text": "[NOISE]", "start": 4069.39, "duration": 0.36}, {"text": "And so you optimize this as function of W.", "start": 4069.75, "duration": 2.46}, {"text": "[NOISE]", "start": 4072.21, "duration": 1.08}, {"text": "And these are Greek alphabets c.", "start": 4073.29, "duration": 2.7}, {"text": "[NOISE]", "start": 4075.99, "duration": 1.32}, {"text": "Um, and if- if you draw a picture,", "start": 4077.31, "duration": 4.065}, {"text": "it turns out that, um,", "start": 4081.375, "duration": 5.055}, {"text": "in this example with that being the optimal decision boundary, um,", "start": 4086.43, "duration": 4.245}, {"text": "it turns out that these examples- [NOISE]", "start": 4090.675, "duration": 3.105}, {"text": "these three examples would be equidistant from this straight line, right?", "start": 4093.78, "duration": 3.63}, {"text": "Because if they weren't, then you can fiddle the straight line", "start": 4097.41, "duration": 2.37}, {"text": "to improve the margin even a little bit more.", "start": 4099.78, "duration": 2.55}, {"text": "It turns out that these few examples have,", "start": 4102.33, "duration": 2.425}, {"text": "um, functional margin exactly equal to 1.", "start": 4104.755, "duration": 2.695}, {"text": "And this example over there,", "start": 4107.45, "duration": 1.65}, {"text": "we have functional margin equal to 2,", "start": 4109.1, "duration": 1.56}, {"text": "and the further away examples of even bigger functional margins.", "start": 4110.66, "duration": 3.225}, {"text": "And what this optimization objective is", "start": 4113.885, "duration": 3.325}, {"text": "saying is that it is okay if you have an example here,", "start": 4117.21, "duration": 3.645}, {"text": "where functional margin so everything right so everything here has functional margin one.", "start": 4120.855, "duration": 6.825}, {"text": "If an example here I have functional margin a little bit less than one.", "start": 4127.68, "duration": 4.17}, {"text": "And this by having- by setting Ci to 0.5 say is", "start": 4131.85, "duration": 3.81}, {"text": "letting me [NOISE] get away with having function module lower than, less than 1.", "start": 4135.66, "duration": 3.63}, {"text": "[NOISE] Um, er, one other reason why,", "start": 4139.29, "duration": 8.64}, {"text": "um, you might want to use the L_1 norm soft margin SVM is the following,", "start": 4147.93, "duration": 5.189}, {"text": "which is, um, [NOISE] let's say you have a data set that looks like this.", "start": 4153.119, "duration": 3.901}, {"text": "[NOISE] You know,", "start": 4157.02, "duration": 5.01}, {"text": "seems like- it seems like that would be a pretty good decision boundary, right?", "start": 4162.03, "duration": 3.825}, {"text": "But, um, if we add just,", "start": 4165.855, "duration": 3.465}, {"text": "you know, measure a lot of examples, a lot of evidence.", "start": 4169.32, "duration": 3.12}, {"text": "But if you have just one outlier,", "start": 4172.44, "duration": 3.465}, {"text": "say over here, then technically the data set is still linearly separable, right?", "start": 4175.905, "duration": 8.025}, {"text": "[NOISE] If you really want to separate this data set,", "start": 4183.93, "duration": 3.63}, {"text": "um, sorry, I seem to be killing these pens myself as well.", "start": 4187.56, "duration": 3.66}, {"text": "[NOISE] All right.", "start": 4191.22, "duration": 1.485}, {"text": "If you want to separate out this data set,", "start": 4192.705, "duration": 2.34}, {"text": "you can actually, you know,", "start": 4195.045, "duration": 3.06}, {"text": "choose that decision boundary.", "start": 4198.105, "duration": 2.64}, {"text": "But the basic optimal margin classifier will allow the presence of one training example", "start": 4200.745, "duration": 5.475}, {"text": "[NOISE] to cause you to have", "start": 4206.22, "duration": 2.1}, {"text": "this dramatic swing in the position of the decision boundaries.", "start": 4208.32, "duration": 3.78}, {"text": "So they are, because", "start": 4212.1, "duration": 1.2}, {"text": "the original optimal margin classifier it optimizes for the worst-case margin,", "start": 4213.3, "duration": 4.725}, {"text": "the concept of optimizing for the worst-case margin allows one example by being", "start": 4218.025, "duration": 4.935}, {"text": "the worst case training examples have a huge impact on", "start": 4222.96, "duration": 3.15}, {"text": "your decision boundary and so the L_1 soft margin SVM,", "start": 4226.11, "duration": 3.975}, {"text": "um, allows the SVM to still keep the decision boundary closer to the blue line,", "start": 4230.085, "duration": 5.43}, {"text": "even when there's one outlier.", "start": 4235.515, "duration": 1.68}, {"text": "And it makes it, um,", "start": 4237.195, "duration": 1.5}, {"text": "much more robust outliers.", "start": 4238.695, "duration": 2.475}, {"text": "Okay. Um, [NOISE] and then if you go through the representer theorem derivation, uh,", "start": 4241.17, "duration": 9.84}, {"text": "you know, represent w as a function of the Alphas and so on, um,", "start": 4251.01, "duration": 3.045}, {"text": "It turns out that the problem then simplifies to the following;", "start": 4254.055, "duration": 4.32}, {"text": "So this is- I'm just [NOISE] right,", "start": 4258.375, "duration": 8.43}, {"text": "after some- some- after, you know,", "start": 4266.805, "duration": 2.055}, {"text": "the whole representing the calc- the whole represents a calculation, [NOISE] derivation.", "start": 4268.86, "duration": 8.91}, {"text": "[NOISE] This is just what we had previously.", "start": 4277.77, "duration": 5.55}, {"text": "[NOISE] I've not changed anything so far.", "start": 4283.32, "duration": 3.0}, {"text": "[NOISE] Right.", "start": 4286.32, "duration": 5.1}, {"text": "This is just exactly what we had.", "start": 4291.42, "duration": 2.71}, {"text": "Um, all right, and, uh- [NOISE]", "start": 4294.98, "duration": 4.3}, {"text": "And it turns out that, um,", "start": 4299.28, "duration": 2.295}, {"text": "the only change to this is we end up with an additional condition on the authorize.", "start": 4301.575, "duration": 8.19}, {"text": "So if- if you go for that simplification, uh,", "start": 4309.765, "duration": 2.91}, {"text": "now that you've changed the algorithm to have this extra term, uh,", "start": 4312.675, "duration": 3.315}, {"text": "then the- the- the new form- this is called the dual form with the optimization problem.", "start": 4315.99, "duration": 4.185}, {"text": "The only change is that you end up with this additional condition, right?", "start": 4320.175, "duration": 4.935}, {"text": "The, the constraints between Alpha are between 0 and C. Um,", "start": 4325.11, "duration": 6.84}, {"text": "and it turns out that, uh,", "start": 4331.95, "duration": 1.995}, {"text": "today there are very good, you know,", "start": 4333.945, "duration": 2.61}, {"text": "packages, uh, software packages which are solving that for you.", "start": 4336.555, "duration": 3.375}, {"text": "I- I- I- I think once upon a time we were doing machine learning,", "start": 4339.93, "duration": 3.435}, {"text": "you need to worry about whether your code for inverting matrices was good enough, right?", "start": 4343.365, "duration": 3.795}, {"text": "And when- when code for inverting", "start": 4347.16, "duration": 1.98}, {"text": "matrices was less mature there's just one thing you had to think about.", "start": 4349.14, "duration": 2.43}, {"text": "But today uh, linear algebra, you know,", "start": 4351.57, "duration": 2.64}, {"text": "packages have gotten good enough that", "start": 4354.21, "duration": 1.905}, {"text": "when you invert the matrix you just invert the matrix.", "start": 4356.115, "duration": 1.935}, {"text": "You don't have to worry too much- when you're", "start": 4358.05, "duration": 1.86}, {"text": "solving you don't have to worry too much about it.", "start": 4359.91, "duration": 2.28}, {"text": "So in the early days of SVM solving this problem was really hard.", "start": 4362.19, "duration": 3.3}, {"text": "You had to worry if your optimization packages were optimizing it.", "start": 4365.49, "duration": 2.85}, {"text": "But I think today there are very good numerical optimization packages.", "start": 4368.34, "duration": 3.06}, {"text": "They just solve this problem for you and you can just code without worrying", "start": 4371.4, "duration": 2.85}, {"text": "about the- the details that much. All right.", "start": 4374.25, "duration": 4.395}, {"text": "So this L1 norm soft margin SVM and, uh,", "start": 4378.645, "duration": 3.495}, {"text": "oh and so, um,", "start": 4382.14, "duration": 1.89}, {"text": "and so this parameter C is something you need to choose.", "start": 4384.03, "duration": 3.48}, {"text": "We'll talk on Wednesday about how to choose this parameter.", "start": 4387.51, "duration": 3.045}, {"text": "But it trades off um- how much you want to insist", "start": 4390.555, "duration": 3.165}, {"text": "on getting the training examples right versus you know,", "start": 4393.72, "duration": 3.345}, {"text": "saying it's okay if you label a few terms out of this one.", "start": 4397.065, "duration": 2.625}, {"text": "[NOISE] We'll- we'll discuss on Wednesday when we discuss bias and variance.", "start": 4399.69, "duration": 4.26}, {"text": "How they choose a parameter like c. All right.", "start": 4403.95, "duration": 6.33}, {"text": "So the last thing I want to- last thing I'd like you to see", "start": 4410.28, "duration": 4.71}, {"text": "today is uh just a few examples of um, SVM kernels.", "start": 4414.99, "duration": 4.74}, {"text": "Uh, let me just give um- all right.", "start": 4419.73, "duration": 4.785}, {"text": "So, uh, it turns out the SVM with the polynomial kernel,", "start": 4424.515, "duration": 4.035}, {"text": "uh, works quite well.", "start": 4428.55, "duration": 1.215}, {"text": "So this is, uh, you know k of x,", "start": 4429.765, "duration": 3.315}, {"text": "z equals x transpose z to the d. This thing is called a polynomial kernel,", "start": 4433.08, "duration": 6.375}, {"text": "um, and this is called a Gaussian kernel which is really", "start": 4439.455, "duration": 2.805}, {"text": "uh- the most widely used one is the Gaussian kernel.", "start": 4442.26, "duration": 3.57}, {"text": "Right. And it turns out that I guess early days of SVMs,", "start": 4445.83, "duration": 3.645}, {"text": "you know, one of the proof points of SVMs was, um,", "start": 4449.475, "duration": 2.925}, {"text": "the field of machine learning was doing a lot of work on", "start": 4452.4, "duration": 2.04}, {"text": "handwritten digit classification so that's", "start": 4454.44, "duration": 2.04}, {"text": "uh- so a- a digit is a matrix of pixels with values that are,", "start": 4456.48, "duration": 5.34}, {"text": "you know, 0 or 1 or maybe grayscale values, right?", "start": 4461.82, "duration": 3.21}, {"text": "And so if you take a list of pixel intensity values and list them,", "start": 4465.03, "duration": 3.18}, {"text": "so this is 0, 0, 0, 1, 1,", "start": 4468.21, "duration": 2.07}, {"text": "0, 0, 0, 0, 0, 1,", "start": 4470.28, "duration": 2.31}, {"text": "0 and just- this is all the pixel intensity values,", "start": 4472.59, "duration": 5.205}, {"text": "then this can be your feature X and you feed it", "start": 4477.795, "duration": 3.075}, {"text": "to an SVM using either of these kernels um,", "start": 4480.87, "duration": 3.225}, {"text": "it'll do not too badly, uh,", "start": 4484.095, "duration": 2.025}, {"text": "as a handwritten digit classification, right?", "start": 4486.12, "duration": 3.36}, {"text": "So there's a classic data set, um, called MNIST,", "start": 4489.48, "duration": 3.315}, {"text": "which is a classic benchmark, uh, in computing- uh,", "start": 4492.795, "duration": 2.325}, {"text": "in- in history of machine learning and, um,", "start": 4495.12, "duration": 3.255}, {"text": "it was a very surprising result many years ago that, you know,", "start": 4498.375, "duration": 3.495}, {"text": "support vector machine with a kernel like this", "start": 4501.87, "duration": 2.85}, {"text": "does very well on handwritten digit classification.", "start": 4504.72, "duration": 3.135}, {"text": "Uh, in the past several years we've found that deep learning algorithms,", "start": 4507.855, "duration": 3.21}, {"text": "specific convolutional neural networks do even better than the SVM.", "start": 4511.065, "duration": 3.15}, {"text": "But for some time, um, SVMs were the best algorithm uh,", "start": 4514.215, "duration": 3.555}, {"text": "and- and they're very easy to use in turnkey.", "start": 4517.77, "duration": 2.37}, {"text": "There aren't a lot of parameters to filter with.", "start": 4520.14, "duration": 1.65}, {"text": "So that's the one very nice property about them.", "start": 4521.79, "duration": 3.345}, {"text": "Um, but more generally,", "start": 4525.135, "duration": 3.475}, {"text": "uh, a lot of the most innovative work in SVMs has been into design of kernels.", "start": 4529.64, "duration": 6.1}, {"text": "So here's one example.", "start": 4535.74, "duration": 1.905}, {"text": "Um, let's say you want a protein sequence classifier, right?", "start": 4537.645, "duration": 8.835}, {"text": "So uh, uh protein sequences are made up of ami- of- of amino acids so,", "start": 4546.48, "duration": 6.045}, {"text": "you know, I guess a lot of our", "start": 4552.525, "duration": 2.055}, {"text": "bodies are made of proteins and proteins are just sequences of", "start": 4554.58, "duration": 2.58}, {"text": "amino acids and there are 20 amino acids, um, but, uh,", "start": 4557.16, "duration": 4.185}, {"text": "in order to simplify the description and really not worry too much of biology,", "start": 4561.345, "duration": 4.665}, {"text": "I hope the biologists don't get mad at me,", "start": 4566.01, "duration": 1.665}, {"text": "I'm gonna pretend there are 26 amino acids", "start": 4567.675, "duration": 2.115}, {"text": "even though there aren't because there are 26 alphabets.", "start": 4569.79, "duration": 1.74}, {"text": "So I'm gonna use the alphabets A through Z to denote amino acids", "start": 4571.53, "duration": 4.575}, {"text": "even though I know there's supposed to be only 20 but it's", "start": 4576.105, "duration": 2.235}, {"text": "just easier to talk with- with 26 alphabets.", "start": 4578.34, "duration": 2.595}, {"text": "And so a protein is a sequence of alphabets.", "start": 4580.935, "duration": 4.315}, {"text": "Right? Because a protein in your body is a sequence", "start": 4589.9, "duration": 3.76}, {"text": "that's made up of a sequence of amino acids and,", "start": 4593.66, "duration": 3.03}, {"text": "uh, amino acids can be very- variable length,", "start": 4596.69, "duration": 2.07}, {"text": "some can be very, very long, some can be very, very short.", "start": 4598.76, "duration": 2.175}, {"text": "So the question is,", "start": 4600.935, "duration": 2.095}, {"text": "how do you represent the feature X?", "start": 4603.03, "duration": 6.44}, {"text": "So it turns out- uh, and so, um,", "start": 4609.71, "duration": 3.025}, {"text": "the goal is to get an input x and make a prediction about this particular protein.", "start": 4612.735, "duration": 6.135}, {"text": "Like, what is the function of this protein, right?", "start": 4618.87, "duration": 2.775}, {"text": "And so- well, here's one way to design a feature vector which is, uh,", "start": 4621.645, "duration": 4.575}, {"text": "I'm going to list out all combinations of four amino acids.", "start": 4626.22, "duration": 7.74}, {"text": "You can tell this will take a while.", "start": 4633.96, "duration": 2.505}, {"text": "Right. Go down to AAAZ and then AABA and so on.", "start": 4636.465, "duration": 7.335}, {"text": "Uh, and eventually, you know,", "start": 4643.8, "duration": 1.92}, {"text": "there'll be a BAJT,", "start": 4645.72, "duration": 2.189}, {"text": "TSTA down to ZZZZ.", "start": 4647.909, "duration": 3.496}, {"text": "Right. Um, and then I'm gonna construct phi of", "start": 4651.405, "duration": 3.495}, {"text": "x according to the number of times I see this sequence in the amino acids.", "start": 4654.9, "duration": 7.29}, {"text": "So for example, BAJT appears twice.", "start": 4662.19, "duration": 5.085}, {"text": "So I'm gonna put 2 there um,", "start": 4667.275, "duration": 2.175}, {"text": "uh, you know, TSTA, oh whatever.", "start": 4669.45, "duration": 4.365}, {"text": "Right. Appears once so I'm gonna put a 1  there and there are no AAAAs,", "start": 4673.815, "duration": 4.575}, {"text": "no AAABs, no AAACs and so on.", "start": 4678.39, "duration": 2.04}, {"text": "Okay? So this is a- uh,", "start": 4680.43, "duration": 1.68}, {"text": "a 20 to the 4, you know,", "start": 4682.11, "duration": 4.02}, {"text": "26 to the 4, 20 to the 4-dimensional feature vector.", "start": 4686.13, "duration": 2.835}, {"text": "So this is a very, very high dimensional feature vector.", "start": 4688.965, "duration": 2.88}, {"text": "And it turns out that, um,", "start": 4691.845, "duration": 2.1}, {"text": "using some statistic as 20 to the 4 is 160,000.", "start": 4693.945, "duration": 3.885}, {"text": "That's pretty high dimensional.", "start": 4697.83, "duration": 1.47}, {"text": "Quite expensive to compute.", "start": 4699.3, "duration": 1.62}, {"text": "And it turns out that using dynamic programming,", "start": 4700.92, "duration": 4.29}, {"text": "given two amino acid sequences you can compute phi of x transpose phi of z,", "start": 4705.21, "duration": 5.055}, {"text": "that's K of x,z.", "start": 4710.265, "duration": 1.77}, {"text": "And there's a- there's a- there's a dynamic programming algorithm for doing this.", "start": 4712.035, "duration": 4.335}, {"text": "Uh, the details aren't important for first-year students,", "start": 4716.37, "duration": 2.31}, {"text": "uh, if any of you, um,", "start": 4718.68, "duration": 1.335}, {"text": "have taken an advanced CS algorithms course and learned", "start": 4720.015, "duration": 2.415}, {"text": "about the Knuth-Morris-Pratt algorithm,", "start": 4722.43, "duration": 2.955}, {"text": "uh, it's- it's- it's quite similar to that.", "start": 4725.385, "duration": 2.265}, {"text": "Uh, so it's Don Knuth, right, Stanford- Stanford professor, emeritus professor here.", "start": 4727.65, "duration": 4.035}, {"text": "So the DP algorithm is quite similar to that and um,", "start": 4731.685, "duration": 3.225}, {"text": "uh using this is actually quite um,", "start": 4734.91, "duration": 4.335}, {"text": "this is actually a pretty decent algorithm for inputting a sequence of, say,", "start": 4739.245, "duration": 4.305}, {"text": "amino acids and training a supervised learning algorithm to", "start": 4743.55, "duration": 3.06}, {"text": "make a call- binary classification on amino acid sequences.", "start": 4746.61, "duration": 3.435}, {"text": "Okay? So as you apply support vector machines one of the things you", "start": 4750.045, "duration": 2.865}, {"text": "see is that depending on the input data you have,", "start": 4752.91, "duration": 3.045}, {"text": "there can be innovative kernels to use, uh,", "start": 4755.955, "duration": 3.09}, {"text": "in order to measure the similarity of two amino acid sequences or", "start": 4759.045, "duration": 4.095}, {"text": "the similarity of two of whatever else and then to use that to um,", "start": 4763.14, "duration": 4.65}, {"text": "build a classifier even on very strange shaped object which,", "start": 4767.79, "duration": 3.9}, {"text": "you now, do not come,", "start": 4771.69, "duration": 1.455}, {"text": "um, as a feature.", "start": 4773.145, "duration": 1.98}, {"text": "Okay? So um, uh,", "start": 4775.125, "duration": 3.48}, {"text": "and- and I think actually- another example- or if the input x is a histogram,", "start": 4778.605, "duration": 4.544}, {"text": "you know, maybe of two different countries.", "start": 4783.149, "duration": 1.681}, {"text": "You have histograms of people's demographics it", "start": 4784.83, "duration": 2.61}, {"text": "turns out that there is a kernel that's taking the min of", "start": 4787.44, "duration": 3.12}, {"text": "the two histograms and then summing up to compute", "start": 4790.56, "duration": 2.76}, {"text": "a kernel function that inputs two histograms that measures how similar they are.", "start": 4793.32, "duration": 3.06}, {"text": "So there are many different kernel functions for", "start": 4796.38, "duration": 2.19}, {"text": "many different unique types of inputs you might want to classify.", "start": 4798.57, "duration": 3.015}, {"text": "Okay? So that's it for SVMs uh,", "start": 4801.585, "duration": 3.255}, {"text": "very useful algorithm and what we'll do on Wednesday is", "start": 4804.84, "duration": 4.08}, {"text": "continue with more advice on now the- all of these learning algorithms.", "start": 4808.92, "duration": 4.02}, {"text": "We'll talk about bias and variance to give you more advice on how to actually apply them.", "start": 4812.94, "duration": 4.11}, {"text": "So let's break and then I'll look forward to seeing you on Wednesday.", "start": 4817.05, "duration": 3.7}]