[{"vid": "tw6cmL5STuY", "cid": "Ugx5y6ZnzFHrTp1fUHh4AaABAg", "comment": "For any one who noticed the missing episode. This is the actual lecture 15. I hope @stanford online could add it https://www.youtube.com/watch?v=I_c6w1SJSJs&t=282s", "votes": "0", "replies": "", "reply": false}, {"vid": "tw6cmL5STuY", "cid": "Ugy0ujpldcyXxcuJTgd4AaABAg", "comment": "I wonder if there was any need for all the derivations starting from 56.00 to arrive the P(x_i) at 1.10.00. The moment you assume a model like x = mu + Lambda*z + epsilon, then X is distributed by P(X) which has a normal density with mean = mu and covariance =Lambda*Lambda^T + Psi. Then, the probability of a particular x_i is plugging in this into normal density formula. Then, it was argued that there is no closed form solution to mu, Lambda and Psi. I can agree on Lambda and Psi, but how about mu (mean)? I think this is not a correct statement. I think one can easily calculate the mean as simply the average of data points (x_1+...+x_M)/M for the considered uni-modular Gaussian model fit", "votes": "0", "replies": "", "reply": false}, {"vid": "tw6cmL5STuY", "cid": "Ugyl-23skODKhpc9CR54AaABAg", "comment": "actually i couldn't understand what did factor analysis do to combat the size problem of our data set. we assumed a latent variable to be a normal distribution and wrote our x in terms of z and added some noise and then predicted mu,sigma,psi to get the x. but conceptually what did factor analysis do", "votes": "2", "replies": "", "reply": false}, {"vid": "tw6cmL5STuY", "cid": "Ugz23ibO0eg3-r421-p4AaABAg", "comment": "Well, thanks for the recap. I finally know what the EM Algorithem does. For the Gaussian Mixture Model, we suppose that our data can be fitted by several Gaussian Distribution. And EM algorithm helps us learning the parameters of Gaussian. In E-step, we use a latent variable z to discribe which Gaussian Distribution is our sample point belongs to. The meaning of this step is to estimate which Gaussian Distribution does a sample point exactly belong to. So, we can get m multinomial distribution, denoted by Q_i(z^{(i)}). The Q_i describe the confidence(probabilty) that an exact sample point belongs to, which is used to assign weight in M-step. Then, in M-step, we can use the distribution of latent variable to maximize our loss function by taking first derivative of loss func. But, compared to taking first order on the primal loss function, M-step use Jensen's Inequality create a tight lower bound, which is easier than the primal one. With iteration, we can find the local optima of our goal.(Why global optima? I don't know. I should figure it out later)", "votes": "7", "replies": "2", "reply": false}, {"vid": "tw6cmL5STuY", "cid": "UgyIpHwnaSjcZEnIOjd4AaABAg", "comment": "Ivy league? This??!!??", "votes": "1", "replies": "1", "reply": false}, {"vid": "tw6cmL5STuY", "cid": "UgxGFhY_at0AzMgHCT54AaABAg", "comment": "Amazing", "votes": "0", "replies": "", "reply": false}, {"vid": "tw6cmL5STuY", "cid": "UgxIh5oV_kH28RiDsa14AaABAg", "comment": "52:50 hahaha", "votes": "4", "replies": "", "reply": false}, {"vid": "tw6cmL5STuY", "cid": "UgwGF9hqBu2MN1g-Mnh4AaABAg", "comment": "34:09 - Is this related to singular value decomposition?", "votes": "1", "replies": "", "reply": false}, {"vid": "tw6cmL5STuY", "cid": "Ugz23ibO0eg3-r421-p4AaABAg.A-Y-i4CGMJ0A-Y0PRNC_Zm", "comment": "oops, that should be \"why local optima?\" In the parenthesis.", "votes": "0", "replies": "", "reply": true}, {"vid": "tw6cmL5STuY", "cid": "Ugz23ibO0eg3-r421-p4AaABAg.A-Y-i4CGMJ0A3PUmYxlERi", "comment": "\u00a0@PRED122\u00a0 Because once you reach a local optima, the algorithm converges.", "votes": "0", "replies": "", "reply": true}, {"vid": "tw6cmL5STuY", "cid": "UgyIpHwnaSjcZEnIOjd4AaABAg.A-5-vZAc_svA2AlXHY7nLH", "comment": "Too easy for you?", "votes": "1", "replies": "", "reply": true}]