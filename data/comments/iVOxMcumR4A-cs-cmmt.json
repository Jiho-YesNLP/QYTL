[{"vid": "iVOxMcumR4A", "cid": "Ugxw-NghKz0nC0cUE8l4AaABAg", "comment": "Great Lecture", "votes": "0", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgyBT7wPj-xCZTLSu7V4AaABAg", "comment": "The only part I disagreed about is him saying \"It's not intuitive\" because he explained it intuitively well! Bias is like the arrogance of your model into thinking it's correct about something while Variance is its level of ADHD!", "votes": "1", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgzAmu0mB52DFVrhkg94AaABAg", "comment": "Great Lecture, really enjoy Anand's type of teaching: more logical and structured.", "votes": "1", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgyCSq4vmJT3h5wHcmB4AaABAg", "comment": "is there a corr btn bias an var? I mean if you increase H the hypothesis space, bias decreses but var increases and vice versa, but what of the cases where we have both high var and high bias?", "votes": "1", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "Ugwnx8mkOCtwDJaYY_l4AaABAg", "comment": "Great lecture!!!!", "votes": "3", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "Ugy3F89Q9UjMRMfB5Ap4AaABAg", "comment": "Great Lecture! Clarified some of the stuff in the notes and Lecture 8. Really wish he had more time to talk about the VC Dimension at the end", "votes": "2", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgwYhkn2yoOQk3qZD_R4AaABAg", "comment": "Very Informative Lecture, thaks!", "votes": "6", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgyW8V7hPPdcLaXiRxR4AaABAg", "comment": "Why do we calculate hoeffding's equality in application?", "votes": "0", "replies": "1", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgxPCuSuJWlu9eLaNtV4AaABAg", "comment": "Very Informative Lecture! Philosophy!", "votes": "2", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgzApvA4dqiVbzD-MKt4AaABAg", "comment": "memo: i didn't get the last half of the lecture. watch again later in the future", "votes": "15", "replies": "", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgwXBQsE_Mocj9-HRNZ4AaABAg", "comment": "I found this lecture (and the notes) a little disappointing in that they answer the question of \"How much data do I need for my model?\" with \"on the order of the number of parameters.\" This is great to know, but if we're actually going to apply learning theory to practicing machine learning, I think we really need to understand what the constant ratio of parameters to training examples should be. 1-to-1? 10-to-1? 1,000-to-1?  Andrew Ng mentioned in the previous lecture that a general rule of thumb is 10-to-1, but it would be nice to see a number like this actually worked out for a model. (I also get the feeling that number was based on experience of what works in practice than learning theory.) Anyone have any suggested resources?", "votes": "6", "replies": "1", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgyhfFO_Sf47or_RoGZ4AaABAg", "comment": "This lecture is hard to grasp :(", "votes": "12", "replies": "2", "reply": false}, {"vid": "iVOxMcumR4A", "cid": "UgyW8V7hPPdcLaXiRxR4AaABAg.9lkBr6PiuBzA2zGwvXsym6", "comment": "inequality and no", "votes": "0", "replies": "", "reply": true}, {"vid": "iVOxMcumR4A", "cid": "UgwXBQsE_Mocj9-HRNZ4AaABAg.9cfa6UIc5-r9fk-kjlhdpe", "comment": "These heuristics come from old school Learning Theory. With Deep Learning, where it is possible to pretrain models with large amounts of unlabelled data using self-supervised techniques, actual learning on the labelled examples wouldn't need anywhere near as many labelled examples as the number of parameters. For example, I have successfully trained and productionized NLP models like BERT which have more than 100 million parameters, using only 0.1M-1M labelled examples. Pretraining with unlabelled examples really biases the model parameters to lie near good local optima for the actual classification problems, and hence reduces the sample complexity drastically.", "votes": "8", "replies": "", "reply": true}, {"vid": "iVOxMcumR4A", "cid": "UgyhfFO_Sf47or_RoGZ4AaABAg.9bDyMzMD6jG9cxVvDI5dSQ", "comment": "this was one f the most important lectures, if you read about bias-variance trade-off in books it would be easier to grasp", "votes": "4", "replies": "", "reply": true}, {"vid": "iVOxMcumR4A", "cid": "UgyhfFO_Sf47or_RoGZ4AaABAg.9bDyMzMD6jG9dxGLUT6NjE", "comment": "Read up on the notes. It clarifies some things better", "votes": "5", "replies": "", "reply": true}]