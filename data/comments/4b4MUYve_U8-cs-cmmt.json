[{"vid": "4b4MUYve_U8", "cid": "Ugzobu4A2W5Td5mYs2Z4AaABAg", "comment": "When will practice lectures start ?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugyr7-3UVe12KrJO74d4AaABAg", "comment": "Is this man teaching aliens ?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwHKxYZJCJ5Wk5eWHZ4AaABAg", "comment": "where can i find lecture notes???", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwHhW1HKUUFH7SaFop4AaABAg", "comment": "12:57 44:00 54:00", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwOOEu7cNyTfHPyHHJ4AaABAg", "comment": "Linear regression and gradient descent are introduced as the first in-depth learning algorithm. The video covers the hypothesis representation, cost function, and optimization using batch and stochastic gradient descent. The normal equation is also derived as an efficient way to fit linear models.\r \r \r Highlights:\r 00:11 Linear regression is a fundamental learning algorithm in supervised learning, used to fit models like predicting house prices. The algorithm involves defining hypotheses, parameters, and training sets to make accurate predictions.\r           -Supervised learning involves mapping inputs to outputs, like predicting house prices based on features. Linear regression is a simple yet powerful algorithm for this task.\r           -In linear regression, hypotheses are defined as linear functions of input features. Parameters like theta are chosen by the learning algorithm to make accurate predictions.\r           -Introducing multiple input features in linear regression expands the model's capabilities. Parameters like theta are adjusted to fit the data accurately.\r 13:01 Linear regression involves choosing parameters Theta to minimize the squared difference between the hypothesis output and the actual values for training examples, achieved through a cost function J of Theta. Gradient descent is used to find the optimal Theta values for minimizing J of Theta.\r           -Explanation of input features X and output Y in linear regression, highlighting the importance of terminology and notation in defining hypotheses.\r           -Defining the cost function J of Theta in linear regression as the squared difference between predicted and actual values, leading to the minimization of this function to find optimal parameters.\r           -Introduction to gradient descent as an algorithm used to minimize the cost function J of Theta and find the optimal parameters for linear regression.\r 18:47 Gradient descent is a method used to minimize a function by iteratively adjusting parameters. It involves taking steps in the direction of steepest descent to reach a local optimum.\r           -Visualization of gradient descent involves finding values for Theta to minimize J of Theta, representing a 3D vector in 2D space.\r           -Gradient descent algorithm involves updating parameters Theta using the learning rate and the partial derivative of the cost function with respect to Theta.\r           -Determining the learning rate in practice involves starting with a common value like 0.01 and adjusting based on feature scaling for optimal function minimization.\r 27:26 Understanding the partial derivative in gradient descent is crucial for updating parameters efficiently. The algorithm iterates through training examples to find the global minimum of the cost function, adjusting Theta values accordingly.\r           -Explanation of the partial derivative calculation in gradient descent and its importance in updating parameters effectively.\r           -Expanding on the concept of gradient descent with multiple training examples and the iterative process of updating Theta values for convergence.\r           -Illustration of how the cost function J of Theta behaves in linear regression models, showing a quadratic function without local optima, aiding in efficient parameter optimization.\r 36:30 Gradient descent is a key algorithm in machine learning, adjusting parameters to minimize errors. It's crucial to choose the right learning rate to efficiently converge. \r           -Visualizing gradient descent with data points and parameter adjustments helps understand the algorithm's progression. \r           -Batch gradient descent processes the entire dataset at once, suitable for small datasets but inefficient for large ones due to extensive computations. \r           -The limitations of batch gradient descent in handling big data sets due to the need for repeated scans, leading to slow convergence and high computational costs. \r 44:58 Stochastic gradient descent updates parameters using one training example at a time, making faster progress on large datasets compared to batch gradient descent, which is slower but more stable.\r           -Comparison of stochastic and batch gradient descent. Stochastic is faster on large datasets but doesn't converge, while batch is slower but more stable. \r           -Mini-batch gradient descent. Using a subset of examples for faster convergence compared to one at a time in stochastic gradient descent.\r           -Importance of decreasing learning rate. Reducing steps size in stochastic gradient descent for smoother convergence towards the global minimum.\r 53:39 The normal equation provides a way to find the optimal parameters in linear regression in one step, leading to the global optimum without iterative algorithms. Linear algebra notation simplifies deriving the normal equation and matrix derivatives for efficient computation.\r           -The normal equation streamlines finding optimal parameters in linear regression, bypassing iterative methods for quick convergence to the global optimum.\r           -Utilizing matrix derivatives and linear algebra notation simplifies the derivation process, reducing complex computations to a few lines for efficiency.\r           -Understanding matrix functions mapping to real numbers and computing derivatives with respect to matrices enhances algorithm derivation and optimization in machine learning.\r 1:03:52 The video explains the concept of the trace of a matrix, its properties, and how it relates to derivatives in matrix calculus, providing examples and proofs. It also demonstrates how to express a cost function in matrix vector notation for machine learning optimization.\r           -Properties of the trace of a matrix are discussed, including the fact that the trace of a matrix is equal to the trace of its transpose, and the cyclic permutation property of the trace of matrix products.\r           -The video delves into the derivative properties of the trace operator in matrix calculus, showcasing how the derivative of a function involving the trace of a matrix can be computed and proven.\r           -The concept of expressing a cost function in matrix vector notation for machine learning optimization is explained, demonstrating how to set up the design matrix and compute the cost function using matrix operations.\r 1:15:15 The video explains the normal equations in linear regression, where the derivative is set to 0 to find the optimum Theta value using matrix derivatives, leading to X transpose X Theta equals X transpose y.\r           -Explanation of the normal equations in linear regression and setting the derivative to 0 to find the optimal Theta value using matrix derivatives.\r           -Addressing the scenario of X being non-invertible due to redundant features and the solution using the pseudo inverse for linearly dependent features.", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyeYZkjEBm-jIvZsdt4AaABAg", "comment": "The voice at 50:38, how is that possible?", "votes": "0", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwD0GQPtTWwuUxCrjN4AaABAg", "comment": "So I come from game development, and I'm simulating a super simplified version of the batch gradient descent in Unity3D just for fun, so I can visualize it. One thing I'm noticing is that, for each X input, the algorithm seems to gradually make h(x) match the exact Y values. So if all the Y plots look like a zig zag, the h(x) plots will just mold over that zigzag and copy it exactly instead of forming a line through it.  What am I doing wrong? Am I misunderstanding theta j?", "votes": "0", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzX6ihl26U-hm5263F4AaABAg", "comment": "Dr. NG is always my best.. keep up motivating with such classes.", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgykZJJwZmxG8cxA4BZ4AaABAg", "comment": "Where can we find the lecture notes for this course?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw19xfYGtWWZL6O-Hx4AaABAg", "comment": "Formula looks like variance formulae ,  will be interested to know why we have that 1/2 of the variances  of the lost of function. Could we just used the variance formula instead or is there a theory behind that. Thanks", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugxs8Cn4WCvGmuIr4ZV4AaABAg", "comment": "1:17:31 Can't we just get rid of the x transverse on both left sides of the equation. As I remember from linear algebra if you have the same matrix on two sides of the equation from the same side that is redundant and can be removed.  The result should be x(theta) =y  => (theta) = x^(-1) y", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwIkV095p2dS0Cjral4AaABAg", "comment": "How to study applications  ? This I only upto theory is it??", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxGZ3h22a-F1JGYPUR4AaABAg", "comment": "Where can I get the lecture notes? I can't access the files in the website.", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwcsCLlR2mZgjlEKkp4AaABAg", "comment": "27:03", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxCeTvkZoD9my7SZkJ4AaABAg", "comment": "The GOAT", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzNFSnBXlaQhYex81h4AaABAg", "comment": "anyone knows where to access the homework assignments as practice?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzCoUwch5CfOXLU0kp4AaABAg", "comment": "okay so the superscript i, ( 1 to m) represents the number of features, right? Because here m = 2 and I don't understand why m = # training examples", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugxqi8nwqg9RLTB0exd4AaABAg", "comment": "Hi, can a gentle soul explain to me why in the linear exampe of the house's price j=2 but in the visualization of the algorithm at 37:25 we have 4 iteration? should the number of iteration always be equal to the number of features?", "votes": "0", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwWONo9HuQ898bzz7d4AaABAg", "comment": "16:00", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugyw2hzARIyOCGCGmrp4AaABAg", "comment": "where can i get the full detail notes?? Anyone who knows this ,reply please.", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyeYZkjEBm-jIvZsdt4AaABAg.A7hKx3GDBN4A8BIfc0uFwr", "comment": "lmfao I was gonna comment it", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgyeYZkjEBm-jIvZsdt4AaABAg.A7hKx3GDBN4A8BIqLswVtZ", "comment": "it's probably microphone glitch (hopefully)", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwD0GQPtTWwuUxCrjN4AaABAg.A6legBQuzKZA6lhahTKRRq", "comment": "Oh, wait. I think I got it -- I was making two separate thetas FOR EACH input, when I should have only been using two thetas for the entire process. For some reason I thought each X vector had to have its own weight for house size and for #bedrooms.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugxqi8nwqg9RLTB0exd4AaABAg.A4g7MtJGaAFA53znspOs9N", "comment": "Number of iterations has no relation with the number of parameters to be calculated. But iteration will depend on :- Is there any possibility to decrease J(\u00d8) by varying the parameters (which is signified by gradient of J(\u00d8) with respect to parameters at a given point) if yes then do next iteration if gradient is zero then stop iteration.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugx9kmBHsA9Je7yyka54AaABAg", "comment": "\ud83c\udfaf Key points for quick navigation:\r \r 00:03 \ud83c\udfe0 Introduction to Linear Regression\r - Linear regression is a learning algorithm used to fit linear models.\r - Motivation for linear regression is explained through a supervised learning problem.\r - Collecting a dataset, defining notations, and building a regression model are important steps.\r 04:04 \ud83d\udcca Designing a Learning Algorithm\r - The process of supervised learning involves inputting a training set and outputting a hypothesis.\r - Key decisions in designing a machine learning algorithm include defining the hypothesis representation.\r - Understanding the workflow, dataset, and hypothesis structure is crucial in creating a successful learning algorithm.\r 07:19 \ud83c\udfe1 Multiple Features in Linear Regression\r - Introducing multiple input features in linear regression models.\r - The importance of adding additional features like the number of bedrooms to enhance prediction accuracy.\r - Notation, such as defining a dummy feature for simplifying hypotheses, is explained.\r 13:03 \ud83c\udfaf Cost Function and Parameter Optimization\r - Choosing parameter values Theta to minimize the cost function J of Theta.\r - The squared error is used in linear regression as a measure of prediction accuracy.\r - Parameters are iteratively adjusted using gradient descent to find the optimal values for the model.\r 24:18 \ud83e\uddee Linear Regression: Gradient Descent Overview\r Explanation of gradient descent in each step:\r - Update Theta values for each feature based on the learning rate and partial derivative of the cost function.\r - Learning rate determination for practical applications.\r - Detailed explanation of the derivative calculation for one training example.\r 27:11 \ud83d\udcc8 Gradient Descent Algorithm\r Derivation of the partial derivative with respect to Theta.\r - Calculating the partial derivative for a simple training example.\r - Update equation for each step of gradient descent using the calculated derivative.\r 33:11 \ud83d\udcc9 Optimization: Convergence and Learning Rate\r Concepts of convergence and learning rate optimization in gradient descent:\r - Explanation of repeat until convergence in gradient descent.\r - Impact of learning rate on the convergence speed and efficiency.\r - Practical approach to determining the optimal learning rate during implementation.\r 41:22 \ud83d\udcca Batch Gradient Descent vs. Stochastic Gradient Descent\r Comparison between batch gradient descent and stochastic gradient descent:\r - Description of batch gradient descent processing the entire training set in one batch.\r - Introduction to stochastic gradient descent processing one example at a time for parameter updates.\r - Illustration of how stochastic gradient descent takes a slightly noisy path towards convergence.\r 47:22 \ud83c\udfc3 Stochastic Gradient Descent vs. Batch Gradient Descent\r - Stochastic gradient descent is used more in practice with very large datasets.\r - Mini-batch gradient descent is another algorithm that can be used with datasets that are too large for batch gradient descent.\r - Stochastic gradient descent is often preferred due to its faster progress in large datasets.\r 53:01 \ud83d\udcc9 Derivation of the Normal Equation for Linear Regression\r - The normal equation allows for the direct calculation of optimal parameter values in linear regression without an iterative algorithm.\r - Deriving the normal equation involves taking derivatives, setting them to zero, and solving for the optimal parameters theta.\r - Matrix derivatives and linear algebra notation play a crucial role in deriving the normal equation.\r 57:52 \ud83e\uddee Matrix Derivatives and Trace Operator\r - The trace operator allows for the sum of diagonal entries in a matrix.\r - Properties of the trace operator include the trace of a matrix being equal to the trace of its transpose.\r - Derivatives with respect to matrices can be computed using the trace operator for functions mapping to real numbers.\r 01:12:49 \ud83d\udcc8 Linear Regression Derivation Summary\r - Deriving the gradient for the cost function J(Theta) involves taking the derivative of a quadratic function.\r 01:15:19 \ud83e\uddee Deriving the Normal Equations\r - Setting the derivative of J(Theta) to 0 leads to the normal equations X^T X Theta = X^T y.\r - Using matrix derivatives helps simplify the final equation for Theta.\r 01:17:09 \ud83d\udd0d Dealing with Non-Invertible X Matrix\r - When X is non-invertible, it indicates redundant features or linear dependence.\r - The pseudo inverse can provide a solution in the case of linearly dependent features.", "votes": "14", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyCnXoiWYM1e9JsXgp4AaABAg", "comment": "i think he did a mistake when he defined the cost function at 16:17 (for \"m\" training examples). He just gave 1/2 as the constant, which works fine for 1 training example. But i felt a bit weird to use this for m training examples. Its like we are adding \"m\" quantities and dividing by 2? shouldn't it be like an average? I searched google and it showed the formuale for cost function. It showed 1/2m as the factor. which makes sense. The 2 is just a trick so that while differentiating it cancels with the power (which is 2). the 2 in the denominator can be adjusted by the learning factor (alpha). but missing the \"m\" in th denominator doesn't feel right.   Can anyone please approve or disprove this??", "votes": "0", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxicY80-7vUUMtXJ-h4AaABAg", "comment": "He suddenly says he is going to do x. Eg minimize theta J but doesn\u2019t tell us why.  He just assumes we would know why. There are sooooooooo many times he does this. So within 30 seconds I get lost. Watch for five more mins and have absolutely no idea what he is doing or why.  A good teacher tells us WHY he is doing stuff.", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx-3qIdT3Dzq79N_Hx4AaABAg", "comment": "i am a student from a poor country, and i am trying to learn from the big universities, the problem is, the maths he is doing, i don't have any background about, even though i've taken statistics and linear algebra classes, like i understand only 10% of the equations, what should i do? how can i  become better?", "votes": "0", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxqRUy1A8LaxpUxhoF4AaABAg", "comment": "Are these youtube lectures same as those 3 modules uploaded on coursera website?", "votes": "0", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw14A6JY7oMfx2Gbbl4AaABAg", "comment": "Andrews Voice is Everything and that blue shirt of his", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugxe6wA6tYN_mVCMr2d4AaABAg", "comment": "Hi. Can anyone recommend any textbook that can help in further study of this course. Thank you", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxzTmqNvIi983WdkW94AaABAg", "comment": "Anyone know where to get the lecture notes for the lecture", "votes": "0", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxDOyBSi2wt9es32T14AaABAg", "comment": "i wish i had access to the problem sets for this course", "votes": "1", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwxZlIdxAQZT1SGVeN4AaABAg", "comment": "\uc2a4\ud0e0\ud3ec\ub4dc \uac00\uace0\uc2f6\ub2e4", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxEof62V3NTR7nHsZd4AaABAg", "comment": "where can i find the notes?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwApuSS82BZ2kygicR4AaABAg", "comment": "miss the sound quality \ud83d\ude15\ud83d\ude15", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxIfXR4mSza_eL4tsV4AaABAg", "comment": "at 40:10, how about if we set the initial value at a point that the gradient is a negative direction, then we should increase theta rather than decrease theta?", "votes": "1", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw4fIkBKMCjVf3oWiZ4AaABAg", "comment": "Hey there Stanford Engineering :hand-pink-waving:. I've uploaded some lecture notes and my own recording for this lecture. I also made a proof of the normal equation (using tensor summation convention).  https://drive.google.com/open?id=1Islzk2_ZE9rRz6sGitIIhDEFxTmW4i1N&usp=drive_fs https://drive.google.com/open?id=1IpFUqw0dL4E39bpup-pwT86XK__xTEbd&usp=drive_fs https://drive.google.com/open?id=1GwfZpooXIYkZ7qSms4yVRcXLc1G175GU&usp=drive_fs  Thank you to Andrew Ng & Stanford. :medal-yellow-first-red:", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwOdF3-BLLB5lYWhlJ4AaABAg", "comment": "Anyone learning here with me ...... Can we join for better understanding of concepts , a little discussion would be nice", "votes": "1", "replies": "3", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxkyfX51mig_pf7GNJ4AaABAg", "comment": "Had to study basic Calculus and Linear algebra at the same time to understand a bit, but don't get it fully yet,", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxAxmCN-B200Venm714AaABAg", "comment": "Is the lecture note available publicly for this? I have been going watching this playlist and I think the lecture note will be very helpful.", "votes": "1", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw4Y16vwTOvxzfsJkh4AaABAg", "comment": "has someone(possibly newbie like me) gone through all the videos and learnt enough to pursue an ML career or created a project? Wondering if a paid class should be taken or these free videos are enough.", "votes": "0", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx2L-8L3LBg2gnMwI94AaABAg", "comment": "Took me quite some time to realize this class was not being taught to darth vader", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyQolJ2RSu_LN0B6Ox4AaABAg", "comment": "One of the greats, a legend in AI & Machine Learning. Up there with Prof. Strang and Prof LeCun.", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx9kmBHsA9Je7yyka54AaABAg.A46ijntGiRlA6CJo6akwqx", "comment": "Thanx Bro for this!!", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgyCnXoiWYM1e9JsXgp4AaABAg.A4-EBgo8oCiA43S7Rl3faI", "comment": "Oh thanku so much you pointed out this I was having doubt in this", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgyCnXoiWYM1e9JsXgp4AaABAg.A4-EBgo8oCiA543mNKuUPG", "comment": "It doesn't matter if we introduce \"m\" in denominator or not. For a given dataset \"m\" is a constant value and the way of minimising variance which you mentioned is done by minimizing numerator only. The only contribution \"m\" and \"2\" will make is reduction of step size in each iteration which will make the computation longer.", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugx-3qIdT3Dzq79N_Hx4AaABAg.A3jF6aL1V0YA3jZqdde_mH", "comment": "Watch few videos of partial derivatives from khan academy, they are short and clear, and if you have trouble with normal derivatives and limits watch them first. That's basically all you need to understand this", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugx-3qIdT3Dzq79N_Hx4AaABAg.A3jF6aL1V0YA541jaNWqAX", "comment": "I think in the first video, the professor already mentioned the prerequisites. So before understanding this specific lecture you should have understanding of :-   1. Basics of statistics (upto variance to understand this lecture). For deeper understanding go for further topics including higher moments and their physical significance.  2. Understand topics like gradients, vector algebra and matrix operations (you can understand without matrix also but the only advantage of matrix is it makes calculation and representation simpler).  3. If you have programming background then great if not try to cover basics of programming. This lecture only have a small snippet of code which is understandable but if you are not getting that then you have to go for programming.  After this you can continue this lecture.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxqRUy1A8LaxpUxhoF4AaABAg.A3eMevdw3XLA3jt5nSyc4-", "comment": "Not really. I think this one dives deeper into the topic, while Coursera one is easy beginner-friendly introduction. For example Support Vector Machine Topic is not discussed on coursera.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxzTmqNvIi983WdkW94AaABAg.A38myeI4YlxA3YvttppEtg", "comment": "just search Stanford machine learning notes, one of the first result will be of pdf from the website cs229", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxDOyBSi2wt9es32T14AaABAg.A2qbiZXP8KnA3qa65CfyEA", "comment": "May be on github...", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxIfXR4mSza_eL4tsV4AaABAg.A1nLwaxWZ2VA4-CHbufZ0B", "comment": "even then we should decrease theta. Why? Reason: see the aim is to find a minima right? So if u start with a negative slope (aka gradient), u need to adjust the values of the parameters (theta) such that the slope approaches zero! (why? since the slope is zero at the minima). and if u see the graph of a quadratic equation, u will immediately understand the logic. it does not matter if u start with a pistive or negative slope. U just need to change theta so that finally ur gradient approaches zero. And for both of these cases we need to decrease the value of theta.", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwOdF3-BLLB5lYWhlJ4AaABAg.A1VMWolH4jnA1tqKXltIyM", "comment": "Where are you from?", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwOdF3-BLLB5lYWhlJ4AaABAg.A1VMWolH4jnA1txZuBMc6w", "comment": "\u00a0@manishcicada\u00a0 India , manish ji", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwOdF3-BLLB5lYWhlJ4AaABAg.A1VMWolH4jnA40Gs6qeGvU", "comment": "You must have finished the course by now", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxAxmCN-B200Venm714AaABAg.A16-gB2kfASA2hrEWQZhH8", "comment": "https://cs229.stanford.edu/main_notes.pdf", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugw4Y16vwTOvxzfsJkh4AaABAg.A0ol-qM22y-A2kL1A0Nhi5", "comment": "i also want to know have you gone through all the videos", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxYciTewF1ohcxKvfF4AaABAg", "comment": "58:00", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxUvsFHEMaZFgePPLN4AaABAg", "comment": "39:38 we're subtracting because to minimize the cost function, the two vectors must be at 180\u2070. So we get a negative from there.", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugys8VXTQa2OZPNVaJJ4AaABAg", "comment": "Why do the students sound like villains", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxsH0508N68vWiM6l14AaABAg", "comment": "\"Wait, AI is just math?\" \"Always has been\"", "votes": "8", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxnXL2W3TztROYSvQd4AaABAg", "comment": "https://cs229.stanford.edu/lectures-spring2022/main_notes.pdf", "votes": "5", "replies": "4", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw_Tuox5YgNJwhq3gx4AaABAg", "comment": "Where can I find the notes and other videos and any material related to this class!?", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzKjSDh_rfIT9VjqKx4AaABAg", "comment": "Does someone know how to get the lecture notes?  They are not available on stanford's website.", "votes": "2", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxJO5v67gHNE1jiirJ4AaABAg", "comment": "How come everybody could understand this? The first lecture was very good. But this one... I couldn't understand a single concept. I have watched half of the video and I don't know what is the meaning of Linear Regression. Professor Andrew Ng. was writing all those equations and I was questioning like what's the meaning of this? Why are we doing this? What's the use of this? I opened comment section and everybody is appreciating Professor's teaching skills and then there's me who couldn't unserstand anything in this video. Where am I going wrong? Why am I different than others?", "votes": "4", "replies": "7", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwGL8LpvFk-nzl6aat4AaABAg", "comment": "why are the student voices like ghosts?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyAzIwxkC-O35R4U894AaABAg", "comment": "Thank you to Stanford and Andrew for a wonderful series of lectures!", "votes": "4", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxzOPTM0tCJbYUzBRN4AaABAg", "comment": "where do I find the lecture notes? Help", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzS-gCzMuYmH_38Nkd4AaABAg", "comment": "is the explanation at 40:00 correct?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxtZYo1i3Eo4ed8e3x4AaABAg", "comment": "Does anyone know which textbook goes well with these lectures?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyjSGuiiBLEL53Nzl94AaABAg", "comment": "How can I implement this?? any references??", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzG6MO819vesbkrM194AaABAg", "comment": "cant download the course class note pls look onto ot", "votes": "2", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyV3VYTQmstitsSyKZ4AaABAg", "comment": "Wondering if lecture notes are also available to download from somewhere ?", "votes": "1", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgygfqlU39X73O9Z1DB4AaABAg", "comment": "why even go to uni, wtf this is so much better than my lectures and it's free and it's recorded lmao wtf unis be doing they are dying fr", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxkoBqUuNMzGKxk9Ll4AaABAg", "comment": "thank you", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugzt2Q0u2ElrxDXnPZt4AaABAg", "comment": "Could you please tell me the actual use of Gradient Descent by minimizing the y(theta)?", "votes": "0", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugy8p2e0il2OAxJXyN54AaABAg", "comment": "In the very last  equatin (Normal equation 1:18:06) Transpose(X)  appears on both sides of the equation, can't this be simplified by dropping transpose(T)?", "votes": "0", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxnXL2W3TztROYSvQd4AaABAg.9zvnEvw4-nJA1VM9CYYEr6", "comment": "Thank you man, I was looking for them all over....", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxnXL2W3TztROYSvQd4AaABAg.9zvnEvw4-nJA1VTH4EAF77", "comment": "no worries, but i am awoman btw :D \u00a0@kitsaruna\u00a0", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxnXL2W3TztROYSvQd4AaABAg.9zvnEvw4-nJA1WOQjO_AHW", "comment": "Nice to meet u awoman , myself Kitsaru and I am just started into machine learning or computer science in general, Wbu?  \u00a0@dimilands\u00a0", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxnXL2W3TztROYSvQd4AaABAg.9zvnEvw4-nJA3qaSNpzdiJ", "comment": "Thanks\ud83c\udf39", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzKjSDh_rfIT9VjqKx4AaABAg.9zq_m36eBCVA-ex7rqDy-Y", "comment": "Same issue for me alsoo....", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxJO5v67gHNE1jiirJ4AaABAg.9zaQ-BOFv6L9zvo2KOaF0D", "comment": "i dont know if i get your question about linear regression, but it is a way to finding a formula that would fit our data most accurately , so that our hypothesis is satisfied (i think)", "votes": "2", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxJO5v67gHNE1jiirJ4AaABAg.9zaQ-BOFv6L9zwdcUyt45o", "comment": "\u00a0@dimilands\u00a0 now I m learning Machine Learning from somebody else and he is teaching very very good. Andrew just sucks.", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxJO5v67gHNE1jiirJ4AaABAg.9zaQ-BOFv6L9zxkv7QIc-J", "comment": "may i ask who is your new 'source' ? \u00a0@Z_nix\u00a0", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxJO5v67gHNE1jiirJ4AaABAg.9zaQ-BOFv6LA3Kx7800NSm", "comment": "\u200b\u200b\u00a0@Z_nix\u00a0what's your source,I can't understand this lecture neither", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxJO5v67gHNE1jiirJ4AaABAg.9zaQ-BOFv6LA3L-1U_4qwi", "comment": "\u00a0@ponugotimanojkumar\u00a0 I'm learning it from YouTube channel 'Campus X'.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxJO5v67gHNE1jiirJ4AaABAg.9zaQ-BOFv6LA7IRSX8VDqg", "comment": "You just haven't seen anything worse.. \ud83d\ude01 I think this course is great and everything is pretty much crystal clear. You just have to rewatch some parts.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxJO5v67gHNE1jiirJ4AaABAg.9zaQ-BOFv6LA7ITsId9z9o", "comment": "\u200b\u00a0@Andrei_Bush\u00a0 I gave up \ud83d\ude1e\ud83d\ude4f", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgyV3VYTQmstitsSyKZ4AaABAg.9xp74k6Dz1T9zATos3qhc5", "comment": "hey bro I found them: https://cs229.stanford.edu/lectures-spring2022/main_notes.pdf", "votes": "4", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgyV3VYTQmstitsSyKZ4AaABAg.9xp74k6Dz1T9zqj4DeHPM2", "comment": "\u00a0@williambrace6885\u00a0thanks a lot!", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugzt2Q0u2ElrxDXnPZt4AaABAg.9wZU4RbS-A49xa46blR8w2", "comment": "Gradient Descent is basically the optimization model that help minimizing the cost of Model. We obtain the cost by calculating the MSE (Mean Squared Error)", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy8p2e0il2OAxJXyN54AaABAg.9wMy_S2Rfxz9zdZHCYReNA", "comment": "no because , x is neccesarily not a square a matrix", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgytovBItLf-JS8DVRN4AaABAg", "comment": "Loving the lectures!!", "votes": "2", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzU3JoisjV0tLjrG1J4AaABAg", "comment": "I am so lost with all the equation but i would love to learn. What course/class should i take to understand the logic behind these equations.pls help", "votes": "1", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgysZR4gZd4LCJfpzF54AaABAg", "comment": "is there thanks sitting in the class??", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx-yyJDxsJxDN15UmJ4AaABAg", "comment": "The pdf link to the problem set says Error Not found. Can someone help Please ?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxyKDfxra8-9cQxWrJ4AaABAg", "comment": "voice is just too low", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwRLeia61Ql6wJkPGZ4AaABAg", "comment": "Knowledge is power", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugxs-2LS2TMAyLNwPl14AaABAg", "comment": "This course saves my life! The lecturer of the ML course I'm attending rn is just going thru those crazy math derivations preassuming that all the students have mastered it all before\ud83d\ude02", "votes": "57", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugy6tPTFr-W-hkgqdnB4AaABAg", "comment": "The partial derivative was incomplete to me. we should take the derivative 2/2 thetha as well? is that term a constant?  shouldn't we go with the product rule!", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxFZRWTsAeQFWsh_N14AaABAg", "comment": "I need that lecture notes ASAP professor", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxC1Ph0LdqQcFKvue54AaABAg", "comment": "Thanks @stanfordonline for sharing this for free!", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzGK8XdpzccbiyCrIh4AaABAg", "comment": "0:41: \ud83d\udcda This class will cover linear regression, batch and stochastic gradient descent, and the normal equations as algorithms for fitting linear regression models.\r 5:35: \ud83c\udfe0 The speaker discusses using multiple input features, such as size and number of bedrooms, to estimate the size of a house.\r 12:03: \ud83d\udcdd The hypothesis is defined as the sum of features multiplied by parameters.\r 18:40: \ud83d\udcc9 Gradient descent is a method to minimize a function J of Theta by iteratively updating the values of Theta.\r 24:21: \ud83d\udcdd Gradient descent is a method used to update values in each step by calculating the partial derivative of the cost function.\r 30:13: \ud83d\udcdd The partial derivative of a term with respect to Theta J is equal to XJ, and one step of gradient descent updates Theta J\r 36:08: \ud83d\udd11 The choice of learning rate in the algorithm affects its convergence to the global minimum.\r 41:45: \ud83d\udcca Batch gradient descent is a method in machine learning where the entire training set is processed as one batch, but it has a disadvantage when dealing with large datasets.\r 47:13: \ud83d\udcc8 Stochastic gradient descent allows for faster progress in large datasets but never fully converges.\r 52:23: \ud83d\udcdd Gradient descent is an iterative algorithm used to find the global optimum, but for linear regression, the normal equation can be used to directly jump to the global optimum.\r 58:59: \ud83d\udcdd The derivative of a matrix function with respect to the matrix itself is a matrix with the same dimensions, where each element is the derivative with respect to the corresponding element in the original matrix.\r 1:05:51: \ud83d\udcdd The speaker discusses properties of matrix traces and their derivatives.\r 1:13:17: \ud83d\udcdd The derivative of the function is equal to one-half times the derivative of Theta multiplied by the transpose of X minus the transpose of y.\r Recap by Tammy AI", "votes": "215", "replies": "4", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyPYcYcgKiUOyw7qIt4AaABAg", "comment": "I love you Sir Andrew, you inspire me a lot haha", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwMI35ZMAwlt6ErVdN4AaABAg", "comment": "can anyone pls explain what do we mean by \"parameters\" that is denoted by theta here?", "votes": "1", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxBVLsDh72pnEhZhIl4AaABAg", "comment": "Very impressive. Somebody knows where are the lecture notes?", "votes": "0", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyZmnZPlNU0Av4zZ9l4AaABAg", "comment": "54:13 Normal Equation", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxvFL9CpUDk9yPbh1Z4AaABAg", "comment": "great except for the guy coughing every 15 seconds", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzsQ3wOhGX1c6Qefdh4AaABAg", "comment": "I asked ChatGPT how to learn machine learning.  #1 Coursera:     Course: \"Machine Learning\" by Andrew Ng (Stanford University)", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwH1ILUCT88fuanbe54AaABAg", "comment": "The notes from the description seem to have vanished. Does anyone have them?", "votes": "2", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx0A72vkL7tLETdgB14AaABAg", "comment": "why in cost function he did 1/2 and not 1/2*m ?", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzSqtbjNL4xTC2x_Vp4AaABAg", "comment": "8:50  notations and symbols\r 13:08  how to choose theta\r 17:50  Gradient descent 8:42 - 14:42 - Terminologies completion 51:00 - batch\r 55:00 problem 1 set\r 57:00 for p 0", "votes": "11", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzU3JoisjV0tLjrG1J4AaABAg.9w9n6IjMqDb9zaPU9yt4DQ", "comment": "same bro", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugxs-2LS2TMAyLNwPl14AaABAg.9vATIUhaIZfA5LWY29q1Ck", "comment": "My man was treating like these top % brains had forgotten simple partial differentiation and ours just don't even care\ud83d\ude22", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzGK8XdpzccbiyCrIh4AaABAg.9uVgK6GjnhG9uylyMD42o_", "comment": "How much we have to pay for your valuable overview on the entire class? Kudos to your efforts \ud83d\udc4d", "votes": "6", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzGK8XdpzccbiyCrIh4AaABAg.9uVgK6GjnhGA30Fyn2TQ2X", "comment": "Thank you so much \ud83d\udc4d\ud83e\udee1", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzGK8XdpzccbiyCrIh4AaABAg.9uVgK6GjnhGA4ssfO_Lfux", "comment": "Legend", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzGK8XdpzccbiyCrIh4AaABAg.9uVgK6GjnhGA6IgozioppH", "comment": "\u00a0@Lucky-vm9dv\u00a0 Bro didn't read the last line, Recap by Tammy AI\ud83d\ude42", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwMI35ZMAwlt6ErVdN4AaABAg.9u4d4HPZQZ69y25PsfOo9n", "comment": "Parameters are TRAINABLE numbers in the model such as weights and bias's, since the prediction of the model is based on some combination of weight and bias values.  So when 'parameters' of 'theta' are changed or 'trained', it means that the weights and bias's are changed or trained.", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxBVLsDh72pnEhZhIl4AaABAg.9tztKUhq1Z39u19hdrS8Mw", "comment": "There is a link in the video description, make sure you click where it says \"...more\"", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwH1ILUCT88fuanbe54AaABAg.9tawMEf42tm9taz--cYXL-", "comment": "same problem", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzSqtbjNL4xTC2x_Vp4AaABAg.9tY4qUMX94v9u7aoJ1dbxL", "comment": "notes are not available on the website ???", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugwf_ZsdFwKo3ZDK-aF4AaABAg", "comment": "Can I get notes for these lectures?", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugy_7opXrkDLpq5SFVV4AaABAg", "comment": "Thank you Stanford for this amazing resource. Pls csn i get a link to the lecture notes. Thanks", "votes": "3", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzYRdRS7ifFboQBIxp4AaABAg", "comment": "the best professor in the world.", "votes": "2", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugz5pOSXRya5rKw7oUF4AaABAg", "comment": "I didn't understand the linear regression algorithm is there any way to understand it better ??", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwhQ8I6q4SXTRInaaZ4AaABAg", "comment": "May I ask, down to 7:50 what does O (teta) represent?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzcvABSHO0hFkR_EN54AaABAg", "comment": "anybody know where the notes are? the link doesnt work for me", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugzc_T1Q9JYsWtA2acZ4AaABAg", "comment": "Andrew Ng, FTW!", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxA61noZE6SVM7vXFp4AaABAg", "comment": "Dude is a multi-millionaire and took valuable time meticulously teaching students and us. Legend.", "votes": "339", "replies": "4", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx1qmWhJ9rEuNzL8214AaABAg", "comment": "how to access the lecture notes:(. they have been removed from standford websites.", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxfI8Ve8e5bDlcfE_94AaABAg", "comment": "27:00", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxPCVqhg9Tf8APSvn54AaABAg", "comment": "This is really cool. \u2764", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx8jCGULAXq0M3O9cl4AaABAg", "comment": "Fantastic. Thank you deeply for sharing", "votes": "2", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxcyKbFqwtvEgB1hGx4AaABAg", "comment": "hey where can i get the notes?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwLN5OwkL87bGUGiZl4AaABAg", "comment": "Why aren't we using the usual  numerical methods(least squares) to fit a straight line to a given set of data points?", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwATnda_v5SXCD3a2l4AaABAg", "comment": "28:51, what is x0 and x1? If we have a single feature, say # of bedrooms, how can we have x0 and x1? Wouldn't x0 be just nothing? I'm confused. Or, in other words, if my Theta0 update function relies on x0 for the update, but x0 doesn't exist, theta0 will always be the initial theta0...", "votes": "0", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwEOKrC8lHM9pGXCD14AaABAg", "comment": "We learn, and teachers give us the information in a way that can help stimulate our learning abilities. So, we always appreciate our teachers and the facilities contributing to our development. Thank you.", "votes": "8", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx5Zza7MFe56G3huLx4AaABAg", "comment": "26:57", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxJ8byMHo7U2rFw-hl4AaABAg", "comment": "1.14.54 my answer is (X^T X\u03b8 )+(X^T \u03b8^T X)-(X^T Y)-(Y X^T) its same or my ans is wrong ?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxZMr7VWK3okctKT7d4AaABAg", "comment": "37:28", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx8SuMuz3zr2AKylgd4AaABAg", "comment": "8:42 - 14:42 - Terminologies completion  17:51 -- Checkpoint 57:00 - run1", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxA61noZE6SVM7vXFp4AaABAg.9rjHHoOS1qw9zR_EyhYMOS", "comment": "Bro needs to train his future employees", "votes": "27", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxA61noZE6SVM7vXFp4AaABAg.9rjHHoOS1qwA-4iOEqpmVY", "comment": "yes bro. i think the more people with the knowledge, the faster the breakthroughs in the field", "votes": "6", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxA61noZE6SVM7vXFp4AaABAg.9rjHHoOS1qwA-UwyLay_l7", "comment": "...and FOR FREE.", "votes": "4", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxA61noZE6SVM7vXFp4AaABAg.9rjHHoOS1qwA61PLEOz7Ne", "comment": "To people like him, money is really irrelevent. These people are really top 0.00001 of people of the world, all that matters to them is how they can contribute to their respective field and help make this world a better place, money is just by-product of that passsion.", "votes": "11", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwATnda_v5SXCD3a2l4AaABAg.9qYRS9gTC5W9rFVVKTISA3", "comment": "The value of x0 is always one 1.  So theta0 can rely on x0 for the update. If we have  single feature  then h(X) =x0*Theta0 + x1* theta1 (which is ultimately equal to theta0 + x1*theta1 as x0=1,  theta0 can also be referred as intercept and theta1 as slope  if you compare it with the equation of a straight line  such that price of house is linear function of  # of bedrooms)", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwATnda_v5SXCD3a2l4AaABAg.9qYRS9gTC5W9rGNo7GZDBS", "comment": "\u00a0@MahakYadav12\u00a0 thank you!!", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwJRRkl5BwbAdhpQ_V4AaABAg", "comment": "Attending Stanford University from Nairobi, Kenya.", "votes": "4", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugwq2mmGqvQ1Jxou6wV4AaABAg", "comment": "That feel when you need to pause the video every n-minutes and need to google the terminology coz highschool was too long ago", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugy7N_ehoXsVAPP_UdV4AaABAg", "comment": "why is it that the cost function has the constant 1/2 before the summation and not 1/2m?", "votes": "1", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgynoN8bgaDjFuzUgFN4AaABAg", "comment": "thanks a lot \u5434\u6069\u8fbe,i learned a lot", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzjBQ_jTR1VTjEc-BN4AaABAg", "comment": "Can you update the lecture notes and assignments in the website for the course? Most of the links to the documents are broken", "votes": "8", "replies": "3", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzT9SDZAL3U-G-olSt4AaABAg", "comment": "why do all the students sound like darth vader", "votes": "8", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw97RYlPTYVYcBy8lF4AaABAg", "comment": "my machine learning lecturer is so dogshit I thought this unit was impossible to understand. Now following these on study break before midsem and this guy is the best. I'd prefer that my uni just refers to these lectures rather than making their own", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw7ZRIbQ1zIrnhPes54AaABAg", "comment": "it's hard, but everything thats worth doing is", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwzHBVdSZcC3zdEqWx4AaABAg", "comment": "why are the background voices like that? i feel like im in backrooms...", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw7REZk27CP9knHj4t4AaABAg", "comment": "Cr\u00e8me de la cr\u00e8me ML course.", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugzl_475brWzjY6owW14AaABAg", "comment": "Very clear explanations. Extra points for sounding like Stewie Griffin", "votes": "5", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzYv7h4VqIHptvveH54AaABAg", "comment": "For course info google stanford cs229. Cs230 is also available.", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgybAXl3Fev-vTE4yp14AaABAg", "comment": "LU\u1eacT NH\u00c2N QU\u1ea2 KH\u00d4NG TR\u1eea 1 AI! \u0110\u1ea5t n\u01b0\u1edbc Vi\u1ec7t Nam ta v\u00e0 TQ n\u00fai li\u1ec1n n\u00fai, s\u00f4ng li\u1ec1n s\u00f4ng 1. Kh\u00f4ng bao gi\u1edd r\u1eddi nhau \u0111\u01b0\u1ee3c 2. C\u1ea7n s\u1ed1ng th\u1ef1c t\u1ebf 3. Kh\u00f4ng n\u00ean m\u01a1 h\u1ed3, \u1ea3o t\u01b0\u1edfng 4. O\u00e1n th\u00f9 kh\u00f4ng di\u1ec7t \u0111\u01b0\u1ee3c o\u00e1n th\u00f9 Ch\u1ec9 c\u00f3 t\u00ecnh th\u01b0\u01a1ng y\u00eau \u0111\u00edch th\u1ef1c di\u1ec7t m\u1ecdi o\u00e1n th\u00f9 5. K\u1ebb ngu si thi\u1ebfu tr\u00ed tu\u1ec7, nu\u00f4i o\u00e1n th\u00f9 = ph\u1ea3i ch\u1ecbu qu\u1ea3 \u0111\u1eafng cay! 6. Chi\u1ebfn tranh l\u00e0 k\u1ebb b\u1ecb li\u1ec7t tu\u1ec7. Th\u00f4ng minh nhi\u1ec1u m\u01b0u m\u00f4, kh\u00f4n ngoan l\u1eafm, oan tr\u00e1i nhi\u1ec1u! 7. Th\u01b0\u01a1ng y\u00eau, tha th\u1ee9 di\u1ec7t m\u1ecdi o\u00e1n th\u00f9!  \u0110\u00f3 l\u00e0 \u0111\u1ecbnh lu\u1eadt ng\u00e0n thu 8. \u0110\u1ea5t n\u01b0\u1edbc ta: +  H\u00e0i h\u00f2a v\u1edbi Trung Qu\u1ed1c + Th\u00e2n M\u1ef9 + Trung l\u1eadp, kh\u00f4ng theo phe ph\u00e1i n\u00e0o, kh\u00f4ng k\u00edch \u0111\u1ed9ng d\u00e2m d\u1ee5c, b\u1ea1o l\u1ef1c, o\u00e1n h\u1eadn ai + L\u00e0m b\u1ea1n v\u1edbi to\u00e0n th\u1ebf gi\u1edbi, h\u1ee3p t\u00e1c c\u00f9ng ph\u00e1t tri\u1ec3n, th\u01b0\u01a1ng y\u00eau L\u00e0o, cambodia, trung qu\u1ed1c l\u00e0 l\u00e1ng gi\u1ec1ng kh\u00f4ng th\u1ec3 r\u1eddi nhau, nh\u01b0 r\u0103ng v\u00e0 m\u00f4i.  + M\u01b0\u1ee3n ti\u1ec1n Nh\u1eadt ODA ph\u00e1t tri\u1ec3n \u0111\u1ea5t n\u01b0\u1edbc H\u00f2a B\u00ecnh mu\u00f4n n\u0103m, \u0111\u1ed9c l\u1eadp, t\u1ef1 ch\u1ee7, t\u1ef1 c\u01b0\u1eddng, d\u00e2n ch\u00fang hi\u1ec1n l\u00e0nh, kh\u00f4ng hung \u00e1c 9. \u0110\u1ea5t n\u01b0\u1edbc ta tr\u00ed tu\u1ec7, \u0111\u1ea1o \u0111\u1ee9c, th\u01b0\u01a1ng y\u00eau. S\u1ed1ng \u0111\u00fang lu\u1eadt nh\u00e2n qu\u1ea3, lu\u1eadt ph\u00e1p, l\u01b0\u01a1ng t\u00e2m 10. Lu\u00f4n ng\u0103n \u00e1c, di\u1ec7t \u00e1c, sinh thi\u1ec7n, t\u0103ng tr\u01b0\u1edfng thi\u1ec7n! 11. Th\u00e2u h\u1ebft tinh hoa kim c\u1ed5 l\u1ea1i, x\u00e2y cao v\u0103n hi\u1ebfn n\u01b0\u1edbc non nh\u00e0!l", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugy0Bz_D7EYpVzykGsV4AaABAg", "comment": "where can I download the lecture notes?", "votes": "1", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzxuxQXJ77ydxGRCxp4AaABAg", "comment": "Hey can I point out how an amazing teacher professor Andrew is?!  Also, I love how he is all excited about the lesson he is giving! It just makes me feel even more interested in the subject.   Thanks for this awesome course!", "votes": "55", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzBvCzzupgg0xClM5t4AaABAg", "comment": "Where do i get the assignments for these lecture series?", "votes": "3", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg", "comment": "when u paying 12k to your own university a year just so you can look up a course from a better school for free", "votes": "334", "replies": "8", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgydZJ6cFGMxWj0eU594AaABAg", "comment": "Which book is he using? and where do we find the homework?", "votes": "3", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzwCED2dEzHnx8_3Al4AaABAg", "comment": "If I was taking this course, I would be sh1tt1ng my pants thinking about how abstract the midterm will be.", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxISOgPtjMwWlM34AV4AaABAg", "comment": "Why do we take the transpose of each row, wouldn't it be stacking columns on top of each other?", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugy7N_ehoXsVAPP_UdV4AaABAg.9p2bTMs2yoy9qNpF2mRACh", "comment": "I think it's because he is taking one learning example and not m learning examples", "votes": "3", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy7N_ehoXsVAPP_UdV4AaABAg.9p2bTMs2yoy9qNqB43zVKP", "comment": "\u00a0@ihebbibani7122\u00a0 ah I see", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzjBQ_jTR1VTjEc-BN4AaABAg.9onN7yGU4Ja9ouZmXgHpVh", "comment": "Hi there, thanks for your comment and feedback. The course website may be helpful to you https://cs229.stanford.edu/ and the notes document https://docs.google.com/spreadsheets/d/12ua10iRYLtxTWi05jBSAxEMM_104nTr8S4nC2cmN9BQ/edit?usp=sharing", "votes": "20", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzjBQ_jTR1VTjEc-BN4AaABAg.9onN7yGU4Ja9phZv2a5-nd", "comment": "\u00a0@stanfordonline\u00a0 Where can I access the problem sets?", "votes": "3", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzjBQ_jTR1VTjEc-BN4AaABAg.9onN7yGU4Ja9q0c2Cz76Nx", "comment": "\u00a0@stanfordonline\u00a0 Please post this in the description to every video. Having this in an obscure reply to a comment will only lead to people missing it while scrolling.", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy0Bz_D7EYpVzykGsV4AaABAg.9n1OBRaRVt99n7p_tRwcou", "comment": "same question.. they're not available on the link in the description", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzxuxQXJ77ydxGRCxp4AaABAg.9n0mi-2hyE49qWFelyW-Rx", "comment": "Look at Coursera, he founded that and has many free courses.", "votes": "2", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg.9mQQyOiyqEu9nF69VXUF_2", "comment": "University cost needs to be as low cost as possible.", "votes": "5", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg.9mQQyOiyqEu9qlsx4b4TAV", "comment": "while youtube have the unlimited free information and courses better than the tech university and colleges \ud83d\ude42", "votes": "11", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg.9mQQyOiyqEu9uxkNpaehLn", "comment": "Hahahahaahaha fucking hell thats what i am doing right fucking now.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg.9mQQyOiyqEu9y_WuUP1LP_", "comment": "which uni is that...", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg.9mQQyOiyqEu9y_liB9UxjN", "comment": "\u00a0@preyumkumar7404\u00a0 University of Toronto", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg.9mQQyOiyqEuA-hpEM93bgy", "comment": "And the students create networks via this and can access to more broad opportunities than you as you didn't pay the money so they won't give you certificate and the facilities they created for their students.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg.9mQQyOiyqEuA4CMGTMIJyr", "comment": "You're just paying for the name, there is no promise of \"better\" education. Too many variables, no pun intended", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzAOKNCgJ7Kd2yX1ft4AaABAg.9mQQyOiyqEuA5cRHzEfD3G", "comment": "I pain 1 lakh per year", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxIyeQILGh0gz0jzjl4AaABAg", "comment": "Any one wondering about link not working. Here is the direct link to notes https://cs229.stanford.edu/notes2022fall/main_notes.pdf", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxKrnLMMLj61qBTGR54AaABAg", "comment": "difficult word : cost function gradient descent convex optimization  hypothesis fx target j of theta = cost/loss function partial derivatives chain row global optimum  batch gradient descent stochastic gradient descent mini batch gradient descent decreasing learning rate parameters oscillating iterative algorithm normal equation trace of a", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzNvih2BDP2v8-804J4AaABAg", "comment": "What's the difference between his course on coursera and the videos that are posted on here ?", "votes": "0", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugz8s4vYvoSdTgh3h2h4AaABAg", "comment": "Dear Dr. Andrew I saw yours other video with the cost function with linear regression by 1/2m but this video 1/2, so what is different between it?(footnote 16:00)", "votes": "1", "replies": "4", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwM4wyEP2xTwV_C9Ed4AaABAg", "comment": "8:50  notations and symbols 13:08  how to choose theta 17:50  Gradient descent", "votes": "45", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwtOInRFljYrBSMYsh4AaABAg", "comment": "Would anyone please share the lecture notes? On clicking on the link for the pdf notes on the course website, its showing an error that the requested URL was not found on the server. It would really be great if someone could help me with finding the class notes.", "votes": "3", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugx9uVXPAmbOZs0WhAF4AaABAg", "comment": "is this useful ?", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzISH4M8-nek2Fz9FJ4AaABAg", "comment": "Feels like sitting in stanford classroom from india ...Thanks stanford. you guys are best", "votes": "89", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwvOjPZuugW8-iT3St4AaABAg", "comment": "Where can I find the lecture notes? Thank you Edit: Reading through comments I got the answer:)", "votes": "2", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugz9-umKlxTC-NG0EMt4AaABAg", "comment": "this men is great teatcher", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugw2AV69viTml9jijNt4AaABAg", "comment": "where can I get the class notes?", "votes": "2", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxFPe7To-ZNg5dNdbF4AaABAg", "comment": "I am not good at math anymore, but I think math is simple if you get the right teachers like you. Tnks.", "votes": "28", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzTHLqdTNTvHdb0HvR4AaABAg", "comment": "How can I get the lecture papers pdf..?", "votes": "2", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwwjZSG7TGf2tE7ruR4AaABAg", "comment": "Thank you so much Dr. Andrew! It took me some time but your stepwise explanation and notes have given me a proper understanding. I'm learning this to make a presentation for my university club. We all are very grateful!", "votes": "28", "replies": "3", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyIBbNKiJOaNUpvaqN4AaABAg", "comment": "El wn escribe como el hoyo", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgydT8hkgXaxb4NDAfp4AaABAg", "comment": "if board is full, slide up the board, if it refuses to go up, pull it back down, erase and continue writing on it.", "votes": "3", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugz42lkicoSsazsqvMh4AaABAg", "comment": "Simple and understandable", "votes": "2", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugws9qj3Dpm6y6OiCU14AaABAg", "comment": "Very clear, but what I don't get is for the multiple data sets when I sum the errors, do I do two passes through the data and choose the error that is less?", "votes": "1", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxI2jzwBvgZU_Yz_Rh4AaABAg", "comment": "1:01:06 Didn't know Darth Vader attended this lectures", "votes": "8", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugwrzaijarbm9cbZz7V4AaABAg", "comment": "Wonder: Is m equals to n+1 \uff1fn stands for  number of inputs, while the m stands for the number of the rows which includes X0 in addition.", "votes": "0", "replies": "3", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzNvih2BDP2v8-804J4AaABAg.9lrnPdIHbrC9ltM5snArkX", "comment": "His Preksha, great question! These videos are lectures from the graduate course at Stanford. Here is a link to course if you are interested: https://online.stanford.edu/courses/cs229-machine-learning  His courses on coursera are more introductory than this graduate level course. Hope this helps, don't hesitate to get in touch with our team if you have more questions https://online.stanford.edu/contact-us", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzNvih2BDP2v8-804J4AaABAg.9lrnPdIHbrC9nF6DFwU91J", "comment": "Short answer: The coursera version is much easier", "votes": "2", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugz8s4vYvoSdTgh3h2h4AaABAg.9ljZSeHLgq29mCkkKuspEz", "comment": "I don't really understand what you mean by 1/2m. However, from my understanding, the 1/2 is just for simplicity when taking the derivative of the cost ftn the power 2 will be multiplied to the equation and cancellyby the half.", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugz8s4vYvoSdTgh3h2h4AaABAg.9ljZSeHLgq29nRDT_MjW7m", "comment": "It should be 1/2m where m is the size of the data set. That's because we'd like to take the average sum of squared differences and not have the cost function depend on the size of the data set m  https://youtu.be/ZzeDtSmrRoU  He explains it here at 6:30 minutes", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugz8s4vYvoSdTgh3h2h4AaABAg.9ljZSeHLgq29qXXR0TxoDr", "comment": "\u00a0@googgab\u00a0 It should be ok if J depends on m since m isn't changing?", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugz8s4vYvoSdTgh3h2h4AaABAg.9ljZSeHLgq29tZWcJBeEcu", "comment": "same question", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwM4wyEP2xTwV_C9Ed4AaABAg.9lHmF4KOdkB9rWSnp3UQno", "comment": "52:50 Normal equations", "votes": "2", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwtOInRFljYrBSMYsh4AaABAg.9l2zkrY2tykA0Mi_iWvwM9", "comment": "I think i found them here : chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cs229.stanford.edu/main_notes.pdf", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzISH4M8-nek2Fz9FJ4AaABAg.9kUr2pPEPlW9vqWcdcN0Ud", "comment": "for real bro, me sitting in panjab, would have never come across how the top uni profs are, this is surreal.", "votes": "5", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzISH4M8-nek2Fz9FJ4AaABAg.9kUr2pPEPlWA0VvK_M5dyY", "comment": "\u200b\u00a0@gurjotsingh3726\u00a0 Sat sri akaal, \u0a16\u0a41\u0a38\u0a3c\u0a15\u0a3f\u0a38\u0a2e\u0a24\u0a40", "votes": "3", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwvOjPZuugW8-iT3St4AaABAg.9kRxPNqeww_9mUIZpgyDnx", "comment": "Where could I get that?", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwvOjPZuugW8-iT3St4AaABAg.9kRxPNqeww_9u8vJCW7fIz", "comment": "\u200b@govardhansathvik5897 the links are broken :(", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugw2AV69viTml9jijNt4AaABAg.9jrPLrfmCCW9k3kVcdifzJ", "comment": "https://cs229.stanford.edu/notes2022fall/main_notes.pdf", "votes": "5", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzTHLqdTNTvHdb0HvR4AaABAg.9iujRCUaIAw9m2dc3UnnC4", "comment": "Google", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwwjZSG7TGf2tE7ruR4AaABAg.9itS3tLBssf9jFm8ukQnuU", "comment": "Hi I was not able to download the notes, 404 error, from the course page in description. Other PDFs are available on the course page. Are you enrolled or where did you download the notes from?", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwwjZSG7TGf2tE7ruR4AaABAg.9itS3tLBssf9jFpaSk6Tzj", "comment": "\u00a0@Amit_Kumar_Trivedi\u00a0 http://cs229.stanford.edu/lectures-spring2022/main_notes.pdf", "votes": "16", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwwjZSG7TGf2tE7ruR4AaABAg.9itS3tLBssf9nMRtfDtWgz", "comment": "\u00a0@anushka.narsima\u00a0 thanks", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugws9qj3Dpm6y6OiCU14AaABAg.9hrG0aW3Pol9hx6LfK2u8z", "comment": "Just continue changing theta till cost function reduces to optimal", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugws9qj3Dpm6y6OiCU14AaABAg.9hrG0aW3Pol9hx6XMIVLjz", "comment": "Yes the goal Is to reach less error and by tweaking theta you can achieve that and make sure you don't overshoot", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugwrzaijarbm9cbZz7V4AaABAg.9gQYmDrJouo9g_5E56QDHa", "comment": "n actually stands for the number of attributes here, or the number of features (columns)", "votes": "3", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugwrzaijarbm9cbZz7V4AaABAg.9gQYmDrJouo9hU2x5wwG7z", "comment": "No not necessarly m is the number of rows, and n is the number of column or features. In his example n is equal to two (Size, and  bedrooms), m can be any number. But i think that in the example m is 50", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugwrzaijarbm9cbZz7V4AaABAg.9gQYmDrJouo9kJ3jdxqccy", "comment": "@Louis Aballea yeah I got it. Thanks !", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzFq66j0_ez-doc3Dl4AaABAg", "comment": "Really easy to understand. Thanks a lot for sharing!", "votes": "19", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzESYL6lIhmDM6EwMZ4AaABAg", "comment": "I am new to ML field just any can tell me whether this course help me to solve modern day problems with Python", "votes": "1", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugz3M-VI1GSrDxZ24wN4AaABAg", "comment": "39:46 for tmr", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwocTuk2wuJkvUtgBR4AaABAg", "comment": "This course has nothing to do with practical practice", "votes": "2", "replies": "5", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugwogd610MujmAivIo54AaABAg", "comment": "Andrew Ng you are the best", "votes": "8", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugws3i9ZDU2BnEqhsEJ4AaABAg", "comment": "47:00 51:00 - batch 55:00 problem 1 set 57:00 for p 0", "votes": "13", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgxySNYJh2tYthCnybN4AaABAg", "comment": "I really don't have a clue about this stuff, but it's interesting and I can concentrate a lot better when I listen to this lecture so I like it", "votes": "28", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwWNm-ra-CtCwoTSWl4AaABAg", "comment": "Thank you!", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwgcUPk8yVm_gmUVdJ4AaABAg", "comment": "Andrew\u8bb2\u5f97\u592a\u597d\u4e86", "votes": "2", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzetdaG7_pW17_8PB94AaABAg", "comment": "Can we get access to class lecture notes? @Stanford Online", "votes": "9", "replies": "6", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyC8FImLJqIP1trjJt4AaABAg", "comment": "Seems like the lagrangian or path of least action theory in physics can be applied to algorythmic manipulations in machine learning as well as economics where isoquant curves and marginal analysis depend on many variables...not being an expert in any field the topics seem very similar and some corelation may exist...perhaps already being used.", "votes": "6", "replies": "1", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugzv6lliYUL1KO8lSs94AaABAg", "comment": "Fred has a one hundred sided die. Fred rolls the dice, once and gets side i. Fred then rolls the dice, again, second roll, and gets side j where side j is not side i. What is the probability of this event e? Assume the one hundred sides of the one hundred sided die all have an equal probability of facing up.", "votes": "3", "replies": "4", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugz50MxggCUurJPpdiJ4AaABAg", "comment": "Ask ur undergrads this question on final exam: 100 side dice. First roll side i. Second to twentieth roll not i. Twentyfirst roll number i. Call this event E. What is the probability of E occuring? Assume 100 side die is the uniform distribution of numbers 1, 2, 3, ..., 100.", "votes": "4", "replies": "2", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgyIegawskFvgsp2jEF4AaABAg", "comment": "\u0e2a\u0e38\u0e14\u0e08\u0e31\u0e14\u0e1b\u0e25\u0e31\u0e14\u0e1a\u0e2d\u0e01", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "Ugy6HFy80BT1Iidv_Uh4AaABAg", "comment": "We define a cost function based on sum of squared errors. The job is minimise this cost function with respect to the parameters. First, we look at (Batch) gradient descent. Second, we look at Stochastic gradient descent, which does not give us the exact value at which the minima is achieved, however, it is much much more effective in dealing with big data. Third, we look at the normal equation. This equation directly gives us the value at which minima is achieved! Linear regression models is one of the few models in which such an equation exist.", "votes": "73", "replies": "7", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgwCMqTL7ydyWt8dj014AaABAg", "comment": "Podemos dar la clase fuera?", "votes": "0", "replies": "", "reply": false}, {"vid": "4b4MUYve_U8", "cid": "UgzFq66j0_ez-doc3Dl4AaABAg.9g5SRTlBDmE9lyKXYNIVjO", "comment": "sure it is, it is high school topic, at least in Italy", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzFq66j0_ez-doc3Dl4AaABAg.9g5SRTlBDmE9mdPRxD2a8N", "comment": "\u00a0@massimovarano407\u00a0 I'm pretty sure multivariate calculus is not a high-school topic in Europe", "votes": "10", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwocTuk2wuJkvUtgBR4AaABAg.9djJV8Rij019dr8cbs78XL", "comment": "Yes even i am thinking the same", "votes": "2", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwocTuk2wuJkvUtgBR4AaABAg.9djJV8Rij019dwtWvpw5C8", "comment": "You need to know the math for practical practice.", "votes": "20", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwocTuk2wuJkvUtgBR4AaABAg.9djJV8Rij019dy8auacxqz", "comment": "\u00a0@vedanthbaliga7686\u00a0 But then you dont understand", "votes": "2", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwocTuk2wuJkvUtgBR4AaABAg.9djJV8Rij019kV7QEsieSC", "comment": "Thats like learning by heart without knowing the basics. You will fall to innovate in your work and just end up copying codes to do your work", "votes": "6", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgwocTuk2wuJkvUtgBR4AaABAg.9djJV8Rij019kudEq-mZST", "comment": "How are you going to use/Practical practice it without knowing how to implement it", "votes": "2", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxySNYJh2tYthCnybN4AaABAg.9cSqG4HcQ8x9ejTDw7sWf2", "comment": "You can see his lecture on coursera about Machine learning. You will surely get what he is saying in this video.", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgxySNYJh2tYthCnybN4AaABAg.9cSqG4HcQ8x9nF5x2WsleQ", "comment": "\u00a0@AHUMAN5\u00a0 yes, that course is beginner-friendly. Everyone with basic high school math can take that course even without knowledge of calculus.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzetdaG7_pW17_8PB94AaABAg.9blfO04Crzc9dDoumIhY67", "comment": "click on show more on the description of the video. the link to class notes is the last link.", "votes": "4", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzetdaG7_pW17_8PB94AaABAg.9blfO04Crzc9dpMYMU5zRt", "comment": "\u00a0@smn7074\u00a0 Does that still work for you? It says \"Not Found\" when I click on a pdf link.", "votes": "2", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzetdaG7_pW17_8PB94AaABAg.9blfO04Crzc9dpaxkzV1B8", "comment": "\u00a0@afmirror01\u00a0 some of the stuff still existed but there were things removed from that website.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzetdaG7_pW17_8PB94AaABAg.9blfO04Crzc9dwfLwtyCKh", "comment": "\u00a0@videowatching9576\u00a0 those aren't the notes for the class of autumn 2018", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzetdaG7_pW17_8PB94AaABAg.9blfO04Crzc9hjDoq8qI45", "comment": "Free at coursera", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgzetdaG7_pW17_8PB94AaABAg.9blfO04Crzc9irTyhZYxkC", "comment": "\u00a0@videowatching9576\u00a0 Tysm this has all the info!!! literally the entire course!!!!", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "UgyC8FImLJqIP1trjJt4AaABAg.9bVSabPVbug9nt1wBsFoY7", "comment": "Do you speak english?", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugzv6lliYUL1KO8lSs94AaABAg.9bTujzvCv289d8N0hLpIqj", "comment": "1 - (1/10000) = 9999/10000", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugzv6lliYUL1KO8lSs94AaABAg.9bTujzvCv289h0AEFCJFn4", "comment": "the probability of getting the same results for two rolls and they are both defined is 1/10000. So that we will subtract that from 1", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugzv6lliYUL1KO8lSs94AaABAg.9bTujzvCv289hFQE0RsFOD", "comment": "Wouldn't it be 99/100? The first roll can be any number so it doesn't really matter what's there. The second roll just needs to be one of the other 99 numbers. The first roll doesn't really change the probability. Of course, I barely know any math so I'm no expert lol", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugzv6lliYUL1KO8lSs94AaABAg.9bTujzvCv289pI7Tnve87T", "comment": "\u00a0@billr5842\u00a0 you're right, the probability calculated above as 1/10000 is the probability of getting the same result for a \"specific side\", like getting \"side 3\" twice. But there are 100 different sides that has the 1/10000 probability to occur twice, so the probability 1/10000 is multiplied by the different side number 100 which makes the probability of getting the same result for two rolls equal to 1/100. Then 1 - 1/100 = 99/100", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugz50MxggCUurJPpdiJ4AaABAg.9bTsjSIZQNv9bwQBiS4muE", "comment": "can you elaborate the question", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugz50MxggCUurJPpdiJ4AaABAg.9bTsjSIZQNv9fHHCpkFwwE", "comment": "This is ambiguous, if you specify an \"i\" beforehand, you are looking at: P(i on 1st roll) * P(not i on 19 rolls) * P(i on 21st roll) 0.01 * 0.99^19 * 0.01 = 0.00008261686238  if the i is specified on the first roll: P(not i on 19 rolls) * P(i on 21st roll) 0.99^19 * 0.01 = 0.008261686238", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy6HFy80BT1Iidv_Uh4AaABAg.9b-_PrFyWld9bucVMPUcIe", "comment": "I wish you sat next to me in class \ud83d\ude02", "votes": "12", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy6HFy80BT1Iidv_Uh4AaABAg.9b-_PrFyWld9dToMyh_Jle", "comment": "Bro who named that equation as normal equation?", "votes": "1", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy6HFy80BT1Iidv_Uh4AaABAg.9b-_PrFyWld9def3fbCfY-", "comment": "\u200b\u00a0@rajvaghasia9942\u00a0 the name \"normal equation\" is because generalizes the concept of perpendiculum  (normal to something means perpendicula to something). In fact \"the normal equation\" represent the projection between the straight line that i draw as a starting point (in the case of LINEAR regression) and the effective sampling data .This projection has , obviously , information about the distances between the real data (sampling data) and my \"starting line\"...hence to find the optimal curve that fit my data i 've to find weight a bias (in this video Theta0 , Theta1 and so on) to minimize this distance. you can minimize this distance using gradient descend (too much the cost), stochastic gradient descend (doing a set of partial derivative not computing all the gradient of loss function) or using the \"normal equations\"...uderstand?... Here an image from wikipedia to understand better (the green line are the famous distances) https://en.wikipedia.org/wiki/File:Linear_least_squares_example2.svg", "votes": "6", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy6HFy80BT1Iidv_Uh4AaABAg.9b-_PrFyWld9euNC6JiQy2", "comment": "\u00a0@rajvaghasia9942\u00a0 because we're in the matrix now bro! ha. For real though. It's about the projection matrix and the matrix representation/method of acquiring the beta coefficients.", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy6HFy80BT1Iidv_Uh4AaABAg.9b-_PrFyWld9euNp7viy30", "comment": "I have been wondering why we need such an algorithm when we could just derive the least squares estimators. Have you seen any research comparing the gradient descent method of selection of parameters with the typical method of deriving the least squares estimators of the coefficient parameters?", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy6HFy80BT1Iidv_Uh4AaABAg.9b-_PrFyWld9hbQzJYFEvb", "comment": "\u00a0@JDMathematicsAndDataScience\u00a0 GD is faster for largeer no of features", "votes": "0", "replies": "", "reply": true}, {"vid": "4b4MUYve_U8", "cid": "Ugy6HFy80BT1Iidv_Uh4AaABAg.9b-_PrFyWld9qI4leCAO1Z", "comment": "\u00a0@rajvaghasia9942\u00a0 Grant Normal, the guy who discovered it in 1942.", "votes": "0", "replies": "", "reply": true}]