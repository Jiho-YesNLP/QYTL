[{"vid": "osh80YCg_GM", "cid": "Ugyix4RI6xuVIRmzAMR4AaABAg", "comment": "@2:23 some nigga stood up , while prof strang is teaching , thats disrespectfull!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyEU9bjn6m3G0wPXm94AaABAg", "comment": "How come that equation comes? by multiplying A transpose? at 21.15", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugz3T3nfUyD7Oc9tHPF4AaABAg", "comment": "Very interesting lecture", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugxu3bKtIwRsa-ThGPN4AaABAg", "comment": "In the case of regression, why is the projection of a point on a regressed line connected by a vertical line (which is parallel to the y-axis) rather than a perpendicular e-line to the regressed line as shown in the chapter of projection?", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwI_NAoR5zD7Gbj_ad4AaABAg", "comment": "One of the best Linear Algebra content available online. Thank you professor Strang and MIT OpenCourseWare", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugx7NGsMHJwXs-T3oAJ4AaABAg", "comment": "Actually it is still hard to visualize why the projection leads to least square, I think those Bs are observed data points and form a hyper-plane, and you want to map their target values to that hyper plane. The closest (in terms of distance) is a projection to it", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugy4fbLQuqGBwTGRLGt4AaABAg", "comment": "I just don\u2019t understand why other mit recordings can\u2019t follow this masterpiece standard", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxR5i893KU3rQDkqGV4AaABAg", "comment": "In 22:50, in case someone is wondering why he tack on the columns like that, it is just out of convenience  to solve for \\hat{x} in A^TA\\hat{x} = A^Tb , you could just do it in a regular way by first multiply all the matrix out and make it in a form of A\\hat{x}=b , then solve it from there.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwk6gONjk9Js_bHJ5V4AaABAg", "comment": "In 7:28, in case someone is wondering why e=(I-P)b , you can derive it as e=b-Pb, which it only meant e=b-p.", "votes": "4", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugxjb_W-T21a2WKEewJ4AaABAg", "comment": "This lecture taught me that finding a projection matrix for the null space of idempotent matrices is the easiest easy clap ever", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyBpEtE437xK2BYKep4AaABAg", "comment": "That (I - P)b gets the component of b in the null space of P is just so... nice. Almost too nice. I - A is the orthogonal complement of A... Gives me the shivers, it's too elegant. How the hell did negating the matrix and adding 1 to the diagonal give me the whole orthogonal complement. The operation is so simple it makes you think there should be an obvious intuition behind it but there's none, even visually.   \"Point the vectors in the opposite direction and add 1 to some component.\" I don't see that this should obviously yield the orthogonal complement. How is it just I - P", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzDS_zJ6WlRq552TIl4AaABAg", "comment": "Professor Strang lost his mind.  He's supposed to be teaching linear algebra,  not magic", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugxu0rAkQf3Aoj7SFgt4AaABAg", "comment": "HES THE GOAT. THE GOAAAAAT", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzHJn8okLnuHaWVuTd4AaABAg", "comment": "Man, you are genius!!!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyDi4bfYfAXXETJl6F4AaABAg", "comment": "I'm watching this 27 years after I took a similar course in my university. Haven't seen linear algebra much during my career. Now when watching, everything seem much clearer to me. Strang is a really good lecturer.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyPDdqj2mu9vUROSTt4AaABAg", "comment": "Can someone explain where i am going wrong? Prof. Strang draws e as a vertical line (residual) but y is not a horizontal line. So, e cannot be perpendicular to y, and p =y. Hasnt Prof. Strang actually performed Deming/orhogonal Regression rather than OLS?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugxy4XADEWHWcVYreRd4AaABAg", "comment": "Professor Gilbert is going to make us a good space traveler\ud83d\ude02Thanks to MIT OCW and Professor Gilbert for bringing such great lectures to us:yougotthis:", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwR9HZ_w8n--q7R8ud4AaABAg", "comment": "There should be rapturous applause to finish this lecture....youth is wasted on the young \ud83d\ude09", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzW8apVH4DpnEg0egt4AaABAg", "comment": "Professor strange cracks me up \ud83d\ude02", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwkCjK2QXRwZ6pW1zR4AaABAg", "comment": "To anyone confused at 11:25, yes, he wrote the wrong b. Instead of (1 2 3) which he wrote, it is actually (1 2 2), otherwise (1 2 3) is a combination of the columns of A (0 times column 1 + 1 times column 2)", "votes": "1", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugy4fbLQuqGBwTGRLGt4AaABAg.9zqmW-U3S0dA-dBlEHyxw0", "comment": "Bro if you don't mind can you explain me at 29:0 min how 5/3  value comes -2/6", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyBpEtE437xK2BYKep4AaABAg.9zOW2HZEYLS9zOY4Yhndnm", "comment": "Ah I see why now, the trick is that you're making sure that 1 is added to different components. So any two vectors that are linearly dependent in the original column space become independent. Independent ones become dependent for intuitive reasons (you can always reduce the independent columns into a subset of columns in I and the negation of the original matrix is important). In the end you get as much independent columns as there used to be dependent columns. Makes sense.  Oh and it only works for idempotent matrices like the ones of the form of the projection matrix", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyPDdqj2mu9vUROSTt4AaABAg.9v2AzmiSEyx9v9Bx0wyIV_", "comment": "Ah, I understand where I was going wrong. I was not considering that we are projecting the whole vector, not individual data points. Also, we are only projecting b, while A is staying the same. Hence, the residual is the vertical distance.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgwkCjK2QXRwZ6pW1zR4AaABAg.9tJUp0J5qNs9uDzh-uxK7o", "comment": "thanks for the remark. i just started getting afraid when he said that b is not in the column space\ud83d\ude02", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugy3-jo1BUV_821dJul4AaABAg", "comment": "To anyone confused at 11:25, yes, he wrote the wrong b. Instead of (1 2 3) which he wrote, it is actually (1 2 2), otherwise (1 2 3) is a combination of the columns of A (0 times column 1 + 1 times column 2)", "votes": "1", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzNUbg7xpd85vy-jD54AaABAg", "comment": "This is amazing", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugxk_VMbZJvxzoA6MZ14AaABAg", "comment": "typo at 11:40, the b vector should be (1, 2, 2) instead of (1, 2, 3)", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyTJ14axuT9MSjw6Vh4AaABAg", "comment": "On 44:00 why A having independent columns means that all columns of A are independent? Why A can't have, say, 2 independent columns and 1 dependent column?", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugx7hJ09wKjAl9j8xo94AaABAg", "comment": "Beautiful lecture, just beautiful. Prof. Strang is drawing the beauty of Linear Algebra on a blackboard.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyTcj5odnPL-LXifC54AaABAg", "comment": "Hay qua\u0301!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugw6DFg-0KUtZ7vM01V4AaABAg", "comment": "Textbook problem13 . If e components of e=b\u2212Ax  averages to zero, then so does (  A^T * A)^(\u22121)  * A^T  *  e.   Why?", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzYuWJznUocwUMSzph4AaABAg", "comment": "Then can i write error matrix as 1 - the projection matrix?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgygFanbD9FeSaLLuRV4AaABAg", "comment": "I loved his sincerity when he thanks god @32:58", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzb1csTAP66TwFVncp4AaABAg", "comment": "Glad I decided to go all the way back to basic LA, such a great and thorough review", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwGGsuLsLtaegqRFvl4AaABAg", "comment": "confusing", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyF2TKH_NcjEjRSeIZ4AaABAg", "comment": "miyvarxar gilbert MIYVARXAR", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwj6ClsJqgOwnH172x4AaABAg", "comment": "so amazing!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugz0bmvD1npmMWl9h6Z4AaABAg", "comment": "17:54 Considering Prof. Strang's art is teaching, he is undoubtedly one of the greatest in the world!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxcBh_ejd1sRzgsnhR4AaABAg", "comment": "thank a lot", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwJQWsxEZGsaOhZUTh4AaABAg", "comment": "One thing that I cannot figure out if somebody can help me. The projection is supposed to be A times that x hat. but when we solve, he does A^TAx = A^Tb. But that is not p, p would be A times all of that, wouldn't it?", "votes": "1", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwiXLr-sOKJiluuQLl4AaABAg", "comment": "https://youtu.be/DDUC23useww", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgywvAFfZVRYC-C4m-d4AaABAg", "comment": "I finally understood OLS in econometrics, now I can say I comprehend what I'm doing, instead of mindlessly applying formulas and rules. Thank you verry much Mr Strang.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugw7QVXZU8B_SovU_UN4AaABAg", "comment": "Excellent , I was amazed to see the one to one correspondence between solving Ax=b when b is not in the column space of A and least squares fitting by a line when all the points doesn't exactly fit in a straight line", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwz61i_a4Do0t558Hh4AaABAg", "comment": "Notes: 1. For Ax=b, we can draw a picture that projection vector p plus e is equal to b. To solve linear regression problems we can calculate the A'Ax=A'b firstly. Then p=Ax, e=b-p. 2. If A has independent columns then A'A is invertible.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugy3-jo1BUV_821dJul4AaABAg.9tJUlqME-dm9w234N08FoO", "comment": "yes, you are correct, otherwise, it has a solution (0,1) but professor said no solution.", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzYuWJznUocwUMSzph4AaABAg.9l84roUMJMz9lrVWgC9W9z", "comment": "You could make it I-P, yes. If you call E the \"error\" matrix E=I-P, then Ex is the error. Kind of a weird matrix to define, though.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgwJQWsxEZGsaOhZUTh4AaABAg.9fHEUhHzZj59fQsBb51uN6", "comment": "I wonder if multivariable calculus is a prerequisite for this course. Could you tell me please", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxGvQBwBLz0FUurylx4AaABAg", "comment": "After watching this course, I wonder if the linear algebra I studied in college was of the same discipline...", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyQJNVoOF3czvJpbEV4AaABAg", "comment": "Learning math is so... delicious!! :D\r I'm not even a math genius. Thanks Prof. Strang, MIT, and YouTube.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugz-7JgKQ1YqysCxOIZ4AaABAg", "comment": "34:29~35:13 It is really helpful for me that he explicitly pointed that out.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzqt-NtxzsumTIpwZ14AaABAg", "comment": "learned this about 30 years ago at Technion Haifa. If I could only have such videos or Instructor then life would have been a breeze", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyGCQxXaguBrXh96xJ4AaABAg", "comment": "Very good", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyFcY4ytdMv4kM8cyl4AaABAg", "comment": "This is a lecture of the best of the best quality. Thrilling", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyIQ9TUN3EDWo2uMYx4AaABAg", "comment": "Well done Mr Gilbert, congratulations", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwaYvLtEP4nna1qhwp4AaABAg", "comment": "Could someone give me a clue why use A(T)AX(hat)=A(T)b to solve [C,D] at 21:05 ? To my understanding, AX(hat) cannot be b if we fit a line which points are P1, P2, P3 instead of b1, b2, b3 or [1 2 2]_t?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugy70eg1z9n1wt5NvOV4AaABAg", "comment": "Nothing new in my comment. EE Grad - did all of this math in undergrad. Don't remember any of it, and never developed an intuition for it.  This is so friggin' amazing!! Dr. Strang is a rockstar!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwieSep3hkzDBwDVKN4AaABAg", "comment": "What interesting lecture is this. you showed how maths is a pillars of statistics. during the lecture I think of the assumptions of least square estimation. it comes from maths (like the independency assumption). great work.  God bless you prof.", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzVi-q3Gfd6AFPOt3R4AaABAg", "comment": "Before this lesson, I liked linear algebra.  Now I LOVE IT!!", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwo6vzglQpzZFYmsah4AaABAg", "comment": "at 32:40...the professor says...oh god, come out correctly!  and a moment later, aha, thank you, God.  haha...", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxQOCwmstVlx3z6aYp4AaABAg", "comment": "Imagine giving this video a thumbs down. Who does that", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyQ3fJLdfy6ebZnjVx4AaABAg", "comment": "This is another great lecture by MIT Professor DR. Gilbert Strang. Least Squares put linear algebra into another world by itself.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyYigz1kP5CKHo0vmJ4AaABAg", "comment": "WE NEED MORE TEACHERS LIKE YOU Mr.Strang!!!! Regards a fellow student you never met", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzMfHzNdnghp7a2fYJ4AaABAg", "comment": "Listening to your voice has been my priority these days.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzQSsCSDuJUb32kAFZ4AaABAg", "comment": "I love it how he does not even need to explain it carefully, but everyone was already interested in Linear Algebra taught by him. He really encourages students to brainstorm other insights based on strong background that he could provide them.", "votes": "2", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxRPoPvtDSCNSv0BT94AaABAg", "comment": "With his lecture, I could sit in my desk all day and study. Math is so great.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwBE3SNxv61E3RAkQR4AaABAg", "comment": "I wonder where the students in these videos are now, 16 years later.", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwjGlkXRIDhw4KVs9R4AaABAg", "comment": "Sen nas\u0131l  bir krals\u0131n yaa.", "votes": "2", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwaYvLtEP4nna1qhwp4AaABAg.9UwT6UcRCQT9UwVwLyfHkN", "comment": "I got it. I messed up the two pictures. This is to solve a invertible squared matrix A(T)Ax= J, where J=A(T)b, and x=[C, D].", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgwjGlkXRIDhw4KVs9R4AaABAg.9NTxkVWAFPA9OIkZ0X_5Rx", "comment": "sjkhdjkshjj", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzhWsnxqLK2y0NAD-J4AaABAg", "comment": "Ahhh, \"deserving\" MIT students. Sleeping through the lecture. Noone to point out fast to the professor that he accidentally wrote 3 instead of 2 as the third coordinate of b. To professor Strang, my eternal gratitude for this wonderful series of gems and for showing us true beauty of linear algebra and math. Thank you so much.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzXhBp_DMDN72ioHDd4AaABAg", "comment": "I had horrible experiences with learning math in elementary school and since then, I've had a negative predisposition to it. This playlist is reversing that predisposition.", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwtDGSvEIq0xV1qg-54AaABAg", "comment": "His lectures are in an endless loop. He comes back to the statements that he has said earlier in the lecture.", "votes": "2", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxAocTQq3PfI6MhkxV4AaABAg", "comment": "Excellent lecture. Professor Strang is a legend.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugx9TM_VC8N1hMTJYzV4AaABAg", "comment": "Thank you professor, Thank you MIT.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzysJVeHaRfN-JsdHR4AaABAg", "comment": "I can't stress my thanks enough. Thanks for everything Prof Strang, MIT.", "votes": "3", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwq3-Z4h9ovSb-6Pbp4AaABAg", "comment": "28:14 I promise not to write another thing on this board xD :) Thank you so much Prof. your lectures are way more intuitive than my college Prof.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwXNQ7-RKYT2WUCKUN4AaABAg", "comment": "Thm. A vector is orthogonal to itself if and only if it is the zero vector. Proof. Backwards. Suppose there exists a vector x s.t. that is orthogonal to itself has a nonzero entry. Then xTx=summation xixi>=bi^2. Analog holds for negative bi. Which contradicts there existing a vector with nonzero entries which has a zero dot product. Forward. Trivial. Then Ax=0 If A has independent columns ie. is of full rank, then the only vector in the N(A)=[0]. \u0394", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugy_ITLg98VaA0cIx4l4AaABAg", "comment": "I can't stop watching these lectures...", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyPj1VMLlCXODiJbNZ4AaABAg", "comment": "Gilbert is really good at teasing the next lecture. I have to force myself to stop watching so I can sleep.", "votes": "2", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwSwm5y-ol4EbXPUht4AaABAg", "comment": "Rediscovering Linear Algebra again with Professor Strang! So intuitive with him.", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzOQAwZz_Wu7RTS0Th4AaABAg", "comment": "Wheres the applause?", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwh0_2SibXsDDVW3FB4AaABAg", "comment": "Given that column space and null space of a transpose are perpendicular to each other, if p is a projection of b on A then shouldn't it be parallel to N(At)? How can it be perpendicular to N(At) as well? any help pls", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwu2GwSEUHAQ5FDI0F4AaABAg", "comment": "36:48 I legitimately was wondering this. Thank Professor Strang for answering my questions from beyond the screen", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxUSjSHiVaxDDt_8FB4AaABAg", "comment": "Could anyone explain why solving A_T A  x_hat = A_T b is equivalent to using calculus? Specially how does the least square error fit in the first method?", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugw2M7zgSGm45wuSjxx4AaABAg", "comment": "Expertise your engineering knowledge here", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwq81QX8HdlNhPlvAZ4AaABAg", "comment": "This is the 3rd time I am taking these lectures in the last 2 years. Thank you, professor, these lectures are amazing.", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzzy0CcNU0pHw5ZOrN4AaABAg", "comment": "@34:03 Dr. Strang can read minds too!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugw9ZT1AizDZ0eg_lS94AaABAg", "comment": "He does not teach Linear Algebra, He teach us to see the MATH as an art form and tells us how to draw math, and admire its beauty.", "votes": "29", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwi2TKEcbJWD5fbrTB4AaABAg", "comment": "Least square approximation", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwh0_2SibXsDDVW3FB4AaABAg.9Gi27po89l69HjLFiozGRR", "comment": "The projection matrix sends b to an image vector that is in the column space of A, and because it is perpendicular to N(A^T), any vector in it is perpendicular to any other vector in N(A^T).", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugw9ZT1AizDZ0eg_lS94AaABAg.9DrakWoBo889EmgDgdaexa", "comment": "Yes absolutely dear", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzTjT9FhRM6NF89Iy94AaABAg", "comment": "An Indian edition of this book is available only in India http://www.wellesleypublishers.com/buy.html", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugz026A__j_NXqj28tZ4AaABAg", "comment": "I always hated linear algebra, but Prof. Strang makes it fun.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwu1y-7KUc8re88P9N4AaABAg", "comment": "this lecture is more immortal than lec. 15", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwjm2HrBhPmGhgxKCh4AaABAg", "comment": "You just cannot miss anything Prof.Strang wrote on the board", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzVm59yci9VxRe6U2d4AaABAg", "comment": "With lectures this good I can watch this instead of Netflix. I have one professor who also hold phenomenal lectures and lectures this good bring me as much joy or even more than playing a good video game or watching a good show. It is interesting and entertaining and it blows my mind. Truly a fantastic job! Thank you professor Strang!", "votes": "169", "replies": "4", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzcq3sABIartS9J6lJ4AaABAg", "comment": "48:04...\"Thank you, God\".  I love this man.", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyrV5eN0zRXng42Ivx4AaABAg", "comment": "How I-P gives projection matrix. Can anyone describe that?", "votes": "2", "replies": "3", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugx3psRF9aO9ncvm3Ix4AaABAg", "comment": "linear algebra is immortal", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugz89bNYZbEtUHrLkeF4AaABAg", "comment": "Thank you so much, MIT", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugznf9QlMN5C1AhNvS54AaABAg", "comment": "Here we are anyhow finding the best approximated solution by this method.  Then why can't we find the same approximated solution even when the rank(A) < n.  By following the same way.  Just by projecting the 'b' on C(A) and then solving.", "votes": "1", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugw3bMkyzAT4sJuRDkd4AaABAg", "comment": "he deserves better than this audience lol", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwaJvoMbCPuIJxpPB14AaABAg", "comment": "What do I do if I want to allow also some dependent column vectors in A so that AtA is not invertible? The projection onto the column space of A surely must still be unique. How do I find the projection matrix using my A in this case?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwTXEl-9KTTehluh4F4AaABAg", "comment": "that asian guy with the time stamps is gone now.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxvdDQoALlPd20Ff4Z4AaABAg", "comment": "Hi, I am just a beginner with not much knowledge.  Toward the End he is doing a proof stating that  xTAT=(Ax)T.  I believe A is a square matrix and x is just a column vector.  To me this doesn't make sense.  I don't see how you can see can perform such an operation X Transpose A Transpose.  Can someone explain?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzzSoeh-84hDYUt6PF4AaABAg", "comment": "I am econ Student and have studied Regression in my statistics class but never was able to understand how exactly was it connected to Nullspace and Column Space . Totally a new persepective  , Thanks a lot for this series Prof Strang and MIT", "votes": "2", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyA1G2Y19lCb8usBMZ4AaABAg", "comment": "The last ten minute discussion in a nutshell : If A has independent columns then Ax=0 only for x=0  Then multiply A^T on both sides of Ax=0  A^T Ax=0 only true for x=0 So no combination of columns of  A^T A gives 0 As A^T A is symmetric  No combination of rows of A^T A gives 0 hence A^T A is invertible bcz if combination of rows give 0 then determinant is 0", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzuxAX65nkqCy6Tj1t4AaABAg", "comment": "why the projection matrix that projects b onto N(A ^ T)  is I - P ? Can somebody please make me see that?", "votes": "2", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugww8xyoR2JAAZF9WDZ4AaABAg", "comment": "I had some trouble connecting the two pictures.  What helped me to understand the connection is rewrite the original equation as Ax = b = e + p.  That means we breaking down b into the error vector and its projection, p, onto A.  We find e1, e2, e3, which are the elements of the error vector, by solving for C and D such as e is in the null space of A transpose.", "votes": "2", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwqlNfBXQlASIngn414AaABAg", "comment": "22:36 : positive semidefinite, not positive definite", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugze59sGldUEYUJg1eB4AaABAg", "comment": "Just curious: is it a coincidence that the sum of the errors in the example is zero (-1/6 + 2/6 -1/6 = 0) ?", "votes": "0", "replies": "2", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzVm59yci9VxRe6U2d4AaABAg.9CDI1BrQ6vA9Emg6j2EXAk", "comment": "Really true dear", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzVm59yci9VxRe6U2d4AaABAg.9CDI1BrQ6vA9ZGSal7ZGQp", "comment": "Guys, let's watch Prof. Strang instead of watching dumb TV shows!!! (well.. I'm dumb too though!)", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzVm59yci9VxRe6U2d4AaABAg.9CDI1BrQ6vA9y0nVkglnDB", "comment": "These kinda people are scary bruh!!! Hats off to you for having this kinda motivation", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzVm59yci9VxRe6U2d4AaABAg.9CDI1BrQ6vA9zwX3V5ogdR", "comment": "\u00a0@starriet\u00a0 You're not dumb, the fact you watch Linear Algebra videos show you're interested in learning, and you are smart:)", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugzcq3sABIartS9J6lJ4AaABAg.9BuBd4OJ0aJ9EmgAiSXlRM", "comment": "Thank you God", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyrV5eN0zRXng42Ivx4AaABAg.9BA4eWS7svZ9BFLJLIpPiR", "comment": "Suppose we have Ax = b as the equation that we want to solve. p is the projection of b onto the column space of A. Let P be the projection matrix that maps vectors to their projections in the column space of A. (all this is notation from the lecture) A projection matrix is any matrix that takes a vector and maps it to the nearest vector in some fixed subspace. 1) e = b - p is a vector in the orthogonal complement of the column space of A, because Pe = P(b-p) = Pb -Pp = p - p = 0. Since the projection of e is 0, e is orthogonal to the column space of A (see 1:00) 2) Since b = p + e,  Ib = Pb + e so, e = Ib - Pb = (I-P) b 3) So, there is a matrix, namely I-P that maps b to a vector in the orthogonal complement of the column space of A (i.e. the left nullspace of A). It maps each vector to the closest vector in the left nullspace of A because it removes the component orthogonal to the left nullspace (namely, the component in the column space, that is p). Thus this is a projection matrix.", "votes": "3", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyrV5eN0zRXng42Ivx4AaABAg.9BA4eWS7svZ9BFLnntGQ47", "comment": "It's quite a clunky proof, anyone got any better ideas?", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyrV5eN0zRXng42Ivx4AaABAg.9BA4eWS7svZ9BNVxFLioq5", "comment": "\u200b\u00a0@shashvatshukla\u00a0 Pretty similar but here goes:   Well some observations are suppose P* is that projection matrix on the left nullspace, then we want P*b + Pb = b, in other words, (P* + P)b = b, which leads us to P* + P = I, or P* = I - P by construction.  Other things we can verify is if I - P acts on a vector in the left nullspace it return it unchanged: If Px = 0 then (I - P)x = x - Px = x  We can also verify that if I-P acts on a vector in the column space of A our original matrix, it should return 0 If Px = x then (I - P)x = x - Px = x - x = 0  We can also verify the property that acting (P*)(P*) = P* i.e. applying the projection twice should be the same as applying it once. (I - P)(I - P) = II + PP - IP - PI = I + P - P - P = I - P  We can also verify that it is symmetric as we should expect: (I - P)^T = I^T - P^T = I - P", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugznf9QlMN5C1AhNvS54AaABAg.994gNKPjGLh9BFMCndfCuc", "comment": "I think the difficulty is that multiple solutions will exist in that case. In the method we start with Ax = b, which is unsolvable. Then we convert b to p and get Ax = p which is solvable.  If A has rank n, then Ax = p is uniquely solvable, otherwise many solutions exist.  Also the projection matrix cannot be written simply because (A^T * A) would not be invertible.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgwaJvoMbCPuIJxpPB14AaABAg.98tctiCFW7p9BFMen6nDQo", "comment": "You could delete the dependent columns (finding them by Gaussian elimination) and then use the method in the lecture. This would neither change the column space of A nor the left nullspace of A. But it would make A invertible, and hence AtA invertible.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxvdDQoALlPd20Ff4Z4AaABAg.98BJfabkeeq9BNWMfV5PEv", "comment": "x is a column vector of size n, you can see it as a n x 1 matrix so xT is basically a 1 x n matrix A is a m x n matrix (not necessarily a square matrix btw) so AT is a n x m matrix   So the dimensional requirements for matrix multiplication work out just fine.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzuxAX65nkqCy6Tj1t4AaABAg.96x0ZaFQmEC97Y2Rw5OXRF", "comment": "Using algebra: p + e = b (Pb) + e = b e = b - Pb e = b(I - P) e = (I - P)b", "votes": "7", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugze59sGldUEYUJg1eB4AaABAg.95qla_-bBgY97nhoM6Lzmb", "comment": "No it's not. Think about it in terms of total lenght on each side of the line. There's equal lenght on both sides so it always equals to 0", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugze59sGldUEYUJg1eB4AaABAg.95qla_-bBgY9NebzY611-3", "comment": "\u00a0@TheTeehee11111\u00a0 thank you!", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugz_uIBSJb6pweYK5g54AaABAg", "comment": "\"please come out right\". \"oh yes!\" \"thank you god\" \ud83d\ude02", "votes": "70", "replies": "2", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwTUdLX6vdt-oY4cX94AaABAg", "comment": "respect for Prof.Strang", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxAdEN37hjlpvPsKv14AaABAg", "comment": "Beautiful lecture and amazing lecturer! Thank you Mr. Strang!", "votes": "1", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzRA-YJ2oxl7uUAHqV4AaABAg", "comment": "The best course of linear algebra. Thanks Prof. Strang!", "votes": "2", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugx_aHt3GODErkPVzKZ4AaABAg", "comment": "Ever great lectures, thanks professor Strang and MIT.Merci la vie!", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugy4gBBrDgFcTUP_CF54AaABAg", "comment": "thanks a lot", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzEBCCy_gjcT_kzR_J4AaABAg", "comment": "I love you so much. Thank you.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugx3wD3gPCQleWjr-sV4AaABAg", "comment": "How is e = I-P ?  I didn't get this part. is there some intuition behind this?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxCS3N6pU0iNo2vRSZ4AaABAg", "comment": "This was a really good lecture. It was packed with insights.  I love how everything is coming together.", "votes": "3", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyoKu_N-_UuqmmnHpt4AaABAg", "comment": "I suggest every colleges' linear algebra course using this course video. Prof Strang makes linear algebra so intuitive, interesting and easy to understand. He plots the pictures and tells you what's going on in the vector space and then he will go back to the theory to make you have a deep comprehension", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxLpOufZLrwpkvTfZl4AaABAg", "comment": "Could someone explain to me why do we wan't A^TA to be an invertible square matrix? at 22:00", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyBjIChQAz4oknNk_t4AaABAg", "comment": "Thank you Prof. Strang for changing the way to study maths rather than cramming now we not only just study matrices but can visualize itbecaue of you. Never seen a wonderful teacher like you.I hope I will meet you somday to show my gratitude. Highly recommended for everyone.", "votes": "3", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzQMTuWyLOjfMSN-WB4AaABAg", "comment": "professor Strang you are excellent. Thanks a lot to you and MIT for these lectures and to all the supporters of OCW.", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgypL_pAH6irJu8v4j54AaABAg", "comment": "Should he add x hat at 12:58?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxdYd2L0Gxt6WpJkRF4AaABAg", "comment": "At 5:38, what if b is not in column space nor perpendicular to column space?", "votes": "0", "replies": "2", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwV6z8ZSzIuQ2XJi_14AaABAg", "comment": "29:14 \u201cYou have to admire the beauty of this answer\u201d. \ud83d\ude02\ud83d\ude02", "votes": "14", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzYks1YaFoZBijbJSx4AaABAg", "comment": "I promise not to write another thing, but I can erase you know... lmao", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugxd4kLhjdh6Mc9m-B54AaABAg", "comment": "Nice explanation", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzxznJLfu3tBgyECQR4AaABAg", "comment": "i learned linear regression in a statistical lecture, but i think the linear algebra way of doing it is nicer and neater.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyOjgrociL1JY3momR4AaABAg", "comment": "so e being perpendicular to p means that e is truly the smallest difference between b and p", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugz_uIBSJb6pweYK5g54AaABAg.95qh38rn_V79Emg2pHelrX", "comment": "Good", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugz_uIBSJb6pweYK5g54AaABAg.95qh38rn_V79LFJvPq7znV", "comment": "I think it's that sort of personality that my teachers in school were missing. They didn't care about math at all.", "votes": "9", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxAdEN37hjlpvPsKv14AaABAg.94W6O2iqyFi9EmgMKvqGvw", "comment": "Thankyou", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzRA-YJ2oxl7uUAHqV4AaABAg.94Isv8SXvI79EmfxJtPkR2", "comment": "Absolutely dear", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugx_aHt3GODErkPVzKZ4AaABAg.943EbrPL1HB9EmgJ8Pzy_f", "comment": "Thanks and bless you", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugx3wD3gPCQleWjr-sV4AaABAg.91Zov0Ei35d91vwVNxLs3I", "comment": "e is not equal to I - P.  e = Pn*b and  Pn = I = P.  This is because  b = p + e p = P*b e = Pn*b b = I*b  b = p + e  =>  I*b = (P + Pn)*b  I  = P + Pn   Hence finally  Pn = I - P", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxLpOufZLrwpkvTfZl4AaABAg.9089K7XJYOu91vx8_D5hFX", "comment": "A^T*A is always a Square matrix.   We want it to be invertible because we want A to have full column rank, i.e. so that we can get a unique projection vector or UNIQUE \"best case solution\" for all b.  Also, P = A*(A^T*A)^-1*A^T , so for P to exist, we need A^T*A to be invertible", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgypL_pAH6irJu8v4j54AaABAg.8ySa1on2AH39-ZsFrQPYe2", "comment": "No, he is talking about the distance, i.e. the error or difference.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxdYd2L0Gxt6WpJkRF4AaABAg.8x4D855SNDe9-ZsuKnV1zx", "comment": "that's the idea of the lecture, you use the minimize error projection into the space created by the linear combination of the columns. With perpendicular he means in the left null space, a subspace so it has to include the origin.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxdYd2L0Gxt6WpJkRF4AaABAg.8x4D855SNDe9-Zt8yUVUFZ", "comment": "if perpendicular when you do the projection to b, you project into the origin, so Pb=0.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzWNvgifViV_4eYZjZ4AaABAg", "comment": "Maybe we can come up with a linear algebra hip-hop artist. We might call her/him PosDef.  me and my ho's, we like to transpose...", "votes": "4", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgypLg_qfh5-zLGDAp14AaABAg", "comment": "maybe using p for both the matrix P and the projection p is a bit unfortunate", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgytvUg78jmvjgPZGVB4AaABAg", "comment": "18:04 and 28:12  It proves Prof. Strang is a man of his words.", "votes": "74", "replies": "2", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzAUEOwlNtzwR2MVz14AaABAg", "comment": "Thank you.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxriirMgcWLtl3_JU94AaABAg", "comment": "is it 14:40 there is a mistake? should it be perpendicular to the line not vertical distance to the line?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyNlcM-StBW9GGAalR4AaABAg", "comment": "At around 18:00, why did he take p1, p2 and p3 on the same vertical line as b1, b2 and b3 respectively? Why not take them on the line perpendicular to the line we drew (I mean why not project them properly??) Bcos the line we drew is not perpendicular to the vertical. Help pls. Thanks :)", "votes": "2", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzq7LkfSnMHXdrRjVV4AaABAg", "comment": "Professor Strang you are the first person who makes me deeply understand linear regression from linear algebra's point of view.", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzui58sLR76C0r7uvl4AaABAg", "comment": "It does not seem like we are going for a perpendicular projection of the data points on the line. Rather we are taking error (e) in vertical directions. Is that still correct?", "votes": "2", "replies": "3", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugwn7sp0lsB_2RfWWCZ4AaABAg", "comment": "This video quitely provides the proof of assumption of regression that features have to be uncorrelated/independent.  I have read that only  in theory but now I can see exactly why. Thank you prof strang", "votes": "4", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzxpcrVe0ZI35s5M6t4AaABAg", "comment": "the man the myth the legend Gilbert Strang", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugx97d9B_uWhZMbLgM94AaABAg", "comment": "I think I am too lazy to use this. Probably will just use mv calculus. Great subject anyway and a very nice intuition to have.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxJVByHLV1ATOHsG2x4AaABAg", "comment": "11:34 made a typo in  b as it should be  [1, 2, 2] right?", "votes": "80", "replies": "7", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyDg_XdAezk-hQff2R4AaABAg", "comment": "Anyone have the intuition about why projection and least squares arrive at the same solution? My only piece of intuition is the notion of \"length\" is kind of \"squared\"", "votes": "1", "replies": "2", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugx_RBwup7Q4-aYzzf94AaABAg", "comment": "Great Lecture. Could anyone answer my question: I understand that error vector (b-p) is smallest when e is orthogonal to column space. What I am confused at is the squared error portion. Using projection, I thought what was solved is the Xhat that could minimize the error vector instead of error square. Could anyone help me clarifying this?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyaVNuxkHSLfpEi_eV4AaABAg", "comment": "bo\u011fazi\u00e7ililere selam olsun", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwLPc647XlPY2NE6lh4AaABAg", "comment": "MIT!! MIT!! \ud83c\uddfa\ud83c\uddf8\ud83c\udfc6\ud83d\udc4c\ud83c\udffc", "votes": "3", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxKuYQHUIwz-5mmZWx4AaABAg", "comment": "An alternative way of deriving the equation \u00a0If we have Ax=b with no solution. lets say c is the best solution so that Ac=proj(b on C(A)). then Ac-b is orthogonal to Ac so that the dot product of Ac-b and Ac is zero. But that can be writing in matrix form as (Ac)^t(Ac-b)=0 c^tA^t(Ac-b)=0, c^t(A^tAc-A^tb)=0 since we don't want c to be zero c^t can't be zero. that means A^tAc-A^tb=0 or A^tAc=A^tb", "votes": "1", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwHgaIeUl387UnupWN4AaABAg", "comment": "Amazing lecture!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgztioIm3rh1ITJ5GUx4AaABAg", "comment": "What's the second way to prove the AtA is invertiable", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgybjIDQW2W2X-h3jN54AaABAg", "comment": "@43:00 I laughed like crazy. I just felt like he was saying : '' Please God let these kids understand the one most basic thign of all this linear algebra. This is about to be on tape! ''. Mr Strang is a great professor I wish I had him as teacher. Learnign alot from this linear algebra onlien course. Thank you MIT for creating open courseware", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgytvUg78jmvjgPZGVB4AaABAg.8s3O_Dmmbhj9Emg-wZ9vvB", "comment": "He definitely is", "votes": "4", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgytvUg78jmvjgPZGVB4AaABAg.8s3O_Dmmbhj9ZGS52u4IRK", "comment": "That's what I wanted to say :)", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxriirMgcWLtl3_JU94AaABAg.8qfXvTzor528s3QkrAozP0", "comment": "It's the vertical distance. Consider C vs t axis. We cannot plot a 'straight line' for points b1, b2, b3 for the corresponding t1, t2, t3. So we plot 'approximate straight line' P with points p1, p2, p3 for t1, t2, t3.  Now, the approximate line is giving p1 instead of b1. So the error is p1 - b1, which is along the vertical line.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyNlcM-StBW9GGAalR4AaABAg.8q-dmDXkwKf8s3PC6Ibh5A", "comment": "I don't think I understood your question but I will try to help anyway. - b1, b2, b3 are not on any straight line. -p1, p2, p3 are on a line P = C + Dt . - b1 is e1 away from p1 (along the vertical axis or C axis ). Similarly for b2 and b3.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugzui58sLR76C0r7uvl4AaABAg.8pJ020L1Ai28qfXpynpt6p", "comment": "same doubt", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugzui58sLR76C0r7uvl4AaABAg.8pJ020L1Ai28qjqk6rIq4s", "comment": "I think he is minimizing error in Y direction (error in Y) = |(\"actual Y value\" - \"Y value given by our line\")| But I guess to minimize overall error we should go for perpendicular projections", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugzui58sLR76C0r7uvl4AaABAg.8pJ020L1Ai28qjr7CExvv9", "comment": "Thank you for the replies... I thought about it and realize that minimizing in y direction is kind of same as minimizing the perpendicular distance, since they are related by Pythagoreans theorem and restricted on a line with same slope.", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugwn7sp0lsB_2RfWWCZ4AaABAg.8oBRqZ-myw99z7ZnxleQhI", "comment": "Its mad that we dont learn statistics this way in class", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxJVByHLV1ATOHsG2x4AaABAg.8kSGIJCUPQ28ul1QGcSdmg", "comment": "yes", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxJVByHLV1ATOHsG2x4AaABAg.8kSGIJCUPQ296ierNI1wbl", "comment": "I think he makes these mistakes on purpose. Unbelievable there is almost no reaction from students.", "votes": "7", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxJVByHLV1ATOHsG2x4AaABAg.8kSGIJCUPQ298tc5xEzOu-", "comment": "Yes, and he got away with it because he remembered [1,2,2] instead of using the erroneous [1,2,3] for b that he put on the blackboard.", "votes": "3", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxJVByHLV1ATOHsG2x4AaABAg.8kSGIJCUPQ29CKjeQT6_24", "comment": "\u00a0@jurgenkoopman9091\u00a0 Maybe he wanted to inspect that if students were careful in the class", "votes": "2", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxJVByHLV1ATOHsG2x4AaABAg.8kSGIJCUPQ29Fqeg3vbY5l", "comment": "I thought I was going crazy since this is not the top comment? Like wait if even the comment section doesn't see it then I have nothing", "votes": "3", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxJVByHLV1ATOHsG2x4AaABAg.8kSGIJCUPQ29JIzlLmYA_4", "comment": "He did not use  his typo expression during the course of lecture later any where- and used [1,2,2] few steps later. Great man! STRANG the genius of a teacher.If there was Nobel  for teaching he and Walter Lewin(physics prof ) both should get it hands down.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxJVByHLV1ATOHsG2x4AaABAg.8kSGIJCUPQ2A6qLW0s6v3F", "comment": "\u00a0@francescocostanzo8225\u00a0 scroll a bit and see multiple comments about this issue", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyDg_XdAezk-hQff2R4AaABAg.8hkw9K9K_tW8itJfBjhJnD", "comment": "Just to make things clear, do you ask for an explanation/intuition for the equivalence of Least Squares (minimization subject to a linear constraint) and the vector projection method (finding the error vector orthogonal to the plane of spanned by the input vectors)?", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyDg_XdAezk-hQff2R4AaABAg.8hkw9K9K_tW8iv3TXd4zop", "comment": "Etsev Nevo yes, that's what I'm asking for", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugx_RBwup7Q4-aYzzf94AaABAg.8fgN_yLovCT91k3q0h2Xmv", "comment": "A year late, but: since the square is monotonic, minimizing the square minimizes the error.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgyaVNuxkHSLfpEi_eV4AaABAg.8fTuKXz3gGx8s3R4IIrtRL", "comment": "Tumne aisi bhasa istamal kiya jo mere samajh ke bahar hai toh mai aisi bhassa istamal karunga jo tumhare bas ke bahar hoga.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgxKuYQHUIwz-5mmZWx4AaABAg.8aKYUIJvvsk8q2nWUVQQsT", "comment": "why the restriction that c can't be 0 though? c can very well be 0 and the projection might have a 0 length?", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgztioIm3rh1ITJ5GUx4AaABAg.8_ccSl1fj0A8g4RtBLCbad", "comment": "A is MxN. Columns of A are independent => rank is N (M>N!). Rank(A)=Rank(A'A). So, Rank(A'A)=N, but A'A is NxN, so A'A is full rank => invertible.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugw5sZoR3PXEmi11zBt4AaABAg", "comment": "43:10 \"please say it\" ahhahah", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxlmkU-6BUvlbT5F-V4AaABAg", "comment": "This is like pure intellectual chocolate. Gilbert Strang should've taken over Wonka's Chocolate Factory, not Charlie.", "votes": "47", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugz-Vx8XC0OjoyK7rDx4AaABAg", "comment": "I am a graduate student majored in electrical and computer engineering. Though most of us have learned linear algebra in undergraduate study, I would like to highly recommend this course to those who are interested in machine learning and signal processing. Thank you Prof.Strang!", "votes": "379", "replies": "7", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugg7eTRa1fntXHgCoAEC", "comment": "Why wouldn't the error be the perpendicular distance to the line? in the video, the prof said it to be the vertical distance to the line.", "votes": "3", "replies": "3", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UghoDt5YrivZE3gCoAEC", "comment": "Is it me or Gilbert Strang's lecture delivery is \"similar\" to Frank Underwood's dialogue delivery ?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgiIsqL--cCZ73gCoAEC", "comment": "This is probably one of the best courses I have ever taken, Prof Gilbert Strang really rocks! Never though linear algebra can be this beautiful", "votes": "22", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UggWmdZxCzv95ngCoAEC", "comment": "I have a question, do we have to solve the psets by hand, or are we allowed to use a program like sagemath?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgiP_snV1atp7HgCoAEC", "comment": "It is crazy... really amazed... (tears drop)", "votes": "25", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugg3wBoLuO499ngCoAEC", "comment": "question, the vertical error of e1,e2,e3 does not represent the distance between b1,b2,b3 's projection on the line.", "votes": "3", "replies": "2", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UghqbzwdHp3u3XgCoAEC", "comment": "\"Make Bases Orthonormal Again!\"", "votes": "97", "replies": "3", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UghnWXdYKfsTFngCoAEC", "comment": "\"Oh god, come out right here!\" ... \"Thank you, God!\" XD I was dying at those parts of the lecture. Not only does he teach skillfully, he's hilarious too.", "votes": "34", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Uggdo2DVzSc_gHgCoAEC", "comment": "linear algebra is so fun in Prof Gilbert Stang 's hand.", "votes": "53", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugj3a2iS2uTsRXgCoAEC", "comment": "For the life of me I couldn't \"get\" the final proof, but now I think I get it. If (A^T A) has linearly independent columns, then A has linearly independent columns. The first is (A^T A)x = 0, and the latter is Ax=0. Remember that linear independence means that the only combination that goes to zero is the zero vector. If C(A) is linearly independent, then Ax=0 means x=0.", "votes": "5", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UggB9UVsaYpemngCoAEC", "comment": "I thought that the projection of the points onto the line (\"error\") was meant to be perpendicular to the line, and not a projection in the y-axis.", "votes": "9", "replies": "4", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Uggmsg-MS6adFngCoAEC", "comment": "He might correct it later, but as I am watching it: I'm afraid Prof. Strang made a tiny error (pun not intended) at about 12:00. According to my understanding the right-hand side of the equation should be (1 2 2)^T not (1 2 3)^T. Can anyone confirm this? Awesome lecture nevertheless.", "votes": "53", "replies": "7", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UghHF0zmObMLUngCoAEC", "comment": "At the last proof (A'A is invertible), why is he allowed to multiply both sides by a row vector? Shouldn't that change the solution set?  For example, suppose [x1 x2 x3 x4]' = 0. Then x = 0 vector. But if I multiply both sides by [1 -1 0 0], then [1 1 0 0]' becomes a solution. Should I assume that this operation can only add solutions, so the proof is still valid (since after the operation, only 0 is in the nullspace)?", "votes": "0", "replies": "2", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgjureDlaMLclHgCoAEC", "comment": "Dual Picture -> Mind = Blown", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugij-4gwYCNzhngCoAEC", "comment": "Why I-P at 8:22?", "votes": "9", "replies": "4", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UghQ8_OBYPocgHgCoAEC", "comment": "how do we estimate the percentage of error in the observations while making the projection approximation?", "votes": "0", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgjdpP2lmclDRngCoAEC", "comment": "Could you make a playlist of just proofs?", "votes": "4", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugz-Vx8XC0OjoyK7rDx4AaABAg.8VudxtSj1F88bed1LRx56Q", "comment": "same for me, I am a grad student too but I learn a lot from these lectures", "votes": "10", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugz-Vx8XC0OjoyK7rDx4AaABAg.8VudxtSj1F88khg5poYVGi", "comment": "I'm finishing college and I'm studying this to get into Machine Learning.", "votes": "10", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugz-Vx8XC0OjoyK7rDx4AaABAg.8VudxtSj1F88q-saPwuWrW", "comment": "I am also a graduate student majored in ECE. machine learning  and numerical linear algebra.", "votes": "4", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugz-Vx8XC0OjoyK7rDx4AaABAg.8VudxtSj1F892VbVhA0jJp", "comment": "Same here. The insights are invaluable - the lecture about projections finally clarified why a color calibration project I had during my undergrad didn\u2019t always work well. These lectures should be used to teach linear algebra everywhere where there\u2019s no really strong linear algebra classes, as image processing/ML tend to require way more command of linear algebra than what the common linear algebra college classes(talking about Texas Tech here) tend to offer.", "votes": "9", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugz-Vx8XC0OjoyK7rDx4AaABAg.8VudxtSj1F89OA8cwb3jF6", "comment": "Same I graduated with an ECE degree, but our curriculum didn't have linear algebra so I'm taking this in order to pursue a masters with a focus in machine learning. The intuition and guidance Prof Strang offers is really great!", "votes": "2", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugz-Vx8XC0OjoyK7rDx4AaABAg.8VudxtSj1F89fqdCbjbZsE", "comment": "\u00a0@johncarloroberto2635\u00a0 For intuition, have you seen these by 3Blue1Brown:  The Essence of Linear Algebra https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab  Neural Networks https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugz-Vx8XC0OjoyK7rDx4AaABAg.8VudxtSj1F89xBmvFDQLwe", "comment": "Revelation 20:14-15 (KJV)  And death and hell were cast into the lake of fire. This is the second death. And whosoever was not found written in the book of life was cast into the lake of fire.                             *     .                     .. . .             .   . .      *         ..   .. ..      . ..                        *                 .  ...         .   .  .                  ...  ..      .   ..  .                  ...  ..       .   .             *                        . ..   ..... .                            ... ..   ...        *                  ' '    '                              ' ' \"                *                       ^~^~ `i' ~^~^                      *~^~^~^~^~^                                                                                     *                                       *   ,                                   ,                         ,", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugg7eTRa1fntXHgCoAEC.8VKeCDlSQEn8iqZ4zjbcPL", "comment": "Hello, I think this is because the regression line is drawn in the 2D space which is the row space of A. The minimization/projection is carried out in the column space of A.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugg7eTRa1fntXHgCoAEC.8VKeCDlSQEn8q-dwzngje1", "comment": "Same query.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugg7eTRa1fntXHgCoAEC.8VKeCDlSQEn8tZHtzHCglu", "comment": "\"Make the error vector perpendicular to the Column Space\" means \"minimize the euclidean Norm of e\". Because the shortest vector from the Column Space of A to b is the perpendicular vector.  Minimizing the euclidean Norm means: \"minimize the Square root of the sum of squares (of the components of e with regard to the m-dimensional space)\".  This happens to be the same thing as minimizing the sum of squares of each of the m residuals (=the vertical distance to the line)", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UghoDt5YrivZE3gCoAEC.8U75n0aQ6Z29DvQuvL2Bfw", "comment": "nahhh frank's delivery is more ominous", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UggWmdZxCzv95ngCoAEC.8Lurj4aR4lu8LxipC0BmsY", "comment": "Some of the problems require MATLAB, but for much of this course it looks like students are expected to do solutions by hand (calculators are not allowed during exams, for example). Check out the full course site for more information: http://ocw.mit.edu/18-06S05.", "votes": "3", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgiP_snV1atp7HgCoAEC.8KL3cknEm5i9N77m1TL7rb", "comment": "Absolutely!", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugg3wBoLuO499ngCoAEC.8JaxUsCAZ598K3pXUw7Xd5", "comment": "e = [e1 e2 e3] is the vector that represents the distance between b (the measured data) and p (the points on the best fit line)", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugg3wBoLuO499ngCoAEC.8JaxUsCAZ5993IYOpfr1Oo", "comment": "the projection is not done in the x-y space, but the projection is done in the column space of A, which is a 2D plane in R^3. The orthogonal distance in R^3 (e1^2 + e2^2 + e3^2) is equivalent to the sum of squared vertical distance in the x-y space.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UghqbzwdHp3u3XgCoAEC.8HIxHfjTMcR8vzBEpt4gme", "comment": "I dare any trumpster to get that.", "votes": "9", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UghqbzwdHp3u3XgCoAEC.8HIxHfjTMcR9DraUWiAcE4", "comment": "Lol", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UghqbzwdHp3u3XgCoAEC.8HIxHfjTMcR9HKFiXZMGih", "comment": "Good one!", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Uggdo2DVzSc_gHgCoAEC.8G25noA-Ulf97B4BI6SWYR", "comment": "Yeah. It's so good. It's senior algebra!", "votes": "3", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugj3a2iS2uTsRXgCoAEC.8FFt7bovig48FeY_XCIeky", "comment": "First he assumes that if A has n independent columns then the row space has rank n, and spans R^n. Therefore, if Ax=0 must be that x=0.  Then he argues that if A'Ax=0 must be that Ax=0 which means that x=0.", "votes": "2", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UggB9UVsaYpemngCoAEC.8DB9X5UzCRW8Ea11P7rY1f", "comment": "I was wondering about the same thing :/", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UggB9UVsaYpemngCoAEC.8DB9X5UzCRW8F3hTjFvgY5", "comment": "I don't think so. This projection is not literally projecting the points onto the line, and the error is calculated as Ax_hat-b, hence has to be along y-axis.", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UggB9UVsaYpemngCoAEC.8DB9X5UzCRW8FeXxGdn-0N", "comment": "The projection, is of the vector b (all 3 y values) onto a vector in the column space. Thus, reassigning p to be the 3 y values. The x values are not changing since they define the matrix. The error is indeed perpendicular to the column space. Here Strang is showing how linear algebra is translated in terms of linear regression.", "votes": "4", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UggB9UVsaYpemngCoAEC.8DB9X5UzCRW8gQoevUm4em", "comment": "Look at it this way: For each x, there is a computed value, and the observed value. Then the difference between the two ordinates is the error. The \"line\" comes later, after we have minimized the total error. Hope I am making sense.", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Uggmsg-MS6adFngCoAEC.8CqKghTeJnC8CvVTmYPOvS", "comment": "+Lucas Wolf Yeah I noticed that too.", "votes": "2", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Uggmsg-MS6adFngCoAEC.8CqKghTeJnC8DIoQhUknnA", "comment": "+Lucas Wolf You are correct. It is Ax=b where b in this case is (1,2,2)", "votes": "2", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Uggmsg-MS6adFngCoAEC.8CqKghTeJnC8DIogwiftC-", "comment": "+Lucas Wolf He does switch it back to (1,2,2) though so the output is still correct.", "votes": "2", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Uggmsg-MS6adFngCoAEC.8CqKghTeJnC8W5b2V0CR98", "comment": "Interesting that no MIT student corrected him on that...", "votes": "7", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Uggmsg-MS6adFngCoAEC.8CqKghTeJnC8f-pXPV4nIE", "comment": "Me too. I was just curious full why they never let him know and fix ..", "votes": "2", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Uggmsg-MS6adFngCoAEC.8CqKghTeJnC96ZcUHb7mPg", "comment": "\u00a0@rongrongmiao4638\u00a0 they probably all noticed, just didn't care enough to correct", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Uggmsg-MS6adFngCoAEC.8CqKghTeJnC9OMcdGyzU4Q", "comment": "Yeah, he sometimes makes these little mistakes but then someone corrects him. So annoying that no one did that this time!", "votes": "1", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UghHF0zmObMLUngCoAEC.8BQ_oYl21GR8CLs7agavCu", "comment": "+Arthur Santana we are solving for x here in the equation A'Ax = 0. So when he multiplied x^t to the left on both sides, he's not multiplying any row vector, he's multiplying the transpose of the SOLUTION. And to your example, if you multiply [1 -1 0 0] on both sides, then [1 1 0 0] will become a solution to the NEW equation: yA'Ax = 0 (let y=[1 -1 0 0]), NOT a solution to the original equation A'Ax=0", "votes": "3", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UghHF0zmObMLUngCoAEC.8BQ_oYl21GR8CM5RK_ebh9", "comment": "+Yang Shen You are absolutely correct, thanks for the answer!", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugij-4gwYCNzhngCoAEC.81aJ-5HafoL7-U6sEkAkR9", "comment": "Ahh, that makes sense. Thanks for clearing that up!", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugij-4gwYCNzhngCoAEC.81aJ-5HafoL8kH01d3QZ3t", "comment": "Why is that? Are there some comments being deleted? I can't see the answers. :(", "votes": "5", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugij-4gwYCNzhngCoAEC.81aJ-5HafoL8kMz18aFiOY", "comment": "lol same here.", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugij-4gwYCNzhngCoAEC.81aJ-5HafoL8kQmZaxhuf6", "comment": "Phoebe Wang : e = b - p = Ib - Pb = (I - P) b where I is identity matrix", "votes": "21", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UghQ8_OBYPocgHgCoAEC.81OpkT8R-y-91egZbu-SpQ", "comment": "In a linear regression model, one of the assumptions is that the dependent variable (the variable corresponding to the vector b) is a linear combination of the independent variables (the variables corresponding to the column vectors of A). However, we cannot observe the exact values of the dependent variable because of measurement errors or some other reasons. What we can observe are the true value of the dependent variable plus some random error.  Because of the random error, the observed dependent variable is not in the space spanned by the independent variables. The least square estimation assumes that the random error is orthogonal to the space spanned by the independent variables.  Therefore, the length of the projection of b onto the null space of the A transpose gives us some idea about the size of the random error, or the percentage of error in the observations (here vector b is the observations of the dependent variable).", "votes": "4", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgjgGtu8dTEDlHgCoAEC", "comment": "32:56 Thank you Prof. Strang.", "votes": "5", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UghZV-rTr_F1kXgCoAEC", "comment": "Mr Strang and MIT ,thank you so much.", "votes": "13", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugi9TmrqTsD2T3gCoAEC", "comment": "Mr. Strang! Ahahaha so nice at 43:10", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugg1Ew1P8EKtX3gCoAEC", "comment": "Around 20:00.\u00a0Why are the errors \"e\" the vertical distance between the line and the \"b\" points? Why don't we look at the shortest distance between b and p which is when you use a distance between b and the line that is perpendicular to the line (not parallel with y axis)?", "votes": "5", "replies": "3", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UggSB1j5knQylXgCoAEC", "comment": "The proof of A^tA being invertible around 39:00 was great!", "votes": "7", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzloNm70t0H7zZzKjR4AaABAg", "comment": "I love how you can reach the same answer also using a calculus approach.. and I LOVE the two pictures for the least squares regression. Beautiful stuff and amazing lecturer.", "votes": "27", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugw99BhHvl2j40n0eHx4AaABAg", "comment": "The bit at 4:54 where b is replaced by Ax giving A(A^TA)\u02c9\u00b9ATAx, which then collapses to Ax is fantastic. It's so high-level and so simple to see.", "votes": "24", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugw1mF_-uMCF-Ql2Pgd4AaABAg", "comment": "holy shit....18.06 for life", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyLKsnEgUk6imwA3XJ4AaABAg", "comment": "This is mind blowing. Great lecture.", "votes": "2", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzann_WTzStyJbfa-l4AaABAg", "comment": "This guy is incredible.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgxLNxb5kcHXz0SMWnR4AaABAg", "comment": "Really an inspiration to me as how the things add up and come together. Great lecture intense easy to follow / understand   ", "votes": "9", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyklnqUoFylkxlxN1d4AaABAg", "comment": "I Really Like The Video Projection Matrices and Least Squares From Your", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzqa7V62wOE8prtmEx4AaABAg", "comment": "omg sir much obliged sir, so much obliged !!", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzH-rhZiwNjDuv2BjB4AaABAg", "comment": "@dylanhoggyt I get that, but that doesn't answer my question. What I mean, is that around minute 14:00 he says that the error is vertical instead of orthogonal to the line. I thought we were trying to minimize the error by orthogonal projection. I'm probably mixing things up, but I don't see it.", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwTyNedJkddqoIo6Yd4AaABAg", "comment": "@j4ckjs What I mean, is that around minute 14:00 he says that the error is vertical instead of orthogonal to the line. I thought we were trying to minimize the error by orthogonal projection. I'm probably mixing things up, but I don't see it.", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugzng1-__lgeZFIdQ014AaABAg", "comment": "@pelemanov from what I understand we are projecting b in to the column space so that we can actually solve the system with a best estimate.  The closest (least squares) projection of b is p which will only be orthogonal if b happens to be in the null space of A transpose, but there is no requirement that this is the case.  In fact if b happens to be in the column space, then the projection doesn't change b at all (i.e. P = I.)", "votes": "1", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwL2J1LI-NJ4l-AtNR4AaABAg", "comment": "Great lecture! But aren't we supposed to make an orthogonal projection? Instead he did a projection parallel to the Y-axis because he calculates p1, p2 and p3 by taking t-values 1, 2 and 3. You can also see it on his drawing. Why does he take this projection instead of the orthogonal one? And how can e turn out to be orthogonal to p anyway?", "votes": "2", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugw8rZ1E0jh_1w1exZN4AaABAg", "comment": "Wish my professors wrote as big as that too -__________-", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgyZj2hEbe9HgabyuiJ4AaABAg", "comment": "padma laxmi is hot", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugxh0uKhm5fUOs3g0pF4AaABAg", "comment": "Prof Strang spoke so much about errors and he did make one ! :P", "votes": "0", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "Ugg1Ew1P8EKtX3gCoAEC.7-H0Z7-IOlb72Z24Unm-6B", "comment": "Because the line is not column space. If you go back to the previous lecture, the goal was to find vector b project on the column space of A for solving Ax = b. In this example, A is a 3x2 matrix, and b is 3x1 vector. b-A*x_hat is perpendicular to the column space of A that is a 2D plane. Here X-Y plane (the plane on the blackboard) is not column space, as it represents the possible solution plane (null space moved by the special solution, check earlier lectures). The line is not column space either. The perpendicularity doesn't happen in this geometrical illustration.  If you are looking for the shortest distance, i.e. error e perpendicular to line, you are projecting each point onto a 1-dimention line, i.e. you are considering the line as column space of a totally different equation.  The example is not a good example indeed, when professor Strang said the \"best line\" close to all three points. It's not the closest line to all three points. What you said is another way of finding the \"best line\", to minimize the least square of the distance to a 1-D line, but it cannot be written in the the same Ax=b format as the professor wrote on board.\u00a0 \u00a0", "votes": "8", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugg1Ew1P8EKtX3gCoAEC.7-H0Z7-IOlb8e2zYiDkfUB", "comment": "That would be Support Vector Machine", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugg1Ew1P8EKtX3gCoAEC.7-H0Z7-IOlb8q2p0dZjT32", "comment": "i think you're confusing it with principal component analysis", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "Ugwfnx6qcGJEPCG8Ard4AaABAg", "comment": "\" so what's up?\" lol", "votes": "5", "replies": "", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwAnxdEjHStCyhqn-54AaABAg", "comment": "What was he doing since 22:45? I don't see why was he augmenting.", "votes": "1", "replies": "2", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgzHXzXwvOQFCuVtnZt4AaABAg", "comment": "The mistake starts at 11:26. The right hand side is (1 2 2), not (1 2 3). But he later uses the (1 2 2) for all other calculations, so not a big deal.", "votes": "16", "replies": "1", "reply": false}, {"vid": "osh80YCg_GM", "cid": "UgwAnxdEjHStCyhqn-54AaABAg.8GbpQ6p77JE8iCNzZHL0bu", "comment": "if you were to perform an elementary operation on the equation Ax=b, then you could solve both sides at once by augmenting. I interpret what he does here as a shortcut to get the constants of the new equation, where the new equation is what you get when A^t is applied to the original equation Ax=b.", "votes": "3", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgwAnxdEjHStCyhqn-54AaABAg.8GbpQ6p77JE9YcpWtT1Y3u", "comment": "Thanks for the explanation! But certainly very clever", "votes": "0", "replies": "", "reply": true}, {"vid": "osh80YCg_GM", "cid": "UgzHXzXwvOQFCuVtnZt4AaABAg.8GbdofCNOduA4RjIs-nskD", "comment": "Not the first time he's made a mistake and not caught it.", "votes": "0", "replies": "", "reply": true}]