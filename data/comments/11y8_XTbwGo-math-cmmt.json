[{"vid": "11y8_XTbwGo", "cid": "UgzA0aHgcHifpBbw5Md4AaABAg", "comment": "may the cemera man understood the  entire course", "votes": "0", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UgwBeJ3_32IjmoBhlnN4AaABAg", "comment": "In 17:50, wouldn't the u vectors be the input and the independet vector be the output of a system? Even more, in dynamic systems, the output is the result of \"passing\" the state vector through the matrix of the system and adding the term Bu with u being the input", "votes": "0", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UgwBkH7ovcLKKuvYIIp4AaABAg", "comment": "This is a great lecture", "votes": "1", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UgzMIJk5HZ4ybMr5ssV4AaABAg", "comment": "The big picture of linear algebra at 21:57 is really a great summary.", "votes": "3", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UgyiaHgYQhH0xJvx0V54AaABAg", "comment": "17:43 When he writes AU=I, why does he call the columns of u impulse responses? Are they impulse responses to the map A^-1? Because naturally since the map is Ax=b, when we write Au=e1, e1 (being an \"impulse\") is the output of the system, so I don't see the analogy with impulse responses in LTI systems.", "votes": "0", "replies": "1", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UgwgCVVsnwvfCblm5Kx4AaABAg", "comment": "excellent lecture. its creative to think [1, 0, 0 ] as the impulse at position 1, but I think its more precise to say its unit load at position 1, since here we just have K matrix but not M, it's a static problem if we really want link the matrix to a physical problem.", "votes": "0", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "Ugzk2NvOdDrStSqqVut4AaABAg", "comment": "I like two people. Dr Strang and Wiz Khalifa", "votes": "1", "replies": "1", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "Ugzdm44C5ddSYbW73954AaABAg", "comment": "One thing I think is missing from this lecture(or perhaps I just missed it) is 'how does computing elimination on an invertible matrix, A, allow us to solve a linear system problem with never having to solve for A inverse'?   Meaning, if I want to solve: K*U = b -> U = K^(-1)*b, how does elimination compute the right-hand-side to yield the U matrix without ever find K inverse??   Much Thanks!!", "votes": "0", "replies": "1", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "Ugz6Ph6NdSSPTRrq1bJ4AaABAg", "comment": "English subtitle helps me a lot. thanks", "votes": "1", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "Ugh59KbeYQ8Pd3gCoAEC", "comment": "someone has his book or something he gives for studies at these classes?", "votes": "0", "replies": "2", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UghTneSv7Fjy7XgCoAEC", "comment": "THANKS A LOT!", "votes": "0", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UgjH7Huz5ukE53gCoAEC", "comment": "why a linear term uTf makes the minimization problem \u2013 min of energy \u201cuTku - uTf = 0\u201d .... so good to produce final Ku = f ? How can one say this is \"the equation\" of minimizing \u201cENERGY\u201d?  (Other than changing the bowl concept?) Why this linear term added without mentioning the \u201cConstraint\u201d?  (constraining  to null space of K and adding f  )Should there be only linear constraints?", "votes": "0", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UghAYsgdR2OdPXgCoAEC", "comment": "I maybe missed something in the first video? But I don't get what the \"free-free\" and \"fixed-free\" and so on e.g. the matrix at 28:13 means. I get that it has to do with boundary conditions. But is it contrasting a u(1)=1, u'(0)=0 problem with a u(1)=1, u(0)=0 problem? Or...something else?  Maybe someone could put up a bubble when he talks about free & fixed pointing to the appropriate part of the video where this was initially defined. TIA", "votes": "0", "replies": "3", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "Ugj8XfiCOeedjXgCoAEC", "comment": "If you're only going to watch one linear algebra review video, this should maybe be it. LU decomp, QR decomp, eigenvalue decomp, SVD decomp @ minute 30", "votes": "4", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UgzFFfVnHPsHn26l4PZ4AaABAg", "comment": "Thanks Professor Strang. I was keen to know a little more about LU factorisation so this video helped me out.", "votes": "3", "replies": "", "reply": false}, {"vid": "11y8_XTbwGo", "cid": "UgyiaHgYQhH0xJvx0V54AaABAg.95FCzAzczY597Do4HqGGvW", "comment": "No when you write a linear system of equations as Ax = b, b is actually the input, i.e. the value is known. You are solving for x, which is, in you terms the response of the system to the forcing. If you are familiar with PDEs from non numerics classes, think of an equation of the form Laplacian(u) = dirac delta, u is the potential for a Dirac delta input", "votes": "1", "replies": "", "reply": true}, {"vid": "11y8_XTbwGo", "cid": "Ugzk2NvOdDrStSqqVut4AaABAg.8u4DsXHEdsS9GTVWx6hs6x", "comment": "another professor?", "votes": "0", "replies": "", "reply": true}, {"vid": "11y8_XTbwGo", "cid": "Ugzdm44C5ddSYbW73954AaABAg.8bQx8nsIGEX8fxmhjw5ddp", "comment": "+Maximillian Sbabo  The reason why a set of linear equations Ax = b is NEVER solved by computing the inverse matrix A\u207b\u00b9 is that it demands way more calculations than when using elimination (at least, for large n). And because of the large number of calculations, it turns out to be even less accurate.  So, how DO we solve such a general set of equations then? Especially if we want to solve it for a number of right hand sides b?  As \"shown\" (I mean, it is not really proved, just shown by two examples) in the video, every non-singular n by n matrix A can be written as the product of two matrices L and U - i.e. A = LU - such that:  - L is lower triangular and has only 1's on the diagonal - U is upper triangular and has only non-zero elements on it's diagonal  So, our original system of equations, Ax = b, is now equivalent to LUx = b. Let's denote Ux by y, so that we have:  - Ly = b - Ux = y  The system of equations Ly = b can easily be solved:  - start with the first equation and you'll find y(1) - use the value of y(1) in the second equation and you'll find y(2) - (etc.)  No that we've found y, we find the solution x of our original problem Ax = b, by solving the set of equations Ux = y as follows:  - start with the last equation and you'll find x(n) - use the value of x(n) in the second-to-last equation and you'll find x(n-1) - (etc.)  And this gives us the solution to the original problem Ax = b. So actually, we've split our problem of Ax = b into two sets of linear equations, namely Ly = b and Ux = y, which are both relatively easy to solve.  Especially when trying to solve a system of linear equations such as Ax = b by using \"pen and paper\", one usually proceeds as follows:  - augment the matrix A with the vector b to form the so-called augmented matrix:     [A|b] (actually, the bar is not needed here, but often written in practice) - perform so-called elementary row operations on this augmented matrix:     Elementary row operations are:         - switching two rows         - replacing a row by a multiple (\u2260 0) of it         - replacing a row by subtracting a multiple of another row from it  It is easy to see that the two operations above will not affect the (original) solution (i.e., we'll end up with a set of equivalent equations) and that (by cleverly applying these row operations), we can end up with an equivalent system of equations, where the matrix A has been turned into an upper triangular matrix. (note that the matrix A has changed, but so has the vector b, since we apply the row operations to full augmented matrix, i.e. including the column where we put the vector b)  This leaves us with a new augmented matrix of the form [A'|b'], which should be read as an equivalent system of equations (equivalent as in having the same solution) A'x = b'. Since A' is now upper triangular, this is easy to solve (see above, solving Ux = y).  Should one feel the need to find the inverse of A, one augments the original matrix with the identity matrix I as follows:     [A|I] (I've left out the dimension 'n' here, so I should be read as I(n)). If we consistently apply elementary row reductions on this augmented matrix in such a way that the first part (the original matrix A) is turned into I, then the \"second matrix\" (which was originally just I) will have turned into A\u207b\u00b9.  If we want to compute the determinant of a matrix, we just apply a number of elementary row operations, until the matrix A is reduced to an upper triangular matrix. The determinant of the original matrix A is then nothing else but the product of the diagonal elements of the resulting upper triangular matrix ... UNLESS... If in the process of getting to such an upper triangular matrix we have  - swapped two rows: then the sign of the determinant flips.   Therefore we have to compensate for this by an additional factor of (-1)^(number of row swaps) - multiplied a row by a constant   (but not in the row operation: \"replace row by row - factor * other row\")   We have to divide the determinant of the upper triangle by the product of all factors applied.  It turns out that when nothing specific is known about the matrix A (e.g. being symmetric), \"practically\" the most efficient way of solving Ax = b, determining A\u207b\u00b9 or determining the determinant of A, is by using the (Gaussian) elimination process. (however, since computers only can approximate real numbers, some minor adjustments have to made (such as partial pivoting) to \"make sure\" that (inevitable) approximation errors made by computers don't get out of control)  (see e.g. https://en.wikipedia.org/wiki/Gaussian_elimination for more information)", "votes": "7", "replies": "", "reply": true}, {"vid": "11y8_XTbwGo", "cid": "Ugh59KbeYQ8Pd3gCoAEC.8Foi36WKIFC8G24QBc4ZV_", "comment": "The textbook for this course is: Strang, Gilbert. Computational Science and Engineering. Wellesley, MA: Wellesley-Cambridge Press, 2007. ISBN: 9780961408817. (https://www.amazon.com/Computational-Science-Engineering-Gilbert-Strang/dp/0961408812?ie=UTF8&redirect=true&tag=mitopencourse-20). See the course on MIT OpenCourseWare for more information: http://ocw.mit.edu/18-085F08", "votes": "2", "replies": "", "reply": true}, {"vid": "11y8_XTbwGo", "cid": "Ugh59KbeYQ8Pd3gCoAEC.8Foi36WKIFC8G26lRk0pOJ", "comment": "+MIT OpenCourseWare thank you!!!", "votes": "0", "replies": "", "reply": true}, {"vid": "11y8_XTbwGo", "cid": "UghAYsgdR2OdPXgCoAEC.7-H0Z7-OEn772KmRqe3ARw", "comment": "Free-free, fixed-free matrices are explained in Lect 1 and 2.\u00a0", "votes": "1", "replies": "", "reply": true}, {"vid": "11y8_XTbwGo", "cid": "UghAYsgdR2OdPXgCoAEC.7-H0Z7-OEn7734Zqan9VMn", "comment": "Thanks NoLongerBe\u00a0", "votes": "0", "replies": "", "reply": true}, {"vid": "11y8_XTbwGo", "cid": "UghAYsgdR2OdPXgCoAEC.7-H0Z7-OEn78HPdrAaKWBa", "comment": "Free means u' =0 either at beginning or end. It corresponds to \"no force\" acting on that boundary.", "votes": "1", "replies": "", "reply": true}]