[{"vid": "het9HFqo1TQ", "cid": "UgyBMEyExHtrbYQsai54AaABAg", "comment": "can someone help me to find partial derivative of L(theta) at 1:01:10?", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwbupZ5RVWxMd2nNkd4AaABAg", "comment": "I couldn't find the Newton's method in lecture notes. Can anybody tell me in which page this belongs?", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxgK9bji38zW-HCBdx4AaABAg", "comment": "Motivation: only 10% will make it to the last video", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzumSz0I143tVgtxXt4AaABAg", "comment": "can anyone tell me how the h thetha(x) which is equal to sigma j = 0 to n (thetha j Xj) can be written as thetha(transpose) into X ? how the transpose came here it should be thetha into x then it makes sense somewhat... how is this transpose imposed on the thetha? (obviously in linear regression)", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwqB0hRbo6D4NEJllZ4AaABAg", "comment": "Awesome totally", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugw4tLCBehIt4XTneOh4AaABAg", "comment": "Notice how views decrease by half on each new lecture, Congratulations on making this far, keep going fellas, we got this.", "votes": "20", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzqWppC5RVBvPpUM2J4AaABAg", "comment": "My boyfriend does this course very diligently \ud83d\ude0a", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyhkKHXZCVtsmGI-3l4AaABAg", "comment": "They changed the voices of students \ud83d\udc69\u200d\ud83c\udf93, at first I was amused why they all talk in a same way \ud83d\ude2e, but now that makes sense", "votes": "12", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyUGF7IGuQZS6Wzu6d4AaABAg", "comment": "How can we access the homework? The link in the syllabus leads to the piazza but it does not get into the classroom?", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwBRRMiZpXS0W0Y4Ml4AaABAg", "comment": "please help me get the problem set", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzFunJLAYhasnFPJid4AaABAg", "comment": "For anyone struggling with the concept.. likelihood indicates how likely a particular population is to produce an observed sample given a particular distibution. For example, if we have data that should follow a Gaussian Distribution with mean=5  and variance = 0.1 but the ACTUAL data in my dataset are all 0.5, well... the likelihood that my data actually follow this distribution is very low! If each ACTUAL data has a high density probability, the overall likelihood will be high!", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugzn8xXralkn1ZBPwoZ4AaABAg", "comment": "\ud1a0\uc0ac\uc7a5 \ub4e4\uc774 \ubcc4\uac78 \ub2e4\ub9cc\ub4dc\ub124 \u314b\u314b", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwO-ToUUOEZXvlzdy94AaABAg", "comment": "Why every student sounds like a giant.\ud83d\ude02", "votes": "0", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugw_C_nQjr65QIshlu94AaABAg", "comment": "he speaks with so much bass that I have to ramp up my volume.", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugyq4P2r4DAucxiZqJB4AaABAg", "comment": "00:10 Today's discussion is about supervised learning and locally weighted regression. 07:48 Locally weighted regression focuses on fitting a straight line to the training examples close to the prediction value. 16:15 Locally weighted linear regression is a good algorithm for low-dimensional datasets 22:30 Assumptions for housing price prediction 29:45 Linear regression falls out naturally from the assumptions made. 36:36 Maximum Likelihood Estimation is equivalent to the least squares algorithm 44:40 Linear regression is not a good algorithm for classification. 51:04 Logistic regression involves calculating the chance of a tumor being malignant or benign 58:30 Logistic regression uses gradient ascent to maximize the log-likelihood. 1:05:36 Newton's method is a faster algorithm than gradient ascent for optimizing the value of theta. 1:12:40 Newton's method is a fast algorithm that converges rapidly near the minimum. Crafted by Merlin AI.", "votes": "3", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzYc95AyL5m5vvQnlB4AaABAg", "comment": "0:28: \ud83d\udcda The video discusses supervised learning, specifically linear regression, locally weighted regression, and logistic regression.\r 5:38: \ud83d\udcda Locally weighted regression is a non-parametric learning algorithm that requires keeping data in computer memory.\r 13:05: \ud83d\udcca Locally weighted regression is a method that assigns different weights to data points based on their distance from the prediction point.\r 19:01: \ud83d\udcda Locally linear regression is a learning algorithm that may not have good results and is not great at extrapolation.\r 24:46: \ud83d\udd0d The video discusses Gaussian density and its application in determining housing prices.\r 31:31: \ud83d\udca1 The likelihood of the parameters is the probability of the data given the parameters, assuming independent and identically distributed errors.\r 36:55: \ud83d\udcca Maximum Likelihood Estimation (MLE) is a commonly used method in statistics to estimate parameters by maximizing the likelihood or log-likelihood of the data.\r 43:44: \ud83d\udcca Applying linear regression to a binary classification problem is not a good idea.\r 49:22: \ud83c\udfaf The video discusses the choice of hypothesis function in learning algorithms and why logistic regression is chosen as a special case of generalized linear models.\r 54:45: \ud83d\udcda The video explains how to compress two equations into one line using a notational trick.\r 1:01:31: \u270f Batch gradient ascent is used to update the parameters in logistic regression.\r 1:07:52: \ud83d\udcda The video explains how to use Newton's method to find the maximum or minimum of a function.\r 1:13:55: \ud83d\udca1 Newton's method is a fast algorithm for finding the place where the first derivative of a function is 0, using the first and second derivatives.\r Recap by Tammy AI", "votes": "14", "replies": "3", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugx4hJiT8iTriJ4t8HF4AaABAg", "comment": "26:37 How is it being implied? Like we are assuming the error term to be a gaussian, from there we jumped to the conditional distribution of  y given x parameterized by theta, I did not understand this implication.", "votes": "0", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxzGMEz_R7vQgi0dsR4AaABAg", "comment": "The simplification of log likelihood function log(L(theta)) to give you back the cost function J(theta) has to be one of the most beautiful transformations I've seen in a while!", "votes": "28", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugz71uWM4LyTYFegR9B4AaABAg", "comment": "Can't find the derivation of the MLE", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxD2EAseegzdl61l2N4AaABAg", "comment": "Can anyone explain what had happened to Andrew's voice at 19:32 ?", "votes": "0", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwO-ToUUOEZXvlzdy94AaABAg.A48kliDcsClA89Agb8qaku", "comment": "It may be to protect the privacy of the students, so special treatment is made for the sound", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzYc95AyL5m5vvQnlB4AaABAg.A3D5xaSCs_eA3yYbTJNfNl", "comment": "you are making my day bra'", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzYc95AyL5m5vvQnlB4AaABAg.A3D5xaSCs_eA5M9-zn486A", "comment": "Thnq\u2764", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzYc95AyL5m5vvQnlB4AaABAg.A3D5xaSCs_eA7thLtKDSN0", "comment": "Which website did you used to find this type of  conclusion ?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "Ugx4hJiT8iTriJ4t8HF4AaABAg.A2RmyOCoKLmA40iRGHO7dM", "comment": "its assumed that the error term is normally distributed", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgxzGMEz_R7vQgi0dsR4AaABAg.A1wxQ0S_zV3A7U3P28ru6s", "comment": "Hehe, I'm certain that the first derivation of the least square as a cost function did not come from a probabilistic interpretation. The goes to prove that if you are right in one angle, you will also be right in all angles. It was interesting to see that too.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgxD2EAseegzdl61l2N4AaABAg.A14Hc2KIG7iA1ozV69aIXE", "comment": "It seems like they applied some audio distortion effect whenever a student asks a question (to preserve anonymity) that makes their voice sound very deep.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwIKhPma69D9rI-uGt4AaABAg", "comment": "ML for Goa'ulds", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxTX4T11LWR8hfLurZ4AaABAg", "comment": "While deriving maximum likelihood for linear regression, the professor modelled a gaussian error term. However, for logistic regression he did not use an error term, does anyone know why that is?", "votes": "0", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyMeVh_dCSd2u9XK4J4AaABAg", "comment": "Thank you for explanation. I don\u2019t know why but it\u2019s so annoying, when lecturer constantly erases and writes the same signs((", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwpFTm-OyiDSTfOxER4AaABAg", "comment": "in our case parameter of learning algorithm(theta) is a cost of our house?", "votes": "0", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyoGb7RboO13yPlxpR4AaABAg", "comment": "Whether anybody knows how to get familiar with these concepts..like where to apply and practice these??", "votes": "1", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugx6cdrcwZzDVvXC1CN4AaABAg", "comment": "What a concept. I just \"wow\"d when MLE was shown. Anyone here familiar with Power System State Estimation?", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxoPN8VLRv9LNVfW0h4AaABAg", "comment": "at 1:16:59 shouldnt the formula have negative sign before the hessian inverse", "votes": "2", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxsAwQEpv6qDg0-Tgl4AaABAg", "comment": "I try to find way how to use this to teach kids.", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg", "comment": "the course page has some problem sets and class notes provided by prof. but are inaccessible . Is there any way to get those ?  p.s. I just need those problem sets", "votes": "1", "replies": "9", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyzTIL_rBObPb5PDqd4AaABAg", "comment": "Very helpful.", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzoI4zqHP_U1tmLaUh4AaABAg", "comment": "Awesome lecture", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxZ8nLYtdRJm1D7hgB4AaABAg", "comment": "i'm an EE student and we don't anything to do with ML except a simple course in the final year and i'm still taking this course wish me luck guys because it's hard reaally hard", "votes": "2", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugwz4kXw-yCurWoVnNV4AaABAg", "comment": "Where we get problemset of this courses", "votes": "1", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgylKyK_NpN3aD6v1H14AaABAg", "comment": "soooooo goood", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyzeCgIZLlOSLmTv2J4AaABAg", "comment": "Check point 44:16", "votes": "0", "replies": "2", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwSslKONYlflVPKt3V4AaABAg", "comment": "i like this guy\u2018s video, it's amazing", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugy6gNnBAjjMhLKLUvh4AaABAg", "comment": "thx", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxNWv9gHsyK70PWKvN4AaABAg", "comment": "now I know why my university teaches optimization techniques for CS program \ud83d\udc80", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugz7fwucMULL0cLmEiB4AaABAg", "comment": "where can i access the problem sets?", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzAVZ_uFza2Tf8Q0KB4AaABAg", "comment": "What a phenomenal lecture! So beautiful, so elegant, just looking like a wow", "votes": "6", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxTX4T11LWR8hfLurZ4AaABAg.A0AtrRvU1CFA56VELj3odx", "comment": "You wont need an error term for logistic regression because in linear regression you are trying to predict the h(x) which can vary based only some real world phenomenon. However, in case of classification(for which logistic regression is used) you are more or less trying to fit the h(x) into few defined classes of output, for instance the true or false of an occurrence. Hence presence of error function does not have any effect on the outcome. In other word, the output h(x) is discrete in classification so theres no requirement of an error term. I may be wrong with this though.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwpFTm-OyiDSTfOxER4AaABAg.A-xbBt2wAAdA56VbQX8agm", "comment": "nope X is the cost of house, parameter are weights of every feature at a given point on x that help you identify the corresponding h(x)", "votes": "1", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyoGb7RboO13yPlxpR4AaABAg.A-iNWzP7Gw5A0TEYOGuOch", "comment": "I'm thinking about ChatGPT. Ask it for exercises and to evaluate your responses.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgxoPN8VLRv9LNVfW0h4AaABAg.9zpWehY9oiWA4hmrCdxd0V", "comment": "We are trying to maximize the likelihood function. Hence the formula has a +ve sign instead of a negative sign.", "votes": "1", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdR9z3OS2NHUpl", "comment": "never mind. got them", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdRA-qCDDULKL0", "comment": "\u00a0@stephendiopter2289\u00a0how did you get them?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdRA10F23X3R2e", "comment": "\u200b\u00a0@stephendiopter2289\u00a0 where did you find them?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdRA1gJZoV-Yv6", "comment": "\u00a0@stephendiopter2289\u00a0 Can you help me where to find the lecture note", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdRA1gJb9NWQnW", "comment": "\u00a0@stephendiopter2289\u00a0 Can you help me where to find the lecture note", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdRA56VxA52Pl6", "comment": "\u00a0@stephendiopter2289\u00a0 how did you get them can you please share?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdRA56g1zwewkA", "comment": "\u00a0@stephendiopter2289\u00a0 how did u get them?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdRA5aiAZMKhw_", "comment": "For notes just search on google Stanford CS229 notes", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzXGuKbEkCbZ6BssJ4AaABAg.9z3E_GzgCdRA803vCPzlr9", "comment": "\u00a0@stephendiopter2289\u00a0 Could you please tell me, where I can find them.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgxZ8nLYtdRJm1D7hgB4AaABAg.9yTPAEmrn9FA-8sWmJm0Mm", "comment": "good luck! and yes, it is very hard", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "Ugwz4kXw-yCurWoVnNV4AaABAg.9yLR2qMp2FwA4cvK8z9pHk", "comment": "Did you find the problem sets?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzeCgIZLlOSLmTv2J4AaABAg.9xMo3VP1xakA-apQapZpOP", "comment": "THanks. Can you explain what he meant at H(x) is different when using logistic function? Is it because it's bounded [0,1]?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgyzeCgIZLlOSLmTv2J4AaABAg.9xMo3VP1xakA4hmhDRYBZL", "comment": "\u00a0@malfuriosstormrage5218\u00a0 h(x) is nothing but our hypothesis function. So depending on the task (classification or regression), our hypothesis function will look different. For example, for linear regression our h(x) was w0 + w1x1 + ... + wnxn. (Here w is same as theta, parameters). But h(x) looked different in logistic function.   Our hypothesis also depends on preferred outcome. Like you mentioned, h(x) looks different because we want to bound the output to [0,1]. Hope this helps.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgxP8G_fAulG4lvdltR4AaABAg", "comment": "0:28: \ud83d\udcda The video discusses supervised learning, specifically linear regression, locally weighted regression, and logistic regression.\r 5:38: \ud83d\udcda Locally weighted regression is a non-parametric learning algorithm that requires keeping data in computer memory.\r 13:05: \ud83d\udcca Locally weighted regression is a method that assigns different weights to data points based on their distance from the prediction point.\r 19:01: \ud83d\udcda Locally linear regression is a learning algorithm that may not have good results and is not great at extrapolation.\r 24:46: \ud83d\udd0d The video discusses Gaussian density and its application in determining housing prices.\r 31:31: \ud83d\udca1 The likelihood of the parameters is the probability of the data given the parameters, assuming independent and identically distributed errors.\r 36:55: \ud83d\udcca Maximum Likelihood Estimation (MLE) is a commonly used method in statistics to estimate parameters by maximizing the likelihood or log-likelihood of the data.\r 43:44: \ud83d\udcca Applying linear regression to a binary classification problem is not a good idea.\r 49:22: \ud83c\udfaf The video discusses the choice of hypothesis function in learning algorithms and why logistic regression is chosen as a special case of generalized linear models.\r 54:45: \ud83d\udcda The video explains how to compress two equations into one line using a notational trick.\r 1:01:31: \u270f Batch gradient ascent is used to update the parameters in logistic regression.\r 1:07:52: \ud83d\udcda The video explains how to use Newton's method to find the maximum or minimum of a function.\r 1:13:55: \ud83d\udca1 Newton's method is a fast algorithm for finding the place where the first derivative of a function is 0, using the first and second derivatives.\r Recap by Tammy AI", "votes": "8", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyRgxY19madrr_xLKl4AaABAg", "comment": "great lecture. ML is fun with you\ud83d\ude00", "votes": "1", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwowZ9eiWfX7Z-4Tfx4AaABAg", "comment": "You know he's thought about just getting slightly shorter sleeves tailored, right?", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzvT24Fqir50KIsyl14AaABAg", "comment": "Your clear explanation of these concepts is greatly appreciated. Thank you so much for sharing", "votes": "5", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxD6mN_xnxQ4hSON0l4AaABAg", "comment": "He needs to get the IBM guys blackboard", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyzO01v7RtMhvtp3Tp4AaABAg", "comment": "1:02:02", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzmhpHjNIgkqUZbTRF4AaABAg", "comment": "Is it better than his course on coursera or it is same?", "votes": "0", "replies": "2", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzjNV_3by_dbWXeY5h4AaABAg", "comment": "Since when these these basic statistics techniques become \u201cmachine learning\u201d??", "votes": "2", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxTxWiKMk10gOnBixR4AaABAg", "comment": "can anyone explain me where that x came from in the final equation of gradient ascent", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwURyD3Nyz08rpql-14AaABAg", "comment": "Thank You Very Much", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugygd28yuaAvf2V71sN4AaABAg", "comment": "someone, buy this guy better markers!", "votes": "4", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugw2aARpEGMddQsDLb54AaABAg", "comment": "Where are lecture notes?", "votes": "0", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzqClIe7TNOQWWhKCh4AaABAg", "comment": "In the links given in description don't have the class notes he keeps mentioning and he tells to read from them. Can anyone help? I mean how do i get those?", "votes": "1", "replies": "2", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwoF_BsJprbLpKU9lR4AaABAg", "comment": "can someone tell me after we derive the maximum likelihood of theta how do we use it to modify all our parameters theta?", "votes": "0", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgySVT2mEOv4jxcqjoN4AaABAg", "comment": "Hi, i'm trying to follow this courses in order to start reading papers for my phd research/preperation, i don't seem to understand most of the mathematic equations, do i really need to understand them to achieve my goal or i just need to understand the concepts and memorize the formulas ?", "votes": "0", "replies": "2", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwxIJafRsdzZ6wIZjt4AaABAg", "comment": "The way it started and the way it is going forward :/ So much math", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugz8ttbEE9GDjp1gPXF4AaABAg", "comment": "I love the videos and Mr Ng explains things clearly, but gosh, the markers he uses are so pale and hard to read", "votes": "20", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugz36yMyqheB4O1HOHl4AaABAg", "comment": "I have a question about locally weighted regression. Imagine we want to calculate studentized residual. we have different hat matrix (projection matrix) for each observation and each hat matrix is a matrix (k by k) which k is a number of the observation in the span. Now I would like to calculate the leverage. I would like to know how to determine leverage for each observation?", "votes": "4", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugwd4zyRsrDw8e_BUUR4AaABAg", "comment": "then what is the difference between the locally weighted regression and polynomial regression? in application", "votes": "1", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwMfabxzonXip-Qb2x4AaABAg", "comment": "Damn , this guy just explains concepts so clearly. love this course", "votes": "91", "replies": "6", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyRgxY19madrr_xLKl4AaABAg.9wcibC6rBk39wp_dvXtD10", "comment": "Thanks for your comment and for watching!", "votes": "1", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzmhpHjNIgkqUZbTRF4AaABAg.9tdmnG4ZWWk9tglYshOLdt", "comment": "Hi there, thanks for your comment! The material on coursera is more introductory level and this lecture is from the graduate course CS229 and covers more advanced topics.", "votes": "6", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzmhpHjNIgkqUZbTRF4AaABAg.9tdmnG4ZWWkA4fmNOGDj6P", "comment": "\u00a0@stanfordonline\u00a0  so what should i prefer?? this course or the one in coursera??", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzjNV_3by_dbWXeY5h4AaABAg.9tbc4F3Qoc0A-rz89HdGSC", "comment": "Its all marketing. To be fair, much of these results fall out of linear system theory that does not require any statistics. So the branding is somewhat subjective.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "Ugw2aARpEGMddQsDLb54AaABAg.9sSyJual4OG9vgZZbI_jxi", "comment": "look up cs229 autumn 2018 on google, you should find the repository maxim5/cs229-2018-autumn", "votes": "1", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzqClIe7TNOQWWhKCh4AaABAg.9rhrdh--SVQA-HPec8PIO1", "comment": "Same question. I think the notes are only available to stanford students because its in their intranet.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzqClIe7TNOQWWhKCh4AaABAg.9rhrdh--SVQA2fYu6FNoJh", "comment": "\u00a0@kinetic_kane9033\u00a0 https://cs229.stanford.edu/lectures-spring2022/main_notes.pdf", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwoF_BsJprbLpKU9lR4AaABAg.9r-jJ0HcYxy9sCCrGRwxFV", "comment": "From MLE of theta we have the function that should be maximized i.e. l(theta) Now use any optimization algorithm(like Gradient descent/ newton's method) to optimize for example using GD theta(new) = theta(old) +alpha * partial derivative of l(theta)", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgySVT2mEOv4jxcqjoN4AaABAg.9oTD8LsweLY9p81CSsWHoS", "comment": "i have the same problem as you , what's your phd research theme ?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgySVT2mEOv4jxcqjoN4AaABAg.9oTD8LsweLY9qs7RU1tnHG", "comment": "Hi, it's highly recommended to have a background in probability and statistics, and linear algebra before studying machine learning. Personally i think that a few knownledge in optimization is sufficiently but no necessary.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "Ugwd4zyRsrDw8e_BUUR4AaABAg.9lR5O0iGj8tA-rzTI8N-nT", "comment": "I got the same question. Im sure with more exposure it will be clear. Polynomial regression and locally weighted regression echoes simularity in concept with gains scheduled control design for nonlinear systems. Same trick, different pony. IE, how can we apply linear theory to non linear systems?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwMfabxzonXip-Qb2x4AaABAg.9kXaQmKWN989tbc79M0Ju7", "comment": "Where else have you seen these techniques explained? It\u2019s not that hard.", "votes": "2", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwMfabxzonXip-Qb2x4AaABAg.9kXaQmKWN989wznX2Fz2h_", "comment": "Damn, chinese  communist teaching in Stanford!!!", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwMfabxzonXip-Qb2x4AaABAg.9kXaQmKWN989x-eYIaQvCh", "comment": "\u00a0@hannukoistinen5329\u00a0 Damn, Cool Joke bro!! You must be really a funny guy.", "votes": "11", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwMfabxzonXip-Qb2x4AaABAg.9kXaQmKWN98A4cv9x-dFCW", "comment": "Do you know where we can get the practice sets for the course?", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwMfabxzonXip-Qb2x4AaABAg.9kXaQmKWN98A56UNDJlJb9", "comment": "\u00a0@ujjolchakrabarty9285\u00a0 trying to find the same thing. Cant access it from the website", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwMfabxzonXip-Qb2x4AaABAg.9kXaQmKWN98A7qkSwGumIe", "comment": "\u200b\u00a0@ujjolchakrabarty9285\u00a0 its on the course website", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwLmkBUdKKxBHKsgkt4AaABAg", "comment": "Are the class notes he mentions throughout the course available anywhere for download?", "votes": "1", "replies": "4", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugw267Lh8xdpZWJB-Kp4AaABAg", "comment": "Okay.", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgyA6tw7KGNQKT-80bt4AaABAg", "comment": "Otu yo", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzuFqbiffKq8PdVcOR4AaABAg", "comment": "32:18 I have a question about this likelihood function. Can somebody help me with it?  According to the IID assumption, the probability of all the observations is equal to the product of each probability . However, isn\u2019t the expression a density instead of a probability of a normal distribution? I am really confused. I think the probability should be the integral of density function. If it's density, what's the meaning of the product of densities?", "votes": "6", "replies": "3", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzUi6XtwoDBPsWYSe94AaABAg", "comment": "What happened at 17:45", "votes": "2", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugx4LCEZF4zPus5JzCZ4AaABAg", "comment": "10:24 - How is this just not a form of interpolation using shape functions?  That doesn't really seem like \"learning\" to me.", "votes": "0", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugz3KHpjmMMJm7C5pzx4AaABAg", "comment": "A lot of the links aren't working on the syllabus linked in description. Is there an updated version with the class notes pdf's, etc.?", "votes": "18", "replies": "3", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgxDMv8Ye5aIUWnAb914AaABAg", "comment": "Thank you!", "votes": "1", "replies": "", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgzLghxfrRnjks7gxnd4AaABAg", "comment": "Looks like in gradient ascent if we replace the scalar learning rate alpha by the inverse H^{-1}, we get the Newton's method.", "votes": "1", "replies": "3", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "Ugx0n3eJZJ2-5x1PbLx4AaABAg", "comment": "A classic tradeoff in locally weighted models between training cost and accuracy, though it seems like the cost really comes from refitting for each x input during testing.", "votes": "10", "replies": "1", "reply": false}, {"vid": "het9HFqo1TQ", "cid": "UgwLmkBUdKKxBHKsgkt4AaABAg.9jtmMzZQOm19k4a9ejt5Tw", "comment": "In the link to the syllabus in the description there are some lecture notes available, although many are dead links", "votes": "1", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwLmkBUdKKxBHKsgkt4AaABAg.9jtmMzZQOm19kSP7oUw2VJ", "comment": "google it and you'll find them. first hit.", "votes": "4", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwLmkBUdKKxBHKsgkt4AaABAg.9jtmMzZQOm19ptc9rSFXsC", "comment": "See what I am doing is to follow the current year course page for assignments as they are mostly working links. Lecture notes can be found in the course page given in the Lect 1 desc.", "votes": "2", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgwLmkBUdKKxBHKsgkt4AaABAg.9jtmMzZQOm19z3Equp5xpp", "comment": "can you share the link of current year course page \u00a0@shashankrana977\u00a0", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzuFqbiffKq8PdVcOR4AaABAg.9fbq0ZVcC179fuzpsf8t3P", "comment": "For I.I.D, P(AB) = P(A)P(B). Your observation about it being the probability density of the Gaussian is correct. When we maximize it, we are trying to find the point which has the highest probability. A point that has the highest density will have the highest probability. So using the probability density function is correct in that regard (I think you are confused by the fact that the density will probably result in a value that is not between 0 and 1 but with a little thought about what I said, hopefully you will be able to see why normalizing the values to be between 0 and 1 do not really matter). I don't know how much help this answer will be to you, I'm simply having a hard time to articulate what I'm trying to say.", "votes": "10", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzuFqbiffKq8PdVcOR4AaABAg.9fbq0ZVcC179fvZPiaMe-w", "comment": "\u00a0@HamzaAsgharKhan\u00a0 Thank you very much! I didn't expect someone would give me such a detailed answer! That's exactly what I thought. The product of density might not really have a meaning in statistics, the density can also be greater than 1 , but it would be enough to find the maximum point. I appreciate it!!", "votes": "4", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzuFqbiffKq8PdVcOR4AaABAg.9fbq0ZVcC179fvzGsFe8Hi", "comment": "\u00a0@liketheblue5082\u00a0 I'm glad it helped! \ud83d\ude0a", "votes": "3", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzUi6XtwoDBPsWYSe94AaABAg.9eDnGgNr9NX9l5MLkw3kvu", "comment": "Probably a mic failure. Not sure though.", "votes": "1", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "Ugz3KHpjmMMJm7C5pzx4AaABAg.9cTozO1FW339dyn3ji3DrW", "comment": "You can refer to the notes of the summer 2019 class. Though the topics were covered in a different order, the content is the same.", "votes": "2", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "Ugz3KHpjmMMJm7C5pzx4AaABAg.9cTozO1FW339nAa0jy6zXf", "comment": "\u00a0@aphievel\u00a0 where?", "votes": "2", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "Ugz3KHpjmMMJm7C5pzx4AaABAg.9cTozO1FW339nOhpAji5JY", "comment": "https://docs.google.com/spreadsheets/d/18pHRegyB0XawIdbZbvkr8-jMfi_2ltHVYPjBEOim-6w/edit#gid=0", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzLghxfrRnjks7gxnd4AaABAg.9bGHImo1WBY9bcA9ua8fVk", "comment": "Also remember that the partial derivative is replaced with the gradient vector, allowing for matrix multiplication.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzLghxfrRnjks7gxnd4AaABAg.9bGHImo1WBY9d3lDURFFw_", "comment": "Can you guys help me out ? I can't get my head around likelihood of theta thing ....why this is equal to product of probabilities of Y", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "UgzLghxfrRnjks7gxnd4AaABAg.9bGHImo1WBY9mIyBD92goH", "comment": "Newton's method uses 2nd order approximation vs the gradient descent uses 1st order approximation, the rationale is quite similar.", "votes": "0", "replies": "", "reply": true}, {"vid": "het9HFqo1TQ", "cid": "Ugx0n3eJZJ2-5x1PbLx4AaABAg.9an4dv8MzqP9qDhG1EHcyq", "comment": "ohhh so thats how it does it, wouldnt this overfit? It's like the start of thinking towards \"forest-lile\" methods, amazing.", "votes": "1", "replies": "", "reply": true}]