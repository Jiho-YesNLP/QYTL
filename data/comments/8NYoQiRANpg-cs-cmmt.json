[{"vid": "8NYoQiRANpg", "cid": "UgyEvMlmxSuDhxmXWHd4AaABAg", "comment": "Where is the lecture Notes..?", "votes": "0", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "Ugzk2JpWB-HTGbZ9Bh14AaABAg", "comment": "18:09", "votes": "0", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "Ugy2Z-KgZNObAJItDsN4AaABAg", "comment": "28:00", "votes": "1", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "Ugx_fx7EoRgSMLgfRP94AaABAg", "comment": "The audio is bad. I am relying on the captions to follow Andrew Ng.", "votes": "4", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "Ugz1VErQktUqKOM33Hx4AaABAg", "comment": "i am coming", "votes": "1", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgyE4MODtARmEz5nj7Z4AaABAg", "comment": "1:00:00", "votes": "1", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgzyiHIEFJPAF9Bb1a14AaABAg", "comment": "Where can I find lecture notes to cs 229?", "votes": "4", "replies": "1", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "Ugx_2qYLY3yujg1MbZZ4AaABAg", "comment": "Poor audio", "votes": "2", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgyBhJLdbZdAELiAv_J4AaABAg", "comment": "What happened here? 17:51", "votes": "8", "replies": "2", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgwtLvLcT6dT23cYtlF4AaABAg", "comment": "Question for the kind hearted people who understood: in the geometric margin, y is either 1 or -1 right? But wasn\u2019t it mentioned before that y is either 1 or 0? I got a little messed up on that.", "votes": "4", "replies": "2", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgyaGWxycELwO49weJV4AaABAg", "comment": "What is that that weird symbol he wrote during L1 soft margin svm? He called it c-i , i guess", "votes": "3", "replies": "1", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgxaVf8q-8m4n-vpKM54AaABAg", "comment": "This is a confusing lecture.", "votes": "3", "replies": "1", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgwfcV7tG20tXXFLVK54AaABAg", "comment": "Amazing. Start of kernel trick: 28:46.", "votes": "14", "replies": "1", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "Ugwa9CZaDtZNPNUgJHt4AaABAg", "comment": "he seems explaining to himself", "votes": "22", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgwMhNH1fZJaVd0-04N4AaABAg", "comment": "Is it really hard to prove that w is a linear combination of the training samples?  The training samples represent some set of vectors in a vector space.  They  might not span all of the space, and in fact they can't span more than N where N is the number of training samples (and they might span less, if there's any linear dependence among them).  But they span some subspace of dimension M.  The vector w defines a hyperplane that divides that space into classes.  Well, the training samples span that subspace, and therefore any vector in that subspace is a linear combination of them.     That leaves the non-spanned dimensions to consider, but your training sample conveys  no information whatsoever about those, so there's nothing to do about them.     I hope linear algebra was a prerequisite for this class.", "votes": "2", "replies": "6", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgwIYz3QadinEx0UGcV4AaABAg", "comment": "The stuff at the beginning here seems silly to me.  We're solving for a LINE in the classification space - it's just standard knowledge that the equation of a line can be multiplied by anything and still be the equation of the same line - it seems wasteful to apply so much attention to that idea.  It's something that just seems like it should be \"taken for granted\" to me.  And if you don't move the line, you haven't changed the classifier.  Obvious.     I feel like a good bit of what I'm seeing here is \"mathematically overdressed.\"  Most of the basic ideas we're talking about are simple applications of elementary probability theory.  There's really no need to layer it with more than the minimum of mathematical notation.  Obviously you want to state things correctly and mathematically, but once you've done that there's no need for further \"decoration.\"  If it's taken too far it starts to make those simple ideas more opaque, and that certainly doesn't seem like a good idea.     Edit: The rest of the lecture (kernel trick) more than made up for the first part.  \ud83d\ude42", "votes": "4", "replies": "2", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgzHSeDV1lHNyme6gu14AaABAg", "comment": "Very clear, thank you for the explanation", "votes": "7", "replies": "", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgyTXibdtfWaAvw7yS14AaABAg", "comment": "hi", "votes": "2", "replies": "1", "reply": false}, {"vid": "8NYoQiRANpg", "cid": "UgzyiHIEFJPAF9Bb1a14AaABAg.9t6yQObMji79tfNupRciS_", "comment": "if you google it it pop up", "votes": "4", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgyBhJLdbZdAELiAv_J4AaABAg.9qcDmoK3QT69rfdkM4L70o", "comment": "You fall a sleep during the class", "votes": "22", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgyBhJLdbZdAELiAv_J4AaABAg.9qcDmoK3QT6A51cSssRZl1", "comment": "\u00a0@zer1230\u00a0 close your eyes for 2 seconds and never catch back up", "votes": "1", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwtLvLcT6dT23cYtlF4AaABAg.9qcCmJiNzny9r93-9gjonQ", "comment": "Yes, y is either 1 or -1. y being 1 or 0 was a quality of the other binary classification algorithms [like logistic regression]", "votes": "2", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwtLvLcT6dT23cYtlF4AaABAg.9qcCmJiNzny9rCrN_mWVnu", "comment": "it is because we multlply the y with (W(t)X+b) so that we know the tyoe of data.", "votes": "1", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgyaGWxycELwO49weJV4AaABAg.9pmm9bbDFOq9r5_L2m6Fx9", "comment": "\u03be Greek letter resembling our acceptance for miss-classifications occurrence", "votes": "0", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgxaVf8q-8m4n-vpKM54AaABAg.9khweHqGWZE9lELe2-62Br", "comment": "You might try the MIT version of  it: https://youtu.be/_PwhiWxHK8o, really helped me", "votes": "0", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwfcV7tG20tXXFLVK54AaABAg.9hydZvNbdXoA7JscEMQH9I", "comment": "thank you..I've learnt lot about A Ng, but this explanation shows his genius because so far I never found a more lucid explanation.", "votes": "0", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwMhNH1fZJaVd0-04N4AaABAg.9eCKWZkoFnF9gyZyGXJZ8k", "comment": "Unfortunely your proof is incorrect. You have to remember that the hyperplane is defined as w^t x + b = 0 (it is affine). You need to prove that the w that solves the optimisation problem problem lies in this span.", "votes": "0", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwMhNH1fZJaVd0-04N4AaABAg.9eCKWZkoFnF9lnNdO6EkWP", "comment": "\"Is it really hard to prove that w is a linear combination of the training samples?\"  Well, let's use our critical thinking skills to answer this question. Given that the professor, multiple times, says that the result is too difficult to prove in this lecture, and he says the lecture notes prove the result (called Representer Theorem), and that upon looking at the notes or Googling this theorem and looking on Wikipedia, you can see for yourself that the proof is really much more involved than what you are making it out to be: maybe it IS that hard. And maybe your failed attempt to convert his drawings for intuition #2 and intuition #3 into a proof is not actually a genius, shortcut proof that he and everybody else somehow missed out on.", "votes": "9", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwMhNH1fZJaVd0-04N4AaABAg.9eCKWZkoFnF9pFK4aEBZfn", "comment": "The intuition seems correct but this is hardly a mathematical proof", "votes": "5", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwMhNH1fZJaVd0-04N4AaABAg.9eCKWZkoFnF9rWLg2DrDnl", "comment": "He did say this, this was essentially intuition #2", "votes": "0", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwMhNH1fZJaVd0-04N4AaABAg.9eCKWZkoFnFA1003jOkk3F", "comment": "\u200b\u00a0@timgoppelsroeder121\u00a0genuine question, why is it not a proper proof and just an intuition?", "votes": "0", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwMhNH1fZJaVd0-04N4AaABAg.9eCKWZkoFnFA112t2VBZ8O", "comment": "\u00a0@mohakkhetan\u00a0 If you compare it to the actual proof on wikipedia that would probably do a better job at anwering your question. I would say mostly due to the the fact that certain things are assumed/deduced without formally showing them. You need to define certain parameters applying certain definitions/axioms and using a methodology to arrive at a mathematically formulated solution for it to be more formal. This means there is less potential for false assumptions to be made .", "votes": "0", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwIYz3QadinEx0UGcV4AaABAg.9eCJMH4-DfP9jZBuATG_SG", "comment": "The point of the scaling comments wasn't that scaling the parameters of the line does not change the line.  Rather it was used to show the intuition on how to to change the non-convex intractable optimization function to another form that can be solved through the use of lagragian multipliers.", "votes": "15", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgwIYz3QadinEx0UGcV4AaABAg.9eCJMH4-DfP9lnGyzFsGrT", "comment": "A) He spends from 7:00 to 8:30 explaining that scaling the vector w and the real number b by the same constant does not change the line. You say it's \"wasteful to apply so much attention to that idea\", but it's not necessarily obvious to everyone and he's just going slowly so that everybody can follow and not spending that much time, really.  B) The rest of the time in the introduction, he is talking about a SPECIFIC clever choice of scaling (which makes ||w|| = 1/gamma) which converts the problem from something that was previously difficult to solve, into something solvable. You entirely missed the point of the scaling talk, despite him repeatedly stating the point in lecture 6 and here in lecture 7.  None of this has to do with \"simple application of elementary probability theory\", and the notation he uses is already as simple as it can be to express the idea. If anything, he is simplifying a lot of math involved and not deriving everything in detail (he signals extra resources such as lecture notes for those interested in reading further).   You might benefit from rewatching the lectures again and paying more attention.", "votes": "10", "replies": "", "reply": true}, {"vid": "8NYoQiRANpg", "cid": "UgyTXibdtfWaAvw7yS14AaABAg.9bEBQwFg83c9qccZOdlJqW", "comment": "Kernel", "votes": "0", "replies": "", "reply": true}]