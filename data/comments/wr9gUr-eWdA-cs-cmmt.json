[{"vid": "wr9gUr-eWdA", "cid": "Ugz-lBDWWQtsKLhiN9x4AaABAg", "comment": "shouldn't the cross entropy loss at 20:00 be the negative sum of y log p(hat), where y is its true classification? I'm not sure why p hat appears twice in this equation", "votes": "0", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgzqovXU18g1KGr1cnJ4AaABAg", "comment": "These days the level of presentations is so good thanks to opencoursewares. Even videos prepared by some random dudes is quite good due to high efforts in preparing and refining them. This particular lecture might look a little on the downside of today's standards but come on it is still pretty descent. I am pretty sure he would also improve after this, after all experience and practice is the only way", "votes": "1", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgwmRB9asM5cnD5np5Z4AaABAg", "comment": "i have a doubt while defining loss, isnt it more like proportion so its 100/(900+100)", "votes": "8", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgzA6KfBw5dcpERKpsp4AaABAg", "comment": "20 minutes in and this is all over the place. Could have been much more thoroughly thought through before the lecture.", "votes": "5", "replies": "2", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgyqJgbdKU7G8jGJP7x4AaABAg", "comment": "nice lecture, which gives me a big picutre of tree models", "votes": "1", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "Ugz2wBiv4cHleMNpENN4AaABAg", "comment": "How to know while partitioning which one is the root?", "votes": "0", "replies": "1", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgyPkrz_snnhdf8TVoR4AaABAg", "comment": "So many markers 33:30 daamn", "votes": "0", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "Ugwkztfai0KR648UUvZ4AaABAg", "comment": "It should be decrease in entropy, not cross entropy, imo. You are using the entropy formula, calling it CE", "votes": "3", "replies": "1", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgyedA16g2h1H9SoqJN4AaABAg", "comment": "40:59", "votes": "0", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgwKue2aVMNxJc22m594AaABAg", "comment": "In cross entropy what if phat goes to 0. It also decreases the loss?", "votes": "1", "replies": "1", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "Ugz-fCUkQTqFjFbRf_h4AaABAg", "comment": "He defines the classification loss as L(R) = 1 - max(p^c) (over all c in C classes) at 10:19. Next, he defines L(parent) -sum(L(children)) , initially stating that it needs to be minimized at 12:05 but later changing it to be maximized at 18:46.  However, when comparing the two trees, he disregards the mentioned concepts and simply sums the values that will be misclassified for each child (100 + 0 for the first tree and 100 + 0 for the second) at 15:00. This approach of summing misclassified values without considering proportions is not accurate.  Taking proportions into account according to L(R) = 1 - max(p^c), it turns out that the quantity to be maximized won't be the same for the two trees. For the first tree: L(R_1) + L(R_2) = (1 - 700/800) - (1 - 200/200) = 0.125. For the second tree: L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.1.  The reason these values differ is because the quantity he defined as L(parent) - sum(L(children)) is incorrect. To have them both be the same value, the weighted average of the number of points in each should be taken. Thus we ditch misclassification loss and go for cross entropy.", "votes": "10", "replies": "3", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "Ugx-SBVIyY8H2lhc0Rp4AaABAg", "comment": "Any updates link to download pdf of the notes. The link given doesn't work.", "votes": "8", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgwmzqRpLG6EplmrTPh4AaABAg", "comment": "Why is the L(R_1) + L(R_2) = 100 + 0 at 14:50? I thought that p-hat-c is the proportion of examples in R that are of class C, not the actual number of examples in class C. Based on the misclassification loss definition L(R_1) + L(R_2) = (1 - 700/800) + (1 - 200/200) = 0.125 and L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.1.", "votes": "16", "replies": "6", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgydHerat3Wsrgi4Uo94AaABAg", "comment": "@9:40 - MIS-CLASSIFICATION LOSS -- R being REGION , and given Class Count is CAPITAL-C , num classes = CAPITAL-C , he has defined - p-hat-c to be the proportion of examples in Region R , which are of Class NotCapital-C .", "votes": "4", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgzXBJ1aQU-bGHUfarV4AaABAg", "comment": "@15:20 -- Cross Entropy Loss", "votes": "1", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "Ugyh_LvBYgLaZsb4-754AaABAg", "comment": "good lecture but waiting for him to finish writing on a white board when ML is done on a computer is incredibly frustrating. what is this 2003?", "votes": "5", "replies": "1", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "Ugx9EiXj77Qx7lSaX554AaABAg", "comment": "Great easy-to-follow lecture.", "votes": "16", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgwNM_aDPmNaJkS_UNN4AaABAg", "comment": "really clear\ud83d\ude06", "votes": "5", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgxDnDejQ6iYlg74VTV4AaABAg", "comment": "Andrew is indeed much easier to understand", "votes": "45", "replies": "5", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgwYx-RF6yllKvc2dd54AaABAg", "comment": "In deeplearning we have thing called dropouts whyn't we use them here.", "votes": "1", "replies": "2", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "UgzA6KfBw5dcpERKpsp4AaABAg.A-yp-9cVK_rA54E1ugJhcJ", "comment": "As a PhD student who has taught lectures for my advisor, I will say that sometimes it doesn't matter how much you prepare for the lecture, you will stumble a bit. It's certainly not an amazing lecture, but your comment isn't helpful and only seems ignorant.", "votes": "1", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgzA6KfBw5dcpERKpsp4AaABAg.A-yp-9cVK_rA57UiX1MiLg", "comment": "\u00a0@mrpotatohed4\u00a0 Providing feedback is absolutely paramount to improve performance. This is helpful for both the lecturer and the students. I would have explained it better and in a much more clearer way if I had spent even half a day learning about it from scratch but this is just pathetic at best.", "votes": "1", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugz2wBiv4cHleMNpENN4AaABAg.9yd6jaiUe_BA30khCYxOtS", "comment": "the one you start with. look at the id3 algorithm for an example", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugwkztfai0KR648UUvZ4AaABAg.9vZj_DML7X1A6g3Dg_qhVO", "comment": "I agree. This is an important thing that might misled alot of people", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwKue2aVMNxJc22m594AaABAg.9rn2wmFmxd49xr3QQl7EHW", "comment": "Yes. When x approaches 0, the limit of xlogx is 0.", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugz-fCUkQTqFjFbRf_h4AaABAg.9pzjxYyKjEq9xjL4A3DM5h", "comment": "Thanks that makes a lot of sense", "votes": "1", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugz-fCUkQTqFjFbRf_h4AaABAg.9pzjxYyKjEq9z75WN0G7-Y", "comment": "I mean you\u2019re right but that\u2019s exactly his point and it\u2019s why he is criticizing misclassification loss, since there we would only be looking at absolute values. It\u2019s also the reason he proposes cross-entropy loss and it automatically addresses your concerns as well.", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugz-fCUkQTqFjFbRf_h4AaABAg.9pzjxYyKjEqA1KdDCPE0Vf", "comment": "small nitpick, L(R_1') + L(R_2') = (1 - 400/500) + (1 - 500/500) = 0.2 not 0.1. But otherwise i agree with you.", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwmzqRpLG6EplmrTPh4AaABAg.9iUrjxgeKaY9kFgMu7305t", "comment": "Actually I think you're right\uff0cyour answer is same as the lecture note of Decision Trees, CS229, 2021", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwmzqRpLG6EplmrTPh4AaABAg.9iUrjxgeKaY9qg2Wq8uGZ2", "comment": "He simply omitted the denominator, 1-700/800 ==> 100/800 ; 1-400/500 ==> 100/500; Not defending him, just explaining his thinking process, but precisely, your calculation is the precise.", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwmzqRpLG6EplmrTPh4AaABAg.9iUrjxgeKaY9rfz_um7c7T", "comment": "I think in this case he is talking about absolute nos of example that they have gotten wrong. Btw you are right it is a bit confusing what he is talking about.", "votes": "4", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwmzqRpLG6EplmrTPh4AaABAg.9iUrjxgeKaY9rg1ucwa4E5", "comment": "Also please refer to 25:30 in this lecture. He calculates the p(hat) instead of the absolute loss.", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwmzqRpLG6EplmrTPh4AaABAg.9iUrjxgeKaY9z75qFVEwbF", "comment": "Yeah it\u2019s what @tariqkhan1518 said, we are only looking at the absolute values at that point since this is also his main argument to instead use a different measurement later on", "votes": "1", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwmzqRpLG6EplmrTPh4AaABAg.9iUrjxgeKaYA1KZfPljVLs", "comment": "in the lecture notes, L(R_p) = (|R1|*L(R1) + |R2|*L(R2)) / (R1 + R2). This will give L(Rp) for first threshold as 100/1000 and L(Rp) for second threshold as 100/1000. L(R1) is 1 - maxp, same as the formula in the lecture. But yeah as the other comment mentioned he dint bother doing all this and just took the absolute number of examples that were misclassified.", "votes": "0", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugyh_LvBYgLaZsb4-754AaABAg.9hkUTOeLn-jA30gQDcy9xU", "comment": "set speed to 1.4 and stop pissing your pants", "votes": "4", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgxDnDejQ6iYlg74VTV4AaABAg.9e7QjpYBJsO9iXgzM3pgUw", "comment": "He has more than 20 years of experience in teaching students lol", "votes": "13", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgxDnDejQ6iYlg74VTV4AaABAg.9e7QjpYBJsO9jlAmhI3N9r", "comment": "There are still better instructors", "votes": "1", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgxDnDejQ6iYlg74VTV4AaABAg.9e7QjpYBJsO9pS6Inh0GRM", "comment": "\u200b\u00a0@lugia8888\u00a0 please suggest some..", "votes": "2", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgxDnDejQ6iYlg74VTV4AaABAg.9e7QjpYBJsO9qDXuuf-bhA", "comment": "\u00a0@nagendranaidu1933\u00a0 I would recommend Kilian Weinberger, he's a professor at Cornell and has a youtube channel with all his machine learning lectures. He's an excellent teacher, one of the best, imo.", "votes": "3", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgxDnDejQ6iYlg74VTV4AaABAg.9e7QjpYBJsO9v2tb0_eUaH", "comment": "\u00a0@T_SULTAN_\u00a0 A plus of Kilian is also that he is very funny and engaging - even often unintentionally so...  :-D", "votes": "2", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwYx-RF6yllKvc2dd54AaABAg.9bWvKfJhBvG9gDsB6Cyv0d", "comment": "You must be talking about pruning", "votes": "4", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "UgwYx-RF6yllKvc2dd54AaABAg.9bWvKfJhBvG9wx5tATpbp7", "comment": "whyn't isn't a word.", "votes": "6", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugzga6U3nV4ksncOfKB4AaABAg", "comment": "What  is the loss function for Decision Tree?", "votes": "4", "replies": "3", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "Ugzbr7FsLTeLxSaPz_N4AaABAg", "comment": "why did the loss of parent in 23:37  become the point projected upwards?.", "votes": "4", "replies": "", "reply": false}, {"vid": "wr9gUr-eWdA", "cid": "Ugzga6U3nV4ksncOfKB4AaABAg.9aa_VEyFKmZ9aatyoi4wCk", "comment": "it is the Entropy or Gini", "votes": "2", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugzga6U3nV4ksncOfKB4AaABAg.9aa_VEyFKmZ9aauSFQ2qaT", "comment": "\u00a0@happytrigger3778\u00a0 and for SVM,XGboost and Random Forest?", "votes": "1", "replies": "", "reply": true}, {"vid": "wr9gUr-eWdA", "cid": "Ugzga6U3nV4ksncOfKB4AaABAg.9aa_VEyFKmZ9bRqZKUPoSU", "comment": "\u00a0@happytrigger3778\u00a0 These are from Classification trees , you can also find other loss functions like the *Mean Square Error* for Regression trees", "votes": "6", "replies": "", "reply": true}]